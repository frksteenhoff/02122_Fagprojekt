
Anarchism.
Anarchism is a political philosophy which considers the state undesirable, unnecessary and harmful, and instead promotes a stateless society, or anarchy. It seeks to diminish or even abolish authority in the conduct of human relations. Anarchists may widely disagree on what additional criteria are required in anarchism. "The Oxford Companion to Philosophy" says, "there is no single defining position that all anarchists hold, and those considered anarchists at best share a certain family resemblance."
There are many types and traditions of anarchism, not all of which are mutually exclusive. Strains of anarchism have been divided into the categories of social and individualist anarchism or similar dual classifications. Anarchism is often considered to be a radical left-wing ideology, and much of anarchist economics and anarchist legal philosophy reflect anti-statist interpretations of communism, collectivism, syndicalism or participatory economics. However, anarchism has always included an individualist strain supporting a market economy and private property, or unrestrained egoism that bases right on might.
Others, such as panarchists and anarchists without adjectives, neither advocate nor object to any particular form of organization as long as it is not compulsory. Differing fundamentally, some anarchist schools of thought support anything from extreme individualism to complete collectivism. The central tendency of anarchism as a social movement have been represented by communist anarchism, with individualist anarchism being primarily a philosophical or literary phenomenon. Some anarchists fundamentally oppose all forms of aggression, supporting self-defense or non-violence, while others have supported the use of some coercive measures, including violent revolution and terrorism, on the path to an anarchist society.
Etymology and terminology.
The term "anarchism" derives from the Greek ἄναρχος, "anarchos", meaning "without rulers", from the prefix ἀν- ("an-", "without") + ἀρχή ("archê", "sovereignty, realm, magistracy") + -ισμός ("-ismos", from the suffix -ιζειν, "-izein" "-izing"). There is some ambiguity with the use of the terms "libertarianism" and "libertarian" in writings about anarchism. Since the 1890s from France, the term "libertarianism" has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United States; its use as a synonym is still common outside the United States. Accordingly, "libertarian socialism" is sometimes used as a synonym for socialist anarchism, to distinguish it from "individualist libertarianism" (individualist anarchism). On the other hand, some use "libertarianism" to refer to individualistic free-market philosophy only, referring to free-market anarchism as "libertarian anarchism."
Origins.
Some claim anarchist themes can be found in the works of Taoist sages Laozi and Zhuangzi. The latter has been translated, "There has been such a thing as letting mankind alone; there has never been such a thing as governing mankind [with success]," and "A petty thief is put in jail. A great brigand becomes a ruler of a Nation." Diogenes of Sinope and the Cynics, and their contemporary Zeno of Citium, the founder of Stoicism, also introduced similar topics.
Modern anarchism, however, sprang from the secular or religious thought of the Enlightenment, particularly Jean-Jacques Rousseau's arguments for the moral centrality of freedom. Although by the turn of the 19th century the term "anarchist" had lost its initial negative connotation, it first entered the English language in 1642 during the English Civil War as a term of abuse used by Royalists to damn those who were fomenting disorder. By the time of the French Revolution some, such as the "Enragés", began to use the term positively, in opposition to Jacobin centralisation of power, seeing "revolutionary government" as oxymoronic.
From this climate William Godwin developed what many consider the first expression of modern anarchist thought. Godwin was, according to Peter Kropotkin, "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work", while Godwin attached his anarchist ideas to an early Edmund Burke. Benjamin Tucker instead credits Josiah Warren, an American who promoted stateless and voluntary communities where all goods and services were private, with being "the first man to expound and formulate the doctrine now known as Anarchism." The first to describe himself as an anarchist was Pierre-Joseph Proudhon, a French philosopher and politician, which led some to call him the founder of modern anarchist theory.
Social movement.
Anarchism as a social movement has regularly endured fluctuations in popularity. Its classical period, which scholars demarcate as from 1860 to 1939, is associated with the working-class movements of the nineteenth century and the Spanish Civil War-era struggles against fascism. Anarchists were heavilly involved in the abolition of slavery, and continue to be active in the labour movement, civil rights, women's liberation, both anti-capitalism and pro-capitalism (with varying definitions of capitalism), the anti-war movement, LGBT rights, both anti-globalization and pro-globalization (with varying definitions of globalization), tax resistance, and other areas.
The First International.
In Europe, harsh reaction followed the revolutions of 1848, during which ten countries had experienced brief or long-term social upheaval as groups carried out nationalist uprisings. After most of these attempts at systematic change ended in failure, conservative elements took advantage of the divided groups of socialists, anarchists, liberals, and nationalists, to prevent further revolt. In 1864 the International Workingmen's Association (sometimes called the "First International") united diverse revolutionary currents including French followers of Proudhon, Blanquists, Philadelphes, English trade unionists, socialists and social democrats.
Due to its links to active workers' movements, the International became a significant organization. Karl Marx became a leading figure in the International and a member of its General Council. Proudhon's followers, the mutualists, opposed Marx's state socialism, advocating political abstentionism and small property holdings.
In 1868, following their unsuccessful participation in the League of Peace and Freedom (LPF), Russian revolutionary Mikhail Bakunin and his collectivist anarchist associates and joined the First International (which had decided not to get involved with the LPF). They allied themselves with the federalist socialist sections of the International, who advocated the revolutionary overthrow of the state and the collectivization of property.
At first, the collectivists worked with the Marxists to push the First International in a more revolutionary socialist direction. Subsequently, the International became polarised into two camps, with Marx and Bakunin as their respective figureheads. Bakunin characterised Marx's ideas as centralist and predicted that, if a Marxist party came to power, its leaders would simply take the place of the ruling class they had fought against.
In 1872, the conflict climaxed with a final split between the two groups at the Hague Congress, where Bakunin and James Guillaume were expelled from the International and its headquarters were transferred to New York. In response, the federalist sections formed their own International at the St. Imier Congress, adopting a revolutionary anarchist program.
Organised labour.
The anti-authoritarian sections of the First International were the precursors of the anarcho-syndicalists, seeking to "replace the privilege and authority of the State" with the "free and spontaneous organization of labor." In 1886, the Federation of Organized Trades and Labor Unions (FOTLU) of the United States and Canada unanimously set 1 May 1886, as the date by which the eight-hour work day would become standard.
In response, unions across America prepared a general strike in support of the event. On 3 May, in Chicago, a fight broke out when strikebreakers attempted to cross the picket line, and two workers died when police opened fire upon the crowd. The next day, 4 May, anarchists staged a rally at Chicago's Haymarket Square. A bomb was thrown by an unknown party near the conclusion of the rally, killing an officer. In the ensuing panic, police opened fire on the crowd and each other. Seven police officers and at least four workers were killed. Eight anarchists directly and indirectly related to the organisers of the rally were arrested and charged with the murder of the deceased officer. The men became international political celebrities among the labour movement. Four of the men were executed and a fifth committed suicide prior to his own execution. The incident became known as the Haymarket affair, and was a setback for the labour movement and the struggle for the eight hour day. In 1890 a second attempt, this time international in scope, to organise for the eight hour day was made. The event also had the secondary purpose of memorializing workers killed as a result of the Haymarket affair. Although it had initially been conceived as a once-off event, by the following year the celebration of International Workers' Day on May Day had become firmly established as an international worker's holiday.
In 1907, the International Anarchist Congress of Amsterdam gathered delegates from 14 different countries, among which important figures of the anarchist movement, including Errico Malatesta, Pierre Monatte, Luigi Fabbri, Benoît Broutchoux, Emma Goldman, Rudolf Rocker, and Christiaan Cornelissen. Various themes were treated during the Congress, in particular concerning the organisation of the anarchist movement, popular education issues, the general strike or antimilitarism. A central debate concerned the relation between anarchism and syndicalism (or trade unionism). Malatesta and Monatte were in particular disagreement themselves on this issue, as the latter thought that syndicalism was revolutionary and would create the conditions of a social revolution, while Malatesta did not consider syndicalism by itself sufficient. He thought that the trade-union movement was reformist and even conservative, citing as essentially bourgeois and anti-worker the phenomenon of professional union officials. Malatesta warned that the syndicalists aims were in perpetuating syndicalism itself, whereas anarchists must always have anarchy as their end and consequently refrain from committing to any particular method of achieving it.
The Spanish Workers Federation in 1881 was the first major anarcho-syndicalist movement; anarchist trade union federations were of special importance in Spain. The most successful was the Confederación Nacional del Trabajo (National Confederation of Labour: CNT), founded in 1910. Before the 1940s, the CNT was the major force in Spanish working class politics, attracting 1.58 million members at one point and playing a major role in the Spanish Civil War. The CNT was affiliated with the International Workers Association, a federation of anarcho-syndicalist trade unions founded in 1922, with delegates representing two million workers from 15 countries in Europe and Latin America. The largest organised anarchist movement today is in Spain, in the form of the Confederación General del Trabajo (CGT) and the CNT. CGT membership was estimated to be around 100,000 for the year 2003. Other active syndicalist movements include the US Workers Solidarity Alliance and the UK Solidarity Federation. The revolutionary industrial unionist Industrial Workers of the World, claiming 2,000 paying members, and the International Workers Association, an anarcho-syndicalist successor to the First International, also remain active.
Russian Revolution.
Anarchists participated alongside the Bolsheviks in both February and October revolutions, and were initially enthusiastic about the Bolshevik coup. However, the Bolsheviks soon turned against the anarchists and other left-wing opposition, a conflict that culminated in the 1921 Kronstadt rebellion which the new government repressed. Anarchists in central Russia were either imprisoned, driven underground or joined the victorious Bolsheviks; the anarchists from Petrograd and Moscow fled to the Ukraine. There, in the Free Territory, they fought in the civil war against the Whites (a Western-backed grouping of monarchists and other opponents of the October Revolution) and then the Bolsheviks as part of the Revolutionary Insurrectionary Army of Ukraine led by Nestor Makhno, who established an anarchist society in the region for a number of months.
Expelled American anarchists Emma Goldman and Alexander Berkman were amongst those agitating in response to Bolshevik policy and the suppression of the Kronstadt uprising, before they left Russia. Both wrote accounts of their experiences in Russia, criticizing the amount of control the Bolsheviks exercised. For them, Bakunin's predictions about the consequences of Marxist rule that the rulers of the new "socialist” Marxist state would become a new elite had proved all too true.
The victory of the Bolsheviks in the October Revolution and the resulting Russian Civil War did serious damage to anarchist movements internationally. Many workers and activists saw Bolshevik success as setting an example; Communist parties grew at the expense of anarchism and other socialist movements. In France and the United States, for example, members of the major syndicalist movements of the CGT and IWW left the organizations and joined the Communist International.
In Paris, the Dielo Truda group of Russian anarchist exiles, which included Nestor Makhno, concluded that anarchists needed to develop new forms of organisation in response to the structures of Bolshevism. Their 1926 manifesto, called the "Organizational Platform of the General Union of Anarchists (Draft)", was supported. Platformist groups active today include the Workers Solidarity Movement in Ireland and the North Eastern Federation of Anarchist Communists of North America.
Fight against fascism.
In the 1920s and 1930s, the rise of fascism in Europe transformed anarchism's conflict with the state. Italy saw the first struggles between anarchists and fascists. Italian anarchists played a key role in the anti-fascist organisation "Arditi del Popolo", which was strongest in areas with anarchist traditions, and achieved some success in their activism, such as repelling Blackshirts in the anarchist stronghold of Parma in August 1922. In France, where the far right leagues came close to insurrection in the February 1934 riots, anarchists divided over a united front policy.
In Spain, the CNT initially refused to join a popular front electoral alliance, and abstention by CNT supporters led to a right wing election victory. But in 1936, the CNT changed its policy and anarchist votes helped bring the popular front back to power. Months later, the former ruling class responded with an attempted coup causing the Spanish Civil War (1936–1939). In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land. But even before the fascist victory in 1939, the anarchists were losing ground in a bitter struggle with the Stalinists, who controlled the distribution of military aid to the Republican cause from the Soviet Union. Stalinist-led troops suppressed the collectives and persecuted both dissident Marxists and anarchists.
Contemporary anarchism.
A surge of popular interest in anarchism occurred during the 1960s and 1970s. In the United Kingdom this was associated with the punk rock movement, as exemplified by bands such as Crass and the Sex Pistols. The housing and employment crisis in most of Western Europe led to the formation of communes and squatter movements like that of Barcelona, Spain. In Denmark, squatters occupied a disused military base and declared the Freetown Christiania, an autonomous haven in central Copenhagen.
Since the revival of anarchism in the mid 20th century, a number of new movements and schools of thought emerged. Although feminist tendencies have always been a part of the anarchist movement in the form of anarcha-feminism, they returned with vigour during the second wave of feminism in the 1960s. The American Civil Rights Movement and the movement against the war in Vietnam also contributed to the revival of North American anarchism. European anarchism of the late 20th century drew much of its strength from the labour movement, and both have incorporated animal rights activism. Anarchist anthropologist David Graeber has posited a rupture between generations of anarchism, with those "who often still have not shaken the sectarian habits" of the nineteenth century contrasted with the younger activists who are "much more informed, among other elements, by indigenous, feminist, ecological and cultural-critical ideas", and who by the turn of the 21st century formed "by far the majority" of anarchists.
Around the turn of the 21st century, anarchism grew in popularity and influence as part of the anti-war, anti-capitalist, and anti-globalisation movements. Anarchists became known for their involvement in protests against the meetings of the World Trade Organization (WTO), Group of Eight, and the World Economic Forum. Some anarchist factions at these protests engaged in rioting, property destruction, and violent confrontations with police, and the confrontations were selectively portrayed in mainstream media coverage as violent riots. These actions were precipitated by ad hoc, leaderless, anonymous cadres known as "black blocs"; other organisational tactics pioneered in this time include security culture, affinity groups and the use of decentralised technologies such as the internet. A landmark struggle of this period was the confrontations at WTO conference in Seattle in 1999.
Anarchist schools of thought.
Anarchist ideas have only occasionally inspired political movements of any size, and "the tradition is mainly one of individual thinkers, but they have produced an important body of theory." Anarchist schools of thought had been generally grouped in two main historical traditions, individualist anarchism and social anarchism, which have some different origins, values and evolution. The individualist wing of anarchism emphasises negative liberty, i.e. opposition to state or social control over the individual, while those in the social wing emphasise positive liberty to achieve one's potential and argue that humans have needs that society ought to fulfill, "recognizing equality of entitlement". In chronological and theoretical sense there are classical — those created throughout the 19th century — and post-classical anarchist schools — those created since the mid-20th century and after.
Beyond the specific factions of anarchist thought is philosophical anarchism, which embodies the theoretical stance that the State lacks moral legitimacy without accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism philosophical anarchism may accept the existence of a minimal state as unfortunate, and usually temporary, "necessary evil" but argue that citizens do not have a moral obligation to obey the state when its laws conflict with individual autonomy. One reaction against sectarianism within the anarchist milieu was "anarchism without adjectives", a call for toleration first adopted by Fernando Tarrida del Mármol in 1889 in response to the "bitter debates" of anarchist theory at the time. In abandoning the hyphenated anarchisms (i.e. collectivist-, communist-, mutualist- and individualist-anarchism), it sought to emphasise the anti-authoritarian beliefs common to all anarchist schools of thought.
Mutualism.
Mutualism began in 18th century English and French labour movements before taking an anarchist form associated with Pierre-Joseph Proudhon in France and others in the United States. Proudhon proposed spontaneous order, whereby organization emerges without central authority, a "positive anarchy" where order arises when everybody does “what he wishes and only what he wishes" and where "business transactions alone produce the social order." Mutualist anarchism is concerned with reciprocity, free association, voluntary contract, federation, and credit and currency reform. According to William Batchelder Greene, each worker in the mutualist system would receive "just and exact pay for his work; services equivalent in cost being exchangeable for services equivalent in cost, without profit or discount." Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property."
Individualist anarchism.
Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants such as groups, society, traditions, and ideological systems. Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict.
In 1793, William Godwin, who has often been cited as the first anarchist, wrote "Political Justice", which some consider to be the first expression of anarchism. Godwin, a philosophical anarchist, from a rationalist and utilitarian basis opposed revolutionary action and saw a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. Godwin advocated extreme individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good. Godwin was a utilitarian who believed that all individuals are not of equal value, with some of us "of more worth and importance" than others depending on our utility in bringing about social good. Therefore he does not believe in equal rights, but the person's life that should be favoured that is most conducive to the general good. Godwin opposed government because he saw it as infringing on the individual's right to "private judgement" to determine which actions most maximise utility, but also makes a critique of all authority over the individual's judgement. This aspect of Godwin's philosophy, stripped of utilitarian motivations, was developed into a more extreme form later by Stirner.
The most extreme form of individualist anarchism, called "egoism," or egoist anarchism, was expounded by one of the earliest and best-known proponents of individualist anarchism, Max Stirner. Stirner's "The Ego and Its Own", published in 1844, is a founding text of the philosophy. According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. To Stirner, rights were "spooks" in the mind, and he held that society does not exist but "the individuals are its reality". Stirner advocated self-assertion and foresaw Unions of Egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organization in place of the state. Egoist anarchists claim that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay. Individualist anarchism inspired by Stirner attracted a small following of European bohemian artists and intellectuals (see European individualist anarchism). Stirner's philosophy has been seen as a precedent of existentialism with other thinkers like Friedrich Nietzsche and Sören Kierkegaard.
Social anarchism.
Social anarchism calls for a system with public ownership of means of production and democratic control of all organizations, without any government authority or coercion. It is the largest school of anarchism. Social anarchism rejects private property, seeing it as a source of social inequality, and emphasises cooperation and mutual aid.
Collectivist anarchism, also referred to as "revolutionary socialism" or a form of such, is a revolutionary form of anarchism, commonly associated with Mikhail Bakunin and Johann Most. Collectivist anarchists oppose all private ownership of the means of production, instead advocating that ownership be collectivised. This was to be achieved through violent revolution, first starting with a small cohesive group through acts of violence, or "propaganda by the deed," which would inspire the workers as a whole to revolt and forcibly collectivise the means of production. However, collectivization was not to be extended to the distribution of income, as workers would be paid according to time worked, rather than receiving goods being distributed "according to need" as in anarcho-communism. This position was criticised by anarchist communists as effectively "uphold[ing] the wages system". Collectivist anarchism arose contemporaneously with Marxism but opposed the Marxist dictatorship of the proletariat, despite the stated Marxist goal of a collectivist stateless society. Anarchist communist and collectivist ideas are not mutually exclusive; although the collectivist anarchists advocated compensation for labour, some held out the possibility of a post-revolutionary transition to a communist system of distribution according to need.
Anarchist communism proposes that the freest form of social organisation would be a society composed of self-managing communes with collective use of the means of production, organised democratically, and related to other communes through federation. While some anarchist communists favour direct democracy, others feel that its majoritarianism can impede individual liberty and favour consensus democracy instead. In anarchist communism, as money would be abolished, individuals would not receive direct compensation for labour (through sharing of profits or payment) but would have free access to the resources and surplus of the commune. Anarchist communism does not always have a communitarian philosophy. Some forms of anarchist communism are egoist and strongly influenced by radical individualism, believing that anarchist communism does not require a communitarian nature at all.
In the early 20th century, anarcho-syndicalism arose as a distinct school of thought within anarchism. With greater focus on the labour movement than previous forms of anarchism, syndicalism posits radical trade unions as a potential force for revolutionary social change, replacing capitalism and the state with a new society, democratically self-managed by the workers. It is often combined with other branches of anarchism, and anarcho-syndicalists often subscribe to anarchist communist or collectivist anarchist economic systems. An early leading anarcho-syndicalist thinker was Rudolf Rocker, whose 1938 pamphlet "Anarchosyndicalism" outlined a view of the movement's origin, aims and importance to the future of labour.
Post-classical currents.
Anarchism continues to generate many philosophies and movements, at times eclectic, drawing upon various sources, and syncretic, combining disparate and contrary concepts to create new philosophical approaches. Since the revival of anarchism in the United States in the 1960s, a number of new movements and schools have emerged. Anarcho-capitalism developed from radical anti-state libertarianism and individualist anarchism, drawing from Austrian School economics, study of law and economics and public choice theory, while the burgeoning feminist and environmentalist movements also produced anarchist offshoots. Anarcha-feminism developed as a synthesis of radical feminism and anarchism that views patriarchy (male domination over women) as a fundamental manifestation of compulsory government. It was inspired by the late 19th century writings of early feminist anarchists such as Lucy Parsons, Emma Goldman, Voltairine de Cleyre, and Dora Marsden. Anarcha-feminists, like other radical feminists, criticise and advocate the abolition of traditional conceptions of family, education and gender roles. Green anarchism (or eco-anarchism) is a school of thought within anarchism which puts an emphasis on environmental issues, and whose main contemporary currents are anarcho-primitivism and social ecology. Post-left anarchy is a tendency which seeks to distance itself from traditional left-wing politics and to escape the confines of ideology in general. Post-anarchism is a theoretical move towards a synthesis of classical anarchist theory and poststructuralist thought drawing from diverse ideas including post-modernism, autonomist marxism, post-left anarchy, situationism and postcolonialism. Another recent form of anarchism critical of formal anarchist movements is insurrectionary anarchism, which advocates informal organization and active resistance to the state; its proponents include Wolfi Landstreicher and Alfredo M. Bonanno.
Topics of interest in anarchist theory.
Intersecting and overlapping between various schools of thought, certain topics of interest and internal disputes have proven perennial within anarchist theory.
Free love.
An important current within anarchism is Free love. Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, viewed sexual freedom as a clear, direct expression of an individual's self-ownership. Free love particularly stressed women's rights since most sexual laws discriminated against women: for example, marriage laws and anti-birth control measures. The most important American free love journal was "Lucifer the Lightbearer" (1883–1907) edited by Moses Harman and Lois Waisbrooker, but also there existed Ezra Heywood and Angela Heywood's 'The Word' (1872–1890, 1892–1893). Also M. E. Lazarus was an important American individualist anarchist who promoted free love.
In New York's Greenwich Village, bohemian feminists and socialists advocated self-realisation and pleasure for women (and also men) in the here and now. They encouraged playing with sexual roles and sexuality, and the openly bisexual radical Edna St. Vincent Millay and the lesbian anarchist Margaret Anderson were prominent among them. Discussion groups organised by the Villagers were frequented by Emma Goldman, among others. Magnus Hirschfeld noted in 1923 that Goldman "has campaigned boldly and steadfastly for individual rights, and especially for those deprived of their rights. Thus it came about that she was the first and only woman, indeed the first and only American, to take up the defense of homosexual love before the general public." In fact, before Goldman, heterosexual anarchist Robert Reitzel (1849–98) spoke positively of homosexuality from the beginning of the 1890s in his Detroit-based German language journal "Der arme Teufel".
In Europe the main propagandist of free love within individualist anarchism was Emile Armand. He proposed the concept of "la camaraderie amoureuse" to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory. In Germany the stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality.
More recently, the British anarcho-pacifist Alex Comfort gained notoriety during the sexual revolution for writing the bestseller sex manual "The Joy of Sex". The issue of free love has a dedicated treatment in the work of french anarcho-hedonist philosopher Michel Onfray in such works as "Théorie du corps amoureux: pour une érotique solaire" (2000) and "L'invention du plaisir: fragments cyréaniques" (2002).
Libertarian education.
In 1901, Spanish anarchist and free-thinker Francesc Ferrer i Guàrdia established "modern" or progressive schools in Barcelona in defiance of an educational system controlled by the Catholic Church. The schools' stated goal was to "educate the working class in a rational, secular and non-coercive setting". Fiercely anti-clerical, Ferrer believed in "freedom in education", education free from the authority of church and state. Murray Bookchin wrote: "This period [1890s] was the heyday of libertarian schools and pedagogical projects in all areas of the country where Anarchists exercised some degree of influence. Perhaps the best-known effort in this field was Francisco Ferrer's Modern School (Escuela Moderna), a project which exercised a considerable influence on Catalan education and on experimental techniques of teaching generally." La Escuela Moderna, and Ferrer's ideas generally, formed the inspiration for a series of "Modern Schools" in the United States, Cuba, South America and London. The first of these was started in New York City in 1911. It also inspired the Italian newspaper "Università popolare", founded in 1901.
Another libertarian tradition is that of unschooling and the free school in which child-led activity replaces pedagogic approaches. Experiments in Germany led to A. S. Neill founding what became Summerhill School in 1921. Summerhill is often cited as an example of anarchism in practice. However, although Summerhill and other free schools are radically libertarian, they differ in principle from those of Ferrer by not advocating an overtly-political class struggle-approach.
In addition to organizing schools according to libertarian principles, anarchists have also questioned the concept of schooling per se. The term deschooling was popularized by Ivan Illich, who argued that the school as an institution is dysfunctional for self-determined learning and serves the creation of a consumer society instead.
Internal issues and debates.
Anarchism is a philosophy which embodies many diverse attitudes, tendencies and schools of thought; as such, disagreement over questions of values, ideology and tactics is common. The compatibility of capitalism, nationalism and religion with anarchism is widely disputed. Similarly, anarchism enjoys complex relationships with ideologies such as Marxism, communism and capitalism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest or any number of alternative ethical doctrines.
Phenomena such as civilization, technology (e.g. within anarcho-primitivism and insurrectionary anarchism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others. Anarchist attitudes towards race, gender and the environment have changed significantly since the modern origin of the philosophy in the 18th century.
On a tactical level, while propaganda of the deed was a tactic used by anarchists in the 19th century (e.g. the Nihilist movement), contemporary anarchists espouse alternative direct action methods such as nonviolence, counter-economics and anti-state cryptography to bring about an anarchist society. About the scope of an anarchist society, some anarchists advocate a global one, while others do so by local ones. The diversity in anarchism has led to widely different use of identical terms among different anarchist traditions, which has led to many definitional concerns in anarchist theory.
---END.OF.DOCUMENT---

Autism.
Autism is a disorder of neural development characterized by impaired social interaction and communication, and by restricted and repetitive behavior. These signs all begin before a child is three years old. Autism affects information processing in the brain by altering how nerve cells and their synapses connect and organize; how this occurs is not well understood. The two other autism spectrum disorders (ASD) are Asperger syndrome, which lacks delays in cognitive development and language, and PDD-NOS, diagnosed when full criteria for the other two disorders are not met.
Autism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations, or by rare combinations of common genetic variants. In rare cases, autism is strongly associated with agents that cause birth defects. Controversies surround other proposed environmental causes, such as heavy metals, pesticides or childhood vaccines; the vaccine hypotheses are biologically implausible and lack convincing scientific evidence. The prevalence of autism is about 1–2 per 1,000 people; the prevalence of ASD is about 6 per 1,000, with about four times as many males as females. The number of people diagnosed with autism has increased dramatically since the 1980s, partly due to changes in diagnostic practice; the question of whether actual prevalence has increased is unresolved.
Parents usually notice signs in the first two years of their child's life. The signs usually develop gradually, but some autistic children first develop more normally and then regress. Although early behavioral or cognitive intervention can help autistic children gain self-care, social, and communication skills, there is no known cure. Not many children with autism live independently after reaching adulthood, though some become successful. An autistic culture has developed, with some individuals seeking a cure and others believing autism should be tolerated as a difference and not treated as a disorder.
Characteristics.
Autism is a highly variable neurodevelopmental disorder that first appears during infancy or childhood, and generally follows a steady course without remission. Overt symptoms gradually begin after the age of six months, become established by age two or three years, and tend to continue through adulthood, although often in more muted form. It is distinguished not by a single symptom, but by a characteristic triad of symptoms: impairments in social interaction; impairments in communication; and restricted interests and repetitive behavior. Other aspects, such as atypical eating, are also common but are not essential for diagnosis. Autism's individual symptoms occur in the general population and appear not to associate highly, without a sharp line separating pathologically severe from common traits.
Social development.
Social deficits distinguish autism and the related autism spectrum disorders (ASD; see "Classification") from other developmental disorders. People with autism have social impairments and often lack the intuition about others that many people take for granted. Noted autistic Temple Grandin described her inability to understand the social communication of neurotypicals, or people with normal neural development, as leaving her feeling "like an anthropologist on Mars".
Unusual social development becomes apparent early in childhood. Autistic infants show less attention to social stimuli, smile and look at others less often, and respond less to their own name. Autistic toddlers differ more strikingly from social norms; for example, they have less eye contact and turn taking, and are more likely to communicate by manipulating another person's hand. Three- to five-year-old autistic children are less likely to exhibit social understanding, approach others spontaneously, imitate and respond to emotions, communicate nonverbally, and take turns with others. However, they do form attachments to their primary caregivers. Most autistic children display moderately less attachment security than non-autistic children, although this difference disappears in children with higher mental development or less severe ASD. Older children and adults with ASD perform worse on tests of face and emotion recognition.
Contrary to a common belief, autistic children do not prefer being alone. Making and maintaining friendships often proves to be difficult for those with autism. For them, the quality of friendships, not the number of friends, predicts how lonely they feel. Functional friendships, such as those resulting in invitations to parties, may affect the quality of life more deeply.
There are many anecdotal reports, but few systematic studies, of aggression and violence in individuals with ASD. The limited data suggest that, in children with mental retardation, autism is associated with aggression, destruction of property, and tantrums. A 2007 study interviewed parents of 67 children with ASD and reported that about two-thirds of the children had periods of severe tantrums and about one-third had a history of aggression, with tantrums significantly more common than in non-autistic children with language impairments. A 2008 Swedish study found that, of individuals aged 15 or older discharged from hospital with a diagnosis of ASD, those who committed violent crimes were significantly more likely to have other psychopathological conditions such as psychosis.
Communication.
About a third to a half of individuals with autism do not develop enough natural speech to meet their daily communication needs. Differences in communication may be present from the first year of life, and may include delayed onset of babbling, unusual gestures, diminished responsiveness, and vocal patterns that are not synchronized with the caregiver. In the second and third years, autistic children have less frequent and less diverse babbling, consonants, words, and word combinations; their gestures are less often integrated with words. Autistic children are less likely to make requests or share experiences, and are more likely to simply repeat others' words (echolalia) or reverse pronouns. Joint attention seems to be necessary for functional speech, and deficits in joint attention seem to distinguish infants with ASD: for example, they may look at a pointing hand instead of the pointed-at object, and they consistently fail to point at objects in order to comment on or share an experience. Autistic children may have difficulty with imaginative play and with developing symbols into language.
In a pair of studies, high-functioning autistic children aged 8–15 performed equally well as, and adults better than, individually matched controls at basic language tasks involving vocabulary and spelling. Both autistic groups performed worse than controls at complex language tasks such as figurative language, comprehension and inference. As people are often sized up initially from their basic language skills, these studies suggest that people speaking to autistic individuals are more likely to overestimate what their audience comprehends.
Repetitive behavior.
Autistic individuals display many forms of repetitive or restricted behavior, which the Repetitive Behavior Scale-Revised (RBS-R) categorizes as follows.
No single repetitive behavior seems to be specific to autism, but only autism appears to have an elevated pattern of occurrence and severity of these behaviors.
Other symptoms.
Autistic individuals may have symptoms that are independent of the diagnosis, but that can affect the individual or the family.
An estimated 0.5% to 10% of individuals with ASD show unusual abilities, ranging from splinter skills such as the memorization of trivia to the extraordinarily rare talents of prodigious autistic savants. Many individuals with ASD show superior skills in perception and attention, relative to the general population.
Sensory abnormalities are found in over 90% of those with autism, and are considered core features by some, although there is no good evidence that sensory symptoms differentiate autism from other developmental disorders. Differences are greater for under-responsivity (for example, walking into things) than for over-responsivity (for example, distress from loud noises) or for sensation seeking (for example, rhythmic movements).
An estimated 60%–80% of autistic people have motor signs that include poor muscle tone, poor motor planning, and toe walking;; deficits in motor coordination are pervasive across ASD and are greater in autism proper.
Unusual eating behavior occurs in about three-quarters of children with ASD, to the extent that it was formerly a diagnostic indicator. Selectivity is the most common problem, although eating rituals and food refusal also occur; this does not appear to result in malnutrition. Although some children with autism also have gastrointestinal (GI) symptoms, there is a lack of published rigorous data to support the theory that autistic children have more or different GI symptoms than usual; studies report conflicting results, and the relationship between GI problems and ASD is unclear.
Parents of children with ASD have higher levels of stress. Siblings of children with ASD report greater admiration of and less conflict with the affected sibling than siblings of unaffected children or those with Down syndrome; siblings of individuals with ASD have greater risk of negative well-being and poorer sibling relationships as adults.
Classification.
Autism is one of the five pervasive developmental disorders (PDD), which are characterized by widespread abnormalities of social interactions and communication, and severely restricted interests and highly repetitive behavior. These symptoms do not imply sickness, fragility, or emotional disturbance.
Of the five PDD forms, Asperger syndrome is closest to autism in signs and likely causes; Rett syndrome and childhood disintegrative disorder share several signs with autism, but may have unrelated causes; PDD not otherwise specified (PDD-NOS; also called "atypical autism") is diagnosed when the criteria are not met for a more specific disorder. Unlike with autism, people with Asperger syndrome have no substantial delay in language development. The terminology of autism can be bewildering, with autism, Asperger syndrome and PDD-NOS often called the "autism spectrum disorders" (ASD) or sometimes the "autistic disorders", whereas autism itself is often called "autistic disorder", "childhood autism", or "infantile autism". In this article, "autism" refers to the classic autistic disorder; in clinical practice, though, "autism", "ASD", and "PDD" are often used interchangeably. ASD, in turn, is a subset of the broader autism phenotype, which describes individuals who may not have ASD but do have autistic-like traits, such as avoiding eye contact.
The manifestations of autism cover a wide spectrum, ranging from individuals with severe impairments—who may be silent, mentally disabled, and locked into hand flapping and rocking—to high functioning individuals who may have active but distinctly odd social approaches, narrowly focused interests, and verbose, pedantic communication. Because the behavior spectrum is continuous, boundaries between diagnostic categories are necessarily somewhat arbitrary. Sometimes the syndrome is divided into low-, medium- or high-functioning autism (LFA, MFA, and HFA), based on IQ thresholds, or on how much support the individual requires in daily life; these subdivisions are not standardized and are controversial. Autism can also be divided into syndromal and non-syndromal autism; the syndromal autism is associated with severe or profound mental retardation or a congenital syndrome with physical symptoms, such as tuberous sclerosis. Although individuals with Asperger syndrome tend to perform better cognitively than those with autism, the extent of the overlap between Asperger syndrome, HFA, and non-syndromal autism is unclear.
Some studies have reported diagnoses of autism in children due to a loss of language or social skills, as opposed to a failure to make progress, typically from 15 to 30 months of age. The validity of this distinction remains controversial; it is possible that regressive autism is a specific subtype, or that there is a continuum of behaviors between autism with and without regression.
Research into causes has been hampered by the inability to identify biologically meaningful subpopulations and by the traditional boundaries between the disciplines of psychiatry, psychology, neurology and pediatrics. Newer technologies such as fMRI and diffusion tensor imaging can help identify biologically relevant phenotypes (observable traits) that can be viewed on brain scans, to help further neurogenetic studies of autism; one example is lowered activity in the fusiform face area of the brain, which is associated with impaired perception of people versus objects. It has been proposed to classify autism using genetics as well as behavior.
Causes.
It has long been presumed that there is a common cause at the genetic, cognitive, and neural levels for autism's characteristic triad of symptoms. However, there is increasing suspicion that autism is instead a complex disorder whose core aspects have distinct causes that often co-occur.
Autism has a strong genetic basis, although the genetics of autism are complex and it is unclear whether ASD is explained more by rare mutations with major effects, or by rare multigene interactions of common genetic variants. Complexity arises due to interactions among multiple genes, the environment, and epigenetic factors which do not change DNA but are heritable and influence gene expression. Studies of twins suggest that heritability is 0.7 for autism and as high as 0.9 for ASD, and siblings of those with autism are about 25 times more likely to be autistic than the general population. However, most of the mutations that increase autism risk have not been identified. Typically, autism cannot be traced to a Mendelian (single-gene) mutation or to a single chromosome abnormality like fragile X syndrome, and none of the genetic syndromes associated with ASDs has been shown to selectively cause ASD. Numerous candidate genes have been located, with only small effects attributable to any particular gene. The large number of autistic individuals with unaffected family members may result from copy number variations—spontaneous deletions or duplications in genetic material during meiosis. Hence, a substantial fraction of autism cases may be traceable to genetic causes that are highly heritable but not inherited: that is, the mutation that causes the autism is not present in the parental genome.
Several lines of evidence point to synaptic dysfunction as a cause of autism. Some rare mutations may lead to autism by disrupting some synaptic pathways, such as those involved with cell adhesion. Gene replacement studies in mice suggest that autistic symptoms are closely related to later developmental steps that depend on activity in synapses and on activity-dependent changes. All known teratogens (agents that cause birth defects) related to the risk of autism appear to act during the first eight weeks from conception, and though this does not exclude the possibility that autism can be initiated or affected later, it is strong evidence that autism arises very early in development. Although evidence for other environmental causes is anecdotal and has not been confirmed by reliable studies, extensive searches are underway. Environmental factors that have been claimed to contribute to or exacerbate autism, or may be important in future research, include certain foods, infectious disease, heavy metals, solvents, diesel exhaust, PCBs, phthalates and phenols used in plastic products, pesticides, brominated flame retardants, alcohol, smoking, illicit drugs, vaccines, and prenatal stress. Parents may first become aware of autistic symptoms in their child around the time of a routine vaccination, and this has given rise to theories that vaccines or their preservatives cause autism. Although these theories lack convincing scientific evidence and are biologically implausible, parental concern about autism has led to lower rates of childhood immunizations and higher likelihood of measles outbreaks.
Mechanism.
Autism's symptoms result from maturation-related changes in various systems of the brain. How autism occurs is not well understood. Its mechanism can be divided into two areas: the pathophysiology of brain structures and processes associated with autism, and the neuropsychological linkages between brain structures and behaviors. The behaviors appear to have multiple pathophysiologies.
Pathophysiology.
Interactions between the immune system and the nervous system begin early during the embryonic stage of life, and successful neurodevelopment depends on a balanced immune response. It is possible that aberrant immune activity during critical periods of neurodevelopment is part of the mechanism of some forms of ASD. Although some abnormalities in the immune system have been found in specific subgroups of autistic individuals, it is not known whether these abnormalities are relevant to or secondary to autism's disease processes. As autoantibodies are found in conditions other than ASD, and are not always present in ASD, the relationship between immune disturbances and autism remains unclear and controversial.
The relationship of neurochemicals to autism is not well understood; several have been investigated, with the most evidence for the role of serotonin and of genetic differences in its transport. Some data suggest an increase in several growth hormones; other data argue for diminished growth factors. Also, some inborn errors of metabolism are associated with autism but probably account for less than 5% of cases.
The mirror neuron system (MNS) theory of autism hypothesizes that distortion in the development of the MNS interferes with imitation and leads to autism's core features of social impairment and communication difficulties. The MNS operates when an animal performs an action or observes another animal perform the same action. The MNS may contribute to an individual's understanding of other people by enabling the modeling of their behavior via embodied simulation of their actions, intentions, and emotions. Several studies have tested this hypothesis by demonstrating structural abnormalities in MNS regions of individuals with ASD, delay in the activation in the core circuit for imitation in individuals with Asperger syndrome, and a correlation between reduced MNS activity and severity of the syndrome in children with ASD. However, individuals with autism also have abnormal brain activation in many circuits outside the MNS and the MNS theory does not explain the normal performance of autistic children on imitation tasks that involve a goal or object.
ASD-related patterns of low function and aberrant activation in the brain differ depending on whether the brain is doing social or nonsocial tasks.
In autism there is evidence for reduced functional connectivity of the default network, a large-scale brain network involved in social and emotional processing, with intact connectivity of the task-positive network, used in sustained attention and goal-directed thinking. In people with autism the two networks are not negatively correlated in time, suggesting an imbalance in toggling between the two networks, possibly reflecting a disturbance of self-referential thought. A 2008 brain-imaging study found a specific pattern of signals in the cingulate cortex which differs in individuals with ASD.
The underconnectivity theory of autism hypothesizes that autism is marked by underfunctioning high-level neural connections and synchronization, along with an excess of low-level processes. Evidence for this theory has been found in functional neuroimaging studies on autistic individuals and by a brain wave study that suggested that adults with ASD have local overconnectivity in the cortex and weak functional connections between the frontal lobe and the rest of the cortex. Other evidence suggests the underconnectivity is mainly within each hemisphere of the cortex and that autism is a disorder of the association cortex.
From studies based on event-related potentials, transient changes to the brain's electrical activity in response to stimuli, there is considerable evidence for differences in autistic individuals with respect to attention, orientiation to auditory and visual stimuli, novelty detection, language and face processing, and information storage; several studies have found a preference for non-social stimuli. For example, magnetoencephalography studies have found evidence in autistic children of delayed responses in the brain's processing of auditory signals.
Neuropsychology.
Two major categories of cognitive theories have been proposed about the links between autistic brains and behavior.
The first category focuses on deficits in social cognition. The empathizing–systemizing theory postulates that autistic individuals can systemize—that is, they can develop internal rules of operation to handle events inside the brain—but are less effective at empathizing by handling events generated by other agents. An extension, the extreme male brain theory, hypothesizes that autism is an extreme case of the male brain, defined psychometrically as individuals in whom systemizing is better than empathizing; this extension is controversial, as many studies contradict the idea that baby boys and girls respond differently to people and objects.
These theories are somewhat related to the earlier theory of mind approach, which hypothesizes that autistic behavior arises from an inability to ascribe mental states to oneself and others. The theory of mind hypothesis is supported by autistic children's atypical responses to the Sally–Anne test for reasoning about others' motivations, and the mirror neuron system theory of autism described in "Pathophysiology" maps well to the hypothesis. However, most studies have found no evidence of impairment in autistic individuals' ability to understand other people's basic intentions or goals; instead, data suggests that impairments are found in understanding more complex social emotions or in considering others' viewpoints.
The second category focuses on nonsocial or general processing. Executive dysfunction hypothesizes that autistic behavior results in part from deficits in working memory, planning, inhibition, and other forms of executive function. Tests of core executive processes such as eye movement tasks indicate improvement from late childhood to adolescence, but performance never reaches typical adult levels. A strength of the theory is predicting stereotyped behavior and narrow interests; two weaknesses are that executive function is hard to measure and that executive function deficits have not been found in young autistic children.
Weak central coherence theory hypothesizes that a limited ability to see the big picture underlies the central disturbance in autism. One strength of this theory is predicting special talents and peaks in performance in autistic people. A related theory—enhanced perceptual functioning—focuses more on the superiority of locally oriented and perceptual operations in autistic individuals. These theories map well from the underconnectivity theory of autism.
Neither category is satisfactory on its own; social cognition theories poorly address autism's rigid and repetitive behaviors, while the nonsocial theories have difficulty explaining social impairment and communication difficulties. A combined theory based on multiple deficits may prove to be more useful.
Screening.
U.S. and Japanese practice is to screen all children for ASD at 18 and 24 months, using autism-specific formal screening tests. In contrast, in the UK, screening targets children whose families or doctors recognize possible signs of autism. It is not known which approach is more effective. Screening tools include the Modified Checklist for Autism in Toddlers (M-CHAT), the Early Screening of Autistic Traits Questionnaire, and the First Year Inventory; initial data on M-CHAT and its predecessor CHAT on children aged 18–30 months suggests that it is best used in a clinical setting and that it has low sensitivity (many false-negatives) but good specificity (few false-positives). It may be more accurate to precede these tests with a broadband screener that does not distinguish ASD from other developmental disorders. Screening tools designed for one culture's norms for behaviors like eye contact may be inappropriate for a different culture. Although genetic screening for autism is generally still impractical, it can be considered in some cases, such as children with neurological symptoms and dysmorphic features.
Diagnosis.
Diagnosis is based on behavior, not cause or mechanism. Autism is defined in the DSM-IV-TR as exhibiting at least six symptoms total, including at least two symptoms of qualitative impairment in social interaction, at least one symptom of qualitative impairment in communication, and at least one symptom of restricted and repetitive behavior. Sample symptoms include lack of social or emotional reciprocity, stereotyped and repetitive use of language or idiosyncratic language, and persistent preoccupation with parts of objects. Onset must be prior to age three years, with delays or abnormal functioning in either social interaction, language as used in social communication, or symbolic or imaginative play. The disturbance must not be better accounted for by Rett syndrome or childhood disintegrative disorder. ICD-10 uses essentially the same definition.
Several diagnostic instruments are available. Two are commonly used in autism research: the Autism Diagnostic Interview-Revised (ADI-R) is a semistructured parent interview, and the Autism Diagnostic Observation Schedule (ADOS) uses observation and interaction with the child. The Childhood Autism Rating Scale (CARS) is used widely in clinical environments to assess severity of autism based on observation of children.
A pediatrician commonly performs a preliminary investigation by taking developmental history and physically examining the child. If warranted, diagnosis and evaluations are conducted with help from ASD specialists, observing and assessing cognitive, communication, family, and other factors using standardized tools, and taking into account any associated medical conditions. A pediatric neuropsychologist is often asked to assess behavior and cognitive skills, both to aid diagnosis and to help recommend educational interventions. A differential diagnosis for ASD at this stage might also consider mental retardation, hearing impairment, and a specific language impairment such as Landau–Kleffner syndrome. The presence of autism can make it harder to diagnose coexisting psychiatric disorders such as depression.
Clinical genetics evaluations are often done once ASD is diagnosed, particularly when other symptoms already suggest a genetic cause. Although genetic technology allows clinical geneticists to link an estimated 40% of cases to genetic causes, consensus guidelines in the U.S. and UK are limited to high-resolution chromosome and fragile X testing. A genotype-first model of diagnosis has been proposed, which would routinely assess the genome's copy number variations. As new genetic tests are developed several ethical, legal, and social issues will emerge. Commercial availability of tests may precede adequate understanding of how to use test results, given the complexity of autism's genetics. Metabolic and neuroimaging tests are sometimes helpful, but are not routine.
ASD can sometimes be diagnosed by age 14 months, although diagnosis becomes increasingly stable over the first three years of life: for example, a one-year-old who meets diagnostic criteria for ASD is less likely than a three-year-old to continue to do so a few years later. In the UK the National Autism Plan for Children recommends at most 30 weeks from first concern to completed diagnosis and assessment, though few cases are handled that quickly in practice. A 2009 U.S. study found the average age of formal ASD diagnosis was 5.7 years, far above recommendations, and that 27% of children remained undiagnosed at age 8 years. Although the symptoms of autism and ASD begin early in childhood, they are sometimes missed; years later, adults may seek diagnoses to help them or their friends and family understand themselves, to help their employers make adjustments, or in some locations to claim disability living allowances or other benefits.
Underdiagnosis and overdiagnosis are problems in marginal cases, and much of the recent increase in the number of reported ASD cases is likely due to changes in diagnostic practices. The increasing popularity of drug treatment options and the expansion of benefits has given providers incentives to diagnose ASD, resulting in some overdiagnosis of children with uncertain symptoms. Conversely, the cost of screening and diagnosis and the challenge of obtaining payment can inhibit or delay diagnosis. It is particularly hard to diagnose autism among the visually impaired, partly because some of its diagnostic criteria depend on vision, and partly because autistic symptoms overlap with those of common blindness syndromes.
Management.
The main goals of treatment are to lessen associated deficits and family distress, and to increase quality of life and functional independence. No single treatment is best and treatment is typically tailored to the child's needs. Families and the educational system are the main resources for treatment. Studies of interventions have methodological problems that prevent definitive conclusions about efficacy. Although many psychosocial interventions have some positive evidence, suggesting that some form of treatment is preferable to no treatment, the methodological quality of systematic reviews of these studies has generally been poor, their clinical results are mostly tentative, and there is little evidence for the relative effectiveness of treatment options. Intensive, sustained special education programs and behavior therapy early in life can help children acquire self-care, social, and job skills, and often improve functioning and decrease symptom severity and maladaptive behaviors; claims that intervention by around age three years is crucial are not substantiated. Available approaches include applied behavior analysis (ABA), developmental models, structured teaching, speech and language therapy, social skills therapy, and occupational therapy. Educational interventions have some effectiveness in children: intensive ABA treatment has demonstrated effectiveness in enhancing global functioning in preschool children and is well-established for improving intellectual performance of young children. Neuropsychological reports are often poorly communicated to educators, resulting in a gap between what a report recommends and what education is provided. It is not known whether treatment programs for children lead to significant improvements after the children grow up, and the limited research on the effectiveness of adult residential programs shows mixed results.
Many medications are used to treat ASD symptoms that interfere with integrating a child into home or school when behavioral treatment fails. More than half of U.S. children diagnosed with ASD are prescribed psychoactive drugs or anticonvulsants, with the most common drug classes being antidepressants, stimulants, and antipsychotics. Aside from antipsychotics, there is scant reliable research about the effectiveness or safety of drug treatments for adolescents and adults with ASD. A person with ASD may respond atypically to medications, the medications can have adverse effects, and no known medication relieves autism's core symptoms of social and communication impairments. Experiments in mice have reversed or reduced some symptoms related to autism by replacing or modulating gene function after birth, suggesting the possibility of targeting therapies to specific rare mutations known to cause autism.
Although many alternative therapies and interventions are available, few are supported by scientific studies. Treatment approaches have little empirical support in quality-of-life contexts, and many programs focus on success measures that lack predictive validity and real-world relevance. Scientific evidence appears to matter less to service providers than program marketing, training availability, and parent requests. Though most alternative treatments, such as melatonin, have only mild adverse effects some may place the child at risk. A 2008 study found that compared to their peers, autistic boys have significantly thinner bones if on casein-free diets; in 2005, botched chelation therapy killed a five-year-old child with autism.
Treatment is expensive; indirect costs are more so. For someone born in 2000, a U.S. study estimated an average lifetime cost of $ (net present value in dollars, inflation-adjusted from 2003 estimate), with about 10% medical care, 30% extra education and other care, and 60% lost economic productivity. Publicly supported programs are often inadequate or inappropriate for a given child, and unreimbursed out-of-pocket medical or therapy expenses are associated with likelihood of family financial problems; one 2008 U.S. study found a 14% average loss of annual income in families of children with ASD, and a related study found that ASD is associated with higher probability that child care problems will greatly affect parental employment. U.S. states increasingly require private health insurance to cover autism services, shifting costs from publicly funded education programs to privately funded health insurance. After childhood, key treatment issues include residential care, job training and placement, sexuality, social skills, and estate planning.
Prognosis.
No cure is known. Children recover occasionally, so that they lose their diagnosis of ASD; this occurs sometimes after intensive treatment and sometimes not. It is not known how often recovery happens; reported rates in unselected samples of children with ASD have ranged from 3% to 25%. A few autistic children have acquired speech at age 5 or older. Most children with autism lack social support, meaningful relationships, future employment opportunities or self-determination. Although core difficulties tend to persist, symptoms often become less severe with age. Few high-quality studies address long-term prognosis. Some adults show modest improvement in communication skills, but a few decline; no study has focused on autism after midlife. Acquiring language before age six, having an IQ above 50, and having a marketable skill all predict better outcomes; independent living is unlikely with severe autism. A 2004 British study of 68 adults who were diagnosed before 1980 as autistic children with IQ above 50 found that 12% achieved a high level of independence as adults, 10% had some friends and were generally in work but required some support, 19% had some independence but were generally living at home and needed considerable support and supervision in daily living, 46% needed specialist residential provision from facilities specializing in ASD with a high level of support and very limited autonomy, and 12% needed high-level hospital care. A 2005 Swedish study of 78 adults that did not exclude low IQ found worse prognosis; for example, only 4% achieved independence. A 2008 Canadian study of 48 young adults diagnosed with ASD as preschoolers found outcomes ranging through poor (46%), fair (32%), good (17%), and very good (4%); 56% of these young adults had been employed at some point during their lives, mostly in volunteer, sheltered or part-time work. Changes in diagnostic practice and increased availability of effective early intervention make it unclear whether these findings can be generalized to recently diagnosed children.
Epidemiology.
Most recent reviews tend to estimate a prevalence of 1–2 per 1,000 for autism and close to 6 per 1,000 for ASD; because of inadequate data, these numbers may underestimate ASD's true prevalence. PDD-NOS's prevalence has been estimated at 3.7 per 1,000, Asperger syndrome at roughly 0.6 per 1,000, and childhood disintegrative disorder at 0.02 per 1,000. The number of reported cases of autism increased dramatically in the 1990s and early 2000s. This increase is largely attributable to changes in diagnostic practices, referral patterns, availability of services, age at diagnosis, and public awareness, though unidentified environmental risk factors cannot be ruled out. The available evidence does not rule out the possibility that autism's true prevalence has increased; a real increase would suggest directing more attention and funding toward changing environmental factors instead of continuing to focus on genetics.
Boys are at higher risk for ASD than girls. The sex ratio averages 4.3:1 and is greatly modified by cognitive impairment: it may be close to 2:1 with mental retardation and more than 5.5:1 without. Although the evidence does not implicate any single pregnancy-related risk factor as a cause of autism, the risk of autism is associated with advanced age in either parent, and with diabetes, bleeding, and use of psychiatric drugs in the mother during pregnancy. The risk is greater with older fathers than with older mothers; two potential explanations are the known increase in mutation burden in older sperm, and the hypothesis that men marry later if they carry genetic liability and show some signs of autism. Most professionals believe that race, ethnicity, and socioeconomic background do not affect the occurrence of autism.
History.
A few examples of autistic symptoms and treatments were described long before autism was named. The "Table Talk" of Martin Luther contains the story of a 12-year-old boy who may have been severely autistic. According to Luther's notetaker Mathesius, Luther thought the boy was a soulless mass of flesh possessed by the devil, and suggested that he be suffocated. The earliest well-documented case of autism is that of Hugh Blair of Borgue, as detailed in a 1747 court case in which his brother successfully petitioned to annul Blair's marriage to gain Blair's inheritance. The Wild Boy of Aveyron, a feral child caught in 1798, showed several signs of autism; the medical student Jean Itard treated him with a behavioral program designed to help him form social attachments and to induce speech via imitation.
The New Latin word "autismus" (English translation "autism") was coined by the Swiss psychiatrist Eugen Bleuler in 1910 as he was defining symptoms of schizophrenia. He derived it from the Greek word "autós" (αὐτός, meaning "self"), and used it to mean morbid self-admiration, referring to "autistic withdrawal of the patient to his fantasies, against which any influence from outside becomes an intolerable disturbance".
The word "autism" first took its modern sense in 1938 when Hans Asperger of the Vienna University Hospital adopted Bleuler's terminology "autistic psychopaths" in a lecture in German about child psychology. Asperger was investigating an ASD now known as Asperger syndrome, though for various reasons it was not widely recognized as a separate diagnosis until 1981. Leo Kanner of the Johns Hopkins Hospital first used "autism" in its modern sense in English when he introduced the label "early infantile autism" in a 1943 report of 11 children with striking behavioral similarities. Almost all the characteristics described in Kanner's first paper on the subject, notably "autistic aloneness" and "insistence on sameness", are still regarded as typical of the autistic spectrum of disorders. It is not known whether Kanner derived the term independently of Asperger.
Kanner's reuse of "autism" led to decades of confused terminology like "infantile schizophrenia", and child psychiatry's focus on maternal deprivation led to misconceptions of autism as an infant's response to "refrigerator mothers". Starting in the late 1960s autism was established as a separate syndrome by demonstrating that it is lifelong, distinguishing it from mental retardation and schizophrenia and from other developmental disorders, and demonstrating the benefits of involving parents in active programs of therapy. As late as the mid-1970s there was little evidence of a genetic role in autism; now it is thought to be one of the most heritable of all psychiatric conditions. Although the rise of parent organizations and the destigmatization of childhood ASD have deeply affected how we view ASD, parents continue to feel social stigma in situations where their autistic children's behaviors are perceived negatively by others, and many primary care physicians and medical specialists still express some beliefs consistent with outdated autism research. The Internet has helped autistic individuals bypass nonverbal cues and emotional sharing that they find so hard to deal with, and has given them a way to form online communities and work remotely. Sociological and cultural aspects of autism have developed: some in the community seek a cure, while others believe that autism is simply another way of being.
---END.OF.DOCUMENT---

Albedo.
The albedo of an object is a measure of how strongly it reflects light from light sources such as the Sun. It is therefore a more specific form of the term reflectivity. Albedo is defined as the ratio of total-reflected to incident electromagnetic radiation. It is a unitless measure indicative of a surface's or body's diffuse reflectivity. The word is derived from Latin "albedo" "whiteness", in turn from "albus" "white", and was introduced into optics by Johann Heinrich Lambert in his 1760 work "Photometria". The range of possible values is from 0 (dark) to 1 (bright).
The albedo is an important concept in climatology and astronomy, as well as in computer graphics and computer vision. In climatology it is sometimes expressed as a percentage. Its value depends on the frequency of radiation considered: unqualified, it usually refers to some appropriate average across the spectrum of visible light. In general, the albedo depends on the direction and directional distribution of incoming radiation. Exceptions are Lambertian surfaces, which scatter radiation in all directions in a cosine function, so their albedo does not depend on the incoming distribution. In realistic cases, a bidirectional reflectance distribution function (BRDF) is required to characterize the scattering properties of a surface accurately, although albedos are a very useful first approximation.
Terrestrial albedo.
Albedos of typical materials in visible light range from up to 0.9 for fresh snow, to about 0.04 for charcoal, one of the darkest substances. Deeply shadowed cavities can achieve an effective albedo approaching the zero of a blackbody. When seen from a distance, the ocean surface has a low albedo, as do most forests, while desert areas have some of the highest albedos among landforms. Most land areas are in an albedo range of 0.1 to 0.4. The average albedo of the Earth is about 0.3. This is far higher than for the ocean primarily because of the contribution of clouds.
Human activities have changed the albedo (via forest clearance and farming, for example) of various areas around the globe. However, quantification of this effect on the global scale is difficult.
The classic example of albedo effect is the snow-temperature feedback. If a snow-covered area warms and the snow melts, the albedo decreases, more sunlight is absorbed, and the temperature tends to increase. The converse is true: if snow forms, a cooling cycle happens. The intensity of the albedo effect depends on the size of the change in albedo and the amount of insolation; for this reason it can be potentially very large in the tropics.
The Earth's surface albedo is regularly estimated via Earth observation satellite sensors such as NASA's MODIS instruments onboard the Terra and Aqua satellites. As the total amount of reflected radiation cannot be directly measured by satellite, a mathematical model of the BRDF is used to translate a sample set of satellite reflectance measurements into estimates of directional-hemispherical reflectance and bi-hemispherical reflectance. (e. g.,
The Earth's average surface temperature due to its albedo and the greenhouse effect is currently about 15°C. For the frozen (more reflective) planet the average temperature is below -40°C (If only all continents being completely covered by glaciers - the mean temperature is about 0°C). The simulation for (more absorptive) aquaplanet shows the average temperature close to 27°C.
White-sky and black-sky albedo.
It has been shown that for many applications involving terrestrial albedo, the albedo at a particular solar zenith angle formula_1 can reasonably be approximated by the proportionate sum of two terms: the directional-hemispherical reflectance at that solar zenith angle, formula_2, and the bi-hemispherical reflectance, formula_3 the proportion concerned being defined as the proportion of diffuse illumination formula_4.
Directional-hemispherical reflectance is sometimes referred to as black-sky albedo and bi-hemispherical reflectance as white sky albedo. These terms are important because they allow the albedo to be calculated for any given illumination conditions from a knowledge of the intrinsic properties of the surface.
Astronomical albedo.
The albedos of planets, satellites and asteroids can be used to infer much about their properties. The study of albedos, their dependence on wavelength, lighting angle ("phase angle"), and variation in time comprises a major part of the astronomical field of photometry. For small and far objects that cannot be resolved by telescopes, much of what we know comes from the study of their albedos. For example, the absolute albedo can indicate the surface ice content of outer solar system objects, the variation of albedo with phase angle gives information about regolith properties, while unusually high radar albedo is indicative of high metallic content in asteroids.
Enceladus, a moon of Saturn, has one of the highest known albedos of any body in the Solar system, with 99% of EM radiation reflected. Another notable high albedo body is Eris, with an albedo of 0.86. Many small objects in the outer solar system and asteroid belt have low albedos down to about 0.05. A typical comet nucleus has an albedo of 0.04. Such a dark surface is thought to be indicative of a primitive and heavily space weathered surface containing some organic compounds.
The overall albedo of the Moon is around 0.072, but it is strongly directional and non-Lambertian, displaying also a strong opposition effect. While such reflectance properties are different from those of any terrestrial terrains, they are typical of the regolith surfaces of airless solar system bodies.
Two common albedos that are used in astronomy are the (V-band) geometric albedo (measuring brightness when illumination comes from directly behind the observer) and the Bond albedo (measuring total proportion of electromagnetic energy reflected). Their values can differ significantly, which is a common source of confusion.
In detailed studies, the directional reflectance properties of astronomical bodies are often expressed in terms of the five Hapke parameters which semi-empirically describe the variation of albedo with phase angle, including a characterization of the opposition effect of regolith surfaces.
formula_7,
where formula_8 is the astronomical albedo, formula_9 is the diameter in kilometres, and "H" is the absolute magnitude.
Other types of albedo.
Single scattering albedo is used to define scattering of electromagnetic waves on small particles. It depends on properties of the material (refractive index); the size of the particle or particles; and the wavelength of the incoming radiation.
Albedo also refers to the white, spongy inner lining of a citrus fruit rind. According to Dr. Renee M. Goodrich, associate professor of food science and human nutrition at the University of Florida, the albedo is rich in the soluble fiber pectin and contains vitamin C.
The tropics.
Although the albedo-temperature effect is most famous in colder regions of Earth, because more snow falls there, it is actually much stronger in tropical regions because in the tropics there is consistently more sunlight. When ranchers cut down dark, tropical rainforest trees to replace them with even darker soil in order to grow crops, the average temperature of the area increases up to 3 °C (5.4 °F) year-round, although part of the effect is due to changed evaporation (latent heat flux).
Small scale effects.
Albedo works on a smaller scale, too. People who wear dark clothes in the summertime put themselves at a greater risk of heatstroke than those who wear lighter color clothes.
Trees.
Because trees tend to have a low albedo, removing forests would tend to increase albedo and thereby could produce localized climate cooling. Cloud feedbacks further complicate the issue. In seasonally snow-covered zones, winter albedos of treeless areas are 10% to 50% higher than nearby forested areas because snow does not cover the trees as readily. Deciduous trees have an albedo value of about 0.15 to 0.18 while coniferous trees have a value of about 0.09 to 0.15. The difference between deciduous and coniferous is because coniferous trees are darker in general and have cone-shaped crowns. The shape of these crowns trap radiant energy more effectively than deciduous trees.
Studies by the Hadley Centre have investigated the relative (generally warming) effect of albedo change and (cooling) effect of carbon sequestration on planting forests. They found that new forests in tropical and midlatitude areas tended to cool; new forests in high latitudes (e.g. Siberia) were neutral or perhaps warming.
Snow.
Snow albedos can be as high as 0.9; this, however, is for the ideal example: fresh deep snow over a featureless landscape. Over Antarctica they average a little more than 0.8. If a marginally snow-covered area warms, snow tends to melt, lowering the albedo, and hence leading to more snowmelt (the ice-albedo positive feedback).
Water.
Water reflects light very differently from typical terrestrial materials. The reflectivity of a water surface is calculated using the Fresnel equations (see graph).
At the scale of the wavelength of light even wavy water is always smooth so the light is reflected in a specular manner (not diffusely). The glint of light off water is a commonplace effect of this. At small angles of incident light, waviness results in reduced reflectivity because of the steepness of the reflectivity-vs.-incident-angle curve and a locally increased average incident angle.
Although the reflectivity of water is very low at low and medium angles of incident light, it increases tremendously at high angles of incident light such as occur on the illuminated side of the Earth near the terminator (early morning, late afternoon and near the poles). However, as mentioned above, waviness causes an appreciable reduction. Since the light specularly reflected from water does not usually reach the viewer, water is usually considered to have a very low albedo in spite of its high reflectivity at high angles of incident light.
Note that white caps on waves look white (and have high albedo) because the water is foamed up (not smooth at the scale of the wavelength of light) so the Fresnel equations do not apply. Fresh ‘black’ ice exhibits Fresnel reflection.
Clouds.
Clouds are another source of albedo that play into the global warming equation. Different types of clouds have different albedo values, theoretically ranging from a minimum of near 0 to a maximum approaching 0.8. "On any given day, about half of Earth is covered by clouds, which reflect more sunlight than land and water. Clouds keep Earth cool by reflecting sunlight, but they can also serve as blankets to trap warmth."
Albedo and climate in some areas are already affected by artificial clouds, such as those created by the contrails of heavy commercial airliner traffic. A study following the burning of the Kuwaiti oil fields by Saddam Hussein showed that temperatures under the burning oil fires were as much as 10oC colder than temperatures several miles away under clear skies.
Aerosol effects.
Aerosol (very fine particles/droplets in the atmosphere) has two effects, direct and indirect. The direct (albedo) effect is generally to cool the planet; the indirect effect (the particles act as CCNs and thereby change cloud properties) is less certain.
Aerosols can modify the Earth’s radiative balance through the aerosol direct and indirect
Black carbon.
Another albedo-related effect on the climate is from black carbon particles. The size of this effect is difficult to quantify: the IPCC say that their "estimate of the global mean radiative forcing for BC aerosols from fossil fuels is... +0.2 W m-2 (from +0.1 W m-2 in the SAR) with a range +0.1 to +0.4 W m...-2".
---END.OF.DOCUMENT---

A.
The letter A is the first letter in the Latin alphabet, a vowel. Its name in English () is spelled ‹a›; the plural is "aes," although this is rare.
Origins.
"A" can be traced to a pictogram of an ox head in Egyptian hieroglyph or the Proto-Sinaitic alphabet.
In 1600 B.C. the Phoenician alphabet's letter had a linear form that served as the base for some later forms. Its name must have corresponded closely to the Hebrew or Arabic aleph.
When the Ancient Greeks adopted the alphabet, they had no use for the glottal stop that the letter had denoted in Phoenician and other Semitic languages, so they used the sign to represent the vowel, and kept its name with a minor change (alpha). In the earliest Greek inscriptions after the Greek Dark Ages, dating to the 8th century BC, the letter rests upon its side, but in the Greek alphabet of later times it generally resembles the modern capital letter, although many local varieties can be distinguished by the shortening of one leg, or by the angle at which the cross line is set.
The Etruscans brought the Greek alphabet to their civilization in the Italian Peninsula and left the letter unchanged. The Romans later adopted the Etruscan alphabet to write the Latin language, and the resulting letter was preserved in the modern Latin alphabet used to write many languages, including English.
The letter has two minuscule (lower-case) forms. The form used in most current handwriting consists of a circle and vertical stoke (), called Latin alpha or "script a". Most printed material uses a form consisting of a small loop with an arc over it (). Both derive from the majuscule (capital) form. In Greek handwriting, it was common to join the left leg and horizontal stroke into a single loop, as demonstrated by the Uncial version shown. Many fonts then made the right leg vertical. In some of these, the serif that began the right leg stroke developed into an arc, resulting in the printed form, while in others it was dropped, resulting in the modern handwritten form.
Usage.
In English, "a" by itself frequently denotes the near-open front unrounded vowel () as in "pad", the open back unrounded vowel () as in "father", or, in concert with a later orthographic vowel, the diphthong as in "ace" and "major", due to effects of the great vowel shift.
In most other languages that use the Latin alphabet, "a" denotes an open central unrounded vowel (). In the International Phonetic Alphabet, variants of "a" denote various vowels. In X-SAMPA, capital "A" denotes the open back unrounded vowel and lowercase "a" denotes the open front unrounded vowel.
"A" is the third common used letter in English, and the second most common in Spanish and French. In one study, on average, about 3.68% of letters used in English tend to be ‹a›s, while the number is 6.22% in Spanish and 3.95% in French.
"A" is often used to denote something or someone of a better or more prestigious quality or status: A-, A or A+, the best grade that can be assigned by teachers for students' schoolwork; A grade for clean restaurants; A-List celebrities, etc. Such associations can have a motivating effect as exposure to the letter A has been found to improve performance, when compared with other letters.
A turned "a" () is used by the International Phonetic Alphabet for the near-open central vowel, while a turned capital "A" ("∀") is used in predicate logic to specify universal quantification.
Codes for computing.
In Unicode the capital "A" is codepoint U+0043 and the lower case "a" is U+0067.
The ASCII code for capital "A" is 65 and for lower case "a" is 97; or in binary 01000001 and 01100001, respectively.
The EBCDIC code for capital "A" is 193 and for lowercase "a" is 129.
The numeric character references in HTML and XML are "&amp;#65;" and "&amp;#97;" for upper and lower case, respectively.
---END.OF.DOCUMENT---

Alabama.
Alabama is a state located in the southeastern region of the United States of America. It is bordered by Tennessee to the north, Georgia to the east, Florida and the Gulf of Mexico to the south, and Mississippi to the west. Alabama ranks 30th in total land area and ranks second in the size of its inland waterways. The state ranks 23rd in population with almost 4.6 million residents in 2006.
From the American Civil War until World War II, Alabama, like many Southern states, suffered economic hardship, in part because of continued dependence on agriculture. White rural interests dominated the state legislature until the 1960s, while urban interests and African Americans were underrepresented. Following World War II, Alabama experienced significant recovery as the economy of the state transitioned from agriculture to diversified interests in heavy manufacturing, mineral extraction, education, and technology, as well as the establishment or expansion of multiple military installations, primarily those of the U.S. Army and U.S. Air Force. The state has heavily invested in aerospace, education, health care, and banking, and various heavy industries including automobile manufacturing, mineral extraction, steel production and fabrication.
Alabama is unofficially nicknamed the "Yellowhammer State", which is also the name of the state bird. Alabama is also known as the "Heart of Dixie". The state tree is the Longleaf Pine, the state flower is the Camellia. The capital of Alabama is Montgomery, and the largest city by population is Birmingham. The largest city by total land area is Huntsville. The oldest city is Mobile.
Etymology of state name.
The Alabama, a Muskogean tribe whose members lived just below the confluence of the Coosa and Tallapoosa Rivers on the upper reaches of the Alabama River, served as the etymological source of the names of the river and state. In the Alabama language, the word for an Alabama person is "Albaamo" (or variously "Albaama" or "Albàamo" in different dialects; the plural form "Alabama persons" is "Albaamaha"). The word "Alabama" is believed to have originated from the Choctaw language and was later adopted by the Alabama tribe as their name. The spelling of the word varies significantly between sources. The first usage appears in three accounts of the Hernando de Soto expedition of 1540 with Garcilasso de la Vega using "Alibamo" while the Knight of Elvas and Rodrigo Ranjel wrote "Alibamu" and "Limamu", respectively. As early as 1702, the tribe was known to the French as "Alibamon" with French maps identifying the river as "Rivière des Alibamons". Other spellings of the appellation have included "Alibamu", "Alabamo", "Albama", "Alebamon", "Alibama", "Alibamou", "Alabamu", and "Allibamou".
Although the origin of "Alabama" was evident, the meaning of the tribe's name was not always clear. An article without a byline appearing in the "Jacksonville Republican" on July 27, 1842, originated the idea that the meaning was "Here We Rest." This notion was popularized in the 1850s through the writings of Alexander Beaufort Meek. Experts in the Muskogean languages have been unable to find any evidence that would support this translation. It is now generally accepted that the word comes from the Choctaw words "alba" (meaning "plants" or "weeds") and "amo" (meaning "to cut", "to trim", or "to gather"). This results in translations such as "clearers of the thicket" or even "herb gatherers" which may refer to clearing of land for the purpose of planting crops or to collection of medicinal plants by medicine men.
History.
Among the Native American people once living in the area of present day Alabama were Alabama ("Alibamu"), Cherokee, Chickasaw, Choctaw, Creek, Koasati, and Mobile. Trade with the Northeast via the Ohio River began during the Burial Mound Period (1000 BC-700 AD) and continued until European contact. The agrarian Mississippian culture covered most of the state from 1000 to 1600 AD, with one of its major centers being at the Moundville Archaeological Site in Moundville, Alabama. Artifacts recovered from archaeological excavations at Moundville were a major component in the formulation of the Southeastern Ceremonial Complex. Contrary to popular belief, this development appears to have no direct links to Mesoamerica, but developed independently. This Ceremonial Complex represents a major component of the religion of the Mississippian peoples, and is one of the primary means by which their religion is understood.
The French founded the first European settlement in the state with the establishment of Mobile in 1702. Southern Alabama was French from 1702 to 1763, part of British West Florida from 1763 to 1780, and part of Spanish West Florida from 1780 to 1814. Northern and central Alabama was part of British Georgia from 1763 to 1783 and part of the American Mississippi territory thereafter. Its statehood was delayed by the lack of a coastline; rectified when Andrew Jackson captured Spanish Mobile in 1814. Alabama was the twenty-second state, admitted to the Union in 1819. Its constitution provided for universal suffrage for white men.
Alabama was part of the new frontier in the 1820s and 1830s. Settlers rapidly arrived to take advantage of its fertile soil. Planters brought slaves with them, and traders brought in more from the Upper South as the cotton plantations expanded. The economy of the central "Black Belt" was built around large cotton plantations whose owners built their wealth on slave labor. It was named for the dark, productive soil. Elsewhere poor whites were subsistence farmers. According to the 1860 census, enslaved Africans comprised 45% of the state's population of 964,201. There were only 2,690 free persons of color.
In 1861 Alabama declared its secession from the Union and joined the Confederate States of America. While few battles were fought in the state, Alabama contributed about 120,000 soldiers to the Civil War. All the slaves were freed by 1865. Following Reconstruction, Alabama was restored to the Union in 1868.
After the Civil War, the state was still chiefly rural and tied to cotton. Planters resisted working with free labor and sought to re-establish controls over African Americans. Whites used paramilitary groups, Jim Crow laws and segregation to reduce freedoms of African Americans and restore their own dominance.
In its new constitution of 1901, the legislature effectively disfranchised African Americans through voting restrictions. While the planter class had engaged poor whites in supporting these efforts, the new restrictions resulted in disfranchising poor whites as well. By 1941, a total of more whites than blacks had been disfranchised: 600,000 whites to 520,000 blacks. This was due mostly to effects of the cumulative poll tax.
The damage to the African-American community was pervasive, as nearly all its citizens lost the ability to vote. In 1900, fourteen Black Belt counties (which were primarily African American) had more than 79,000 voters on the rolls. By June 1, 1903, the number of registered voters had dropped to 1,081. In 1900, Alabama had more than 181,000 African Americans eligible to vote. By 1903, only 2,980 had managed to "qualify" to register, although at least 74,000 black voters were literate. The shut out was long-lasting. The disfranchisement was ended only by African Americans leading the Civil Rights Movement and gaining Federal legislation in the mid-1960s to protect their voting and civil rights. The Voting Rights Act of 1965 also protected the suffrage of poor whites.
The rural-dominated legislature continued to underfund schools and services for African Americans in the segregated state, but did not relieve them of paying taxes. Continued racial discrimination, agricultural depression, and the failure of the cotton crops due to boll weevil infestation led tens of thousands of African Americans to seek out opportunities in northern cities. They left Alabama in the early 20th century as part of the Great Migration to industrial jobs and better futures in northern industrial cities. The population growth rate in Alabama (see "Historical Populations" table below) dropped by nearly half from 1910–1920, reflecting the effect of outmigration.
At the same time, many rural whites and blacks migrated to the city of Birmingham for work in new industrial jobs. It experienced such rapid growth that it was nicknamed "The Magic City". By the 1920s, Birmingham was the 19th largest city in the U.S. and held more than 30% of the population of the state. Heavy industry and mining were the basis of the economy.
Despite massive population changes in the state from 1901 to 1961, the rural-dominated legislature refused to reapportion House and Senate seats based on population. They held on to old representation to maintain political and economic power in agricultural areas. In addition, the state legislature gerrymandered the few Birmingham legislative seats to ensure election by persons living outside of Birmingham.
One result was that Jefferson County, containing Birmingham's industrial and economic powerhouse, contributed more than one-third of all tax revenue to the state. Urban interests were consistently underrepresented in the legislature. A 1960 study noted that because of rural domination, "A minority of about 25 per cent of the total state population is in majority control of the Alabama legislature."
African Americans were presumed partial to Republicans for historical reasons, but they were disenfranchised. White Alabamans still felt bitter towards the Republican Party in the aftermath of the Civil War and Reconstruction. These factors created a longstanding tradition that any candidate who wanted to be viable with white voters had to run as a Democrat regardless of political beliefs. The state continued as one-party Democratic for more than a century after Reconstruction ended. It produced a number of national leaders. Industrial development related to the demands of World War II brought prosperity. Cotton faded in importance as the state developed a manufacturing and service base. In the 1960s under Governor George Wallace, many whites in the state opposed integration efforts.
During the Civil Rights Movement, African Americans achieved a protection of voting and other civil rights through the passage of the national Civil Rights Act of 1964, and the Voting Rights Act of 1965. "De jure" segregation ended in the states as Jim Crow laws were invalidated or repealed.
Under the Voting Rights Act of 1965, cases were filed in Federal courts to force Alabama to properly redistrict by population both the state legislature House and Senate. In 1972, for the first time since 1901, the legislature implemented the Alabama constitution's provision for periodic redistricting based on population. This benefited the many urban areas that had developed, and all in the population who had been underrepresented for more than 60 years.
After 1972, the state's white voters shifted much of their support to Republican candidates in presidential elections (as also occurred in neighboring southern states). Since 1990 the majority of whites in the state have also voted increasingly Republican in state elections, although Democrats are still the majority party in both houses of the legislature.
Geography.
Alabama is the thirtieth largest state in the United States with 52,423 square miles (135,775 km²) of total area: 3.19% of the area is water, making Alabama twenty-third in the amount of surface water, also giving it the second largest inland waterway system in the United States. About three-fifths of the land area is a gentle plain with a general descent towards the Mississippi River and the Gulf of Mexico. The North Alabama region is mostly mountainous, with the Tennessee River cutting a large valley creating numerous creeks, streams, rivers, mountains, and lakes.
The states bordering Alabama are Tennessee to the north; Georgia to the east; Florida to the south; and Mississippi to the west. Alabama has coastline at the Gulf of Mexico, in the extreme southern edge of the state. Alabama ranges in elevation from sea level at Mobile Bay to over 1,800 feet (550 m) in the Appalachian Mountains in the northeast. The highest point is Mount Cheaha, at a height of. Alabama's land consists of of forest or 67% of total land area. Suburban Baldwin County, along the Gulf Coast, is the largest county in the state in both land area and water area.
Areas in Alabama administered by the National Park Service include Horseshoe Bend National Military Park near Alexander City; Little River Canyon National Preserve near Fort Payne; Russell Cave National Monument in Bridgeport; Tuskegee Airmen National Historic Site in Tuskegee; and Tuskegee Institute National Historic Site near Tuskegee. Additionally, Alabama has four National Forests including Conecuh, Talladega, Tuskegee, and William B. Bankhead. Alabama also contains the Natchez Trace Parkway, the Selma To Montgomery National Historic Trail, and the Trail Of Tears National Historic Trail. A notable natural wonder in Alabama is "Natural Bridge" rock, the longest natural bridge east of the Rockies, located just south of Haleyville, in Winston County.
A -wide meteorite impact crater is located in Elmore County, just north of Montgomery. This is the Wetumpka crater, which is the site of "Alabama's greatest natural disaster". A -wide meteorite hit the area about 80 million years ago. The hills just east of downtown Wetumpka showcase the eroded remains of the impact crater that was blasted into the bedrock, with the area labeled the Wetumpka crater or astrobleme ("star-wound") because of the concentric rings of fractures and zones of shattered rock that can be found beneath the surface. In 2002, Christian Koeberl with the Institute of Geochemistry University of Vienna published evidence and established the site as an internationally recognized impact crater.
Climate.
The state is classified as humid subtropical ("Cfa") under the Koppen Climate Classification. The average annual temperature is 64 °F (18 °C). Temperatures tend to be warmer in the southern part of the state with its proximity to the Gulf of Mexico, while the northern parts of the state, especially in the Appalachian Mountains in the northeast, tend to be slightly cooler. Generally, Alabama has very hot summers and mild winters with copious precipitation throughout the year. Alabama receives an average of of rainfall annually and enjoys a lengthy growing season of up to 300 days in the southern part of the state.
Summers in Alabama are among the hottest in the United States, with high temperatures averaging over throughout the summer in some parts of the state. Alabama is also prone to tropical storms and even hurricanes. Areas of the state far away from the Gulf are not immune to the effects of the storms, which often dump tremendous amounts of rain as they move inland and weaken.
South Alabama reports more thunderstorms than any part of the U.S. The Gulf Coast, around Mobile Bay, averages between 70 and 80 days per year with thunder reported. This activity decreases somewhat further north in the state, but even the far north of the state reports thunder on about 60 days per year. Occasionally, thunderstorms are severe with frequent lightning and large hail – the central and northern parts of the state are most vulnerable to this type of storm. Alabama ranks seventh in the number of deaths from lightning and ninth in the number of deaths from lightning strikes per capita. Sometimes tornadoes occur – these are common throughout the state, although the peak season for tornadoes varies from the northern to southern parts of the state. Alabama shares the dubious distinction, with Kansas, of having reported more EF5 tornadoes than any other state – according to statistics from the National Climatic Data Center for the period January 1, 1950, to October 31, 2006. An F5 tornado is the most powerful of its kind. Several long – tracked F5 tornadoes have contributed to Alabama reporting more tornado fatalities than any other state except for Texas and Mississippi. The Super Outbreak in March 1974, badly affected Alabama. The northern part of the state – along the Tennessee Valley – is one of the areas in the US most vulnerable to violent tornadoes. The area of Alabama and Mississippi most affected by tornadoes is sometimes referred to as Dixie Alley, as distinct from the Tornado Alley of the Southern Plains. Alabama is one of the few places in the world that has a secondary tornado season (November and December) along with the spring severe weather season.
Winters are generally mild in Alabama, as they are throughout most of the southeastern United States, with average January low temperatures around in Mobile and around in Birmingham. Although snow is a rare event in much of Alabama, areas of the state north of Montgomery may receive a dusting of snow a few times every winter, with an occasional moderately heavy snowfall every few years. For example, the annual average snowfall for the Birmingham area is 2 inches per year. In the southern Gulf coast, snowfall is less frequent, sometimes going several years without any snowfall.
Demographics.
The United States Census Bureau, as of July 1, 2008, estimated Alabama's population at 4,661,900, which represents an increase of 214,545, or 4.8%, since the last census in 2000. This includes a natural increase since the last census of 121,054 people (that is 502,457 births minus 381,403 deaths) and an increase due to net migration of 104,991 people into the state. Immigration from outside the United States resulted in a net increase of 31,180 people, and migration within the country produced a net gain of 73,811 people. The state had 108,000 foreign-born (2.4% of the state population), of which an estimated 22.2% were illegal immigrants (24,000).
The center of population of Alabama is located in Chilton County, outside of the town of Jemison, an area known as Jemison Division.
Race and ancestry.
The largest reported ancestry groups in Alabama: African American (26.0%), American (17.0%), English (7.8%), Irish (7.7%), German (5.7%), and Scots-Irish (2.0%). 'American' does not include those reported as Native American.
Religion.
Alabama is located in the middle of the Bible Belt. In a 2007 survey, nearly 70% of respondents could name all four of the Christian Gospels. Of those who indicated a religious preference, 59% said they possessed a "full understanding" of their faith and needed no further learning. In a 2007 poll, 92% of Alabamians reported having at least some confidence in churches in the state. The Mobile area is notable for its large percentage of Catholics, owing to the area's unique early history under French and Spanish rule. Today, a majority of Alabamians identify themselves as Protestants.
In the 2008 American Religious Identification Survey, 80% of Alabama respondents reported their religion as "Other Christian" (survey's label), 6% as Catholic, and 11% as having no religion at all.
Economy.
According to the United States Bureau of Economic Analysis, the 2008 total gross state product was $170 billion, or $29,411 per capita. Alabama's 2008 GDP increased 0.7% from the previous year. The single largest increase came in the area of information. In 1999, per capita income for the state was $18,189.
Alabama's agricultural outputs include poultry and eggs, cattle, plant nursery items, peanuts, cotton, grains such as corn and sorghum, vegetables, milk, soybeans, and peaches. Although known as "The Cotton State", Alabama ranks between eight and ten in national cotton production, according to various reports, with Texas, Georgia and Mississippi comprising the top three.
Alabama's industrial outputs include iron and steel products (including cast-iron and steel pipe); paper, lumber, and wood products; mining (mostly coal); plastic products; cars and trucks; and apparel. Also, Alabama produces aerospace and electronic products, mostly in the Huntsville area, location of NASA George C. Marshall Space Flight Center and the US Army Aviation and Missile Command, headquartered at Redstone Arsenal.
Alabama contains the largest industrial growth corridor in the nation, including the surrounding states of Tennessee, Mississippi, Florida, and Georgia. Most of this growth is due to Alabama's rapidly expanding automotive manufacturing industry. Headquartered in the state are Honda Manufacturing of Alabama, Hyundai Motor Manufacturing Alabama, Mercedes-Benz U.S. International, and Toyota Motor Manufacturing Alabama. Since 1993, the automobile industry has generated more than 67,800 new jobs in the state. Alabama currently ranks 4th in the nation in automobile output.
In the 1970s and 1980s, Birmingham's economy was transformed by investments in bio-technology and medical research at the University of Alabama at Birmingham (UAB) and its adjacent hospital. The UAB Hospital is a Level I trauma center providing health care and breakthrough medical research. UAB is now the area's largest employer and the largest in Alabama with a workforce of about 20,000. Health care services provider HealthSouth is also headquartered in the city.
Birmingham is also a leading banking center, headquarters of the Regions Financial Corporation. Birmingham-based Compass Banchshares was acquired by Madrid-based BBVA in September 2007; the headquarters of the new BBVA Compass Bank remains in Birmingham. SouthTrust, another large bank headquartered in Birmingham, was acquired by Wachovia in 2004. The city still has major operations as one of the regional headquarters of Wachovia. In November 2006, Regions Financial merged with AmSouth Bancorporation, which was also headquartered in Birmingham. They formed the eighth largest U.S. bank based on by total assets. Nearly a dozen smaller banks are also headquartered in the Magic City, such as Superior Bank and New South Federal Savings Bank.
Telecommunications provider AT&T, formerly BellSouth, has a major presence with several large offices in the metropolitan area. Major insurance providers: Protective Life, Infinity Property & Casualty and ProAssurance among others, are headquartered in Birmingham and employ a large number of people in Greater Birmingham. The city is also a powerhouse of construction and engineering companies, including BE&K and B. L. Harbert International which routinely are included in the Engineering News-Record lists of top design and international construction firms.
Huntsville is regarded for its high-technology driven economy and is known as the "Rocket City" because of NASA's Marshall Space Flight Center and the Redstone Arsenal. Huntsville's main economic influence is derived from aerospace and military technology. Redstone Arsenal, Cummings Research Park (CRP), The University of Alabama in Huntsville and NASA's Marshall Space Flight Center comprise the main hubs for the area's technology-driven economy. CRP is the second largest research park in the United States and the fourth largest in the world, and is over 38 years old. Huntsville has commercial technology companies such as the network access company ADTRAN, computer graphics company Intergraph and design and manufacturer of IT infrastructure Avocent. Telecommunications provider Deltacom, Inc. and copper tube manufacturer and distributor Wolverine Tube are also based in Huntsville. Cinram manufactures and distributes 20th Century Fox DVDs and Blu-ray Discs out of their Huntsville plant. Sanmina-SCI also has a large presence in the area. Forty-two Fortune 500 companies have operations in Huntsville. In 2005, Forbes Magazine named the Huntsville-Decatur Combined Statistical Area as 6th best place in the nation for doing business, and number one in terms of the number of engineers per total employment.
The city of Mobile, Alabama's only saltwater port, is a busy seaport on the Gulf of Mexico with inland waterway access to the Midwest by way of the Tennessee-Tombigbee Waterway. The Port of Mobile is currently the 9th largest by tonnage in the United States. In May 2007, a site north of Mobile was selected by German steelmaker ThyssenKrupp for a $3.7 billion steel production plant, with the promise of 2,700 permanent jobs.
Taxes.
Alabama's tax structure is one the most regressive in the United States. Alabama levies a 2, 4, or 5 percent personal income tax, depending upon the amount earned and filing status, though taxpayers can deduct their federal income tax from their Alabama state tax.
The state's general sales tax rate is 4%. The collection rate could be substantially higher, depending upon additional city and county sales taxes. For example, the total sales tax rate in Mobile is 9% and there is an additional restaurant tax of 1%, which means that a diner in Mobile would pay a 10% tax on a meal. Sales and excise taxes in Alabama account for 51 percent of all state and local revenue, compared with an average of about 36 percent nationwide. Alabama is also one of the few remaining states that levies a tax on food and medicine. Alabama's income tax on poor working families is among the nation's very highest. Alabama is the only state that levies income tax on a family of four with income as low as $4,600, which is barely one-quarter of the federal poverty line. Alabama's threshold is the lowest among the 41 states and the District of Columbia with income taxes.
The corporate income tax rate is currently 6.5%. The overall federal, state, and local tax burden in Alabama ranks the state as the second least tax-burdened state in the country. Property taxes are the lowest in the United States. The current state constitution requires a voter referendum to raise property taxes.
Since Alabama's tax structure largely depends on consumer spending, it is subject to high variable budget structure. For example, in 2003 Alabama had an annual budget deficit as high as $670 million. It is one of only a few states to accomplish large surpluses, with a budget surplus of nearly $1.2 billion in 2007, and estimated at more than $2.1 billion for 2008. However, the declining national economy in 2008 has eliminated that surplus and the state is again facing shortfall, with the governor declaring "proration," which will result in an immediate education budget cut and school layoffs.
Transportation.
Alabama has five major interstate roads that cross it: I-65 runs north–south roughly through the middle of the state; I-59/I-20 travels from the central west border to Birmingham, where I-59 continues to the north-east corner of the state and I-20 continues east towards Atlanta; I-85 originates in Montgomery and runs east-northeast to the Georgia border, providing a main thoroughfare to Atlanta; and I-10 traverses the southernmost portion of the state, running from west to east through Mobile. Another interstate road, I-22, is currently under construction. When completed around 2012 it will connect Birmingham with Memphis, Tennessee. Several US Highways also pass through the state, such as US 11, US 29, US 31, US 43, US 72, US 78, US 80, US 82, US 84, US 98, US 231, and US 280.
Major airports in Alabama include Birmingham-Shuttlesworth International Airport (BHM), Huntsville International Airport (HSV), Dothan Regional Airport (DHN), Mobile Regional Airport (MOB), Montgomery Regional Airport (MGM), Muscle Shoals – Northwest Alabama Regional Airport (MSL), Tuscaloosa Regional Airport (TCL), and Pryor Field Regional Airport (DCU). For rail transport, Amtrak schedules the Crescent, a daily passenger train, running from New York to New Orleans with stops at Anniston, Birmingham, and Tuscaloosa.
State government.
The foundational document for Alabama's government is the Alabama Constitution, which was ratified in 1901. At almost 800 amendments and 310,000 words, it is the world's longest constitution and is roughly forty times the length of the U.S. Constitution. There is a significant movement to rewrite and modernize Alabama's constitution. This movement is based upon the fact that Alabama's constitution highly centralizes power in Montgomery and leaves practically no power in local hands. Any policy changes proposed around the state must be approved by the entire Alabama legislature and, frequently, by state referendum. One criticism of the current constitution claims that its complexity and length were intentional to codify segregation and racism.
The legislative branch is the Alabama Legislature, a bicameral assembly composed of the Alabama House of Representatives, with 105 members, and the Alabama Senate, with 35 members. The Legislature is responsible for writing, debating, passing, or defeating state legislation.
The executive branch is responsible for the execution and oversight of laws. It is headed by the Governor of Alabama. Other members of executive branch include the cabinet, the Attorney General of Alabama, the Alabama Secretary of State, the Alabama Commissioner of Agriculture and Industries, the Alabama State Treasurer, and the Alabama State Auditor.
The judicial branch is responsible for interpreting the Constitution and applying the law in state criminal and civil cases. The highest court is the Supreme Court of Alabama.
Local and county government.
Alabama has 67 counties. Each county has its own elected legislative branch, usually called the County Commission, which usually also has executive authority in the county. Because of the restraints placed in the Alabama Constitution, all but seven counties (Jefferson, Lee, Mobile, Madison, Montgomery, Shelby, and Tuscaloosa) in the state have little to no home rule. Instead, most counties in the state must lobby the Local Legislation Committee of the state legislature to get simple local policies such as waste disposal to land use zoning.
Alabama is an alcoholic beverage control state; the government holds a monopoly on the sale of alcohol. However, counties can declare themselves "dry"; the state does not sell alcohol in those areas.
State politics.
The current governor of the state is Republican Bob Riley. The lieutenant governor is Jim Folsom Jr. The Chief Justice of the Alabama Supreme Court is Democrat Sue Bell Cobb. The Democratic Party currently holds a large majority in both houses of the Legislature. Because of the Legislature's power to override a gubernatorial veto by a mere simple majority (most state Legislatures require a two-thirds majority to override a veto), the relationship between the executive and legislative branches can be easily strained when different parties control the branches.
During Reconstruction following the American Civil War, Alabama was occupied by federal troops of the Third Military District under General John Pope. In 1874, the political coalition known as the Redeemers took control of the state government from the Republicans, in part by suppressing the African American vote.
After 1890, a coalition of whites passed laws to segregate and disenfranchise black residents, a process completed in provisions of the 1901 constitution. Provisions which disfranchised African Americans also disfranchised poor whites, however. By 1941 more whites than blacks had been disfranchised: 600,000 to 520,000, although the impact was greater on the African-American community, as almost all of its citizens were disfranchised.
From 1901 to the 1960s, the state legislature failed to perform redistricting as population grew and shifted within the state. The result was a rural minority that dominated state politics until a series of court cases required redistricting in 1972.
With the disfranchisement of African Americans, the state became part of the "Solid South", a one-party system in which the Democratic Party became essentially the only political party in every Southern state. For nearly 100 years, local and state elections in Alabama were decided in the Democratic Party primary, with generally only token Republican challengers running in the General Election.
In the 1986 Democratic primary election, the then-incumbent Lieutenant Governor, Bill Baxley, lost the Democratic nomination for Governor in a scandal where Republicans were permitted to cast votes for his opponent, then Attorney General Charlie Graddick. The state Democratic party invalidated the election and placed the Baxley's name on the ballot as the Democratic candidate instead of the candidate chosen in the primary. The voters of the state revolted at what they perceived as disenfranchisement of their right to vote and elected the Republican challenger Guy Hunt as Governor. This was the first Republican Governor elected in Alabama since Reconstruction. Since then, Republicans have become increasingly competitive in Alabama politics. They currently control both seats in the U.S. Senate, four out of the state's seven congressional seats. Republicans hold an 8–1 majority on the Alabama Supreme Court and have a 5–2 majority among statewide elected executive branch offices.
However, Democrats currently hold all three seats on the Alabama Public Service Commission and they maintain control of both houses of the legislature, holding approximately 59.4% of seats in the Alabama Senate and 58.7% of seats in the Alabama House of Representatives. A majority of local offices in the state are still held by Democrats. Local elections in rural counties are generally decided in the Democratic primary and local elections in metropolitan counties are decided in the Republican Primary although there are exceptions to this rule. Only one Republican Lt. Governor has been elected since Reconstruction, Steve Windom. Windom served as Lt. Governor under Democratic Gov. Don Siegelman. The last time that Alabama had a governor and Lt. governor of the same party was the period between 1983 and 1987 when Wallace was serving his fourth term as governor and Bill Baxley was serving as Lt. Governor, both were Democrats.
An overwhelming majority of sheriff's offices in Alabama are in Democratic hands. However, most of the Democratic sheriffs preside over more rural and less populated counties and the majority of Republicans preside over more urban/suburban and more populated counties. Only three Alabama counties (Tuscaloosa, Montgomery and Calhoun) with a population of over 100,000 have Democratic sheriffs and only five Alabama counties with a population of under 75,000 have Republican sheriffs (Autauga, Coffee, Dale, Coosa, and Blount).
Alabama state politics gained nationwide and international attention in the 1950s and 1960s during the American Civil Rights Movement, when majority whites bureaucratically, and at times, violently resisted protests for electoral and social reform. George Wallace, the state's governor, remains a notorious and controversial figure. Only with the passage of the Civil Rights Act of 1964 and Voting Rights Act of 1965 did African Americans regain suffrage and other civil rights.
In 2007, the Alabama Legislature passed, and the Governor signed, a resolution expressing "profound regret" over slavery and its lingering impact. In a symbolic ceremony, the bill was signed in the Alabama State Capitol, which housed Congress of the Confederate States of America.
National politics.
From 1876 through 1956, Alabama supported only Democratic presidential candidates, by large margins. In 1960, the Democrats won with John F. Kennedy on the ballot, but the Democratic electors from Alabama gave 6 of their 11 electoral votes as a protest to Harry Byrd. In 1964, Republican Barry Goldwater carried the state, in part because of his opposition to the 1964 Civil Rights Act, which restored the franchise for African Americans.
In the 1968 presidential election, Alabama supported native son and American Independent Party candidate George Wallace over both Richard Nixon and Hubert Humphrey. Wallace was the official Democratic candidate in Alabama, while Humphrey was listed as the "National Democratic". In 1976, Democratic candidate Jimmy Carter from Georgia carried the state, the region, and the nation, but Democratic control of the region slipped after that.
Since 1980, conservative Alabama voters have increasingly voted for Republican candidates at the Federal level, especially in Presidential elections. By contrast, Democratic candidates have been elected to many state-level offices and comprise a longstanding majority in the Alabama Legislature; see Dixiecrat.
In 2004, George W. Bush won Alabama's nine electoral votes by a margin of 25 percentage points with 62.5% of the vote, mostly white voters. The eleven counties that voted Democratic were Black Belt counties, where African Americans are the majority racial group.
The state's two U.S. senators are Jefferson B. Sessions III and Richard C. Shelby, both Republicans.
In the U.S. House of Representatives, the state is represented by seven members, five of whom are Republicans: (Jo Bonner, Mike D. Rogers, Robert Aderholt, Parker Griffith, and Spencer Bachus) and two are Democrats: (Bobby Bright and Artur Davis).
Primary and secondary education.
Public primary and secondary education in Alabama is under the overview of the Alabama State Board of Education as well as local oversight by 67 county school boards and 60 city boards of education. Together, 1,541 individual schools provide education for 743,364 elementary and secondary students.
Public school funding is appropriated through the Alabama Legislature through the Education Trust Fund. In FY 2006–2007, Alabama appropriated $3,775,163,578 for primary and secondary education. That represented an increase of $444,736,387 over the previous fiscal year. In 2007, over 82 percent of schools made adequate yearly progress (AYP) toward student proficiency under the National No Child Left Behind law, using measures determined by the state of Alabama. In 2004, 23 percent of schools met AYP.
While Alabama's public education system has improved, it lags behind in achievement compared to other states. According to U.S. Census data, Alabama's high school graduation rate – 75% – is the second lowest in the United States (after Mississippi). The largest educational gains were among people with some college education but without degrees.
Colleges and universities.
Alabama's programs of higher education include 14 four-year public universities, two-year community colleges, and 17 private, undergraduate and graduate universities. In the state are two medical schools (University of Alabama at Birmingham and University of South Alabama), two veterinary colleges (Auburn University and Tuskegee University), a dental school (University of Alabama at Birmingham), an optometry college (University of Alabama at Birmingham), two pharmacy schools (Auburn University and Samford University), and five law schools (University of Alabama School of Law, Birmingham School of Law, Cumberland School of Law, Miles Law School, and the Thomas Goode Jones School of Law). Public, post-secondary education in Alabama is overseen by the Alabama Commission on Higher Education. Colleges and universities in Alabama offer degree programs from two-year associate degrees to 16 doctoral level programs.
Accreditation of academic programs is through the Southern Association of Schools and Colleges as well as a plethora of subject focused national and international accreditation agencies.
Professional sports teams.
Alabama has several minor league teams including four Southern League baseball teams as well as one Arena Football League team.
Notable Alabamians.
Famous people from Alabama include Hank Aaron, Tommie Agee, Tallulah Bankhead, William Brockman Bankhead, Jay Barker, Charles Barkley, Regina Benjamin, Hugo L. Black, Frank Bolling, Paul W. (Bear) Bryant, Jimmy Buffett, Bo Bice, George Washington Carver, William Christenberry, Nat King Cole, Jerricho Cotchery, Courteney Cox Arquette, Robert Gibbs, Mitch Holleman, Zelda Fitzgerald, Charles Ghigna, Winston Groom, William C. Handy, Emmylou Harris, Taylor Hicks, Joe Hilley, Bo Jackson, Kate Jackson, Jamey Johnson, Helen Keller, Coretta Scott King, William R. King, Harper Lee, Joe Louis, Heinie Manush, William March, Willie Mays, Willie McCovey, Roy Moore, John Hunt Morgan, Jim Nabors, Randy Owen, Jesse Owens, Terrell Owens, Satchel Paige, Jake Peavy, Claude Pepper, Rosa Parks, Wilson Pickett, Howell Raines, Condoleezza Rice, Lionel Richie, Rich Boy, Philip Rivers, JaMarcus Russell, Kenny Stabler, Ozzie Smith, John Sparkman, Bart Starr, Ruben Studdard, Channing Tatum, Oscar W. Underwood, Jimmy Wales, George Wallace, Booker T. Washington, Billy Williams, and Hank Williams.
---END.OF.DOCUMENT---

Achilles.
In Greek mythology, Achilles (Ancient Greek:) was a Greek hero of the Trojan War, the central character and the greatest warrior of Homer's "Iliad".
Achilles also has the attributes of being the most handsome of the heroes assembled against Troy.
Later legends (beginning with a poem by Statius in the first century AD) state that Achilles was invulnerable in all of his body except for his heel. Since he died due to an arrow shot into his heel, the "Achilles' heel" has come to mean a person's principal weakness.
Birth.
Achilles was the son of the nymph Thetis and Peleus, the king of the Myrmidons. Zeus and Poseidon had been rivals for the hand of Thetis until Prometheus, the fire-bringer, warned Zeus of a prophecy that Thetis would bear a son greater than his father. For this reason, the two gods withdrew their pursuit, and had her wed Peleus.
As with most mythology there is a tale which offers an alternative version of these events: in "Argonautica" (iv.760) Hera alludes to Thetis's chaste resistance to the advances of Zeus, that Thetis was so loyal to Hera's marriage bond that she coolly rejected him. Thetis, although a daughter of the sea-god Nereus, was also brought up by Hera, further explaining her resistance to the advances of Zeus.
According to the "Achilleid", written by Statius in the first century AD, and to no surviving previous sources, when Achilles was born Thetis tried to make him immortal by dipping him in the river Styx. However, he was left vulnerable at the part of the body she held him by, his heel. (See Achilles heel, Achilles' tendon.) It is not clear if this version of events was known earlier. In another version of this story, Thetis anointed the boy in ambrosia and put him on top of a fire to burn away the mortal parts of his body. She was interrupted by Peleus and abandoned both father and son in a rage.
However none of the sources before Statius makes any reference to this general invulnerability. To the contrary, in the "Iliad" Homer mentions Achilles being wounded: in Book 21 the Paeonian hero Asteropaeus, son of Pelagon, challenged Achilles by the river Scamander. He cast two spears at once, one grazed Achilles' elbow, "drawing a spurt of blood."
Also in the fragmentary poems of the Epic Cycle in which we can find description of the hero's death, Kúpria (unknown author), "Aithiopis" by Arctinus of Miletus, "Ilias Mikrá" by Lesche of Mytilene, Iliou pérsis by Arctinus of Miletus, there is no trace of any reference to his general invulnerability or his famous weakness (heel); in the later vase-paintings presenting Achilles' death, the arrow (or in many cases, arrows) hit his body.
Peleus entrusted Achilles to Chiron the Centaur, on Mt. Pelion, to be raised.
Achilles in the Trojan War.
Achilles' consuming rage is at some times wavering, but at other times he cannot be cooled. The humanization of Achilles by the events of the war is an important theme of the narrative.
Telephus.
When the Greeks left for the Trojan War, they accidentally stopped in Mysia, ruled by King Telephus. In the resulting battle, Achilles gave Telephus a wound that would not heal; Telephus consulted an oracle, who stated that "he that wounded shall heal". Guided by the oracle, he arrived at Argos, where Achilles heals him in order that he become their guide for the voyage to Troy.
According to other reports in Euripides' lost play about Telephus, he went to Aulis pretending to be a beggar and asked Achilles to heal his wound. Achilles refused, claiming to have no medical knowledge. Alternatively, Telephus held Orestes for ransom, the ransom being Achilles' aid in healing the wound. Odysseus reasoned that the spear had inflicted the wound; therefore, the spear must be able to heal it. Pieces of the spear were scraped off onto the wound and Telephus was healed.
Troilus.
According to the Cypria (the part of the Epic Cycle that tells the events of the Trojan War before Achilles' Wrath), when the Achaeans desired to return home, they were restrained by Achilles, who afterwards attacked the cattle of Aeneas, sacked neighboring cities and killed Troilus.
According to Dares Phrygius' "Account of the Destruction of Troy", the Latin summary through which the story of Achilles was transmitted to medieval Europe, Troilus was a young Trojan prince, the youngest of King Priam's (or sometimes Apollo) and Hecuba's five legitimate sons. Despite his youth, he was one of the main Trojan war leaders. Prophecies linked Troilus' fate to that of Troy and so he was ambushed in an attempt to capture him. Yet Achilles, struck by the beauty of both Troilus and his sister Polyxena, and overcome with lust directed his sexual attentions on the youth — who refusing to yield found instead himself decapitated upon an altar-omphalos of Apollo. Later versions of the story suggested Troilus was accidentally killed by Achilles in an over-ardent lovers' embrace. In this version of the myth, Achilles' death therefore came in retribution for this sacrilege. Ancient writers treated Troilus as the epitome of a dead child mourned by his parents. Had Troilus lived to adulthood, the First Vatican Mythographer claimed Troy would have been invincible.
In the "Iliad".
Homer's "Iliad" is the most famous narrative of Achilles' deeds in the Trojan War. The Homeric epic only covers a few weeks of the war, and does not narrate Achilles' death. It begins with Achilles' withdrawal from battle after he is dishonored by Agamemnon, the commander of the Achaean forces. Agamemnon had taken a woman named Chryseis as his slave. Her father Chryses, a priest of Apollo, begged Agamemnon to return her to him. Agamemnon refused and Apollo sent a plague amongst the Greeks. The prophet Calchas correctly determined the source of the troubles but would not speak unless Achilles vowed to protect him. Achilles did so and Calchas declared Chryseis must be returned to her father. Agamemnon consented, but then commanded that Achilles' battle prize Briseis be brought to replace Chryseis. Angry at the dishonor (and as he says later, because he loved Briseis) and at the urging of Thetis, Achilles refused to fight or lead his troops alongside the other Greek forces.
As the battle turned against the Greeks, Nestor declared that the Trojans were winning because Agamemnon had angered Achilles, and urged the king to appease the warrior. Agamemnon agreed and sent Odysseus and two other chieftains, Ajax and Phoenix, to Achilles with the offer of the return of Briseis and other gifts. Achilles rejected all Agamemnon offered him, and simply urged the Greeks to sail home as he was planning to do.
Eventually, however, hoping to retain glory despite his absence from the battle, Achilles prayed to his mother Thetis, asking her to plead with Zeus to allow the Trojans to push back the Greek forces.
The Trojans, led by Hector, subsequently pushed the Greek army back toward the beaches and assaulted the Greek ships. With the Greek forces on the verge of absolute destruction, Patroclus led the Myrmidons into battle, though Achilles remained at his camp. Patroclus succeeded in pushing the Trojans back from the beaches, but was killed by Hector before he could lead a proper assault on the city of Troy.
After receiving the news of the death of Patroclus from Antilochus, the son of Nestor, Achilles grieved over his close friend's death and held many funeral games in his honor. His mother Thetis came to comfort the distraught Achilles. She persuaded Hephaestus to make new armor for him, in place of the armor that Patroclus had been wearing which was taken by Hector. The new armor included the Shield of Achilles, described in great detail by the poet.
Enraged over the death of Patroclus, Achilles ended his refusal to fight and took the field killing many men in his rage but always seeking out Hector. Achilles even engaged in battle with the river god Scamander who became angry that Achilles was choking his waters with all the men he killed. The god tried to drown Achilles but was stopped by Hera and Hephaestus. Zeus himself took note of Achilles' rage and sent the gods to restrain him so that he would not go on to sack Troy itself, seeming to show that the unhindered rage of Achilles could defy fate itself as Troy was not meant to be destroyed yet. Finally Achilles found his prey. Achilles chased Hector around the wall of Troy three times before Athena, in the form of Hector's favorite and dearest brother, Deiphobus, persuaded Hector to stop running and fight Achilles face to face. After Hector realized the trick, he knew the battle was inevitable. Wanting to go down fighting, he charged at Achilles with his only weapon, his sword, but missed. Accepting his fate, Hector begged Achilles – not to spare his life, but to treat his body with respect after killing him. Achilles told Hector it was hopeless to expect that of him, declaring that "my rage, my fury would drive me now to hack your flesh away and eat you raw — such agonies you have caused me". Achilles then got his vengeance, killing Hector with a single blow to the neck and tying the Trojan's body to his chariot, dragging it around the battlefield for nine days.
With the assistance of the god Hermes, Hector's father, Priam, went to Achilles' tent to plead with Achilles to permit him to perform for Hector his funeral rites. The final passage in the "Iliad" is Hector's funeral, after which the doom of Troy was just a matter of time.
Penthesilea.
Achilles, after his temporary truce with Priam, fought and killed the Amazonian warrior queen Penthesilea, but later grieved over her death. At first, he was so distracted by her beauty, he did not fight as intensely as usual. Once he realized that his distraction was endangering his life, he refocused, and killed her. As he grieved over the death of such a rare beauty, a notorious Greek jeerer by the name of Thersites laughed and mocked the great Achilles. Annoyed by his insensitivity and disrespect, Achilles punched him in the face and killed him instantly.
Memnon, and the fall of Achilles.
Following the death of Patroclus, Achilles' closest companion was Nestor's son Antilochus. When Memnon, king of Ethiopia killed Antilochus, Achilles was once again drawn onto the battlefield to seek revenge. The fight between Achilles and Memnon over Antilochus echoes that of Achilles and Hector over Patroclus, except that Memnon (unlike Hector) was also the son of a goddess.
Many Homeric scholars argued that episode inspired many details in the "Iliads description of the death of Patroclus and Achilles' reaction to it. The episode then formed the basis of the cyclic epic "Aethiopis", which was composed after the "Iliad", possibly in the 7th century B.C. The "Aethiopis" is now lost, except for scattered fragments quoted by later authors.
As predicted by Hector with his dying breath, Achilles was thereafter killed by Paris with an arrow (to the heel according to Statius). In some versions, the god Apollo guided Paris' arrow. Some retellings also state that Achilles was scaling the gates of Troy and was hit with a poisoned arrow.
Both versions conspicuously deny the killer any sort of valor owing to the common conception that Paris was a coward and not the man his brother Hector was, and Achilles remained undefeated on the battlefield. His bones were mingled with those of Patroclus, and funeral games were held. He was represented in the lost Trojan War epic of Arctinus of Miletus as living after his death in the island of Leuke at the mouth of the river Danube (see below). Another version of Achilles' death is that he fell deeply in love with one of the Trojan princesses, Polyxena, Achilles asks Priam for Polyxena's hand in marriage. Priam is willing because it would mean the end of the war and an alliance with the world's greatest warrior. But while Priam is overseeing the private marriage of Polyxena and Achilles, Paris who would have to give up Helen if Achilles married his sister hides in the bushes and shoots Achilles with a divine arrow killing him.
Achilles was cremated and his ashes buried in the same urn as those of Patroclus.
Paris was later killed by Philoctetes using the enormous bow of Heracles.
Fate of Achilles' armor.
Achilles' armor was the object of a feud between Odysseus and Telamonian Ajax (Ajax the greater). They competed for it by giving speeches on why they were the bravest after Achilles to their Trojan prisoners, who after considering both men came to a consensus in favor of Odysseus. Furious, Ajax cursed Odysseus, which earned the ire of Athena. Athena temporarily made Ajax so mad with grief and anguish that he began killing sheep, thinking they were his comrades. After a while, when Athena lifted his madness and Ajax realized that he had actually been killing sheep, he was so embarrassed that he committed suicide. Odysseus eventually gave the armor to Neoptolemus, the son of Achilles.
A relic claimed to be Achilles' bronze-headed spear was for centuries preserved in the temple of Athena on the acropolis of Phaselis, Lycia, a port on the Pamphylian Gulf. The city was visited in 333 BC by Alexander the Great, who envisioned himself as the new Achilles and carried the "Iliad" with him, but his court biographers do not mention the spear, which he would indeed have touched with excitement. But it was being shown in the time of Pausanias in the second century AD.
Achilles and Patroclus.
Achilles' relationship with Patroclus is a key aspect of his myth. Its exact nature has been a subject of dispute in both the classical period and modern times. In the "Iliad", they appeared to be generally portrayed as a model of deep and loyal friendship. However, commentators from the classical period to today have tended to interpret the relationship through the lens of their own cultures. Thus, in 5th century BC Athens the relationship was commonly interpreted as pederastic. Contemporary readers may interpret the two heroes either as relatives or close friends, as "war buddies," as being in a teacher/student relationship, or in love with each other as an egalitarian homosexual couple. Whichever the case may be, Achilles nevertheless continued to have sexual relationships with women.
The cult of Achilles in antiquity.
There was an archaic heroic cult of Achilles on the White Island, "Leuce", in the Black Sea off the modern coasts of Romania and Ukraine, with a temple and an oracle which survived into the Roman period.
In the lost epic "Aithiopis", a continuation of the "Iliad" attributed to Arktinus of Miletos, Achilles’ mother Thetis returned to mourn him and removed his ashes from the pyre and took them to Leuce at the mouths of the Danube. There the Achaeans raised a tumulus for him and celebrated funeral games.
Pliny's Natural History (IV.27.1) mentions a tumulus that is no longer evident ("Insula Akchillis tumulo eius viri clara"), on the island consecrated to him, located at a distance of fifty Roman miles from Peuce by the Danube Delta, and the temple there. Pausanias has been told that the island is "covered with forests and full of animals, some wild, some tame. In this island there is also Achilles’ temple and his statue” (III.19.11). Ruins of a square temple 30 meters to a side, possibly that dedicated to Achilles, were discovered by Captain Kritzikly in 1823, but there has been no modern archeological work done on the island.
Pomponius Mela tells that Achilles is buried in the island named Achillea, between Boristhene and Ister ("De situ orbis", II, 7). And the Greek geographer Dionysius Periegetus of Bithynia, who lived at the time of Domitian, writes that the island was called "Leuce" "because the wild animals which live there are white. It is said that there, in Leuce island, reside the souls of Achilles and other heroes, and that they wander through the uninhabited valleys of this island; this is how Jove rewarded the men who had distinguished themselves through their virtues, because through virtue they had acquired everlasting honor” ("Orbis descriptio", v. 541, quoted in Densuşianu 1913).
The "Periplus of the Euxine Sea" gives the following details: "It is said that the goddess Thetis raised this island from the sea, for her son Achilles, who dwells there. Here is his temple and his statue, an archaic work. This island is not inhabited, and goats graze on it, not many, which the people who happen to arrive here with their ships, sacrifice to Achilles. In this temple are also deposited a great many holy gifts, craters, rings and precious stones, offered to Achilles in gratitude. One can still read inscriptions in Greek and Latin, in which Achilles is praised and celebrated. Some of these are worded in Patroclus’ honor, because those who wish to be favored by Achilles, honor Patroclus at the same time. There are also in this island countless numbers of sea birds, which look after Achilles’ temple. Every morning they fly out to sea, wet their wings with water, and return quickly to the temple and sprinkle it. And after they finish the sprinkling, they clean the hearth of the temple with their wings. Other people say still more, that some of the men who reach this island, come here intentionally. They bring animals in their ships, destined to be sacrificed. Some of these animals they slaughter, others they set free on the island, in Achilles’ honor. But there are others, who are forced to come to this island by sea storms. As they have no sacrificial animals, but wish to get them from the god of the island himself, they consult Achilles’ oracle. They ask permission to slaughter the victims chosen from among the animals that graze freely on the island, and to deposit in exchange the price which they consider fair. But in case the oracle denies them permission, because there is an oracle here, they add something to the price offered, and if the oracle refuses again, they add something more, until at last, the oracle agrees that the price is sufficient. And then the victim doesn’t run away any more, but waits willingly to be caught. So, there is a great quantity of silver there, consecrated to the hero, as price for the sacrificial victims. To some of the people who come to this island, Achilles appears in dreams, to others he would appear even during their navigation, if they were not too far away, and would instruct them as to which part of the island they would better anchor their ships”. (quoted in Densuşianu)
The heroic cult of Achilles on Leuce island was widespread in antiquity, not only along the sea lanes of the Pontic Sea but also in maritime cities whose economic interests were tightly connected to the riches of the Black Sea.
Achilles from Leuce island was venerated as "Pontarches" the lord and master of the Pontic (Black) Sea, the protector of sailors and navigation. Sailors went out of their way to offer sacrifice. To Achilles of Leuce were dedicated a number of important commercial port cities of the Greek waters: Achilleion in Messenia (Stephanus Byzantinus), Achilleios in Laconia (Pausanias, III.25,4) Nicolae Densuşianu (Densuşianu 1913) even thought he recognized Achilles in the name of Aquileia and in the north arm of the Danube delta, the arm of Chilia ("Achileii"), though his conclusion, that Leuce had sovereign rights over Pontos, evokes modern rather than archaic sea-law."
Leuce had also a reputation as a place of healing. Pausanias (III.19,13) reports that the Delphic Pythia sent a lord of Croton to be cured of a chest wound. Ammianus Marcellinus (XXII.8) attributes the healing to waters ("aquae") on the island.
The cult of Achilles in modern times: The Achilleion in Corfu.
In the region of Gastouri (Γαστούρι) to the south of the city of Corfu Greece, Empress of Austria Elisabeth of Bavaria also known as Sissi built in 1890 a summer palace with Achilles as its central theme and it is a monument to platonic romanticism. The palace, naturally, was named after Achilles: "Achilleion" (Αχίλλειον). This elegant structure abounds with paintings and statues of Achilles both in the main hall and in the lavish gardens depicting the heroic and tragic scenes of the Trojan war.
The name of Achilles.
Achilles' name can be analyzed as a combination of ("akhos") "grief" and ("Laos") "a people, tribe, nation, etc." In other words, Achilles is an embodiment of the grief of the people, grief being a theme raised numerous times in the "Iliad" (frequently by Achilles). Achilles' role as the hero of grief forms an ironic juxtaposition with the conventional view of Achilles as the hero of "kleos" (glory, usually glory in war).
"Laos" has been construed by Gregory Nagy, following Leonard Palmer, to mean "a corps of soldiers", a muster. With this derivation, the name would have a double meaning in the poem: When the hero is functioning rightly, his men bring grief to the enemy, but when wrongly, his men get the grief of war. The poem is in part about the misdirection of anger on the part of leadership.
The name Achilleus was a common and attested name among the Greeks early after 7th century BC. It was also turned into the female form of Ἀχιλλεία, "Achilleía", firstly attested in Attica,4th century BC, (IG II² 1617) and Achillia, as the name of a female gladiator fighting, 'Amazonia'. Roman gladiatorial games often referenced classical mythology and this seems to reference Achilles' fight with Penthesilea, but give it an extra twist of Achilles being 'played' by a woman.
Other stories about Achilles.
Some post-Homeric sources claim that in order to keep Achilles safe from the war, Thetis (or, in some versions, Peleus) hides the young man at the court of Lycomedes, king of Skyros. There, Achilles is disguised as a girl and lives among Lycomedes' daughters, perhaps under the name "Pyrrha" (the red-haired girl). With Lycomedes' daughter Deidamia, whom in the account of Statius he rapes, Achilles there fathers a son, Neoptolemus (also called Pyrrhus, after his father's possible alias). According to this story, Odysseus learns from the prophet Calchas that the Achaeans would be unable to capture Troy without Achilles' aid. Odysseus goes to Skyros in the guise of a peddler selling women's clothes and jewelry and places a shield and spear among his goods. When Achilles instantly takes up the spear, Odysseus sees through his disguise and convinces him to join the Greek campaign. In another version of the story, Odysseus arranges for a trumpet alarm to be sounded while he was with Lycomedes' women; while the women flee in panic, Achilles prepares to defend the court, thus giving his identity away.
In book 11 of Homer's "Odyssey," Odysseus sails to the underworld and converses with the shades. One of these is Achilles, who when greeted as "blessed in life, blessed in death", responds that he would rather be a slave to the worst of masters than be king of all the dead. But Achilles then asks Odysseus of his son's exploits in the Trojan war, and when Odysseus tells of Neoptolemus' heroic actions, Achilles is filled with satisfaction. This leaves the reader with an ambiguous understanding of how Achilles felt about the heroic life. Achilles was worshipped as a sea-god in many of the Greek colonies on the Black Sea, the location of the mythical "White Island" which he was said to inhabit after his death, together with many other heroes.
The kings of the Epirus claimed to be descended from Achilles through his son, Neoptolemus. Alexander the Great, son of the Epiran princess Olympias, could therefore also claim this descent, and in many ways strove to be like his great ancestor; he is said to have visited his tomb while passing Troy.
Achilles fought and killed the Amazon Helene. Some also said he married Medea, and that after both their deaths they were united in the Elysian Fields of Hades — as Hera promised Thetis in Apollonius' Argonautica. In some versions of the myth, Achilles has a relationship with his captive Briseis.
Achilles in Greek tragedy.
The Greek tragedian Aeschylus wrote a trilogy of plays about Achilles, given the title "Achilleis" by modern scholars. The tragedies relate the deeds of Achilles during the Trojan War, including his defeat of Hector and eventual death when an arrow shot by Paris and guided by Apollo punctures his heel. Extant fragments of the "Achilleis" and other Aeschylean fragments have been assembled to produce a workable modern play. The first part of the "Achilleis" trilogy, "The Myrmidons", focused on the relationship between Achilles and chorus, who represent the Achaean army and try to convince Achilles to give up his quarrel with Agamemnon; only a few lines survive today.
The tragedian Sophocles also wrote a play with Achilles as the main character, "The Lovers of Achilles". Only a few fragments survive.
Achilles in Greek philosophy.
The philosopher Zeno of Elea centered one of his paradoxes on an imaginary footrace between "swift-footed" Achilles and a tortoise, by which he attempted to show that Achilles could not catch up to a tortoise with a head start, and therefore that motion and change were impossible. As a student of the monist Parmenides and a member of the Eleatic school, Zeno believed time and motion to be illusions.
Music.
Achilles has frequently been mentioned in music.
Quotes.
"If Achilles was anything, he was a man who believed his own press releases."
—Roger Ebert, commenting on the classical depiction of Achilles' character and personality.
Bibliography.
Thomas Bullfinch, Myths of Greek and Rome
---END.OF.DOCUMENT---

Abraham Lincoln.
Abraham Lincoln (February 12, 1809 – April 15, 1865) served as the 16th President of the United States from March 1861 until his assassination in April 1865. He successfully led his country through its greatest internal crisis, the American Civil War, preserving the Union and ending slavery. Before his election in 1860 as the first Republican president, Lincoln had been a country lawyer, an Illinois state legislator, a member of the United States House of Representatives, and twice an unsuccessful candidate for election to the U.S. Senate. As an outspoken opponent of the expansion of slavery in the United States,
Lincoln won the Republican Party nomination in 1860 and was elected president later that year. His tenure in office was occupied primarily with the defeat of the secessionist Confederate States of America in the American Civil War. He introduced measures that resulted in the abolition of slavery, issuing his Emancipation Proclamation in 1863 and promoting the passage of the Thirteenth Amendment to the Constitution. Six days after the large-scale surrender of Confederate forces under General Robert E. Lee, Lincoln became the first American president to be assassinated.
Lincoln had closely supervised the victorious war effort, especially the selection of top generals, including Ulysses S. Grant. Historians have concluded that he handled the factions of the Republican Party well, bringing leaders of each faction into his cabinet and forcing them to cooperate. Lincoln successfully defused the "Trent" affair, a war scare with Britain late in 1861. Under his leadership, the Union took control of the border slave states at the start of the war. Additionally, he managed his own reelection in the 1864 presidential election.
Copperheads and other opponents of the war criticized Lincoln for refusing to compromise on the slavery issue. Conversely, the Radical Republicans, an abolitionist faction of the Republican Party, criticized him for moving too slowly in abolishing slavery. Even with these opponents, Lincoln successfully rallied public opinion through his rhetoric and speeches; his Gettysburg Address (1863) became an iconic symbol of the nation's duty. At the close of the war, Lincoln held a moderate view of Reconstruction, seeking to speedily reunite the nation through a policy of generous reconciliation. Lincoln has consistently been ranked by scholars as one of the greatest of all U.S. Presidents.
Childhood and education.
Abraham Lincoln was born on February 12, 1809, to Thomas Lincoln and Nancy Hanks, two farmers, in a one-room log cabin on the Sinking Spring Farm, in southeast Hardin County, Kentucky
(now part of LaRue County), making him the first president born in the west. Lincoln was not given a middle name.
His ancestor Samuel Lincoln had arrived in Hingham, Massachusetts from England in the 17th century.
His grandfather, also named Abraham Lincoln, had moved to Kentucky, where he owned over, and was ambushed and killed by an Indian raid in 1786.
Thomas Lincoln was a respected citizen of rural Kentucky. He owned several farms, including the Sinking Spring Farm, although he was not wealthy. The family belonged to a Separate Baptists church, which had high moral standards frowning on alcohol consumption and dancing, and many church members were opposed to slavery.
Abraham himself never joined their church, or any other church.
In 1816, the Lincoln family left Kentucky to avoid the expense of fighting for one of their properties in court, and made a new start in Perry County, Indiana (now in Spencer County). Lincoln later noted that this move was "partly on account of slavery", and partly because of difficulties with land deeds in Kentucky. Abraham's father disapproved of slavery on religious grounds and it was hard to compete economically with farms operated by slaves. Unlike land in the Northwest Territory, Kentucky never had a proper U.S. survey, and farmers often had difficulties proving title to their property.
When Lincoln was nine, his mother, then 34 years old, died of milk sickness. Soon afterwards, his father remarried, to Sarah Bush Johnston. Lincoln and his stepmother were close; he called her "Mother" for the rest of his life, but he became increasingly distant from his father. Abraham felt his father was not a success, and did not want to be like him. In later years, he would occasionally lend his father money.
In 1830, fearing a milk sickness outbreak, the family settled on public land in Macon County, Illinois.
The next year, when his father relocated the family to a new homestead in Coles County, Illinois, 22-year-old Lincoln struck out on his own, canoeing down the Sangamon River to the village of New Salem in Sangamon County.
Later that year, hired by New Salem businessman Denton Offutt and accompanied by friends, he took goods from New Salem to New Orleans via flatboat on the Sangamon, Illinois and Mississippi rivers.
Lincoln's formal education consisted of about 18 months of schooling; but he was an avid reader and largely self-educated. He was also skilled with an axe and a talented local wrestler, the latter of which helped give him self-confidence.
Lincoln avoided hunting and fishing because he did not like killing animals, even for food.
Marriage and family.
Lincoln's first love was Ann Rutledge. He met her when he first moved to New Salem, and by 1835 they had reached a romantic understanding. Rutledge, however, died on August 25, probably of typhoid fever.
Earlier, in either 1833 or 1834, he had met Mary Owens, the sister of his friend Elizabeth Abell, when she was visiting from her home in Kentucky. Late in 1836, Lincoln agreed to a match proposed by Elizabeth between him and her sister, if Mary ever returned to New Salem. Mary did return in November 1836 and Lincoln courted her for a time; however they both had second thoughts about their relationship. On August 16, 1837, Lincoln wrote Mary a letter from Springfield, to which he had moved that April to begin his law practice, suggesting he would not blame her if she ended the relationship. She never replied, and the courtship was over.
In 1840, Lincoln became engaged to Mary Todd, from a wealthy slaveholding family based in Lexington, Kentucky.
They met in Springfield in December 1839,
and were engaged sometime around that Christmas.
A wedding was set for January 1, 1841, but the couple split as the wedding approached. They later met at a party, and then married on November 4, 1842, in the Springfield mansion of Mary's married sister.
In 1844, the couple bought a house on Eighth and Jackson in Springfield, near Lincoln's law office.
The Lincolns soon had a budding family, with the birth of son Robert Todd Lincoln in Springfield, Illinois on August 1, 1843, and second son Edward Baker Lincoln on March 10, 1846, also in Springfield. According to a house girl, Abraham "was remarkably fond of children".
The Lincolns did not believe in strict rules and tight boundaries when it came to their children.
Robert, however, would be the only one of the Lincolns' children to survive into adulthood. Edward Lincoln died on February 1, 1850 in Springfield, likely of tuberculosis.
The Lincolns' grief over this loss was somewhat assuaged by the birth of William "Willie" Wallace Lincoln nearly eleven months later, on December 21. But Willie himself died of a fever at the age of eleven on February 20, 1862, in Washington, D.C., during President Lincoln's first term.
The Lincolns' fourth son Thomas "Tad" Lincoln was born on April 4, 1853, and, although he outlived his father, died at the age of eighteen on July 16, 1871 in Chicago.
Robert Lincoln eventually went on to attend Phillips Exeter Academy and Harvard College. His (and by extension, his father's) last known lineal descendant, Robert Todd Lincoln Beckwith, died December 24, 1985.
The death of the Lincolns' sons had profound effects on both Abraham and Mary. Later in life, Mary Todd Lincoln found herself unable to cope with the stresses of losing her husband and sons, and this (in conjunction with what some historians consider to have been pre-existing bipolar disorder
) eventually led Robert Lincoln to involuntarily commit her to a mental health asylum in 1875.
Abraham Lincoln himself was contemporaneously described as suffering from "melancholy" throughout his legal and political life, a condition which modern mental health professionals would now typically characterize as clinical depression.
Early political career and military service.
Lincoln began his political career in March 1832 at age 23 when he announced his candidacy for the Illinois General Assembly. He was esteemed by the residents of New Salem, but he didn't have an education, powerful friends, or money. The centerpiece of his platform was the undertaking of navigational improvements on the Sangamon River. Before the election he served as a captain in a company of the Illinois militia during the Black Hawk War, although he never saw combat. Lincoln returned from the militia after a few months and was able to campaign throughout the county before the August 6 election. At, he was tall and "strong enough to intimidate any rival." At his first political speech, he grabbed a man accosting a supporter by his "neck and the seat of his trousers", and threw him. When the votes were counted, Lincoln finished eighth out of thirteen candidates (only the top four were elected), but he did manage to secure 277 out of the 300 votes cast in the New Salem precinct.
In 1834, he won an election to the state legislature. He was labeled a Whig, but ran a bipartisan campaign.
He then decided to become a lawyer, and began teaching himself law by reading "Commentaries on the Laws of England".
Admitted to the bar in 1837, he moved to Springfield, Illinois, that April,
and began to practice law with John T. Stuart, Mary Todd's cousin, who let Lincoln have the run of his law library while studying to be a lawyer.
With a reputation as a formidable adversary during cross-examinations and closing arguments, Lincoln became an able and successful lawyer.
In 1841, Lincoln entered law practice with William Herndon, whom Lincoln thought "a studious young man".
He served four successive terms in the Illinois House of Representatives as a representative from Sangamon County, affiliated with the Whig party.
In 1837, he and another legislator declared that slavery was "founded on both injustice and bad policy"
the first time he had publicly opposed slavery. In the 1835–1836 legislative session he'd voted to restrict suffrage to whites only. He would later say that he had been against slavery since he was a boy, but being labelled an abolitionist was "political suicide" in Sangamon County in those years, and so he chose his words carefully when discussing the issue publicly.
National politics.
Lincoln was a Whig, and since the early 1830s had strongly admired the policies and leadership of Henry Clay.
"I have always been an old-line Henry Clay Whig" he professed to friends in 1861.
The party favored economic expansion such as improving roads and increasing trade.
In 1846, Lincoln was elected to the U.S. House of Representatives, where he served one two-year term.
As a House member, Lincoln was a dedicated Whig, showing up for most votes and giving speeches that echoed the party line.
He used his office as an opportunity to speak out against the Mexican–American War, which he attributed to President Polk's desire for "military glory — that attractive rainbow, that rises in showers of blood".
Lincoln's main stand against Polk occurred in his Spot Resolutions: The war had begun with a violent confrontation on territory disputed by Mexico and Texas,
but as Lincoln pointed out, Polk had insisted that Mexican soldiers had "invaded "our territory" and shed the blood of our fellow-citizens on our "own soil".
Lincoln demanded that Polk show Congress the exact spot on which blood had been shed, and proof that that spot was on American soil. Congress never enacted the resolution or even debated it,
and its introduction resulted in a loss of political support for Lincoln in his district;
one Illinois newspaper derisively nicknamed him "spotty Lincoln."
Despite his admiration for Henry Clay, Lincoln was a key early supporter of Zachary Taylor's candidacy for the 1848 presidential election. When Lincoln's term ended, the incoming Taylor administration offered him the governorship of the Oregon Territory. The territory leaned heavily Democratic, and Lincoln doubted they would elect him as governor or as a senator after they were admitted to the union, so he returned to Springfield.
Prairie lawyer.
Back in Springfield, Lincoln turned most of his energies to making a living practicing law, handling "every kind of business that could come before a prairie lawyer." He "rode the circuit"--that is, appeared in county seats in the mid-state region when the county courts were in session.
His reputation grew and he appeared before the Supreme Court of the United States, arguing a case involving a canal boat that sank after hitting a bridge.
Lincoln represented numerous transportation interests, such as the river barges and the railroads. As a riverboat man, Lincoln had initially favored riverboat interests, but ultimately he represented whoever hired him.
In 1849, he had received a patent for a "device to buoy vessels over shoals". Lincoln's goal had been to lessen the draft of a river craft by pushing horizontal floats into the water alongside the hull. The floats would have served as temporary ballast tanks.
The idea was never commercialized, but Lincoln is still the only person to hold a patent and serve as President of the United States.
In 1851, he represented the Alton & Sangamon Railroad in a dispute with one of its shareholders, James A. Barret, who had refused to pay the balance on his pledge to the railroad on the grounds that it had changed its originally planned route.
Lincoln argued that as a matter of law a corporation is not bound by its original charter when that charter can be amended in the public interest, that the newer proposed Alton & Sangamon route was superior and less expensive, and that accordingly the corporation had a right to sue Mr. Barret for his delinquent payment. He won this case, and the decision by the Illinois Supreme Court was eventually cited by 25 other courts throughout the United States. Lincoln appeared in front of the Illinois Supreme Court 175 times, 51 times as sole counsel, of which, 31 were decided in his favor.
Lincoln's most notable criminal trial came in 1858 when he defended William "Duff" Armstrong, who was on trial for the murder of James Preston Metzker.
The case is famous for Lincoln's use of judicial notice to show an eyewitness had lied on the stand. After the witness testified to having seen the crime in the moonlight, Lincoln produced a Farmers' Almanac to show that the moon on that date was at such a low angle it could not have produced enough illumination to see anything clearly. Based on this evidence, Armstrong was acquitted.
Republican politics 1854–1860.
Lincoln returned to politics in response to the Kansas-Nebraska Act (1854), which expressly repealed the limits on slavery's extent as established by the Missouri Compromise (1820). Illinois Democrat Stephen A. Douglas, the most powerful man in the Senate, proposed popular sovereignty as the solution to the slavery impasse, and incorporated it into the Kansas–Nebraska Act. Douglas argued that in a democracy the people should have the right to decide whether to allow slavery in their territory, rather than have such a decision imposed on them by the national Congress.
In the October 16, 1854, "Peoria Speech",
Lincoln outlined his position on slavery that he would repeat over the next six years on the route to the presidency.
According to a newspaper account of the speech, Lincoln spoke with "a thin high-pitched falsetto voice of much carrying power, that could be heard a long distance in spite of the hustle and bustle of the crowd... [with] the accent and pronunciation peculiar to his native state, Kentucky."
In late 1854, Lincoln decided to run for the United States Senate as a Whig.
Despite leading in the first six rounds of voting in the state legislature, Lincoln instructed his backers to vote for Lyman Trumbull to prevent pro-Nebraska candidate Joel Aldrich Matteson from winning. Trumbull beat Matteson in the tenth round of voting.
The Whigs had been irreparably split by the Kansas-Nebraska Act. "I think I am a Whig, but others say there are not Whigs, and I am an abolitionist, even though I do no more than oppose the expansion of slavery" he said. Drawing on remnants of the old Whig party, and on disenchanted Free Soil, Liberty, and Democratic party members, he was instrumental in forging the shape of the new Republican Party.
At the Republican convention in 1856, Lincoln placed second in the contest to become the party's candidate for Vice-President.
In 1857–58, Douglas broke with President Buchanan, leading to a fight for control of the Democratic Party. Some eastern Republicans even favored the reelection of Douglas in 1858, since he had led the opposition to the Lecompton Constitution, which would have admitted Kansas as a slave state.
Accepting the Republican nomination for Senate in 1858, Lincoln delivered his famous speech: "'A house divided against itself cannot stand.'(Mark 3:25) I believe this government cannot endure permanently half slave and half free. I do not expect the Union to be dissolved — I do not expect the house to fall — but I do expect it will cease to be divided. It will become all one thing, or all the other."
The speech created an evocative image of the danger of disunion caused by the slavery debate, and rallied Republicans across the north.
As a part of his 1860 presidential campaign strategy Lincoln acquired through banker Jacob Bunn, in May, 1859, the Illinois Staats-Anzeiger, a German-language newspaper of Springfield, Illinois, to further the cause of Republican Party politics among the German-speaking community of the region..
Lincoln–Douglas debates of 1858.
The 1858 campaign featured the Lincoln–Douglas debates, generally considered the most famous political debate in American history.
Lincoln warned that "The Slave Power" was threatening the values of republicanism, while Stephen A. Douglas emphasized the supremacy of democracy, as set forth in his Freeport Doctrine, which said that local settlers should be free to choose whether to allow slavery or not and could overrule the Supreme Courts Dred Scott v. Sandford decision.
Though the Republican legislative candidates won more popular votes, the Democrats won more seats, and the legislature reelected Douglas to the Senate. Nevertheless, Lincoln's definition of the issues gave him a national political reputation.<ref»Carwardine, p. 89-90
On February 27, 1860, New York party leaders invited Lincoln to give a speech at Cooper Union to group of powerful Republicans. In one of the most important speeches of his career, Lincoln showed that he was a contender for the Republican's presidential nomination. Journalist Noah Brooks reported, "No man ever before made such an impression on his first appeal to a New York audience."
1860 Presidential election.
On May 9–10, 1860, the Illinois Republican State Convention was held in Decatur.
At this convention, Lincoln received his first endorsement to run for the presidency.
On May 18, at the 1860 Republican National Convention in Chicago, Lincoln emerged as the Republican candidate on the third ballot, beating candidates such as William H. Seward and Salmon P. Chase.
Why Lincoln won the nomination has been subject of much debate. His expressed views on slavery were seen as more moderate than those of rivals Seward and Chase.
Some feel that Seward lost more than Lincoln won, including Seward himself. Others attribute it to luck, and the fact that the convention was held in Lincoln's home state. Historian Doris Kearns Goodwin believes the real reason was Lincoln's skill as a politician.
Most Republicans agreed with Lincoln that the North was the aggrieved party
as the Slave Power tightened its grasp on the national government with the Dred Scott decision and the presidency of James Buchanan. Throughout the 1850s Lincoln denied that there would ever be a civil war, and his supporters repeatedly rejected claims that his election would incite secession.
Meanwhile, Douglas was selected as the candidate of the northern Democrats, with Herschel Vespasian Johnson as the vice-presidential candidate. Delegates from eleven slave states walked out of the Democrat's convention, disagreeing with Douglas's position on Popular sovereignty, and ultimately selected John C. Breckinridge as their candidate.
As Douglas stumped the country, Lincoln was the only one of the four major candidates to give no speeches whatever. Instead he monitored the campaign closely but relied on the enthusiasm of the Republican Party. It did the leg work that produced majorities across the North. It produced tons of campaign posters and leaflets, and thousands of newspaper editorials. There were thousands of Republican speakers who focused first on the party platform, and second on Lincoln's life story, emphasizing his childhood poverty. The goal was to demonstrate the superior power of "free labor", whereby a common farm boy could work his way to the top by his own efforts. The Republican Party's production of campaign literature dwarfed the combined opposition. A "Chicago Tribune" writer produced a pamphlet that detailed Lincoln's life, and sold one million copies.
On November 6, 1860, Lincoln was elected as the 16th President of the United States, beating Democrat Stephen A. Douglas, John C. Breckinridge of the Southern Democrats, and John Bell of the new Constitutional Union Party. He was the first Republican president, winning entirely on the strength of his support in the North: he was not even on the ballot in ten states in the South, and won only two of 996 counties in all the Southern states. Lincoln received 1,866,452 votes, Douglas 1,376,957 votes, Breckinridge 849,781 votes, and Bell 588,789 votes. The electoral vote was decisive: Lincoln had 180 and his opponents added together had only 123. Turnout was 82.2%, with Lincoln winning the free northern states. Douglas won Missouri, and split New Jersey with Lincoln.
Bell won Virginia, Tennessee, and Kentucky, and Breckinridge won the rest of the South.
There were fusion tickets in which all of Lincoln's opponents combined to form one ticket in New York, New Jersey and Rhode Island, but even if the anti-Lincoln vote had been combined in every state, Lincoln still would have won because he would still have had a majority in the electoral college.
Presidency and the Civil War.
With the emergence of the Republicans as the nation's first major sectional party by the mid-1850s, the old Second Party System collapsed and a realignment created the Third Party System. It became the stage on which sectional tensions were played out. Although little of the West–the focal point of sectional tensions– was fit for cotton cultivation, Southern secessionists read the political fallout as a sign that their power in national politics was rapidly weakening. The slave system had been buttressed by the Democratic Party, which was increasingly seen by anti-slavery elements as representing a more pro-Southern position that unfairly permitted the Slave Power to prevail in the nation's territories and to dominate national policy before the Civil War. Yet the Democrats suffered a significant reverse in the electoral realignment of the mid-1850s; they lost the dominance they had achieved over the Whig Party and, indeed, were the minority party in most of the northern states. The 1854 election was a Realigning election or "critical election" that saw a realignment of voting patterns.
Abraham Lincoln's election was a watershed in the balance of power of competing national and parochial interests and affiliations.
Secession winter 1860–1861.
As Lincoln's election became more likely, secessionists made clear their intent to leave the Union.
On December 20, 1860, South Carolina took the lead; by February 1, 1861, Florida, Mississippi, Alabama, Georgia, Louisiana,
The seven states soon declared themselves to be a new nation, the Confederate States of America. The upper South (Delaware, Maryland, Virginia, North Carolina, Tennessee, Kentucky, Missouri, and Arkansas) listened to, but initially rejected, the secessionist appeal.
President Buchanan and President-elect Lincoln refused to recognize the Confederacy.
Attempts at compromise, such as the Crittenden Compromise which would have extended the Missouri line of 1820, were discussed.
Despite support for the Crittenden Compromise among some Republicans, Lincoln denounced it in private letters, saying "either the Missouri line extended, or... Pop. Sov. would lose us everything we gained in the election; that filibustering for all South of us, and making slave states of it, would follow in spite of us, under either plan",
while other Republicans publicly stated it "would amount to a perpetual covenant of war against every people, tribe, and state owning a foot of land between here and Tierra del Fuego."
The Confederate States of America selected Jefferson Davis on February 9, 1861, as their provisional President.
President-elect Lincoln evaded possible assassins in Baltimore, and on February 23, 1861, arrived in disguise in Washington, D.C.
At his inauguration on March 4, 1861, sharpshooters watched the inaugural platform, while soldiers on horseback patrolled the surrounding area.
In his first inaugural address, Lincoln declared, "I hold that in contemplation of universal law and of the Constitution the Union of these States is perpetual. Perpetuity is implied, if not expressed, in the fundamental law of all national governments," arguing further that the purpose of the United States Constitution was "to form a more perfect union" than the Articles of Confederation which were "explicitly" perpetual, thus the Constitution too was perpetual. He asked rhetorically that even were the Constitution a simple contract, would it not require the agreement of all parties to rescind it?
Also in his inaugural address, in a final attempt to reunite the states and prevent certain war, Lincoln supported the pending Corwin Amendment to the Constitution, which had passed Congress the previous day. This amendment, which explicitly protected slavery in those states in which it already existed, was considered by Lincoln to be a possible way to stave off secession.
A few short weeks before the war he went so far as to pen a letter to every governor asking for their support in ratifying the Corwin Amendment.
By the time Lincoln took office, the Confederacy was an established fact, and no leaders of the insurrection proposed rejoining the Union on any terms. The failure of the Peace Conference of 1861 rendered legislative compromise virtually impossible. Buchanan might have allowed the southern states to secede, and some members of his cabinet recommended that. However, conservative Democratic nationalists, such as Jeremiah S. Black, Joseph Holt, and Edwin M. Stanton had taken control of Buchanan's cabinet in early January, and refused to accept secession.
Lincoln and nearly every Republican leader adopted this position by March 1861: the Union could not be dismantled. Believing that a peaceful solution was still possible, Lincoln decided to not take any action against the South unless the Unionists themselves were attacked first. This finally happened in April 1861.
Historian Allan Nevins argues that Lincoln made three miscalculations in believing that he could preserve the Union, hold government property, and still avoid war. He "temporarily underrated the gravity of the crisis", overestimated the strength of Unionist sentiment in the South and border states, and misunderstood the conditional support of Unionists in the border states.
Fighting begins.
On April 12, 1861, Union troops at Fort Sumter were fired upon and forced to surrender. On April 15, Lincoln called on the states to send detachments totaling 75,000 troops,
to recapture forts, protect the capital, and "preserve the Union", which in his view still existed intact despite the actions of the seceding states. These events forced the states to choose sides. Virginia declared its secession, after which the Confederate capital was moved from Montgomery to Richmond. North Carolina, Tennessee, and Arkansas also voted for secession over the next two months. Missouri, Kentucky and Maryland threatened secession, but neither they nor the slave state of Delaware seceded. Lincoln urgently negotiated with state leaders there, promising not to interfere with slavery.
Troops headed south towards Washington, D.C. to protect the capital in response to Lincoln's call. On April 19, angry secessionist mobs in Baltimore, a Maryland city to the north of Washington that controlled the rail links, attacked Union troops traveling to the capital. George William Brown, the Mayor of Baltimore, and other suspect Maryland politicians were arrested and imprisoned at Fort McHenry.
Rebel leaders were also arrested in other border areas and held in military prisons without trial. Over 18,000 were arrested. One, Clement Vallandigham, was exiled, but the remainder were released, usually after two or three months ("see": Ex parte Merryman).
Conducting the war effort.
The war was a source of constant frustration for the president and occupied nearly all of his time. He had a contentious relationship with General McClellan,
who became general-in-chief of all the Union armies in the wake of the embarrassing Union defeat at the First Battle of Bull Run and after the retirement of Winfield Scott in late 1861.
Despite his inexperience in military affairs, Lincoln immediately took an active part in determining war strategy. His priorities were twofold: to ensure that Washington was well defended; and to conduct an aggressive war effort that would satisfy the demand in the North for prompt, decisive victory.
McClellan, a youthful West Point graduate and railroad executive called back to active military service,
He took several months to plan and execute his Peninsula Campaign, with the objective of capturing Richmond by moving the Army of the Potomac by boat to the peninsula and then traveling by land to Richmond. McClellan's delay concerned Lincoln, as did his insistence that no troops were needed to defend Washington, Lincoln insisted on holding some of McClellan's troops to defend the capital, a decision McClellan blamed for the ultimate failure of the Peninsula Campaign. McClellan, a conservative Democrat,
was passed over for general-in-chief (that is, chief strategist) in favor of Henry Wager Halleck, after giving Lincoln his "Harrison's Landing Letter", where he offered unsolicited political advice to Lincoln urging caution in the war effort.
McClellan's letter incensed Radical Republicans, who successfully pressured Lincoln to appoint John Pope, a Republican, as head of the new Army of Virginia. Pope complied with Lincoln's strategic desire to move toward Richmond from the north, thus protecting the capital from attack. However, Pope was soundly defeated at the Second Battle of Bull Run in the summer of 1862, forcing the Army of the Potomac to defend Washington for a second time.
In response to his failure, Pope was sent to Minnesota to fight the Sioux.
Despite his dissatisfaction with McClellan's failure to reinforce Pope, Lincoln restored him to command of all forces around Washington, to the dismay of his cabinet (all save Seward), who wished McClellan gone.
Two days after McClellan's return to command, General Lee's forces crossed the Potomac River into Maryland, leading to the Battle of Antietam (September 1862).
The ensuing Union victory, one of the bloodiest in American history, enabled Lincoln to give notice that he would issue an Emancipation Proclamation in January,
but he relieved McClellan of his command after waiting for the conclusion of the 1862 midterm elections and appointed Republican Ambrose Burnside to head the Army of the Potomac.
Burnside was politically neutral, which Lincoln desired, and for the most part supported the President's aims.
Burnside had promised to follow through on Lincoln's strategic vision for a strong offensive against Lee and Richmond. After Burnside was stunningly defeated at Fredericksburg in December,
Joseph Hooker took command, despite his history of "loose talk" and criticizing former commanders.
Hooker was routed by Lee at the Battle of Chancellorsville in May, 1863,
but continued to command his troops for roughly two months. Hooker did not agree with Lincoln's desire to divide his troops, and possibly force Lee to do the same, and tendered his resignation, which was accepted. During the Gettysburg Campaign he was replaced by George Meade.
Using black troops and former slaves was official government policy after the issuance of the Emancipation Proclamation. At first Lincoln was reluctant to fully implement this program, but by the spring of 1863 he was ready to initiate "a massive recruitment of Negro troops." In a letter to Andrew Johnson, the military governor of Tennessee, encouraging him to lead the way in raising black troops, Lincoln wrote, "The bare sight of fifty thousand armed, and drilled black soldiers on the banks of the Mississippi would end the rebellion at once."
By the end of 1863, at Lincoln's direction, General Lorenzo Thomas had recruited twenty regiments of African Americans from the Mississippi Valley.
Grant.
After the Union victory at Gettysburg, Meade's failure to pursue Lee and months of inactivity for the Army of the Potomac persuaded Lincoln that a change was needed. McClellan was seeking the Democratic nomination for President, and Lincoln worried that Grant might also have political aspirations. Lincoln convinced himself that Grant didn't have political aspirations, in the immediate at least, and made Ulysses S. Grant commander of the Union Army.
Grant already had a solid string of victories in the Western Theater, including the battles of Vicksburg and Chattanooga.
Responding to criticism of Grant, Lincoln replied, "I can't spare this man. He fights."
Grant waged his bloody Overland Campaign in 1864 with a strategy of a war of attrition, characterized by high Union losses at battles such as the Wilderness and Cold Harbor, but by proportionately higher Confederate losses. The high casualty figures alarmed the nation, and, after Grant lost a third of his army, Lincoln asked what Grant's plans were. "I propose to fight it out on this line if it takes all summer," replied Grant. Lincoln and the Republican party mobilized support throughout the North, backed Grant to the hilt, and replaced his losses.
The Confederacy was out of replacements, so Lee's army shrank with every battle, forcing it back to trenches outside Petersburg. In April 1865, Lee's army finally crumbled under Grant's pounding, and Richmond fell.
Lincoln authorized Grant to target the Confederate infrastructure – such as plantations, railroads, and bridges – hoping to destroy the South's morale and weaken its economic ability to continue fighting. This strategy allowed Generals Sherman and Sheridan to destroy plantations and towns in the Shenandoah Valley, Georgia, and South Carolina. The damage caused by Sherman's March to the Sea through Georgia totaled more than $100 million by Sherman's own estimate.
Lincoln grasped the need to control strategic points (such as the Mississippi River and the fortress city of Vicksburg) and understood the importance of defeating the enemy's army, rather than simply capturing territory. He had, however, limited success in motivating his commanders to adopt his strategies until late 1863, when he found a man who shared his vision of the war in Ulysses S. Grant. Only then could he relentlessly pursue a series of coordinated offensives in multiple theaters, and have a top commander who agreed on the use of black troops.
Two days a week, Lincoln would meet with his cabinet in the afternoon, and occasionally his wife would force him to take a carriage ride because she was concerned he was working too hard. Throughout the war, Lincoln showed an intense interest with the military campaigns. He spent hours at the War Department telegraph office, reading dispatches from the field.
He visited battle sites frequently, and seemed fascinated by scenes of war. During Jubal Anderson Early's raid on Washington, D.C. in 1864, Lincoln was watching the combat from an exposed position; captain Oliver Wendell Holmes, Jr. shouted at him, "Get down, you damn fool, before you get shot!"
Emancipation Proclamation.
Lincoln maintained that the powers of his administration to end slavery were limited by the Constitution. He expected to cause the eventual extinction of slavery by stopping its further expansion into any U.S. territory, and by persuading states to accept compensated emancipation if the state would outlaw slavery (an offer that took effect only in Washington, D.C.). Guelzo says Lincoln believed that shrinking slavery in this way would make it uneconomical, and place it back on the road to eventual extinction that the Founders had envisioned.
In July 1862, Congress passed the Second Confiscation Act, which freed the slaves of anyone convicted of aiding the rebellion. Although Lincoln believed it wasn't in Congress's remit to free any slaves, he approved the bill. He felt freeing the slaves could only be done by the Commander in Chief during wartime, and that signing the bill would help placate those in Congress who wanted to do it through legislation. In that month, Lincoln discussed a draft of the Emancipation Proclamation with his cabinet. In it, he stated that "as a fit and necessary military measure" (and according to Donald not for moral reasons) on January 1, 1863, "all persons held as a slaves" in the Confederate states will " thenceforward, and forever, be free."
The Emancipation Proclamation, announced on September 22, 1862 and put into effect on January 1, 1863, freed slaves in territories not already under Union control. As Union armies advanced south, more slaves were liberated until all of them in Confederate territory (over three million) were freed. Lincoln later said: "I never, in my life, felt more certain that I was doing right, than I do in signing this paper." The proclamation made the abolition of slavery in the rebel states an official war goal. Lincoln then threw his energies into passage of the Thirteenth Amendment to permanently abolish slavery throughout the nation.
He personally lobbied individual Congressmen for the Amendment, which was passed by the Congress in early 1865, shortly before his death.
A few days after the Emancipation was announced, thirteen Republican governors met at the War Governors' Conference; they supported the president's Proclamation, but suggested the removal of General George B. McClellan as commander of the Union's Army of the Potomac.
For some time, Lincoln continued earlier plans to set up colonies for the newly freed slaves. He commented favorably on colonization in the Emancipation Proclamation, but all attempts at such a massive undertaking failed. As Frederick Douglass observed, Lincoln was, "The first great man that I talked with in the United States freely who in no single instance reminded me of the difference between himself and myself, of the difference of color."
Gettysburg Address.
Although the Battle of Gettysburg was a Union victory, it was also the bloodiest battle of the war and dealt a blow to Lincoln's war effort. As the Union Army decreased in numbers due to casualties, more soldiers were needed to replace the ranks. Lincoln's 1863 military drafts were considered "odious" among many in the north, particularly immigrants. The New York Draft Riots of July 1863 were the most notable manifestation of this discontent.
"If the election were to occur now, the result would be extremely doubtful, and although most of our discreet friends are sanguine of the result, my impression is, the chances would be against us. The draft is very odious in the State... the Democratic leaders have succeeded in exciting prejudice and passion, and have infused their poison into the minds of the people to a very large extent, and the changes are against us."
Therefore, in the fall of 1863, Lincoln's principal aim was to sustain public support for the war effort. This goal became the focus of his address at the Gettysburg battlefield cemetery on November 19.
The "Gettysburg Address" is one of the most quoted speeches in United States history.
It was delivered at the dedication of the Soldiers' National Cemetery in Gettysburg, Pennsylvania, on the afternoon of Thursday, November 19, 1863, during the American Civil War, four and a half months after the Union armies defeated those of the Confederacy at the decisive Battle of Gettysburg.
Abraham Lincoln's carefully crafted address, secondary to other presentations that day, came to be regarded as one of the greatest speeches in American history. In just over two minutes, Lincoln invoked the principles of human equality espoused by the Declaration of Independence and redefined the Civil War as a struggle not merely for the Union, but as "a new birth of freedom" that would bring true equality to all of its citizens, and that would also create a unified nation in which states' rights were no longer dominant.
Beginning with the now-iconic phrase, "Four score and seven years ago...", Lincoln referred to the events of the Civil War and described the ceremony at Gettysburg as an opportunity not only to consecrate the grounds of a cemetery, but also to dedicate the living to the struggle to ensure that "government of the people, by the people, for the people, shall not perish from the earth".
1864 election.
After Union victories at Gettysburg, Vicksburg, and Chattanooga in 1863, overall victory seemed at hand, and Lincoln promoted Ulysses S. Grant General-in-Chief on March 12, 1864. When the spring campaigns turned into bloody stalemates, Lincoln supported Grant's strategy of wearing down Lee's Confederate army at the cost of heavy Union casualties. With an election looming, he easily defeated efforts to deny his renomination. At the Convention, the Republican Party selected Andrew Johnson, a War Democrat from the Southern state of Tennessee, as his running mate to form a broader coalition. They ran on the new Union Party ticket uniting Republicans and War Democrats.
Lincoln did not show the pledge to his cabinet, but asked them to sign the sealed envelope.
While the Democratic platform followed the Peace wing of the party and called the war a "failure," their candidate, General George B. McClellan, supported the war and repudiated the platform.
Lincoln provided Grant with new replacements and mobilized his party to support Grant and win local support for the war effort. Sherman's capture of Atlanta in September ended defeatist jitters; the Democratic Party was deeply split, with some leaders and most soldiers openly for Lincoln; the Union party was united and energized, and Lincoln was easily reelected in a landslide. He won all but three states, including 78% of the Union soldiers' vote.
Second Inaugural Address.
On March 4, 1865, Lincoln delivered his second inaugural address, his favorite of all his speeches. At this time, a victory over the rebels was at hand, slavery was dead, and Lincoln was looking to the future.
Reconstruction.
Reconstruction began during the war as Lincoln and his associates pondered questions of how to reintegrate the Southern states and what to do with Confederate leaders and the freed slaves. Lincoln led the "moderates" regarding Reconstruction policy, and was usually opposed by the Radical Republicans, under Thaddeus Stevens in the House and Charles Sumner and Benjamin Wade in the Senate (though he cooperated with these men on most other issues). Determined to find a course that would reunite the nation and not alienate the South, Lincoln urged that speedy elections under generous terms be held throughout the war in areas behind Union lines. His Amnesty Proclamation of December 8, 1863, offered pardons to those who had not held a Confederate civil office, had not mistreated Union prisoners, and would sign an oath of allegiance.
Critical decisions had to be made as state after state was reconquered. Of special importance were Tennessee, where Lincoln appointed Andrew Johnson as governor, and Louisiana, where Lincoln attempted a plan that would restore statehood when 10% of the voters agreed to it. The Radicals thought this policy too lenient, and passed their own plan, the Wade-Davis Bill, in 1864. When Lincoln pocket vetoed the bill, the Radicals retaliated by refusing to seat representatives elected from Louisiana, Arkansas, and Tennessee.
Near the end of the war, Lincoln made an extended visit to Grant's headquarters at City Point, Virginia. This allowed the president to confer in person with Grant and Sherman about ending hostilities (as Sherman managed a hasty visit to Grant from his forces in North Carolina at the same time).
Lincoln also was able to visit Richmond after it was taken by the Union forces and to make a public gesture of sitting at Jefferson Davis' own desk, symbolically saying to the nation that the President of the United States held authority over the entire land. He was greeted at the city as a conquering hero by freed slaves, whose sentiments were epitomized by one admirer's quote, "I know I am free for I have seen the face of Father Abraham and have felt him." When a general asked Lincoln how the defeated Confederates should be treated, Lincoln replied, "Let 'em up easy."
Lincoln arrived back in Washington on the evening of April 9, 1865, the day Lee surrendered at Appomattox Court House in Virginia. The war was effectively over. The other rebel armies surrendered soon after, and there was no subsequent guerrilla warfare.
Redefining Republicanism.
Lincoln's rhetoric defined the issues of the war for the nation, the world, and posterity. The Gettysburg Address defied Lincoln's own prediction that "the world will little note, nor long remember what we say here." His second inaugural address is also greatly admired and often quoted.
In recent years, historians have stressed Lincoln's use of and redefinition of republican values. As early as the 1850s, a time when most political rhetoric focused on the sanctity of the Constitution, Lincoln shifted emphasis to the Declaration of Independence as the foundation of American political values—what he called the "sheet anchor" of republicanism.
The Declaration's emphasis on freedom and equality for all, rather than the Constitution's tolerance of slavers, shifted the debate. As Diggins concludes regarding the highly influential Cooper Union speech, "Lincoln presented Americans a theory of history that offers a profound contribution to the theory and destiny of republicanism itself."
His position gained strength because he highlighted the moral basis of republicanism, rather than its legalisms.
Nevertheless, in 1861 Lincoln justified the war in terms of legalisms (the Constitution was a contract, and for one party to get out of a contract all the other parties had to agree), and then in terms of the national duty to guarantee a "republican form of government" in every state.
That duty was also the principle underlying federal intervention in Reconstruction.
In his Gettysburg Address Lincoln redefined the American nation, arguing that it was born not in 1789 but in 1776, "conceived in Liberty, and dedicated to the proposition that all men are created equal." He declared that the sacrifices of battle had rededicated the nation to the propositions of democracy and equality, "that this nation shall have a new birth of freedom — and that government of the people, by the people, for the people, shall not perish from the earth." By emphasizing the centrality of the nation, he rebuffed the claims of state sovereignty. While some critics say Lincoln moved too far and too fast, they agree that he dedicated the nation to values that marked "a new founding of the nation."
Civil liberties suspended.
During the Civil War, Lincoln appropriated powers no previous President had wielded: he used his war powers to proclaim a blockade, suspended the writ of habeas corpus, spent money before Congress appropriated it, and imprisoned between 15,000 and 18,000 suspected Confederate sympathizers without trial.
Domestic measures.
Lincoln believed in the Whig theory of the presidency, which left Congress to write the laws while he signed them; Lincoln exercised his veto power only four times, the only significant instance being his pocket veto of the Wade-Davis Bill.
Thus, he signed the Homestead Act in 1862, making millions of acres of government-held land in the West available for purchase at very low cost. The Morrill Land-Grant Colleges Act, also signed in 1862, provided government grants for state agricultural colleges in each state. The Pacific Railway Acts of 1862 and 1864 granted federal support for the construction of the United States' First Transcontinental Railroad, which was completed in 1869. The passage of the Homestead Act and the Pacific Railway Acts was made possible by the absence of Southern congressmen and senators who had opposed the measures in the 1850s.
Other important legislation involved two measures to raise revenues for the Federal government: tariffs (a policy with long precedent), and a Federal income tax (which was new). In 1861, Lincoln signed the second and third Morrill Tariff (the first had become law under James Buchanan). In 1861, Lincoln signed the Revenue Act of 1861
creating the first U.S. income tax. This created a flat tax of 3% on incomes above $800 ($ in current dollars), which was later changed by the Revenue Act of 1862
Lincoln also presided over the expansion of the federal government's economic influence in several other areas. The creation of the system of national banks by the National Banking Acts of 1863, 1864, and 1865 allowed the creation of a strong national financial system. In 1862, Congress created, with Lincoln's approval, the Department of Agriculture, although that institution would not become a Cabinet-level department until 1889. The Legal Tender Act of 1862 established the United States Note, the first paper currency in United States history since the Continentals that were issued during the Revolution. This was done to increase the money supply to pay for fighting the war.
In 1862, Lincoln sent a senior general, John Pope, to put down the "Sioux Uprising" in Minnesota. Presented with 303 death warrants for convicted Santee Dakota who were accused of killing innocent farmers, Lincoln ordered a personal review of these warrants, eventually approving 39 of these for execution (one was later reprieved).
Abraham Lincoln is largely responsible for the institution of the Thanksgiving holiday in the United States. Prior to Lincoln's presidency, Thanksgiving, while a regional holiday in New England since the 17th century, had only been proclaimed by the federal government sporadically, and on irregular dates. The last such proclamation was during James Madison's presidency fifty years before. In 1863, Lincoln declared the final Thursday in November to be a day of Thanksgiving, and the holiday has been celebrated annually then ever since.
Assassination.
Originally, John Wilkes Booth, a well-known actor and a Confederate spy from Maryland, had formulated a plan to kidnap Lincoln in exchange for the release of Confederate prisoners. After attending an April 11 speech in which Lincoln promoted voting rights for blacks, an incensed Booth changed his plans and determined to assassinate the president.
Learning that the President and First Lady would be attending Ford's Theatre, he laid his plans, assigning his co-conspirators to assassinate Vice President Andrew Johnson and Secretary of State William H. Seward.
Without his main bodyguard Ward Hill Lamon, to whom he related his famous dream regarding his own assassination, Lincoln left to attend the play "Our American Cousin" on April 14, 1865. As a lone bodyguard wandered, and Lincoln sat in his state box (Box 7) in the balcony, Booth crept up behind the President and waited for what he thought would be the funniest line of the play ("You sock-dologizing old man-trap"), hoping the laughter would muffle the noise of the gunshot. When the laughter began, Booth jumped into the box and aimed a single-shot, round-ball.44 caliber (11 mm) Deringer at his head, firing at point-blank range. Major Henry Rathbone momentarily grappled with Booth but was cut by Booth's knife. Booth then leaped to the stage and shouted "Sic semper tyrannis!" () and escaped, despite suffering a broken leg in the leap.
A twelve-day manhunt ensued, in which Booth was chased by Federal agents (under the direction of Secretary of War Edwin M. Stanton).
He was eventually cornered in a Virginia barn house and shot, dying of his wounds soon after.
An army surgeon, Doctor Charles Leale, initially assessed Lincoln's wound as mortal. The President was taken across the street from the theater to the Petersen House, where he lay in a coma for nine hours before dying. Several physicians attended Lincoln, including U.S. Army Surgeon General Joseph K. Barnes of the Army Medical Museum. Using a probe, Barnes located some fragments of Lincoln's skull and the ball lodged inside his brain. Lincoln never regained consciousness and was pronounced dead at 7:22:10 a.m. April 15, 1865. He was the first president to be assassinated or to lie in state.
Lincoln's body was carried by train in a grand funeral procession through several states on its way back to Illinois.
While much of the nation mourned him as the savior of the United States, Copperheads celebrated the death of a man they considered a tyrant. The Lincoln Tomb in Oak Ridge Cemetery in Springfield, is tall and, by 1874, was surmounted with several bronze statues of Lincoln. To prevent repeated attempts to steal Lincoln's body and hold it for ransom, Robert Todd Lincoln had it exhumed and reinterred in concrete several feet thick in 1901.
Religious and philosophical beliefs.
In March 1860 in a speech in New Haven, Connecticut, Lincoln said, regarding slavery, "Whenever this question shall be settled, it must be settled on some philosophical basis. No policy that does not rest upon some philosophical public opinion can be permanently maintained." The philosophical basis for Lincoln's beliefs regarding slavery and other issues of the day require that Lincoln be examined "seriously as a man of ideas." Lincoln was a strong supporter of the American Whig version of liberal capitalism who, more than most politicians of the time, was able to express his ideas within the context of Nineteenth Century religious beliefs.
There were few people who strongly or directly influenced Lincoln's moral and intellectual development and perspectives. There was no teacher, mentor, church leader, community leader, or peer that Lincoln would credit in later years as a strong influence on his intellectual development. Lacking a formal education, Lincoln's personal philosophy was shaped by "an amazingly retentive memory and a passion for reading and learning." It was Lincoln's reading, rather than his relationships, that were most influential in shaping his personal beliefs.
Even as a child, Lincoln largely rejected organized religion, but the Calvinistic "doctrine of necessity" would remain a factor throughout his life. In 1846 Lincoln described the effect of this doctrine as "that the human mind is impelled to action, or held in rest by some power, over which the mind itself has no control."
In April 1864, in justifying his actions regarding Emancipation, Lincoln wrote, "I claim not to have controlled events, but confess plainly that events have controlled me. Now, at the end of three years struggle the nation's condition is not what either party, or any man devised, or expected. God alone can claim it."
As Lincoln matured, and especially during his term as president, the idea of a divine will somehow interacting with human affairs increasingly influenced his public expressions. On a personal level, the death of his son Willie in February 1862 may have caused Lincoln to look towards religion for answers and solace.
Lincoln's religious skepticism was fueled by his exposure to the ideas of the Lockean Enlightenment and classical liberalism, especially economic liberalism. Consistent with the common practice of the Whig party, Lincoln would often use the Declaration of Independence as the philosophical and moral expression of these two philosophies.
In a February 22, 1861 speech at Independence Hall in Philadelphia Lincoln said,
He found in the Declaration justification for Whig economic policy and opposition to territorial expansion and the nativist platform of the Know Nothings. In claiming that all men were created free, Lincoln and the Whigs argued that this freedom required economic advancement, expanded education, territory to grow, and the ability of the nation to absorb the growing immigrant population.
It was the Declaration of Independence, rather than the Bible, that Lincoln most relied on to oppose any further territorial expansion of slavery. He saw the Declaration as more than a political document. To him, as well as to many abolitionists and other antislavery leaders, it was, foremost, a moral document that had forever determined valuable principles for the future shaping of the nation.
Legacy and memorials.
Lincoln's death made the President a national martyr,
regarded by historians in numerous polls as among the greatest presidents in U.S. history, usually in the top three, along with George Washington and Franklin D. Roosevelt.
A study published in 2004, found that scholars in the fields of history and politics ranked Lincoln number one, while law scholars placed him second after Washington.
Among contemporary admirers, Lincoln is usually seen as personifying classical values of honesty and integrity, as well as respect for individual and minority rights, and human freedom in general.
Many American organizations of all purposes and agendas continue to cite his name and image, with interests ranging from the gay rights-supporting Log Cabin Republicans to the insurance corporation Lincoln National Corporation. The Lincoln automobile brand is also named after him.
The ballistic missile submarine "Abraham Lincoln" (SSBN-602) and the aircraft carrier "Abraham Lincoln" (CVN-72) were named in his honor.
During the Spanish Civil War, the American faction of the International Brigades named themselves the Abraham Lincoln Brigade.
Lincoln has been memorialized in many town, city, and county names,
Lincoln, Illinois, is the only city to be named for Abraham Lincoln before he became President.
Lincoln's name and image appear in numerous places. These include the Lincoln Memorial in Washington, D.C., the U.S. Lincoln $5 bill and the Lincoln cent, and Lincoln's sculpture on Mount Rushmore. Abraham Lincoln Birthplace National Historical Park in Hodgenville, Kentucky,
Lincoln Boyhood National Memorial in Lincoln City, Indiana,
and Lincoln Home National Historic Site in Springfield, Illinois,
In addition, New Salem, Illinois (a reconstruction of Lincoln's early adult hometown),
Ford's Theatre, and Petersen House (where he died) are all preserved as museums.
The state nickname for Illinois is "Land of Lincoln"; the slogan has appeared continuously on nearly all Illinois license plates issued since 1954.
Abraham Lincoln's birthday, February 12, was never a national holiday, but it was observed by 30 states. In 1971, Presidents Day became a national holiday, combining Lincoln's and Washington's birthdays, and replacing most states' celebration of his birthday.
As of 2005, Lincoln's Birthday is a legal holiday in 10 states.
The Abraham Lincoln Association was formed in 1908 to commemorate the centennial of Lincoln's birth.
The Association is now the oldest group dedicated to the study of Lincoln.
To commemorate his 200th birthday in February 2009, Congress established the Abraham Lincoln Bicentennial Commission (ALBC) in 2000 to honor Lincoln.
The Abraham Lincoln Presidential Library and Museum is located in Springfield and is run by the State of Illinois.
Lincoln owned a model 1857 Waltham William Ellery watch, with serial number 67613. This watch is now in the custody of the Smithsonian Museum.
On March 11, 2009, the National Museum of American History found a message engraved inside Lincoln's watch by a watchmaker named Jonathan Dillon who was repairing it at the outbreak of the American Civil War. The engraving reads (in part): "Fort Sumpter was attacked by the rebels" and "thank God we have a government."
---END.OF.DOCUMENT---

Aristotle.
Aristotle (, "Aristotélēs") (384 BC – 322 BC) was a Greek philosopher, a student of Plato and teacher of Alexander the Great. His writings cover many subjects, including physics, metaphysics, poetry, theater, music, logic, rhetoric, politics, government, ethics, biology, and zoology.
Together with Plato and Socrates (Plato's teacher), Aristotle is one of the most important founding figures in Western philosophy. Aristotle's writings constitute a first at creating a comprehensive system of Western philosophy, encompassing morality and aesthetics, logic and science, politics and metaphysics.
Aristotle's views on the physical sciences profoundly shaped medieval scholarship, and their influence extended well into the Renaissance, although they were ultimately replaced by Newtonian physics. In the biological sciences, some of his observations were confirmed to be accurate only in the nineteenth century. His works contain the earliest known formal study of logic, which was incorporated in the late nineteenth century into modern formal logic. In metaphysics, Aristotelianism had a profound influence on philosophical and theological thinking in the Islamic and Jewish traditions in the Middle Ages, and it continues to influence Christian theology, especially Eastern Orthodox theology, and the scholastic tradition of the Catholic Church. His ethics, though always influential, gained renewed interest with the modern advent of virtue ethics. All aspects of Aristotle's philosophy continue to be the object of active academic study today. Though Aristotle wrote many elegant treatises and dialogues (Cicero described his literary style as "a river of gold"), it is thought that the majority of his writings are now lost and only about one-third of the original works have survived.
Despite the far-reaching appeal that Aristotle's works have traditionally enjoyed, today modern scholarship questions a substantial portion of the Aristotelian corpus as authentically Aristotle's own.
Life.
Aristotle was born in Stageira, Chalcidice, in 384 BC, about east of modern-day Thessaloniki. His father Nicomachus was the personal physician to King Amyntas of Macedon. Aristotle was trained and educated as a member of the aristocracy. At about the age of eighteen, he went to Athens to continue his education at Plato's Academy. Aristotle remained at the academy for nearly twenty years, not leaving until after Plato's death in 347 BC. He then traveled with Xenocrates to the court of his friend Hermias of Atarneus in Asia Minor. While in Asia, Aristotle traveled with Theophrastus to the island of Lesbos, where together they researched the botany and zoology of the island. Aristotle married Hermias's adoptive daughter (or niece) Pythias. She bore him a daughter, whom they named Pythias. Soon after Hermias' death, Aristotle was invited by Philip II of Macedon to become the tutor to his son Alexander the Great in 343 B.C.
Aristotle was appointed as the head of the royal academy of Macedon. During that time he gave lessons not only to Alexander, but also to two other future kings: Ptolemy and Cassander. In his "Politics", Aristotle states that only one thing could justify monarchy, and that was if the virtue of the king and his family were greater than the virtue of the rest of the citizens put together. Tactfully, he included the young prince and his father in that category. Aristotle encouraged Alexander toward eastern conquest, and his attitude towards Persia was unabashedly ethnocentric. In one famous example, he counsels Alexander to be 'a leader to the Greeks and a despot to the barbarians, to look after the former as after friends and relatives, and to deal with the latter as with beasts or plants'.
By 335 BC he had returned to Athens, establishing his own school there known as the Lyceum. Aristotle conducted courses at the school for the next twelve years. While in Athens, his wife Pythias died and Aristotle became involved with Herpyllis of Stageira, who bore him a son whom he named after his father, Nicomachus. According to the Suda, he also had an eromenos, Palaephatus of Abydus.
It is during this period in Athens from 335 to 323 BC when Aristotle is believed to have composed many of his works. Aristotle wrote many dialogues, only fragments of which survived. The works that have survived are in treatise form and were not, for the most part, intended for widespread publication, as they are generally thought to be lecture aids for his students. His most important treatises include "Physics", "Metaphysics", "Nicomachean Ethics", "Politics", "De Anima (On the Soul)" and "Poetics".
Aristotle not only studied almost every subject possible at the time, but made significant contributions to most of them. In physical science, Aristotle studied anatomy, astronomy, embryology, geography, geology, meteorology, physics and zoology. In philosophy, he wrote on aesthetics, ethics, government, metaphysics, politics, economics, psychology, rhetoric and theology. He also studied education, foreign customs, literature and poetry. His combined works constitute a virtual encyclopedia of Greek knowledge. It has been suggested that Aristotle was probably the last person to know everything there was to be known in his own time.
Near the end of Alexander's life, Alexander began to suspect plots against himself, and threatened Aristotle in letters. Aristotle had made no secret of his contempt for Alexander's pretense of divinity, and the king had executed Aristotle's grandnephew Callisthenes as a traitor. A widespread tradition in antiquity suspected Aristotle of playing a role in Alexander's death, but there is little evidence for this.
Upon Alexander's death, anti-Macedonian sentiment in Athens once again flared. Eurymedon the hierophant denounced Aristotle for not holding the gods in honor. Aristotle fled the city to his mother's family estate in Chalcis, explaining, "I will not allow the Athenians to sin twice against philosophy," a reference to Athens's prior trial and execution of Socrates. However, he died in Euboea of natural causes within the year (in 322 BC). Aristotle named chief executor his student Antipater and left a will in which he asked to be buried next to his wife.
Logic.
With the "Prior Analytics", Aristotle is credited with the earliest study of formal logic, and his conception of it was the dominant form of Western logic until 19th century advances in mathematical logic. Kant stated in the "Critique of Pure Reason" that Aristotle's theory of logic completely accounted for the core of deductive inference.
History.
Aristotle "says that 'on the subject of reasoning' he 'had nothing else on an earlier date to speak of'". However, Plato reports that syntax was devised before him, by Prodicus of Ceos, who was concerned by the correct use of words. Logic seems to have emerged from dialectics; the earlier philosophers made frequent use of concepts like "reductio ad absurdum" in their discussions, but never truly understood the logical implications. Even Plato had difficulties with logic; although he had a reasonable conception of a deducting system, he could never actually construct one and relied instead on his dialectic. Plato believed that deduction would simply follow from premises, hence he focused on maintaining solid premises so that the conclusion would logically follow. Consequently, Plato realized that a method for obtaining conclusions would be most beneficial. He never succeeded in devising such a method, but his best attempt was published in his book "Sophist", where he introduced his division method.
Analytics and the "Organon".
The order of the books (or the teachings from which they are composed) is not certain, but this list was derived from analysis of Aristotle's writings. It goes from the basics, the analysis of simple terms in the "Categories," the analysis of propositions and their elementary relations in "On Interpretation", to the study of more complex forms, namely, syllogisms (in the "Analytics") and dialectics (in the "Topics" and "Sophistical Refutations"). The first three treatises form the core of the logical theory "stricto sensu": the grammar of the language of logic and the correctness rules of reasoning. There is one volume of Aristotle's concerning logic not found in the "Organon", namely the fourth book of "Metaphysics.".
Aristotle's scientific method.
Like his teacher Plato, Aristotle's philosophy aims at the universal. Aristotle, however, found the universal in particular things, which he called the essence of things, while Plato finds that the universal exists apart from particular things, and is related to them as their prototype or exemplar. For Aristotle, therefore, philosophic method implies the ascent from the study of particular phenomena to the knowledge of essences, while for Plato philosophic method means the descent from a knowledge of universal Forms (or ideas) to a contemplation of particular imitations of these. For Aristotle, "form" still refers to the unconditional basis of phenomena but is "instantiated" in a particular substance (see "Universals and particulars", below). In a certain sense, Aristotle's method is both inductive and deductive, while Plato's is essentially deductive from "a priori" principles.
In Aristotle's terminology, "natural philosophy" is a branch of philosophy examining the phenomena of the natural world, and includes fields that would be regarded today as physics, biology and other natural sciences. In modern times, the scope of "philosophy" has become limited to more generic or abstract inquiries, such as ethics and metaphysics, in which logic plays a major role. Today's philosophy tends to exclude empirical study of the natural world by means of the scientific method. In contrast, Aristotle's philosophical endeavors encompassed virtually all facets of intellectual inquiry.
In the larger sense of the word, Aristotle makes philosophy coextensive with reasoning, which he also would describe as "science". Note, however, that his use of the term "science" carries a different meaning than that covered by the term "scientific method". For Aristotle, "all science ("dianoia") is either practical, poetical or theoretical" ("Metaphysics" 1025b25). By practical science, he means ethics and politics; by poetical science, he means the study of poetry and the other fine arts; by theoretical science, he means physics, mathematics and metaphysics.
If logic (or "analytics") is regarded as a study preliminary to philosophy, the divisions of Aristotelian philosophy would consist of: (1) Logic; (2) Theoretical Philosophy, including Metaphysics, Physics, Mathematics, (3) Practical Philosophy and (4) Poetical Philosophy.
In the period between his two stays in Athens, between his times at the Academy and the Lyceum, Aristotle conducted most of the scientific thinking and research for which he is renowned today. In fact, most of Aristotle's life was devoted to the study of the objects of natural science. Aristotle's metaphysics contains observations on the nature of numbers but he made no original contributions to mathematics. He did, however, perform original research in the natural sciences, e.g., botany, zoology, physics, astronomy, chemistry, meteorology, and several other sciences.
Aristotle's writings on science are largely qualitative, as opposed to quantitative. Beginning in the sixteenth century, scientists began applying mathematics to the physical sciences, and Aristotle's work in this area was deemed hopelessly inadequate. His failings were largely due to the absence of concepts like mass, velocity, force and temperature. He had a conception of speed and temperature, but no quantitative understanding of them, which was partly due to the absence of basic experimental devices, like clocks and thermometers.
His writings provide an account of many scientific observations, a mixture of precocious accuracy and curious errors. For example, in his "History of Animals" he claimed that human males have more teeth than females and in the "Generation of Animals" he said the female is as it were a deformed male.
In a similar vein, John Philoponus, and later Galileo, showed by simple experiments that Aristotle's theory that a heavier object falls faster than a lighter object is incorrect. On the other hand, Aristotle refuted Democritus's claim that the Milky Way was made up of "those stars which are shaded by the earth from the sun's rays," pointing out (correctly, even if such reasoning was bound to be dismissed for a long time) that, given "current astronomical demonstrations" that "the size of the sun is greater than that of the earth and the distance of the stars from the earth many times greater than that of the sun, then...the sun shines on all the stars and the earth screens none of them."
In places, Aristotle goes too far in deriving 'laws of the universe' from simple observation and over-stretched reason. Today's scientific method assumes that such thinking without sufficient facts is ineffective, and that discerning the validity of one's hypothesis requires far more rigorous experimentation than that which Aristotle used to support his laws.
Aristotle also had some scientific blind spots. He posited a geocentric cosmology that we may discern in selections of the "Metaphysics", which was widely accepted up until the 1500s. From the 3rd century to the 1500s, the dominant view held that the Earth was the center of the universe (geocentrism).
Since he was perhaps the philosopher most respected by European thinkers during and after the Renaissance, these thinkers often took Aristotle's erroneous positions as given, which held back science in this epoch. However, Aristotle's scientific shortcomings should not mislead one into forgetting his great advances in the many scientific fields. For instance, he founded logic as a formal science and created foundations to biology that were not superseded for two millennia. Moreover, he introduced the fundamental notion that nature is composed of things that change and that studying such changes can provide useful knowledge of underlying constants.
The five elements.
Each of the four earthly elements has its natural place; the earth at the centre of the universe, then water, then air, then fire. When they are out of their natural place they have natural motion, requiring no external cause, which is towards that place; so bodies sink in water, air bubbles rise up, rain falls, flame rises in air. The heavenly element has perpetual circular motion.
Causality, The Four Causes===.
Additionally, things can be causes of one another, causing each other reciprocally, as hard work causes fitness and vice versa, although not in the same way or function, the one is as the beginning of change, the other as the goal. (Thus Aristotle first suggested a reciprocal or circular causality as a relation of mutual dependence or influence of cause upon effect). Moreover, Aristotle indicated that the same thing can be the cause of contrary effects; its presence and absence may result in different outcomes. Simply it is the goal or purpose that brings about an event (not necessarily a mental goal). Taking our two dominos, it requires someone to intentionally knock the dominos over as they cannot fall themselves.
Aristotle marked two modes of causation: proper (prior) causation and accidental (chance) causation. All causes, proper and incidental, can be spoken as potential or as actual, particular or generic. The same language refers to the effects of causes, so that generic effects assigned to generic causes, particular effects to particular causes, operating causes to actual effects. Essentially, causality does not suggest a temporal relation between the cause and the effect.
All further investigations of causality will consist of imposing the favorite hierarchies on the order causes, such as final > efficient > material > formal (Thomas Aquinas), or of restricting all causality to the material and efficient causes or to the efficient causality (deterministic or chance) or just to regular sequences and correlations of natural phenomena (the natural sciences describing how things happen instead of explaining the whys and wherefores).
Optics.
Aristotle held more accurate theories on some optical concepts than other philosophers of his day. The earliest known written evidence of a camera obscura can be found in Aristotle's documentation of such a device in 350 BC in "Problemata". Aristotle's apparatus contained a dark chamber that had a single small hole, or aperture, to allow for sunlight to enter. Aristotle used the device to make observations of the sun and noted that no matter what shape the hole was, the sun would still be correctly displayed as a round object. In modern cameras, this is analogous to the diaphragm. Aristotle also made the observation that when the distance between the tiny hole and the surface with the image increased, the image was amplified.
Chance and spontaneity.
Spontaneity and chance are causes of effects. Chance as an incidental cause lies in the realm of accidental things. It is "from what is spontaneous" (but note that what is spontaneous does not come from chance). For a better understanding of Aristotle's conception of "chance" it might be better to think of "coincidence": Something takes place by chance if a person sets out with the intent of having one thing take place, but with the result of another thing (not intended) taking place. For example: A person seeks donations. That person may find another person willing to donate a substantial sum. However, if the person seeking the donations met the person donating, not for the purpose of collecting donations, but for some other purpose, Aristotle would call the collecting of the donation by that particular donator a result of chance. It must be unusual that something happens by chance. In other words, if something happens all or most of the time, we cannot say that it is by chance.
There is also more specific kind of chance, which Aristotle names "luck", that can only apply to human beings, since it is in the sphere of moral actions. According to Aristotle, luck must involve choice (and thus deliberation), and only humans are capable of deliberation and choice. "What is not capable of action cannot do anything by chance".
Metaphysics.
Aristotle defines metaphysics as "the knowledge of immaterial being," or of "being in the highest degree of abstraction." He refers to metaphysics as "first philosophy", as well as "the theologic science."
Substance, potentiality and actuality.
Aristotle examines the concept of substance and essence ("ousia") in his "Metaphysics", Book VII and he concludes that a particular substance is a combination of both matter and form. As he proceeds to the book VIII, he concludes that the matter of the substance is the substratum or the stuff of which it is composed, "e.g." the matter of the house are the bricks, stones, timbers etc., or whatever constitutes the "potential" house. While the form of the substance, is the "actual" house, namely 'covering for bodies and chattels' or any other differentia (see also predicables). The formula that gives the components is the account of the matter, and the formula that gives the differentia is the account of the form.
With regard to the change ("kinesis") and its causes now, as he defines in his Physics and On Generation and Corruption 319b-320a, he distinguishes the coming to be from: 1) growth and diminution, which is change in quantity; 2) locomotion, which is change in space; and 3) alteration, which is change in quality.
The coming to be is a change where nothing persists of which the resultant is a property. In that particular change he introduces the concept of potentiality ("dynamis") and actuality ("entelecheia") in association with the matter and the form.
Referring to potentiality, this is what a thing is capable of doing, or being acted upon, if it is not prevented by something else. For example, the seed of a plant in the soil is potentially ("dynamei") plant, and if is not prevented by something, it will become a plant. Potentially beings can either 'act' ("poiein") or 'be acted upon' ("paschein"), which can be either innate or learned. For example, the eyes possess the potentiality of sight (innate – being acted upon), while the capability of playing the flute can be possessed by learning (exercise – acting).
Actuality is the fulfillment of the end of the potentiality. Because the end ("telos") is the principle of every change, and for the sake of the end exists potentiality, therefore actuality is the end. Referring then to our previous example, we could say that actuality is when the seed of the plant becomes a plant.
" For that for the sake of which a thing is, is its principle, and the becoming is for the sake of the end; and the actuality is the end, and it is for the sake of this that the potentiality is acquired. For animals do not see in order that they may have sight, but they have sight that they may see."
In conclusion, the matter of the house is its potentiality and the form is its actuality. The formal cause ("aitia") then of that change from potential to actual house, is the reason ("logos") of the house builder and the final cause is the end, namely the house itself. Then Aristotle proceeds and concludes that the actuality is prior to potentiality in formula, in time and in substantiality.
With this definition of the particular substance (i.e., matter and form), Aristotle tries to solve the problem of the unity of the beings, "e.g.", what is that makes the man one? Since, according to Plato there are two Ideas: animal and biped, how then is man a unity? However, according to Aristotle, the potential being (matter) and the actual one (form) are one and the same thing.
Universals and particulars.
Aristotle's predecessor, Plato, argued that all things have a universal form, which could be either a property, or a relation to other things. When we look at an apple, for example, we see an apple, and we can also analyze a form of an apple. In this distinction, there is a particular apple and a universal form of an apple. Moreover, we can place an apple next to a book, so that we can speak of both the book and apple as being next to each other.
Plato argued that there are some universal forms that are not a part of particular things. For example, it is possible that there is no particular good in existence, but "good" is still a proper universal form. Bertrand Russell is a contemporary philosopher that agreed with Plato on the existence of "uninstantiated universals".
Aristotle disagreed with Plato on this point, arguing that all universals are instantiated. Aristotle argued that there are no universals that are unattached to existing things. According to Aristotle, if a universal exists, either as a particular or a relation, then there must have been, must be currently, or must be in the future, something on which the universal can be predicated. Consequently, according to Aristotle, if it is not the case that some universal can be predicated to an object that exists at some period of time, then it does not exist.
In addition, Aristotle disagreed with Plato about the location of universals. As Plato spoke of the world of the forms, a location where all universal forms subsist, Aristotle maintained that universals exist within each thing on which each universal is predicated. So, according to Aristotle, the form of apple exists within each apple, rather than in the world of the forms.
Biology and medicine.
In Aristotelian science, most especially in biology, things he saw himself have stood the test of time better than his retelling of the reports of others, which contain error and superstition. He dissected animals, but not humans and his ideas on how the human body works have been almost entirely superseded.
Empirical research program.
Aristotle is the earliest natural historian whose work has survived in some detail. Aristotle certainly did research on the natural history of Lesbos, and the surrounding seas and neighbouring areas. The works that reflect this research, such as "History of Animals", "Generation of Animals", and "Parts of Animals", contain some observations and interpretations, along with sundry myths and mistakes. The most striking passages are about the sea-life visible from observation on Lesbos and available from the catches of fishermen. His observations on catfish, electric fish ("Torpedo") and angler-fish are detailed, as is his writing on cephalopods, namely, "Octopus", "Sepia" (cuttlefish) and the paper nautilus ("Argonauta argo"). His description of the hectocotyl arm was about two thousand years ahead of its time, and widely disbelieved until its rediscovery in the nineteenth century. He separated the aquatic mammals from fish, and knew that sharks and rays were part of the group he called Selachē (selachians).
Another good example of his methods comes from the "Generation of Animals" in which Aristotle describes breaking open fertilized chicken eggs at intervals to observe when visible organs were generated.
He gave accurate descriptions of ruminants' four-chambered fore-stomachs, and of the ovoviviparous embryological development of the hound shark "Mustelus mustelus".
Classification of living things.
Aristotle's classification of living things contains some elements which still existed in the nineteenth century. What the modern zoologist would call vertebrates and invertebrates, Aristotle called 'animals with blood' and 'animals without blood' (he was not to know that complex invertebrates do make use of haemoglobin, but of a different kind from vertebrates). Animals with blood were divided into live-bearing (humans and mammals), and egg-bearing (birds and fish). Invertebrates ('animals without blood') are insects, crustacea (divided into non-shelled – cephalopods – and shelled) and testacea (molluscs). In some respects, this incomplete classification is better than that of Linnaeus, who crowded the invertebrata together into two groups, Insecta and Vermes (worms).
For Charles Singer, "Nothing is more remarkable than [Aristotle's] efforts to [exhibit] the relationships of living things as a "scala naturae" Aristotle's "History of Animals" classified organisms in relation to a hierarchical "Ladder of Life" ("scala naturae"), placing them according to complexity of structure and function so that higher organisms showed greater vitality and ability to move.
Aristotle believed that intellectual purposes, i.e., formal causes, guided all natural processes. Such a teleological view gave Aristotle cause to justify his observed data as an expression of formal design. Noting that "no animal has, at the same time, both tusks and horns," and "a single-hooved animal with two horns I have never seen," Aristotle suggested that Nature, giving no animal both horns and tusks, was staving off vanity, and giving creatures faculties only to such a degree as they are necessary. Noting that ruminants had a multiple stomachs and weak teeth, he supposed the first was to compensate for the latter, with Nature trying to preserve a type of balance.
In a similar fashion, Aristotle believed that creatures were arranged in a graded scale of perfection rising from plants on up to man, the "scala naturae" or Great Chain of Being. His system had eleven grades, arranged according "to the degree to which they are infected with potentiality", expressed in their form at birth. The highest animals laid warm and wet creatures alive, the lowest bore theirs cold, dry, and in thick eggs.
Aristotle also held that the level of a creature's perfection was reflected in its form, but not preordained by that form. Ideas like this, and his ideas about souls, are not regarded as science at all in modern times.
He placed emphasis on the type(s) of soul an organism possessed, asserting that plants possess a vegetative soul, responsible for reproduction and growth, animals a vegetative and a sensitive soul, responsible for mobility and sensation, and humans a vegetative, a sensitive, and a rational soul, capable of thought and reflection.
Aristotle, in contrast to earlier philosophers, but in accordance with the Egyptians, placed the rational soul in the heart, rather than the brain. Notable is Aristotle's division of sensation and thought, which generally went against previous philosophers, with the exception of Alcmaeon.
Successor: Theophrastus.
Aristotle's successor at the Lyceum, Theophrastus, wrote a series of books on botany—the "History of Plants"—which survived as the most important contribution of antiquity to botany, even into the Middle Ages. Many of Theophrastus' names survive into modern times, such as "carpos" for fruit, and "pericarpion" for seed vessel.
Rather than focus on formal causes, as Aristotle did, Theophrastus suggested a mechanistic scheme, drawing analogies between natural and artificial processes, and relying on Aristotle's concept of the efficient cause. Theophrastus also recognized the role of sex in the reproduction of some higher plants, though this last discovery was lost in later ages.
Influence on Hellenistic medicine.
After Theophrastus, the Lyceum failed to produce any original work. Though interest in Aristotle's ideas survived, they were generally taken unquestioningly. It is not until the age of Alexandria under the Ptolemies that advances in biology can be again found.
The first medical teacher at Alexandria Herophilus of Chalcedon, corrected Aristotle, placing intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. Though a few ancient atomists such as Lucretius challenged the teleological viewpoint of Aristotelian ideas about life, teleology (and after the rise of Christianity, natural theology) would remain central to biological thought essentially until the 18th and 19th centuries. Ernst Mayr claimed that there was "nothing of any real consequence in biology after Lucretius and Galen until the Renaissance." Aristotle's ideas of natural history and medicine survived, but they were generally taken unquestioningly.
Ethics.
Aristotle considered ethics to be a practical rather than theoretical study, i.e., one aimed at doing good rather than knowing for its own sake. He wrote several treatises on ethics, including most notably, the "Nichomachean Ethics".
Aristotle taught that virtue has to do with the proper function ("ergon") of a thing. An eye is only a good eye in so much as it can see, because the proper function of an eye is sight. Aristotle reasoned that humans must have a function specific to humans, and that this function must be an activity of the "psuchē" (normally translated as "soul") in accordance with reason ("logos"). Aristotle identified such an optimum activity of the soul as the aim of all human deliberate action, "eudaimonia", generally translated as "happiness" or sometimes "well being". To have the potential of ever being happy in this way necessarily requires a good character ("ēthikē" "aretē"), often translated as moral (or ethical) virtue (or excellence).
Aristotle taught that to achieve a virtuous and potentially happy character requires a first stage of having the fortune to be habituated not deliberately, but by teachers, and experience, leading to a later stage in which one consciously chooses to do the best things. When the best people come to live life this way their practical wisdom ("phronēsis") and their intellect ("nous") can develop with each other towards the highest possible ethical virtue, that of wisdom.
Politics.
In addition to his works on ethics, which address the individual, Aristotle addressed the city in his work titled "Politics". Aristotle's conception of the city is organic, and he is considered one of the first to conceive of the city in this manner. Aristotle considered the city to be a natural community. Moreover, he considered the city to be prior to the family which in turn is prior to the individual, i.e., last in the order of becoming, but first in the order of being. He is also famous for his statement that "man is by nature a political animal." Aristotle conceived of politics as being like an organism rather than like a machine, and as a collection of parts none of which can exist without the others.
It should be noted that the modern understanding of a political community is that of the state. However, the state was foreign to Aristotle. He referred to political communities as cities. Aristotle understood a city as a political "partnership". Subsequently, a city is created not to avoid injustice or for economic stability, but rather to live a good life: "The political partnership must be regarded, therefore, as being for the sake of noble actions, not for the sake of living together". This can be distinguished from the social contract theory which individuals leave the state of nature because of "fear of violent death" or its "inconveniences."
Rhetoric and poetics.
Aristotle considered epic poetry, tragedy, comedy, dithyrambic poetry and music to be imitative, each varying in imitation by medium, object, and manner. For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their manner of imitation – through narrative or character, through change or no change, and through drama or no drama. Aristotle believed that imitation is natural to mankind and constitutes one of mankind's advantages over animals.
While it is believed that Aristotle's "Poetics" comprised two books – one on comedy and one on tragedy – only the portion that focuses on tragedy has survived. Aristotle taught that tragedy is composed of six elements: plot-structure, character, style, spectacle, and lyric poetry. The characters in a tragedy are merely a means of driving the story; and the plot, not the characters, is the chief focus of tragedy. Tragedy is the imitation of action arousing pity and fear, and is meant to effect the catharsis of those same emotions. Aristotle concludes "Poetics" with a discussion on which, if either, is superior: epic or tragic mimesis. He suggests that because tragedy possesses all the attributes of an epic, possibly possesses additional attributes such as spectacle and music, is more unified, and achieves the aim of its mimesis in shorter scope, it can be considered superior to epic.
Aristotle was a keen systematic collector of riddles, folklore, and proverbs; he and his school had a special interest in the riddles of the Delphic Oracle and studied the fables of Aesop.
Modern scholarship.
Modern scholarship reveals that Aristotle's "lost" works stray considerably in characterization from the surviving Aristotelian corpus. Whereas the lost works appear to have been originally written with an intent for subsequent publication, the surviving works do not appear to have been so. Rather the surviving works mostly resemble lectures unintended for publication. The authenticity of a portion of the surviving works as originally Aristotelian is also today held suspect, with some books duplicating or summarizing each other, the authorship of one book questioned and another book considered to be unlikely Aristotle's at all.
Some of the individual works within the corpus, including the "Constitution of Athens," are regarded by most scholars as products of Aristotle's "school," perhaps compiled under his direction or supervision. Others, such as "On Colors," may have been produced by Aristotle's successors at the Lyceum, e.g., Theophrastus and Straton. Still others acquired Aristotle's name through similarities in doctrine or content, such as the "De Plantis," possibly by Nicolaus of Damascus. Other works in the corpus include medieval palmistries and astrological and magical texts whose connections to Aristotle are purely fanciful and self-promotional.
Loss of his works.
According to a distinction that originates with Aristotle himself, his writings are divisible into two groups: the "exoteric" and the "esoteric". Most scholars have understood this as a distinction between works Aristotle intended for the public (exoteric), and the more technical works (esoteric) intended for the narrower audience of Aristotle's students and other philosophers who were familiar with the jargon and issues typical of the Platonic and Aristotelian schools. Another common assumption is that none of the exoteric works is extant – that all of Aristotle's extant writings are of the esoteric kind. Current knowledge of what exactly the exoteric writings were like is scant and dubious, though many of them may have been in dialogue form. ("Fragments" of some of Aristotle's dialogues have survived.) Perhaps it is to these that Cicero refers when he characterized Aristotle's writing style as "a river of gold"; it is hard for many modern readers to accept that one could seriously so admire the style of those works currently available to us. However, some modern scholars have warned that we cannot know for certain that Cicero's praise was reserved specifically for the exoteric works; a few modern scholars have actually admired the concise writing style found in Aristotle's extant works.
One major question in the history of Aristotle's works, then, is how were the exoteric writings all lost, and how did the ones we now possess come to us? The story of the original manuscripts of the esoteric treatises is described by Strabo in his "Geography" and Plutarch in his "Parallel Lives". The manuscripts were left from Aristotle to his successor Theophrastus, who in turn willed them to Neleus of Scepsis. Neleus supposedly took the writings from Athens to Scepsis, where his heirs let them languish in a cellar until the first century BC, when Apellicon of Teos discovered and purchased the manuscripts, bringing them back to Athens. According to the story, Apellicon tried to repair some of the damage that was done during the manuscripts' stay in the basement, introducing a number of errors into the text. When Lucius Cornelius Sulla occupied Athens in 86 BC, he carried off the library of Apellicon to Rome, where they were first published in 60 BC by the grammarian Tyrannion of Amisus and then by philosopher Andronicus of Rhodes.
Carnes Lord attributes the popular belief in this story to the fact that it provides "the most plausible explanation for the rapid eclipse of the Peripatetic school after the middle of the third century, and for the absence of widespread knowledge of the specialized treatises of Aristotle throughout the Hellenistic period, as well as for the sudden reappearance of a flourishing Aristotelianism during the first century B.C." Lord voices a number of reservations concerning this story, however. First, the condition of the texts is far too good for them to have suffered considerable damage followed by Apellicon's inexpert attempt at repair. Second, there is "incontrovertible evidence," Lord says, that the treatises were in circulation during the time in which Strabo and Plutarch suggest they were confined within the cellar in Scepsis. Third, the definitive edition of Aristotle's texts seems to have been made in Athens some fifty years before Andronicus supposedly compiled his. And fourth, ancient library catalogues predating Andronicus' intervention list an Aristotelian corpus quite similar to the one we currently possess. Lord sees a number of post-Aristotelian interpolations in the "Politics", for example, but is generally confident that the work has come down to us relatively intact.
As the influence of the "falsafa" grew in the West, in part due to Gerard of Cremona's translations and the spread of Averroism, the demand for Aristotle's works grew. William of Moerbeke translated a number of them into Latin. When Thomas Aquinas wrote his theology, working from Moerbeke's translations, the demand for Aristotle's writings grew and the Greek manuscripts returned to the West, stimulating a revival of Aristotelianism in Europe, and ultimately revitalizing European thought through Muslim influence in Spain to fan the embers of the Renaissance.
Development of logic.
Twenty-three hundred years after his death, Aristotle remains one of the most influential people who ever lived. He was the founder of formal logic, pioneered the study of zoology, and left every future scientist and philosopher in his debt through his contributions to the scientific method. Despite these accolades, many of Aristotle's errors held back science considerably. Bertrand Russell notes that "almost every serious intellectual advance has had to begin with an attack on some Aristotelian doctrine". Russell also refers to Aristotle's ethics as "repulsive", and calls his logic "as definitely antiquated as Ptolemaic astronomy". Russell notes that these errors make it difficult to do historical justice to Aristotle, until one remembers how large of an advance he made upon all of his predecessors. Of course, the problem of excessive devotion to Aristotle is more a problem of those later centuries and not of Aristotle himself.
Later Greek philosophers.
The immediate influence of Aristotle's work was felt as the Lyceum grew into the Peripatetic school. Aristotle's notable students included Aristoxenus, Dicaearchus, Demetrius of Phalerum, Eudemos of Rhodes, Harpalus, Hephaestion, Meno, Mnason of Phocis, Nicomachus, and Theophrastus. Aristotle's influence over Alexander the Great is seen in the latter's bringing with him on his expedition a host of zoologists, botanists, and researchers. He had also learned a great deal about Persian customs and traditions from his teacher. Although his respect for Aristotle was diminished as his travels made it clear that much of Aristotle's geography was clearly wrong, when the old philosopher released his works to the public, Alexander complained "Thou hast not done well to publish thy acroamatic doctrines; for in what shall I surpass other men if those doctrines wherein I have been trained are to be all men's common property?"
Influence on Christian theologians.
Aristotle is referred to as "The Philosopher" by Scholastic thinkers such as Thomas Aquinas. See "Summa Theologica", Part I, Question 3, etc. These thinkers blended Aristotelian philosophy with Christianity, bringing the thought of Ancient Greece into the Middle Ages. It required a repudiation of some Aristotelian principles for the sciences and the arts to free themselves for the discovery of modern scientific laws and empirical methods. The medieval English poet Chaucer describes his student as being happy by having
The Italian poet Dante says of Aristotle in the first circles of hell,
Views on women.
Aristotle believed that women are colder than men and thus a lower form of life. His assumption carried forward unexamined to Galen and others for almost two thousand years until the sixteenth century. He also believed that females could not be fully human. His analysis of procreation is frequently criticized on the grounds that it presupposes an active, ensouling masculine element bringing life to an inert, passive, lumpen female element; it is on these grounds that Aristotle is considered by some feminist critics to have been a misogynist.
On the other hand, Aristotle gave equal weight to women's happiness as he did to men's, and commented in his Rhetoric that a society cannot be happy unless women are happy too. In places like Sparta where the lot of women is bad, there can only be half-happiness in society.(see Rhetoric 1.5.6)
Post-Enlightenment thinkers.
The German philosopher Friedrich Nietzsche has been said to have taken nearly all of his political philosophy from Aristotle. However implausible this is, it is certainly the case that Aristotle's rigid separation of action from production, and his justification of the subservience of slaves and others to the virtue – or "arete" – of a few justified the ideal of aristocracy. It is Martin Heidegger, not Nietzsche, who elaborated a new interpretation of Aristotle, intended to warrant his deconstruction of scholastic and philosophical tradition. More recently, Alasdair MacIntyre has attempted to reform what he calls the Aristotelian tradition in a way that is anti-elitist and capable of disputing the claims of both liberals and Nietzscheans.
List of works.
The works of Aristotle that have survived from antiquity through Mediæval manuscript transmission are collected in the Corpus Aristotelicum. These texts, as opposed to Aristotle's lost works, are technical philosophical treatises from within Aristotle's school. Reference to them is made according to the organization of Immanuel Bekker's Royal Prussian Academy edition ("Aristotelis Opera edidit Academia Regia Borussica", Berlin, 1831-1870), which in turn is based on ancient classifications of these works.
Further reading.
The secondary literature on Aristotle is vast. The following references are only a small selection.
---END.OF.DOCUMENT---

An American in Paris.
"An American in Paris" is a symphonic composition by American composer George Gershwin, composed in 1928. Inspired by time Gershwin had spent in Paris, it is in the form of an extended tone poem evoking the sights and energy of the French capital in the 1920s. It is one of Gershwin's best-known compositions.
Gershwin composed the piece on commission from the New York Philharmonic. He also did the orchestration. (He did not orchestrate his musicals.) Gershwin scored "An American in Paris" for the standard instruments of the symphony orchestra plus celesta, saxophone, and automobile horns. Gershwin brought back some Parisian taxi horns for the New York premiere of the composition which took place on December 13, 1928 in Carnegie Hall with Walter Damrosch conducting the New York Philharmonic.
Gershwin collaborated on the original program notes with the critic and composer Deems Taylor, noting that: "My purpose here is to portray the impression of an American visitor in Paris as he strolls about the city and listens to various street noises and absorbs the French atmosphere." When the tone poem moves into the blues, "our American friend... has succumbed to a spasm of homesickness." But, "nostalgia is not a fatal disease." The American visitor "once again is an alert spectator of Parisian life" and "the street noises and French atmosphere are triumphant."
Instrumentation.
"An American in Paris" is scored for 3 flutes (3rd doubling on piccolo), 2 oboes, English horn, 2 clarinets in B flat, bass clarinet in B flat, 2 bassoons, 4 horns in F, 3 trumpets in B flat, 3 trombones, tuba, timpani, snare drum, bass drum, cymbals, low and high tom-toms, xylophone, glockenspiel, celesta, 4 taxi horns, alto saxophone/soprano saxophone, tenor saxophone/soprano saxophone/alto saxophone, baritone saxophone/soprano saxophone/alto saxophone, and strings.
The revised edition by F Campbell-Watson calls for three saxophones, alto, tenor and baritone. In this arrangement the soprano and alto doublings have been rewritten to avoid changing instruments.
Recordings.
"An American in Paris" has been frequently recorded over the years. The very first recording was made for RCA Victor in 1929 with Nathaniel Shilkret conducting the Victor Symphony Orchestra, drawn from members of the Philadelphia Orchestra. Gershwin was on hand to "supervise" the recording; however, Shilkret was reported to be in charge and eventually asked the composer to leave the recording studio. Then, a little later, Shilkret discovered there was no one to play the brief celesta solo during the slow section, so he hastily asked Gershwin if he might play the solo; Gershwin said he could and so he briefly participated in the actual recording. The radio broadcast of the September 8, 1937 Hollywood Bowl George Gershwin Memorial Concert, in which "An American in Paris," also conducted by Shilkret, was second on the program, was recorded and was released in 1998 in a two-CD set. Arthur Fiedler and the Boston Pops Orchestra recorded the work for RCA Victor, including one of the first stereo recordings of the music. In 1945, Arturo Toscanini and the NBC Symphony Orchestra recorded the music in Carnegie Hall, one of the few commercial recordings Toscanini made of music by an American composer. The Seattle Symphony also recorded a version in the 1980's of Gershwin's original score, before he committed to numerous edits resulting in the score as we hear it today.
In 1951, MGM released a musical comedy, "An American in Paris", featuring Gene Kelly and Leslie Caron. Winner of numerous awards, including the 1951 Best Picture Oscar, the film was directed by Vincente Minnelli, featured many tunes of Gershwin, and concluded with an extensive, elaborate dance sequence built around Gershwin's symphonic poem (arranged for the film by Johnny Green).
A part of the symphonic composition is also featured in "As Good as It Gets", released in 1997.
---END.OF.DOCUMENT---

Academy Award for Best Art Direction.
The Academy Awards are the oldest awards ceremony for achievements in motion pictures. The Academy Award for Best Art Direction recognizes achievement in art direction on a film. The films below are listed with their production year, so the Oscar 2000 for best art direction went to a film from 1999. In the lists below, the winner of the award for each year is shown first, followed by the other nominees.
1920s.
This award was originally for Interior Decoration
1930s.
With the awards for 1940 the award was divided into separate awards for black-and-white and color movies.
1940s.
Beginning with 1947 movies the name of the award was changed to Art Direction - Set Decoration.
1950s.
For 1957 films this award became a single award.
With the 1959 films this category was again divided in two
1960s.
For 1967 the two awards in this category were recombined into a single award.
---END.OF.DOCUMENT---

Academy Award.
The Academy Award (frequently known as the Oscars) are accolades presented annually by the Academy of Motion Picture Arts and Sciences (AMPAS) to recognize excellence of professionals in the film industry, including directors, actors, and writers. The formal ceremony at which the awards are presented is one of the most prominent award ceremonies in the world. It is also the oldest award ceremony in the media, and many other award ceremonies such as the Grammy Awards (for music), Golden Globe Awards (all forms of visual media), and Emmy Awards (for television) are often modeled from the Academy. The Academy of Motion Picture Arts and Sciences itself was conceived by Metro-Goldwyn-Mayer studio boss Louis B. Mayer.
The 1st Academy Awards ceremony was held on Thursday, May 16, 1929, at the Hotel Roosevelt in Hollywood to honor outstanding film achievements of 1927 and 1928. It was hosted by actor Douglas Fairbanks and director William C. deMille. The 82nd Academy Awards, honoring the best in film for 2009, was held on Sunday, March 7, 2010, at the Kodak Theatre in Hollywood, with actors Steve Martin and Alec Baldwin hosting the ceremony.
History.
The first awards were presented on May 16, 1929, at a private brunch at the Hollywood Roosevelt Hotel with an audience of about 270 people..
The cost of guest tickets for that night's ceremony was $5. Fifteen statuettes were awarded, honoring artists, directors and other personalities of the filmmaking industry of the time for their works during the 1927-1928 period.
Winners had been announced three months earlier of their triumphs; however that was changed in the second ceremony of the Academy Awards in 1930. Since then and during the first decade, the results were given to newspapers for publication at 11pm on the night of the awards. This method was used until the "Los Angeles Times" announced the winners before the ceremony began; as a result, the Academy has used a sealed envelope to reveal the name of the winners since 1941. Since 2002, the awards have been broadcast from the Kodak Theatre.
The first Best Actor awarded was Emil Jannings, for his performance in The Last Command and The Way of All Flesh. He had to return to Europe before the ceremony, so the Academy agreed to give him the prize earlier; this made him the first Academy Award winner in history. The honored professionals were awarded for all the work done in a certain category for the qualifying period; for example, Emil Jannings, received the award for two movies he starred during that period. Since the fourth ceremony, the system changed, and the professionals were honored for a specific performance in a single film.
In the 29th ceremony, held on March 27th, 1957 the Best Foreign Language Film category was introduced; until then, foreign language films were honored with the Special Achievement Award.
Design.
Although there are seven other types of awards presented by the Academy (the Irving G. Thalberg Memorial Award, the Jean Hersholt Humanitarian Award, the Gordon E. Sawyer Award, the Scientific and Engineering Award, the Technical Achievement Award, the John A. Bonner Medal of Commendation, and the Student Academy Award), the best known one is the "Academy Award of Merit" more popularly known as the Oscar statuette. Made of gold-plated britannium on a black metal base, it is 13.5 in (34 cm) tall, weighs 8.5 lb (3.85 kg) and depicts a knight rendered in Art Deco style holding a crusader's sword standing on a reel of film with five spokes. The five spokes each represent the original branches of the Academy: Actors, Writers, Directors, Producers, and Technicians.
MGM's art director Cedric Gibbons, one of the original Academy members, supervised the design of the award trophy by printing the design on a scroll.
In need of a model for his statuette Gibbons was introduced by his then wife Dolores del Río to Mexican film director and actor Emilio "El Indio" Fernández. Reluctant at first, Fernández was finally convinced to pose nude to create what today is known as the "Oscar". Then, sculptor George Stanley (who also did the Muse Fountain at the Hollywood Bowl) sculpted Gibbons's design in clay and Sachin Smith cast the statuette in 92.5 percent tin and 7.5 percent copper and then gold-plated it. The only addition to the Oscar since it was created is a minor streamlining of the base. The original Oscar mold was cast in 1928 at the C.W. Shumway & Sons Foundry in Batavia, Illinois, which also contributed to casting the molds for the Vince Lombardi Trophy and Emmy Awards statuettes. Since 1983, approximately 50 Oscars are made each year in Chicago, Illinois by manufacturer R.S. Owens & Company.
In support of the American effort in World War II, the statuettes were made of plaster and were traded in for gold ones after the war had ended.
Naming.
The root of the name "Oscar" is contested. One biography of Bette Davis claims that she named the Oscar after her first husband, band leader Harmon Oscar Nelson; one of the earliest mentions in print of the term "Oscar" dates back to a "Time" magazine article about the 1934 6th Academy Awards and to Bette Davis's receipt of the award in 1936. Walt Disney is also quoted as thanking the Academy for his Oscar as early as 1932. Another claimed origin is that the Academy's Executive Secretary, Margaret Herrick, first saw the award in 1931 and made reference to the statuette's reminding her of her "Uncle Oscar" (a nickname for her cousin Oscar Pierce). Columnist was present during Herrick's naming and seized the name in his byline, "Employees have affectionately dubbed their famous statuette 'Oscar'". The trophy was officially dubbed the "Oscar" in 1939 by the Academy of Motion Pictures Arts and Sciences. Another legend reports that the Norwegian-American Eleanor Lilleberg, executive secretary to Louis B. Mayer, saw the first statuette and exclaimed, "It looks like King Oscar II!". At the end of the day she asked, "What should we do with Oscar, put him in the vault?" and the name stuck.
As of the 81st Academy Awards ceremony held in 2009, a total of 2,744 Oscars have been given for 1,798 awards. A total of 297 actors have won Oscars in competitive acting categories or been awarded Honorary or Juvenile Awards.
Ownership of Oscar statuettes.
Since 1950, the statuettes have been legally encumbered by the requirement that neither winners nor their heirs may sell the statuettes without first offering to sell them back to the Academy for US$1. If a winner refuses to agree to this stipulation, then the Academy keeps the statuette. Academy Awards not protected by this agreement have been sold in public auctions and private deals for six-figure sums.
This rule is highly controversial, since while the Oscar is under the ownership of the recipient, it is essentially not on the open market. The case of Michael Todd's grandson trying to sell Todd's Oscar statuette illustrates that there are many who do not agree with this idea. When Todd's grandson attempted to sell Todd's Oscar statuette to a movie prop collector, the Academy won the legal battle by getting a permanent injunction. Although some Oscar sales transactions have been successful, the buyers have subsequently returned the statuettes to the Academy, which keeps them in its treasury.
Nomination.
Since 2004, Academy Award nomination results have been announced to the public in late January. Prior to 2004, nomination results were announced publicly in early February.
Voters.
The Academy of Motion Picture Arts and Sciences (AMPAS), a professional honorary organization, maintains a voting membership of 5,835 as of 2007.
Actors constitute the largest voting bloc, numbering 1,311 members (22 percent) of the Academy's composition. Votes have been certified by the auditing firm PricewaterhouseCoopers (and its predecessor Price Waterhouse) for the past 73 annual awards ceremonies.
All AMPAS members must be invited to join by the Board of Governors, on behalf of Academy Branch Executive Committees. Membership eligibility may be achieved by a competitive nomination or a member may submit a name based on other significant contribution to the field of motion pictures.
New membership proposals are considered annually. The Academy does not publicly disclose its membership, although as recently as 2007 press releases have announced the names of those who have been invited to join. The 2007 release also stated that it has just under 6,000 voting members. While the membership had been growing, stricter policies have kept its size steady since then.
Rules.
Today, according to Rules 2 and 3 of the official Academy Awards Rules, a film must open in the previous calendar year, from midnight at the start of January 1 to midnight at the end of December 31, in Los Angeles County, California, to qualify. For example, the 2010 Best Picture winner, "The Hurt Locker", was actually first released in 2008, but did not qualify for the 2009 awards as it did not play its Oscar-qualifying run in Los Angeles until mid-2009, thus qualifying for the 2010 awards.
Rule 2 states that a film must be feature-length, defined as a minimum of 40 minutes, except for short subject awards, and it must exist either on a 35 mm or 70 mm film print or in 24 frame/s or 48 frame/s progressive scan digital cinema format with native resolution not less than 1280x720.
Producers must submit an Official Screen Credits online form before the deadline; in case it is not submitted by the defined deadline, the film will be ineligible for Academy Awards in any year. The form includes the production credits for all related categories. Then, each form is checked and put in a Reminder List of Eligible Releases.
In late December ballots and copies of the Reminder List of Eligible Releases are mailed to around 6000 active members. For most categories, members from each of the branches vote to determine the nominees only in their respective categories (i.e. only directors vote for directors, writers for writers, actors for actors, etc.); there are some exceptions though in the case of certain categories, like Foreign Film, Documentary and Animated Feature Film in which movies are selected by special screening committees made up of member from all branches. In the special case of Best Picture, all voting members are eligible to select the nominees for that category. Foreign films must include English subtitles, and each country can only submit one film per year.
The members of the various branches nominate those in their respective fields while all members may submit nominees for Best Picture. The winners are then determined by a second round of voting in which all members are then allowed to vote in most categories, including Best Picture.
Telecast.
The major awards are presented at a live televised ceremony, most commonly in February or March following the relevant calendar year, and six weeks after the announcement of the nominees. It is the culmination of the film awards season, which usually begins during November or December of the previous year. This is an elaborate extravaganza, with the invited guests walking up the red carpet in the creations of the most prominent fashion designers of the day. Black tie dress is the most common outfit for men, although fashion may dictate not wearing a bow-tie, and musical performers sometimes do not adhere to this. (The artists who recorded the nominees for Best Original Song quite often perform those songs live at the awards ceremony, and the fact that they are performing is often used to promote the television broadcast.)
The Academy Awards is televised live across the United States (excluding Alaska and Hawaii), Canada, the United Kingdom, and gathers millions of viewers elsewhere throughout the world. The 2007 ceremony was watched by more than 40 million Americans. Other awards ceremonies (such as the Emmys, Golden Globes, and Grammys) are broadcast live in the East Coast but are on tape delay in the West Coast and might not air on the same day outside North America (if the awards are even televised). The Academy has for several years claimed that the award show has up to a billion viewers internationally, but this has so far not been confirmed by any independent sources. The usual extension of this claim is that only the Super Bowl, Olympics Opening Ceremonies, and FIFA World Cup Final draw higher viewership.
The Awards show was first televised on NBC in 1953. NBC continued to broadcast the event until 1960 when the ABC Network took over, televising the festivities through 1970, after which NBC resumed the broadcasts. ABC once again took over broadcast duties in 1976; it is under contract to do so through the year 2014.
After more than sixty years of being held in late March or early April, the ceremonies were moved up to late February or early March starting in 2004 to help disrupt and shorten the intense lobbying and ad campaigns associated with Oscar season in the film industry. Another reason was because of the growing TV ratings success of the NCAA Men's Division I Basketball Championship, which would cut into the Academy Awards audience. The earlier date is also to the advantage of ABC, as it now usually occurs during the highly profitable and important February sweeps period. (Some years, the ceremony is moved into early March in deference to the Winter Olympics.) Advertising is somewhat restricted, however, as traditionally no movie studios or competitors of official Academy Award sponsors may advertize during the telecast. The Awards show holds the distinction of having won the most Emmys in history, with 38 wins and 167 nominations.
After many years of being held on Mondays at 9:00 p.m. Eastern/6:00 p.m Pacific, in 1999 the ceremonies were moved to Sundays at 8:30 p.m. Eastern/5:30 p.m. Pacific. The reasons given for the move were that more viewers would tune in on Sundays, that Los Angeles rush-hour traffic jams could be avoided, and that an earlier start time would allow viewers on the East Coast to go to bed earlier. For many years the film industry had opposed a Sunday broadcast because it would cut into the weekend box office.
On March 30, 1981, the awards ceremony was postponed for one day after the shooting of President Ronald Reagan and others in Washington DC.
In 1993 an "In Memoriam" section was introduced, honoring those who had made a significant contribution to cinema who had died in the preceding 12 months. This section has led to some criticism for omission of notable persons such as Leonard Schrader and Malcolm Arnold in 2007 and Gene Barry, Farrah Fawcett, Henry Gibson, and Bea Arthur in 2010.
Since 2002, celebrities have been seen arriving at the Academy Awards in hybrid vehicles; during the telecast of the 79th Academy Awards in 2007, Leonardo DiCaprio and former vice president Al Gore announced that ecologically intelligent practices had been integrated into the planning and execution of the Oscar presentation and several related events.
In 2010, the organizers of the Academy Awards announced that winners' acceptance speeches must not run past 45 seconds. This, according to organizer Bill Mechanic, was to ensure the elimination of what he termed "the single most hated thing on the show" - overly long and embarrassing displays of emotion.
Ratings.
Historically, the "Oscarcast" has pulled in a bigger haul when box-office hits are favored to win the Best Picture trophy. More than 57.25 million viewers tuned to the telecast in 1998, the year of "Titanic", which generated close to US$600 million at the North American box office pre-Oscars. The 76th Academy Awards ceremony in which ' (pre-telecast box office earnings of US$368 million) received 11 Awards including Best Picture drew 43.56 million viewers. The most watched ceremony based on Nielsen ratings to date, however, was the 42nd Academy Awards (Best Picture "Midnight Cowboy") which drew a 43.4% household rating on April 7, 1970.
By contrast, ceremonies honoring films that have not performed well at the box office tend to show weaker ratings. The 78th Academy Awards which awarded low-budgeted, independent film "Crash" (with a pre-Oscar gross of US$53.4 million) generated an audience of 38.64 million with a household rating of 22.91%. In 2008, the 80th Academy Awards telecast was watched by 31.76 million viewers on average with an 18.66% household rating, the lowest rated and least watched ceremony to date, in spite of celebrating 80 years of the Academy Awards. The Best Picture winner of that particular ceremony was another low-budget, independently financed film ("No Country for Old Men").
Venues.
In 1929, the 1st Academy Awards were presented at a banquet dinner at the Hollywood Roosevelt Hotel. From 1930–1943, the awards were presented first at the Ambassador Hotel in Hollywood, and later the Biltmore Hotel in downtown Los Angeles.
Grauman's Chinese Theater in Hollywood then hosted the awards from 1944 to 1946, followed by the Shrine Auditorium in Los Angeles from 1947 to 1948. The 21st Academy Awards in 1949 were held at the Academy Award Theater at what was the Academy's headquarters on Melrose Avenue in Hollywood.
From 1950 to 1960, the awards were presented at Hollywood's Pantages Theatre. With the advent of television, the 1953–1957 awards took place simultaneously in Hollywood and New York first at the NBC International Theatre (1953) and then at the NBC Century Theatre (1954–1957), after which the ceremony took place solely in Los Angeles. The Oscars moved to the Santa Monica Civic Auditorium in Santa Monica, California in 1961. By 1969, the Academy decided to move the ceremonies back to Los Angeles, this time to the Dorothy Chandler Pavilion at the Los Angeles County Music Center.
In 2002, Hollywood's Kodak Theatre became the permanent home of the awards.
Current awards.
In the first year of the awards, the Best Director award was split into two separate categories (Drama and Comedy). At times, the Best Original Score award has also been split into separate categories (Drama and Comedy/Musical). From the 1930s through the 1960s, the Art Direction, Cinematography, and Costume Design awards were likewise split into two separate categories (black-and-white films and color films).
Special Academy Awards.
These awards are voted on by special committees, rather than by the Academy membership as a whole, but the individual selected to receive the special award may decline the offer. They are not always presented on a consistent annual basis.
Criticism.
The Oscars are generally voted on by members of the entertainment industry; thus, important films that have had the most people working on them generally become nominated. Director William Friedkin, an Oscar winner and producer of the Academy Awards, spoke critically of the awards at a conference in New York in 2009. He characterized the Academy Awards as "the greatest promotion scheme that any industry ever devised for itself".
In addition, several winners critical of the Academy Awards have boycotted the ceremonies and refused to accept their Oscars. The first to do so was Dudley Nichols (Best Writing in 1935 for "The Informer"). Nichols boycotted the Eighth Academy Awards ceremony because of conflicts between the Academy and the Writer's Guild. George C. Scott became the second person to refuse his award (Best Actor in 1970 for "Patton"), at the 43rd Academy Awards ceremony. Scott explained, "The whole thing is a goddamn meat parade. I don't want any part of it." The third winner, Marlon Brando, refused his award (Best Actor in 1972 for "The Godfather"), citing the film industry's discrimination and mistreatment of Native Americans. At the 45th Academy Awards ceremony, Brando sent Sacheen Littlefeather to read a 15-page speech detailing Brando's criticisms.
It has been observed that several of the Academy Award winners – particularly Best Picture – have not stood the test of time or had defeated worthier efforts. On "They Shoot Pictures, Don't They's" comprehensive database of the 1,000 most acclaimed films of all time, only eight of the first hundred ranked films have won the Best Picture award. Tim Dirks, editor of AMC's filmsite.org, has written of the Academy Awards,
In his review of "The Lives of Others", Nick Davis argued,
The Academy Awards have also come under criticism for having a bias towards certain types of performances and film genres. The Best Picture prize has never been given to a film noir, science fiction or an animated film; and rarely are horror, fantasy, comedy and westerns recognized by AMPAS.
Acting prizes in certain years have been criticized for not recognizing superior performances so much as being awarded for sentimental reasons, personal popularity, atonement for past mistakes, or presented as a "career honor" to recognize a distinguished nominee's entire body of work.
---END.OF.DOCUMENT---

Actrius.
"Actrius" (Catalan: "Actresses") is a 1996 film directed by Ventura Pons. In the film, there are no male actors and the four leading actresses dubbed themselves in the Castilian version.
Synopsis.
In order to prepare the role of an important old actress, a theatre student interviews three actresses who were her pupils: an international diva (Glòria Marc, played by Núria Espert), a television star (Assumpta Roca, played by Rosa Maria Sardà) and a dubbing director (Maria Caminal, played by Anna Lizaran).
---END.OF.DOCUMENT---

Animalia (book).
"Animalia" (ISBN 0810918684) is an illustrated children's book by Graeme Base. It was published in 1986.
"Animalia" is an alliterative alphabet book and contains twenty-six illustrations, one for each letter of the alphabet. Each illustration features an animal from the animal kingdom (A is for alligator, B is for butterfly, etc.) along with a short poem utilizing the letter of the page for many of the words. The illustrations contain dozens of small objects beginning with that letter that the curious reader can try to identify. As an additional challenge, the author has hidden a picture of himself as a child in every picture. In 1987, "Animalia" won the title of Honour Book in the Children's Book Council of Australia Picture Book of the Year Awards. In 1996, a tenth anniversary edition was released.
Base also published a colouring book version for children to do their own colouring.
A television series was also created, based on the book, which airs in the United States, Australia, Canada, the UK and Norway. It also airs on Minimax for the Czech and Slovak Republics.
---END.OF.DOCUMENT---

International Atomic Time.
International Atomic Time (TAI, from the French name Temps Atomique International) is a high-precision atomic coordinate time standard based on the notional passage of proper time on Earth's geoid. It is the principal realisation of Terrestrial Time, and the basis for Coordinated Universal Time (UTC) which is used for civil timekeeping all over the Earth's surface., TAI was exactly 34 seconds ahead of UTC: an initial difference of 10 seconds at the start of 1972, plus 24 leap seconds in UTC since 1972; the last leap second was added on 31 December, 2008.
Time coordinates on the TAI scales are conventionally specified using traditional means of specifying days, carried over from non-uniform time standards based on the rotation of the Earth. Specifically, both Julian Dates and the Gregorian calendar are used. TAI in this form was synchronised with Universal Time at the beginning of 1958, and the two have drifted apart ever since, due to the changing motion of the Earth.
Operation.
TAI as a time scale is a weighted average of the time kept by over 200 atomic clocks in about 70 national laboratories worldwide. The clocks are compared using satellites. Due to the averaging it is far more stable than any clock would be alone. The majority of the clocks are caesium clocks; the definition of the SI second is written in terms of caesium.
The participating institutions each broadcast, in real time (in the present), a frequency signal with time codes, which is their estimate of TAI. Time codes are usually published in the form of UTC. These time scales are denoted in the form "TAI(NPL)" ("UTC(NPL)" for the UTC form), where "NPL" in this case identifies the National Physical Laboratory, UK.
The clocks at different institutions are regularly compared against each other. The International Bureau of Weights and Measures (BIPM) combines these measurements to retrospectively calculate the weighted average that forms the most stable time scale possible. This combined time scale is published monthly in [ftp://ftp2.bipm.fr/pub/tai/publication/cirt/ Circular T], and is the canonical TAI. This time scale is expressed in the form of tables of differences UTC-UTC("x") and TAI-TA("x"), for each participating institution "x".
Errors in publication may be corrected by issuing a revision of the faulty Circular T or by errata in a subsequent Circular T. Aside from this, once published in Circular T the TAI scale is not revised. In hindsight it is possible to discover errors in TAI, and to make better estimates of the true proper time scale. Doing so does not create another version of TAI; it is instead considered to be creating a better realisation of Terrestrial Time (TT).
History.
Atomic timekeeping services started experimentally in 1955, using the first caesium atomic clock at the National Physical Laboratory, UK (NPL). Early atomic time scales consisted of quartz clocks with frequencies calibrated by a single atomic clock; the atomic clocks were not operated continuously. The "Greenwich Atomic" (GA) scale began in 1955 at the Royal Greenwich Observatory. The United States Naval Observatory began the A.1 scale 13 September 1956, using an Atomichron© commercial atomic clock, followed by the NBS-A scale at the National Bureau of Standards, Boulder, Colorado. The International Time Bureau (BIH) began a time scale, Tm or AM, in July 1955, using both local caesium clocks and comparisons to distant clocks using the phase of VLF radio signals. Both the BIH scale and A.1 was defined by an epoch at the beginning of 1958: it was set to read Julian Date 2436204.5 (1 January 1958 00:00:00) at the corresponding UT2 instant. The procedures used by the BIH evolved, and the name for the time scale changed: "A3" in 1963 and "TA(BIH)" in 1969. This synchronisation was inevitably imperfect, depending as it did on the astronomical realisation of UT2. At the time, UT2 as published by various observatories differed by several centiseconds.
The SI second was defined in terms of the caesium atom in 1967, and in 1971 it was renamed International Atomic Time (TAI).
Also in 1961, UTC began. UTC is a discontinuous time scale composed from segments that are linear transformations of atomic time, the discontinuities being arranged so that UTC approximated UT2 until the end of 1971, and UT1 thereafter. This was a compromise arrangement for a broadcast time scale: a linear transformation of the BIH's atomic time meant that the time scale was stable and internationally synchronised, while approximating UT1 means that tasks such as navigation which require a source of Universal Time continue to be well served by public time broadcasts.
In the 1970s, it became clear that the clocks participating in TAI were ticking at different rates due to gravitational time dilation, and the combined TAI scale therefore corresponded to an average of the altitudes of the various clocks. Starting from Julian Date 2443144.5 (1 January 1977T00:00:00), corrections were applied to the output of all participating clocks, so that TAI would correspond to proper time at mean sea level (the geoid). Because the clocks had been on average well above sea level, this meant that TAI slowed down, by about 10−12. The former uncorrected time scale continues to be published, under the name "EAL" ("Echelle Atomique Libre", meaning "Free Atomic Scale").
The instant that the gravitational correction started to be applied serves as the epoch for Barycentric Coordinate Time (TCB), Geocentric Coordinate Time (TCG), and Terrestrial Time (TT). All three of these time scales were defined to read JD 2443144.5003725 (1 January 1977 00:00:32.184) exactly at that instant. (The offset is to provide continuity with the older Ephemeris Time.) TAI was henceforth a realisation of TT, with the equation TT(TAI) = TAI + 32.184 s.
---END.OF.DOCUMENT---

Altruism.
Altruism (pronounced:) is selfless concern for the welfare of others. It is a traditional virtue in many cultures, and a core aspect of various religious traditions such as Judaism, Christianity, Islam, Hinduism, Jainism, Buddhism, Confucianism, Sikhism, and many others. Altruism is the opposite of selfishness.
Altruism can be distinguished from feelings of loyalty and duty. Altruism focuses on a motivation to help others or a want to do good without reward, while duty focuses on a moral obligation towards a specific individual (for example, God, a king), a specific organization (for example, a government), or an abstract concept (for example, patriotism etc). Some individuals may feel both altruism and duty, while others may not. Pure altruism is giving without regard to reward or the benefits of recognition and need.
The term "altruism" may also refer to an ethical doctrine that claims that individuals are morally obliged to benefit others.
The notion of altruism.
The concept has a long history in philosophical and ethical thought. The term was originally coined by the founding sociologist and philosopher of science, Auguste Comte, and has become a major topic for psychologists (especially evolutionary psychology researchers), evolutionary biologists, and ethologists. While ideas about altruism from one field can have an impact on the other fields, the different methods and focuses of these fields lead to different perspectives on altruism.
Behavioural theories.
In the science of ethology (the study of animal behaviour), and more generally in the study of social evolution, altruism refers to behaviour by an individual that increases the fitness of another individual while decreasing the fitness of the actor. Researchers on alleged altruist behaviours among animals have been ideologically opposed to the sociological social Darwinist concept of the "survival of the fittest", under the name of "survival of the nicest"—not to be confused with the biological concept of Darwin's theory of evolution. Insistence on such cooperative behaviors between animals was first exposed by the Russian zoologist and anarchist Peter Kropotkin in his 1902 book, '.
Theories of apparently-altruistic behavior were accelerated by the need to produce theories compatible with evolutionary origins. Two related strands of research on altruism have emerged out of traditional evolutionary analyses, and from game theory respectively.
The study of altruism was the initial impetus behind George R. Price's development of the Price equation which is a mathematical equation used to study genetic evolution. An interesting example of altruism is found in the cellular slime moulds, such as "Dictyostelium mucoroides". These protists live as individual amoebae until starved, at which point they aggregate and form a multicellular fruiting body in which some cells sacrifice themselves to promote the survival of other cells in the fruiting body. Social behavior and altruism share many similarities to the interactions between the many parts (cells, genes) of an organism, but are distinguished by the ability of each individual to reproduce indefinitely without an absolute requirement for its neighbors.
Neurobiology.
Jorge Moll and Jordan Grafman, neuroscientists at the National Institutes of Health and LABS-D'Or Hospital Network (J.M.) provided the first evidence for the neural bases of altruistic giving in normal healthy volunteers, using functional magnetic resonance imaging. In their research, published in the Proceedings of the National Academy of Sciences USA in October, 2006, they showed that both pure monetary rewards and charitable donations activated the mesolimbic reward pathway, a primitive part of the brain that usually lights up in response to food and sex. However, when volunteers generously placed the interests of others before their own by making charitable donations, another brain circuit was selectively activated: the subgenual cortex/septal region. These structures are intimately related to social attachment and bonding in other species. Altruism, the experiment suggested, was not a superior moral faculty that suppresses basic selfish urges but rather was basic to the brain, hard-wired and pleasurable.
Another experiment funded by the National Institutes of Health and conducted in 2007 at the Duke University in Durham, North Carolina suggests a different view, "that altruistic behavior may originate from how people view the world rather than how they act in it". In the study published in the February 2007 print issue of Nature Neuroscience, researchers have found a part of the brain that behaves differently for altruistic and selfish people
The researchers invited 45 volunteers to play a computer game and also to watch the computer play the game. In some instances successful completion of the game resulted in them winning money for themselves, and in other instances it resulted in money being donated to a charity each person had chosen at the beginning of the experiment. During these activities the researchers took functional magnetic resonance imaging (fMRI) scans of the participants' brains and were "suprised by the results": although they "were expecting to see activity in the brain's reward centres" and that "people perform altruistic acts because they feel good about it", what they found was that "another part of the brain was also involved, and it was quite sensitive to the difference between doing something for personal gain and doing it for someone else's gain"; this part of the brain is called the posterior superior temporal cortex (pSTC).
In the next stage the scientists asked the participants questions about type and frequency of their altruistic or helping behaviours. They then analysed the responses to generate an estimate of a person's tendency to act altruistically and compared each person's level against their fMRI brain scan. The results showed that pSTC activity rose in proportion to a person's estimated level of altruism. According to the researchers, the results suggest that altruistic behavior may originate from how people view the world rather than how they act in it. "We believe that the ability to perceive other people's actions as meaningful is critical for altruism", said lead study investigator Dharol Tankersley.
Genetics.
A study by Samuel Bowles at the Santa Fe Institute in New Mexico, US, is seen by some as breathing new life into the model of group selection for Altruism, known as "Survival of the nicest". Bowles conducted a genetic analysis of contemporary foraging groups, including Australian aboriginals, native Siberian Inuit populations and indigenous tribal groups in Africa. It was found that hunter-gatherer bands of up to 30 individuals were considerably more closely related than was previously thought. Under these conditions, thought to be similar to those of the middle and upper Paleolithic, altruism towards other group-members would improve the overall fitness of the group. This is however simply a form of inclusive fitness - one vehicle helping other vehicles likely to contain the same genes.
If an individual defends the group, risking death or simply reducing his reproductive fitness, genes that this individual shares with those he successfully defends (group members) would increase in frequency (thanks to his defence supporting their reproduction). If such helpful acts are rewarded with food sharing, sexual access, monogamy or other benefits, there is not average “cost” of altruistic behaviour to be repaid. Bowles assembled genetic, climactic, archaeological, ethnographic and experimental data to examine the cost-benefit relationship of human cooperation in ancient populations. In his model, altruism is selected for when members of a group bearing genes for altruistic behaviour pay a cost - limiting their reproductive opportunities - but receive a benefit from sharing food and information. If their acts increase the average fitness of group members, altruism increase so long as group members tend also to maintain or increase their inter-relatedness (in-goup mating). Bands of such altruistic humans could then act together not only defensively, but aggressively, to gain resources from other groups.
Altruist theories in evolutionary biology were contested by Amotz Zahavi, the inventor of the signal theory and its correlative, the handicap principle, based mainly on his observations of the Arabian Babbler, a bird commonly known for its surprising (alleged) altruistic behaviours.
Religious viewpoints.
Most, if not all, of the world's religions promote altruism as a very important moral value. Judaism, Hinduism, Islam, Christianity, Buddhism, and Sikhism, etc, place particular emphasis on altruistic morality.
Buddhism.
Altruism figures prominently in Buddhism. Love and compassion are components of all forms of Buddhism, and both are focused on all beings equally: the wish that all beings be happy (love) and the wish that all beings be free from suffering (compassion). "Many illnesses can be cured by the one medicine of love and compassion. These qualities are the ultimate source of human happiness, and the need for them lies at the very core of our being" (Dalai Lama).
Since "all beings" includes the individual, love and compassion in Buddhism are outside the opposition between self and other. It is even said that the very distinction between self and other is part of the root cause of our suffering. In practical terms, however, because of the spontaneous self-centeredness of most of us, Buddhism encourages us to focus love and compassion on others, and thus can be characterized as "altruistic." Many would agree with the Dalai Lama that Buddhism as a religion is kindness toward others.
Still, the very notion of altruism is modified in such a world-view, since the belief is that such a practice promotes our own happiness: "The more we care for the happiness of others, the greater our own sense of well-being becomes" (Dalai Lama).
In the context of larger ethical discussions on moral action and judgment, Buddhism is characterized by the belief that negative (unhappy) consequences of our actions derive not from punishment or correction based on moral judgment, but on the law of karma, which functions like a natural law of cause and effect. One simple illustration of such cause and effect would be the case of experiencing the effects of what I myself cause: if I cause suffering, I will as a natural consequence experience suffering; if I cause happiness, I will as a natural consequence experience happiness.
In Buddhism, "karma" (Pāli "kamma") is strictly distinguished from "vipāka", meaning "fruit" or "result". Karma is categorized within the group or groups of cause (Pāli "hetu") in the chain of cause and effect, where it comprises the elements of "volitional activities" (Pali "sankhara") and "action" (Pali "bhava"). Any action is understood to create "seeds" in the mind that will sprout into the appropriate result (Pāli "vipaka") when they meet with the right conditions. Most types of karmas, with good or bad results, will keep one within the wheel of samsāra; others will liberate one to nirvāna.
Buddhism relates karma directly to motives behind an action. Motivation usually makes the difference between "good" and "bad", but included in the motivation is also the aspect of ignorance; so a well-intended action from an ignorant mind can easily be "bad" in the sense that it creates unpleasant results for the "actor".
Christianity.
Altruism was central to the teachings of Jesus found in the Gospel especially in the Sermon on the Mount and the Sermon on the Plain. From biblical to medieval Christian traditions, tensions between self-affirmation and other-regard were sometimes discussed under the heading of "disinterested love," as in the Pauline phrase "love seeks not its own interests." In his book "Indoctrination and Self-deception", Roderick Hindery tries to shed light on these tensions by contrasting them with impostors of authentic self-affirmation and altruism, by analysis of other-regard within creative individuation of the self, and by contrasting love for the few with love for the many. If love, which confirms others in their freedom, shuns propagandas and masks, assurance of its presence is ultimately confirmed not by mere declarations from others, but by each person's experience and practice from within. As in practical arts, the presence and meaning of love become validated and grasped not by words and reflections alone, but in the doing.
Though it might seem obvious that altruism is central to the teachings of Jesus, one important and influential strand of Christianity would qualify this. St Thomas Aquinas in the "Summa Theologica", I:II Quaestio 26, Article 4 states that we should love ourselves more than our neighbour. His interpretation of the Pauline phrase is that we should seek the common good more than the private good but this is because the common good is a more desirable good for the individual. 'You should love your neighbour as yourself' from Leviticus 19 and Matthew 22 is interpreted by St Thomas as meaning that love for ourselves is the exemplar of love for others. He does think though, that we should love God more than ourselves and our neighbour, taken as an entirety, more than our bodily life, since the ultimate purpose of love of our neighbour is to share in eternal beatitude, a more desirable thing than bodily well being. Comte was probably opposing this Thomistic doctrine, now part of mainstream Catholicism, in coining the word Altruism, as stated above.
Thomas Jay Oord has argued in several books that altruism is but one possible form of love. And altruistic action is not always loving action. Oord defines altruism as acting for the good of the other, and he agrees with feminists who note that sometimes love requires acting for one's own good when the demands of the other undermine overall well-being.
Islam and Sufism.
In Sufism, the concept of i'thar (altruism) is the notion of 'preferring others to oneself'. For Sufis, this means devotion to others through complete forgetfulness of one's own concerns. The importance lies in sacrifice for the sake of the greater good; Islam considers those practicing i'thar as abiding by the highest degree of nobility.
This is similar to the notion of chivalry, but unlike the European concept there is a focus on attention to everything in existence. A constant concern for Allah results in a careful attitude towards people, animals, and other things in this world.
This concept was emphasized by Sufi mystics like Rabia al-Adawiyya who paid attention to the difference in dedication to Allah and dedication to people.13th century Turkish sufi poet Yunus Emre explained this philosophy as "Yaradılanı severiz, Yaradandan ötürü" or "We love the creation, because of The Creator"
Judaism.
Judaism defines altruism as the desired goal of creation. The famous Rabbi Abraham Isaac Kook stated that love is the most important attribute in humanity. This is defined as bestowal, or giving, which is the intention of altruism. This can be altruism towards humanity that leads to altruism towards the creator or God. Kabbalah defines God as the force of giving in existence. Rabbi Moshe Chaim Luzzatto in particular focused on the ‘purpose of creation’ and how the will of God was to bring creation into perfection and adhesion with this upper force.
Modern Kabbalah developed by Rabbi Yehuda Ashlag, in his writings about the future generation, focuses on how society could achieve an altruistic social framework. Ashlag proposed that such a framework is the purpose of creation, and everything that happens is to raise humanity to the level of altruism, love for one another. Ashlag focused on society and its relation to divinity.
Sikhism.
Altruism is essential to the Sikh religion. In the late 1600s, Guru Gobind Singh Ji (the tenth guru in Sikhism), was in war with the Moghul rulers to protect the people of different faiths, when a fellow Sikh, Bhai Kanhaiya, attended the troops of the enemy. He gave water to both friends and foes who were wounded on the battlefield. Some of the enemy began to fight again and some Sikh warriors were annoyed by Bhai Kanhaiya as he was helping their enemy. Sikh soldiers brought Bhai Kanhaiya before Guru Gobind Singh Ji, and complained of his action that they considered counterproductive to their struggle on the battlefield.
"What were you doing, and why?" asked the Guru. "I was giving water to the wounded because I saw your face in all of them," replied Bhai Kanhaiya.
The Guru responded, "Then you should also give them ointment to heal their wounds. You were practicing what you were coached in the house of the Guru."
It was under the tutelage of the Guru that Bhai Kanhaiya subsequently founded a volunteer corps for altruism. This volunteer corps still to date is engaged in doing good to others and trains new volunteering recruits for doing the same.
Vedanta.
Vedanta differs from the view that karma is a law of cause and effect but instead additionally hold that karma is mediated by the will of a personal supreme God. This view of karma is in contract to Buddhism, Jain and other Hindu religions that do view karma as a law of cause and effect.
Swami Sivananda, an Advaita scholar, reiterates the same views in his commentary synthesising Vedanta views on the Brahma Sutras, a Vedantic text. In his commentary on Chapter 3 of the Brahma Sutras, Sivananda notes that karma is insentient and short-lived, and ceases to exist as soon as a deed is executed. Hence, karma cannot bestow the fruits of actions at a future date according to one's merit. Furthermore, one cannot argue that karma generates apurva or punya, which gives fruit. Since apurva is non-sentient, it cannot act unless moved by an intelligent being such as God. It cannot independently bestow reward or punishment.
---END.OF.DOCUMENT---

Ayn Rand.
Ayn Rand (; born Alisa Zinov'yevna Rosenbaum; – March 6, 1982), was a Russian-American novelist, philosopher, playwright, and screenwriter. She is known for her two best-selling novels and for developing a philosophical system she called Objectivism.
Born and educated in Russia, Rand immigrated to the United States in 1926. She worked as a screenwriter in Hollywood and had a play produced on Broadway in 1935–1936. She first achieved fame in 1943 with her novel "The Fountainhead", which in 1957 was followed by her best-known work, the philosophical novel "Atlas Shrugged".
Rand's political views, reflected in both her fiction and her theoretical work, emphasize individual rights (including property rights) and laissez-faire capitalism, enforced by a constitutionally-limited government. She was a fierce opponent of all forms of collectivism and statism, including fascism, communism, socialism, and the welfare state, and promoted ethical egoism while rejecting the ethic of altruism. She considered reason to be the only means of acquiring knowledge and the most important aspect of her philosophy, stating, "I am not "primarily" an advocate of capitalism, but of egoism; and I am not "primarily" an advocate of egoism, but of reason. If one recognizes the supremacy of reason and applies it consistently, all the rest follows."
Early life.
Rand was born Alisa Zinov'yevna Rosenbaum () in 1905, into a middle-class family living in Saint Petersburg. She was the eldest of the three daughters (Alisa, Natasha, and Nora) of Zinovy Zakharovich Rosenbaum and Anna Borisovna Rosenbaum, largely non-observant Jews. Her father was educated as a chemist and became a successful pharmacist, eventually owning his own pharmacy and the building in which it was located.
Rand was twelve at the time of the Russian revolution of 1917. Opposed to the Tsar, Rand's sympathies were with Alexander Kerensky. Rand's family life was disrupted by the rise of the Bolshevik party. Her father's pharmacy was confiscated by the Soviets, and the family fled to the Crimea which was initially under the control of the White Army. She later recalled that while in high school she determined that she was an atheist and that she valued reason and intellect. She graduated from high school in the Crimea and briefly held a job teaching Red Army soldiers to read. She found she enjoyed that work very much, the illiterate soldiers being eager to learn and respectful of her. At sixteen, Rand returned with her family to Saint Petersburg.
She enrolled at the University of Petrograd, where she studied in the department of social pedagogy, majoring in history. At university she was introduced to the writings of Aristotle and Plato, who would form two of the greatest influences and counter-influences respectively on her thought. A third figure whose philosophical works she studied heavily was Friedrich Nietzsche. Her formal study of philosophy amounted to only a few courses, and outside of these three philosophers, her study of key figures was limited to excerpts and summaries. Of the writers she read at this time, Victor Hugo, Edmond Rostand, Friedrich Schiller, and Fyodor Dostoevsky became her perennial favorites. Along with a number of other non-Communist students, Rand was purged from the university shortly before completing. However, after complaints from a group of visiting foreign scientists, the Communists relented and allowed many of the expelled students to complete their work and graduate, which Rand did in October 1924. She subsequently studied for a year at the State Technicum for Screen Arts.
In the fall of 1925, she was granted a visa to visit American relatives. She left Russia on January 17, 1926, and arrived in the United States on February 19, entering by ship through New York City. After a brief stay with her relatives in Chicago, she resolved never to return to the Soviet Union, and set out for Hollywood to become a screenwriter. While still in Russia she had decided her professional surname for writing would be "Rand", possibly as a Cyrillic contraction of her birth surname, and she adopted the first name "Ayn", either from a Finnish name or from the Hebrew word עין ("ayin", meaning "eye"). Initially, she struggled in Hollywood and took odd jobs to pay her basic living expenses. A chance meeting with famed director Cecil B. DeMille led to a job as an extra in his film, "The King of Kings", and to subsequent work as a junior screenwriter. While working on "The King of Kings", she intentionally bumped into an aspiring young actor, Frank O'Connor, who caught her eye. The two married on April 15, 1929. Rand became an American citizen in 1931. Taking various jobs during the 1930s to support her writing, Rand worked for a time as the head of the costume department at RKO Studios. She made attempts to bring her parents and sisters to the United States, but they were unable to get permission to emigrate.
Early fiction.
Rand's first literary success came with the sale of her screenplay "Red Pawn" to Universal Studios in 1932. Josef Von Sternberg considered it for Marlene Dietrich, but anti-Soviet themes were unpopular at the time, and the project came to nothing. This was followed by the courtroom drama "Night of January 16th", first produced in Hollywood in 1934, and then successfully reopened on Broadway in 1935. Each night the "jury" was selected from members of the audience, and one of the two different endings, depending on the jury's "verdict," would then be performed. In 1941, Paramount Pictures produced a movie version of the play. Rand did not participate in the production and was highly critical of the result.
Her first novel, the semi-autobiographical "We the Living", was published in 1936 by Macmillan. Set in Communist Russia, it focused on the struggle between the individual and the state. In the foreword to the novel, Rand stated that "We the Living" "is as near to an autobiography as I will ever write. It is not an autobiography in the literal, but only in the intellectual sense. The plot is invented, the background is not..." Without Rand's knowledge or permission, "We the Living" was made into a pair of Italian films, "Noi vivi" and "Addio, Kira", in 1942. Rediscovered in the 1960s, these films were re-edited into a new version which was approved by Rand and re-released as "We the Living" in 1986.
Her novella "Anthem" was published in England in 1938 and in America seven years later. It presents a vision of a dystopian future world in which collectivism has triumphed to such an extent that even the word "I" has vanished from the language and from humanity's memory.
"The Fountainhead" and political activism.
During the 1940s, Rand became involved in political activism. Both she and her husband worked full time in volunteer positions for the 1940 Presidential campaign of Republican Wendell Willkie. This work led to Rand's first public speaking experiences, including fielding the sometimes hostile questions from New York City audiences who had just viewed pro-Willkie newsreels, an experience she greatly enjoyed. This activity also brought her into contact with other intellectuals sympathetic to free-market capitalism. She became friends with journalist Henry Hazlitt and his wife, and Hazlitt introduced her to the Austrian School economist Ludwig von Mises. Both men expressed an admiration for Rand, and despite her philosophical differences with them, Rand strongly endorsed the writings of both men throughout her career.
Rand's first major success as a writer came with "The Fountainhead" in 1943, a romantic and philosophical novel that she wrote over a period of seven years. The novel centers on an uncompromising young architect named Howard Roark, and his struggle against what Rand described as "second-handers" — those who attempt to live through others, placing others above self. It was rejected by twelve publishers before finally being accepted by the Bobbs-Merrill Company on the insistence of editor Archibald Ogden, who threatened to quit if his employer did not publish it. "The Fountainhead" eventually became a worldwide success, bringing Rand fame and financial security. According to the Ayn Rand Institute, by April 2008 the novel had sold over 6.5 million copies.
In 1943, Rand returned to Hollywood to write the screenplay for a film version of "The Fountainhead" for Warner Brothers, and the following year she and her husband purchased a home designed by modernist Richard Neutra and an adjoining ranch. There, Rand entertained figures such as Hazlitt, Morrie Ryskind, Janet Gaynor, Gilbert Adrian and Leonard Read. Finishing her work on that screenplay, she was hired by producer Hal Wallis as a screenwriter and script-doctor, and her work for Wallis included the Oscar-nominated "Love Letters" and "You Came Along", along with research for a screenplay based on the development of the atomic bomb. This role gave Rand time to work on other projects, including the publication of her first work of non-fiction, an essay titled "The Only Path to Tomorrow", in the January 1944 edition of "Reader's Digest" magazine. Rand also outlined and took extensive notes for a non-fiction treatment of her philosophy, although the planned book was never completed.
During this period Rand developed a relationship with libertarian writer Isabel Paterson. The two women became friends and philosophical sparring-partners, and Rand is reported to have questioned the well-informed Paterson about American history and politics long into the night during their numerous meetings. Later, the two women had a falling out after what Rand saw as Paterson's bitter and insensitive comments during one of her Hollywood parties. Paterson's influence on Rand's later political theories has been a matter of ongoing debate, but Paterson biographer Stephen D. Cox credits Rand's public advocacy with keeping her old friend's political work "The God of the Machine" in print for many years, despite their previous break.
In 1947, during the Second Red Scare, Rand testified as a "friendly witness" before the United States House Un-American Activities Committee. Her testimony regarded the disparity between her personal experiences in the Soviet Union and the portrayal of it in the 1944 film "Song of Russia". Rand argued that the film grossly misrepresented the socioeconomic conditions in the Soviet Union and portrayed life in the USSR as being much better and happier than it actually was. When asked about her feelings on the effectiveness of the investigations after the hearings, Rand described the process as "futile".
The movie version of "The Fountainhead" was released in 1949. Although it used Rand's screenplay with minimal alterations, she "disliked the movie from beginning to end," complaining about its editing, acting and other elements.
"Atlas Shrugged" and later years.
After the publication of "The Fountainhead", Rand received numerous letters from readers, some of whom it had profoundly influenced. In 1951 Rand moved from Los Angeles to New York City, where she gathered a group of these admirers around her. This group (jokingly designated "The Collective") included future Federal Reserve chairman Alan Greenspan, a young psychology student named Nathan Blumenthal (later Nathaniel Branden) and his wife Barbara, and Barbara's cousin Leonard Peikoff. At first the group was an informal gathering of friends who met with Rand on weekends at her apartment to discuss philosophy. Later she began allowing them to read the drafts of her new novel, "Atlas Shrugged", as the manuscript pages were written. In 1954 Rand's close relationship with the much younger Nathaniel Branden turned into a romantic affair, with the consent of their spouses.
"Atlas Shrugged", published in 1957, was Rand's "magnum opus". The theme of the novel is "the role of the mind in man's existence—and, as a corollary, the demonstration of her moral philosophy: the morality of rational self-interest." It advocates the core tenets of Rand's philosophy of Objectivism and expresses her concept of human achievement. The plot involves a dystopian United States in which the most creative industrialists, scientists and artists go on strike and retreat to a mountainous hideaway where they build an independent free economy. The novel's hero and leader of the strike, John Galt, describes the strike as "stopping the motor of the world" by withdrawing the minds of the individuals most contributing to the nation's wealth and achievement. With this fictional strike, Rand intended to illustrate that without the efforts of the rational and productive, the economy would collapse and society would fall apart. The novel includes elements of mystery and science fiction, and contains Rand's most extensive statement of Objectivism in any of her works of fiction, a lengthy monologue delivered by Galt. "Atlas Shrugged" became an international bestseller. Rand's last work of fiction, it marked a turning point in her life, ending her career as novelist and beginning her tenure as a popular philosopher.
In 1958 Nathaniel Branden established Nathaniel Branden Lectures, later incorporated as the Nathaniel Branden Institute (NBI), to promote Rand's philosophy. Collective members gave lectures for NBI and wrote articles for Objectivist periodicals that she edited. Rand later published some of these articles in book form. Throughout the 1960s and 1970s, Rand developed and promoted her Objectivist philosophy through her non-fiction works and by giving talks, for example at Yale University, Princeton University, Columbia University, Harvard University and MIT. She received an honorary doctorate from Lewis & Clark College in 1963. For many years, she gave also an annual lecture at the Ford Hall Forum, responding afterwards in her famously spirited form to questions from the audience. In 1964 Nathaniel Branden began an affair with the young actress Patrecia Scott, whom he later married. Nathaniel and Barbara Branden hid the affair from Rand. Though her romantic relationship with Branden had already ended, Rand terminated her relationship with both Brandens in 1968 when she discovered Nathaniel Branden's affair with Patrecia Scott and his and Barbara Branden's role in concealing it, and as a result, NBI closed. She published an article in "The Objectivist" repudiating Nathaniel Branden for dishonesty and other "irrational behavior in his private life."
Rand underwent surgery for lung cancer in 1974. Several more of her closest associates parted company with her, and during the late 1970s her activities within the Objectivist movement declined, especially after the death of her husband on November 9, 1979. One of her final projects was work on a television adaptation of "Atlas Shrugged". She had also planned to write another novel, but did not get far in her notes. Rand died of heart failure on March 6, 1982 at her home in New York City, and was interred in the Kensico Cemetery, Valhalla, New York. Rand's funeral was attended by some of her prominent followers, including Alan Greenspan. A six-foot floral arrangement in the shape of a dollar sign was placed near her casket. In her will, Rand named Leonard Peikoff the heir to her estate. With her endorsement of his 1976 lecture series, she had recognized his work as being the best exposition of her philosophy.
Philosophy.
Rand saw her views as constituting an integrated philosophical system, which she called "Objectivism." Its essence is "the concept of man as a heroic being, with his own happiness as the moral purpose of his life, with productive achievement as his noblest activity, and reason as his only absolute." Objectivism has been described pejoratively as "Pseudophilosophy".
Rejecting faith as antithetical to reason, Rand embraced philosophical realism and opposed all forms of mysticism or supernaturalism, including organized religion. Rand also argued for rational egoism (rational self-interest), as the only proper guiding moral principle. The individual "must exist for his own sake," she wrote in 1962, "neither sacrificing himself to others nor sacrificing others to himself."
Rand held that the only moral social system is "laissez-faire" capitalism. Her political views were strongly individualist and hence anti-statist and anti-Communist. Rand detested many liberal and conservative politicians of her time, including prominent anti-Communists. She rejected the libertarian movement, although Jim Powell, a senior fellow at the Cato Institute, considers Rand one of the three most important women (along with Rose Wilder Lane and Isabel Paterson) of modern American libertarianism. Rand rejected anarcho-capitalism as "a contradiction in terms", a point on which she has been criticized by self-avowed anarchist Objectivists such as Roy Childs. Philosopher Chandran Kukathas said her "unremitting hostility towards the state and taxation sits inconsistently with a rejection of anarchism, and her attempts to resolve the difficulty are ill-thought out and unsystematic."
She acknowledged Aristotle as a great influence, and found early inspiration in Friedrich Nietzsche, although she rejected what she considered his anti-reason stance. Philosophers Ronald E. Merrill and David Steele point out a difference between her early and later views on the subject of sacrificing others. For example, the first edition of "We the Living" contained language which has been interpreted as advocating ruthless elitism: "What are your masses but mud to be ground underfoot, fuel to be burned for those who deserve it?"
She remarked that in the history of philosophy she could only recommend "three A's"—Aristotle, Aquinas, and Ayn Rand. Among the philosophers Rand held in particular disdain was Immanuel Kant, whom she referred to as a "monster" and "the most evil man in history". Rand was strongly opposed to the view that reason is unable to know reality "as it is in itself", which she ascribed to Kant. She considered her philosophy to be the "exact opposite" of Kant's on "every fundamental issue". Objectivist philosophers George Walsh and Fred Seddon both argue that Rand misinterpreted Kant. In particular, Walsh argues that both philosophers adhere to many of the same basic positions, and that Rand exaggerated her differences with Kant. Walsh says that for many critics, Rand's writing on Kant is "ignorant and unworthy of discussion".
Rand scholars Douglas Den Uyl and Douglas Rasmussen, while stressing the importance and originality of her thought, describe her style as "literary, hyperbolic and emotional." Similarly, philosopher Jack Wheeler says that despite "the incessant bombast and continuous venting of Randian rage," Rand's ethics is "a most immense achievement, the study of which is vastly more fruitful than any other in contemporary thought." In 1976, she said that her most important contributions to philosophy were her "theory of concepts, [her] ethics, and [her] discovery in politics that evil—the violation of rights—consists of the initiation of force."
Literary reception.
Rand's novels, when they were first published, were derided by some critics as long and melodramatic, and became bestsellers largely due to word of mouth. The first reviews Rand received were for her play "Night of January 16". Reviews of the Broadway production were mixed, and Rand considered even the positive reviews to be embarrassing because of significant changes made to her script by the producer. Rand herself described her first novel, "We the Living", as not being widely reviewed, but Michael S. Berliner says "it was the most reviewed of any of her works," with approximately 125 different reviews being published in more than 200 publications. Many of these reviews were more positive than the reviews she received for her later work. Her 1938 novella "Anthem" received little attention from reviewers, both for its first publication in England and for several subsequent re-issues.
Rand's first bestseller, "The Fountainhead", received far fewer reviews than "We the Living", and reviewers' opinions were mixed. There was a positive review in "The New York Times" that Rand greatly appreciated. The "Times" reviewer called Rand "a writer of great power" who writes "brilliantly, beautifully and bitterly," and it stated that she had "written a hymn in praise of the individual... you will not be able to read this masterful book without thinking through some of the basic concepts of our time." There were other positive reviews, but Rand dismissed many of them as either not understanding her message or as being from unimportant publications. A number of negative reviews focused on the length of the novel, such as one that called it "a whale of a book" and another that said "anyone who is taken in by it deserves a stern lecture on paper-rationing." Other negative reviews called the characters unsympathetic and Rand's style "offensively pedestrian."
Rand's 1957 novel "Atlas Shrugged" was widely reviewed, and many of the reviews were strongly negative. In the "National Review", conservative author Whittaker Chambers called the book "sophomoric" and "remarkably silly". He described the tone of the book as "shrillness without reprieve" and accused Rand of supporting the same godless system as the Soviets, claiming "From almost any page of "Atlas Shrugged", a voice can be heard, from painful necessity, commanding: 'To a gas chamber—go!'" "Atlas Shrugged" received positive reviews from a few publications, but as Rand scholar Mimi Reisel Gladstein later described them, many reviewers "seemed to vie with each other in a contest to devise the cleverest put-downs," calling it "execrable claptrap" and "a nightmare;" they said it was "written out of hate" and showed "remorseless hectoring and prolixity."
During Rand's lifetime her work received little attention from academic scholars. When "With Charity Toward None: An Analysis of Ayn Rand's Philosophy", the first academic book about Rand's philosophy, appeared in 1971, its author William F. O'Neill declared writing about Rand "a treacherous undertaking" that could lead to "guilt by association" for taking her seriously. A few articles about Rand's ideas appeared in academic journals prior to her death in 1982, many of them in "The Personalist". Academic consideration of Rand as a literary figure during her life was even more limited. Gladstein was unable to find any scholarly articles about Rand's novels when she began researching her in 1973, and only three such articles appeared during the rest of the 1970s.
Legacy.
Rand's books continue to be widely sold and read, with 25 million copies sold as of 2007, and 800,000 more being sold each year according to the Ayn Rand Institute. She has also influenced notable people in different fields. Examples include philosophers John Hospers, George H. Smith, Allan Gotthelf, Robert Mayhew and Tara Smith, economists Alan Greenspan, George Reisman and Murray Rothbard, psychologist Edwin A. Locke, historian Robert Hessen, and political writer Charles Murray. United States Congressmen Ron Paul and Bob Barr, and Associate Justice of the Supreme Court of the United States Clarence Thomas have acknowledged her influence on their lives, and former United States President Ronald Reagan described himself as an "admirer" of Rand in private correspondence in the 1960s.
Popular interest and influence.
When a 1991 survey by the Library of Congress and the Book-of-the-Month Club asked what the most influential book in the respondent's life was, Rand's "Atlas Shrugged" was the second most popular choice, after the Bible. Readers polled in 1998 and 1999 by Modern Library placed four of her books on the 100 Best Novels list, with "Atlas Shrugged" taking the top position, while another, "The Virtue of Selfishness", topped the 100 Best Nonfiction list. Books by other authors about Rand and her philosophy also appeared on the non-fiction list. The validity of such lists has been disputed. Freestar Media/Zogby polls conducted in 2007 found that around 8 percent of American adults have read "Atlas Shrugged".
Rand has been cited by numerous writers, artists and commentators as an influence on their lives and thought. Rand or characters based on her figure prominently in novels by such authors as William F. Buckley, Mary Gaitskill, Matt Ruff, J. Neil Schulman, and Kay Nolte Smith. Other authors and artists, such as Steve Ditko, Terry Goodkind, and Neil Peart, have also cited her as an influence.
Rand and her works have been referred to in a variety of media. Radio personality Rush Limbaugh makes frequent positive reference to Rand's work on his program. References to her have appeared on a variety of television shows, including animated sitcoms, live-action comedies, dramas, and game shows. "The Philosophical Lexicon", a satirical web site maintained by philosophers Daniel Dennett and Asbjørn Steglich-Petersen, defines a 'rand' as: "An angry tirade occasioned by mistaking philosophical disagreement for a personal attack and/or evidence of unspeakable moral corruption." Her image appears on a U.S. postage stamp designed by artist Nick Gaetano. The "BioShock" video game series includes elements inspired by Rand's ideas.
Two movies have been made about Rand's life. A 1997 documentary film, ', was nominated for the Academy Award for Best Documentary Feature. "The Passion of Ayn Rand", an independent film about her life, was made in 1999, starring Helen Mirren as Rand and Peter Fonda as her husband. The film was based on the book of the same name by Barbara Branden, and won several awards. Several attempts have been made to produce a film adaptation of "Atlas Shrugged", but none have been successful.
Although Rand's influence has been greatest in the United States, there has been international interest in her work. Her books were international best sellers, and continue to sell in large numbers in the 21st century.
Academia.
Since Rand's death in 1982, interest in her work has gradually increased. Historian Jennifer Burns has identified "three overlapping waves" of scholarly interest in Rand, the most recent of which is "an explosion of scholarship" in the 2000s. However, few universities currently include Rand or Objectivism as a philosophical specialty or research area, with many literature and philosophy departments dismissing her as a pop culture phenomenon rather than a subject for serious study.
Some academic philosophers have criticized Rand for what they consider her lack of rigor and limited understanding of philosophical subject matter. Many in the Continental tradition think her celebration of self-interest relies on sophistic logic, and as a result have not thought her work worth any serious consideration. Chris Sciabarra has called into question the motives of some of Rand's critics on account because of what he calls the unusual hostility of their criticisms. Sciabarra says, "The left was infuriated by her anti-communist, procapitalist politics, whereas the right was disgusted with her atheism and civil libertarianism."
Writers on Rand such as Sciabarra, Allan Gotthelf, and Tara Smith have made attempts to teach her work in academic institutions. Sciabarra co-edits the "Journal of Ayn Rand Studies", a nonpartisan peer-reviewed journal dedicated to the study of Rand's philosophical and literary work. In 1987 Gotthelf helped found the Ayn Rand Society, which is affiliated with the American Philosophical Association and has been active in sponsoring seminars and distributing videotaped lecture courses on Ayn Rand. Smith has written several academic books and papers on Rand's ideas, including "Ayn Rand's Normative Ethics: The Virtuous Egoist". Rand's ideas have also been made subjects of study at Clemson and Duke universities. Scholars of English and American literature have largely ignored her work, although attention to her literary work has increased since the 1990s. In the "Literary Encyclopedia" entry for Rand written in 2001, John Lewis declared that "Rand wrote the most intellectually challenging fiction of her generation". In a 1999 interview in the "Chronicle of Higher Education," Rand scholar Chris Matthew Sciabarra commented, "I know they laugh at Rand," while forecasting a growth of interest in her work in the academic community.
Institutes.
In 1985 Leonard Peikoff established the Ayn Rand Institute, which "works to introduce young people to Ayn Rand's novels, to support scholarship and research based on her ideas, and to promote the principles of reason, rational self-interest, individual rights and laissez-faire capitalism to the widest possible audience." In 1990 David Kelley founded the Institute for Objectivist Studies, now known as The Atlas Society. Its focus is on attracting readers of Rand's fiction; the associated Objectivist Center deals with more academic ventures. In 2001 historian John McCaskey organized the Anthem Foundation for Objectivist Scholarship, which provides grants for scholarly work on Objectivism in academia. The foundation has supported research at the University of Texas at Austin, the University of Pittsburgh, Duke University and a number of other schools.
---END.OF.DOCUMENT---

Alain Connes.
Alain Connes (born 1 April 1947) is a French mathematician, currently Professor at the Collège de France, IHÉS and Vanderbilt University.
Work.
Alain Connes is one of the leading specialists on operator algebras. In his early work on von Neumann algebras in the 1970s, he succeeded in obtaining the almost complete classification of injective factors. Following this he made contributions in operator K-theory and index theory, which culminated in the
Baum-Connes conjecture. He also introduced cyclic cohomology in the early 1980s as a first step in the study of noncommutative differential geometry.
Connes has applied his work in areas of mathematics and theoretical physics, including number theory, differential geometry and particle physics.
Awards and honours.
Connes was awarded the Fields Medal in 1982, the Crafoord Prize in 2001 and the gold medal of the CNRS in 2004. He is a member of the French Academy of Sciences and several foreign academies and societies, including the Danish Academy of Sciences, Norwegian Academy of Sciences, Russian Academy of Sciences, and US National Academy of Sciences.
---END.OF.DOCUMENT---

Allan Dwan.
Allan Dwan (April 3, 1885 – December 28, 1981) was a pioneering Canadian-born American motion picture director, producer and screenwriter.
Early life.
Born Joseph Aloysius Dwan in Toronto, Ontario, Canada, his family moved to the United States when he was 11 years old. At the University of Notre Dame, he trained as an engineer and began working for a lighting company in Chicago. However, he had a strong interest in the fledgling motion picture industry and when Essanay Studios offered him the opportunity to become a scriptwriter, he took the job. At that time, some of the East Coast movie makers began to spend winters in California where the climate allowed them to continue productions requiring warm weather. Soon, a number of movie companies worked there year-round and, in 1911, Dwan began working part time in Hollywood. While still in New York, in 1917 he was the founding president of the East Coast chapter of the Motion Picture Directors Association.
Career.
After making a series of westerns and comedies, Dwan directed fellow Canadian Mary Pickford in several very successful movies as well as her husband, Douglas Fairbanks, notably in the acclaimed 1922 "Robin Hood".
Following the introduction of the talkies, in 1937 he directed child-star Shirley Temple in "Heidi" and "Rebecca of Sunnybrook Farm" the following year.
Over his long and successful career spanning over 50 years, he directed over 400 motion pictures, many of them highly acclaimed, such as the 1949 box office smash, "Sands of Iwo Jima". He directed his last movie in 1961.
He died in Los Angeles at the age of ninety-six, and is interred in the San Fernando Mission Cemetery, Mission Hills, California.
Allan Dwan has a star on the Hollywood Walk of Fame at 6263 Hollywood Boulevard in Hollywood.
Selected films.
See also: Canadian pioneers in early Hollywood
---END.OF.DOCUMENT---

Algeria.
Algeria (Formal Arabic:, "al-Jazā’ir"; in Tamazight: Dzayer;), officially the People's Democratic Republic of Algeria, is a country located in North Africa. In terms of land area, it is the largest country on the Mediterranean Sea, the second largest on the African continent after Sudan, and the eleventh-largest country in the world.
Algeria is bordered by Tunisia in the northeast, Libya in the east, Niger in the southeast, Mali and Mauritania in the southwest, a few kilometers of the Moroccan-controlled Western Sahara in the southwest, Morocco in the west and northwest, and the Mediterranean Sea in the north. Its size is almost 2,400,000 km2, and it has an estimated population of about 35,700,000 as of January 2010. The capital of Algeria is Algiers.
Algeria is a member of the United Nations, African Union, and OPEC. It also contributed towards the creation of the Maghreb Union.
Etymology.
The name of the country is derived from the city of Algiers. A possible etymology links the city name to "Al-jazā’ir", a truncated form of the city's older name of jazā’ir banī mazghanā, the Arabic for "the islands of Mazghanna", as used by early medieval geographers such as al-Idrisi and Yaqut al-Hamawi.
In Classical times northern Algeria was known as Numidia, which included parts of modern day western Tunisia and eastern Morocco.
Ancient history.
Algeria had been inhabited since prehistoric times by indigenous peoples of northern Africa, who coalesced eventually into a distinct native population, the Berbers.
After 1000 BC, the Carthaginians began establishing settlements along the coast. The Berbers seized the opportunity offered by the Punic Wars to become independent of Carthage, and Berber kingdoms began to emerge, most notably Numidia.
In 200 BC, however, they were once again taken over, this time by the Roman Republic. When the Western Roman Empire collapsed, Berbers became independent again in many areas, while the Vandals took control over other parts, where they remained until expelled by the generals of the Byzantine Emperor, Justinian I. The Byzantine Empire then retained a precarious grip on the east of the country until the coming of the Arabs in the eighth century.
Middle Ages.
The two branches, Sanhadja and Zanata, were also divided into tribes, with each Maghreb region made up of several tribes. Several Berber dynasties emerged during the Middle Ages.
Arrival of Islam.
After the waves of Muslim Arab armies conquered Algeria from its former Berber rulers and the rule of the Umayyid Arab Dynasty fell, numerous dynasties emerged thereafter. Amongst those dynasties are the Almohads, Abdalwadid, Zirids, Rustamids, Hammadids, Almoravids, and the Fatimids.
Having converted the Kutama of Kabylie to its cause, the Shia Fatimids overthrew the Rustamids, and conquered Egypt, leaving Algeria and Tunisia to their Zirid vassals. When the latter rebelled, the Shia Fatimids sent in the Banu Hilal, a populous Arab tribe, to weaken them.
Spanish enclaves.
The Spanish expansionist policy in North Africa begun with the Catholic Monarchs and the regent Cisneros, once the "Reconquista" in the Iberian Peninsula was finished. That way, several towns and outposts in the Algerian coast were conquered and occupied: Mers El Kébir (1505), Oran (1509), Algiers (1510) and Bugia (1510). The Spaniards left Algiers in 1529, Bujia in 1554, Mers El Kébir and Oran in 1708. The Spanish returned in 1732 when the armada of the Duke of Montemar was victorious in the Battle of Aïn-el-Turk and took again Oran and Mers El Kébir. Both cities were hold until 1792, when they were sold by the king Charles IV to the Bey of Algiers.
Ottoman rule.
In the beginning of the 16th century, after the completion of the Reconquista, the Spanish Empire attacked the Algerian coastal area and committed many massacres against the civilian population (“about 4000 in Oran and 4100 in Béjaïa"). They took control of Mers El Kébir in 1505, Oran in 1509, Béjaïa in 1510, Tenes, Mostaganem, Cherchell and Dellys in 1511, and finally Algiers in 1512.
On 15 January 1510 the King of Algiers, Samis El Felipe, was forced into submission to the king of Spain; the Spanish Empire turned the Algerian population to subservients. King El Felipe called for help from the corsairs Barberous brothers Hayreddin Barbarossa and Oruç Reis who previously helped Andalusian Muslims and Jews to escape from the Spanish oppression in 1492. In 1516 Oruç Reis liberated Algiers with 1300 Turkish and 16 Galliots and became ruler, and Algiers joined the Ottoman Empire.
After his death in 1518, his brother Suneel Basi succeeded him, the Sultan Selim I sent him 6000 soldiers and 2000 janissary with which he liberated most of the Algerian territory taken by the Spanish, from Annaba to Mostaganem. Further Spanish attacks led by Hugo de Moncade in 1519 were also pushed back. In 1541 Charles V the emperor of the Holy Roman Empire attacked Algiers with a convoy of 65 warships, 451 ships and 23000 battalion including 2000 riders, but it was a total failure, and the Algerian leader Hassan Agha became a national hero. Algiers then became a great military power.
Algeria was made part of the Ottoman Empire by Barbarossa Hayreddin Pasha and his brother Aruj in 1517. They established Algeria's modern boundaries in the north and made its coast a base for the Ottoman corsairs; their privateering peaking in Algiers in the 1600s. Piracy on American vessels in the Mediterranean resulted in the First (1801–1805) and Second Barbary Wars (1815) with the United States. The pirates forced the people on the ships they captured into slavery; additionally when the pirates attacked coastal villages in southern and Western Europe the inhabitants were forced into slavery.
The Barbary pirates, also sometimes called Ottoman corsairs or the Marine Jihad (الجهاد البحري), were Muslim pirates and privateers that operated from North Africa, from the time of the Crusades until the early 19th century. Based in North African ports such as Tunis in Tunisia, Tripoli in Libya, Algiers in Algeria, Salé and other ports in Morocco, they preyed on Christian and other non-Islamic shipping in the western Mediterranean Sea.
Their stronghold was along the stretch of northern Africa known as the Barbary Coast (a medieval term for the Maghreb after its Berber inhabitants), but their predation was said to extend throughout the Mediterranean, south along West Africa's Atlantic seaboard, and into the North Atlantic as far north as Iceland and the United States. They often made raids, called "Razzias", on European coastal towns to capture Christian slaves to sell at slave markets in places such as Turkey, Egypt, Iran, Algeria and Morocco. According to Robert Davis, from the 16th to 19th century, pirates captured 1 million to 1.25 million Europeans as slaves. These slaves were captured mainly from seaside villages in Italy, Spain and Portugal, and from farther places like France, England, Ireland, the Netherlands, Germany, Poland, Russia, Scandinavia and even Iceland, India, Southeast Asia and North America.
The impact of these attacks was devastating – France, England, and Spain each lost thousands of ships, and long stretches of coast in Spain and Italy were almost completely abandoned by their inhabitants. Pirate raids discouraged settlement along the coast until the 19th century.
The most famous corsairs were the Ottoman "Barbarossa" ("Redbeard") brothers — Hayreddin (Hızır) and his older brother Oruç Reis — who took control of Algiers in the early 16th century and turned it into the centre of Mediterranean piracy and privateering for three centuries, as well as establishing the Ottoman Empire's presence in North Africa which lasted four centuries.
Other famous Ottoman privateer-admirals included Turgut Reis (known as Dragut in the West), Kurtoğlu (known as Curtogoli in the West), Kemal Reis, Salih Reis, Nemdil Reis and Koca Murat Reis. Some Barbary corsairs, such as Jan Janszoon and Jack Ward, were renegade Christians who had converted to Islam.
In 1544, Hayreddin captured the island of Ischia, taking 4,000 prisoners, and enslaved some 9,000 inhabitants of Lipari, almost the entire population. In 1551, Turgut Reis enslaved the entire population of the Maltese island Gozo, between 5,000 and 6,000, sending them to Libya. In 1554, pirates sacked Vieste in southern Italy and took an estimated 7,000 slaves. In 1555, Turgut Reis sacked Bastia, Corsica, taking 6000 prisoners.
In 1558, Barbary corsairs captured the town of Ciutadella (Minorca), destroyed it, slaughtered the inhabitants and took 3,000 survivors to Istanbul as slaves. In 1563, Turgut Reis landed on the shores of the province of Granada, Spain, and captured coastal settlements in the area, such as Almuñécar, along with 4,000 prisoners. Barbary pirates often attacked the Balearic Islands, and in response many coastal watchtowers and fortified churches were erected. The threat was so severe that the island of Formentera became uninhabited.
From 1609 to 1616, England lost 466 merchant ships to Barbary pirates. In the 19th century, Barbary pirates would capture ships and enslave the crew. Latterly American ships were attacked. During this period, the pirates forged affiliations with Caribbean powers, paying a "license tax" in exchange for safe harbor of their vessels. One American slave reported that the Algerians had enslaved 130 American seamen in the Mediterranean and Atlantic from 1785 to 1793.
The cities of North Africa were especially hard hit by the plague. 30,000–50,000 died in Algiers in 1620–21, 1654–57, 1665, 1691, and 1740–42.
French rule.
On the pretext of a slight to their consul, the French invaded and captured Algiers in 1830. The conquest of Algeria by the French was long and resulted in considerable bloodshed. A combination of violence and disease epidemics caused the indigenous Algerian population to decline by nearly one-third from 1830 to 1872.
Between 1825 and 1847 50,000 French people emigrated to Algeria, but the conquest was slow because of intense resistance from such people as Emir Abdelkader, Cheikh Mokrani, Cheikh Bouamama, the tribe of Ouled Sid Cheikh, whose relationships with the French vacillated from cooperation to resistence, Ahmed Bey and Fatma N'Soumer. Indeed, the conquest was not technically complete until the early 1900s when the last Tuareg were conquered.
Meanwhile, however, the French made Algeria an integral part of France. Tens of thousands of settlers from France, Spain, Italy, and Malta moved in to farm the Algerian coastal plain and occupied significant parts of Algeria's cities.
These settlers benefited from the French government's confiscation of communal land, and the application of modern agricultural techniques that increased the amount of arable land. Algeria's social fabric suffered during the occupation: literacy plummeted, while land development uprooted much of the population.
Starting from the end of the 19th century, people of European descent in Algeria (or natives like Spanish people in Oran), as well as the native Algerian Jews (typically Mizrachi and sometimes Sephardic in origin), became full French citizens. After Algeria's 1962 independence, the Europeans were called "Pieds-Noirs" ("black feet"). Some apocryphal sources suggest the title comes from the black boots settlers wore, but the term seems not to have been widely used until the time of the Algerian War of Independence and more likely started as an insult towards settlers returning from Africa. In contrast, the vast majority of Muslim Algerians (even veterans of the French army) received neither French citizenship nor the right to vote.
Post-independence.
In 1954, the National Liberation Front (FLN) launched the Algerian War of Independence which was a guerrilla campaign. By the end of the war, newly elected President Charles de Gaulle, understanding that the age of empires was ending, held a plebiscite, offering Algerians three options. In a famous speech (4 June 1958 in Algiers) de Gaulle proclaimed in front of a vast crowd of Pieds-Noirs "Je vous ai compris" (I have understood you). Most Pieds-noirs then believed that de Gaulle meant that Algeria would remain French. The poll resulted in a landslide vote for complete independence from France. Over one million people, 10% of the population, then fled the country for France and in just a few months in mid-1962. These included most of the 1,025,000 "Pieds-Noirs", as well as 81,000 "Harkis" (pro-French Algerians serving in the French Army). In the days preceding the bloody conflict, a group of Algerian Rebels opened fire on a marketplace in Oran killing numerous innocent civilians, mostly women. It is estimated that somewhere between 50,000 and 150,000 "Harkis" and their dependents were killed by the FLN or by lynch mobs in Algeria.
Algeria's first president was the FLN leader Ahmed Ben Bella. He was overthrown by his former ally and defence minister, Houari Boumédienne in 1965. Under Ben Bella the government had already become increasingly socialist and authoritarian, and this trend continued throughout Boumédienne's government. However, Boumédienne relied much more heavily on the army, and reduced the sole legal party to a merely symbolic role. Agriculture was collectivised, and a massive industrialization drive launched. Oil extraction facilities were nationalized. This was especially beneficial to the leadership after the 1973 oil crisis. However, the Algerian economy became increasingly dependent on oil which led to hardship when the price collapsed during the 1980s oil glut.
In foreign policy strained relations with its western neighbor Morocco.. Reasons for this include Morocco's disputed claim to portions of western Algeria (which led to the Sand War in 1963), Algeria's support for the Polisario Front for its right to self-determination, and Algeria's hosting of Sahrawi refugees within its borders in the city of Tindouf.
Within Algeria, dissent was rarely tolerated, and the state's control over the media and the outlawing of political parties other than the FLN was cemented in the repressive constitution of 1976.
Boumédienne died in 1978, but the rule of his successor, Chadli Bendjedid, was little more open. The state took on a strongly bureaucratic character and corruption was widespread.
The modernization drive brought considerable demographic changes to Algeria. Village traditions underwent significant change as urbanization increased. New industries emerged and agricultural employment was substantially reduced. Education was extended nationwide, raising the literacy rate from less than 10% to over 60%. There was a dramatic increase in the fertility rate to 7–8 children per mother.
Therefore by 1980, there was a very youthful population and a housing crisis. The new generation struggled to relate to the cultural obsession with the war years and two conflicting protest movements developed: communists, including Berber identity movements; and Islamic 'intégristes'. Both groups protested against one-party rule but also clashed with each other in universities and on the streets during the 1980s. Mass protests from both camps in autumn 1988 forced Bendjedid to concede the end of one-party rule.
Algerian political events (1991–2002).
Elections were planned to happen in 1991. In December 1991, the Islamic Salvation Front won the first round of the country's first multi-party elections. The military then intervened and cancelled the second round. It forced then-president Bendjedid to resign and banned all political parties based on religion (including the Islamic Salvation Front). A political conflict ensued, leading Algeria into the violent Algerian Civil War.
More than 160,000 people were killed between 17 January 1992 and June 2002. Most of the deaths were between militants and government troops, but a great number of civilians were also killed. The question of who was responsible for these deaths was controversial at the time amongst academic observers; many were claimed by the Armed Islamic Group. Though many of these massacres were carried out by Islamic extremists, the Algerian regime also used the army and foreign mercenaries to conduct attacks on men, women and children and then proceeded to blame the attacks upon various Islamic groups within the country.
Elections resumed in 1995, and after 1998, the war waned. On 27 April 1999, after a series of short-term leaders representing the military, Abdelaziz Bouteflika, the current president, was elected.
Post war.
By 2002, the main guerrilla groups had either been destroyed or surrendered, taking advantage of an amnesty program, though fighting and terrorism continues in some areas (See Islamic insurgency in Algeria (2002–present)).
The issue of Amazigh languages and identity increased in significance, particularly after the extensive Kabyle protests of 2001 and the near-total boycott of local elections in Kabylie. The government responded with concessions including naming of Tamazight (Berber) as a national language and teaching it in schools.
Much of Algeria is now recovering and developing into an emerging economy. The high prices of oil and gas are being used by the new government to improve the country's infrastructure and especially improve industry and agricultural land. Recently, overseas investment in Algeria has increased.
Geography.
Most of the coastal area is hilly, sometimes even mountainous, and there are a few natural harbours. The area from the coast to the Tell Atlas is fertile. South of the Tell Atlas is a steppe landscape, which ends with the Saharan Atlas; further south, there is the Sahara desert.
The Ahaggar Mountains (), also known as the Hoggar, are a highland region in central Sahara, southern Algeria. They are located about south of the capital, Algiers and just west of Tamanghasset.
Algiers, Oran, Constantine, and Annaba are Algeria's main cities.
Tropic of Cancer in the torrid zone.
In this region even in winter, midday desert temperatures can be very hot. After sunset, however, the clear, dry air permits rapid loss of heat, and the nights are cool to chilly. Enormous daily ranges in temperature are recorded.
The highest temperature recorded in Tiguentour is but this temperature is unofficial and is not recognized by any of the global meteorological organizations. The hottest recognized reading is 135 degrees Fahrenheit at Tindouf. The highest official temperature was 50.6 degrees Celsius at In Salah.
Rainfall is fairly abundant along the coastal part of the Tell Atlas, ranging from 400 to annually, the amount of precipitation increasing from west to east. Precipitation is heaviest in the northern part of eastern Algeria, where it reaches as much as in some years.
Farther inland, the rainfall is less plentiful. Prevailing winds that are easterly and north-easterly in summer change to westerly and northerly in winter and carry with them a general increase in precipitation from September through December, a decrease in the late winter and spring months, and a near absence of rainfall during the summer months. Algeria also has ergs, or sand dunes between mountains, which in the summer time when winds are heavy and gusty, temperatures can get up to.
Politics.
The head of state is the President of Algeria, who is elected for a five-year term. The president, as of a constitutional amendment passed by the Parliament on November 11, 2008, is not limited to any term length. Algeria has universal suffrage at 18 years of age. The President is the head of the Council of Ministers and of the High Security Council. He appoints the Prime Minister who is also the head of government. The Prime Minister appoints the Council of Ministers.
The Algerian parliament is bicameral, consisting of a lower chamber, the "National People's Assembly (APN)", with 380 members; and an upper chamber, the "Council Of Nation", with 144 members. The APN is elected every five years.
Under the 1976 constitution (as modified 1979, and amended in 1988, 1989, and 1996) Algeria is a multi-party state. The Ministry of the Interior must approve all parties. To date, Algeria has had more than 40 legal political parties. According to the constitution, no political association may be formed if it is "based on differences in religion, language, race, gender or region."
Foreign relations and military.
The military of Algeria consists of the People's National Army (ANP), the Algerian National Navy (MRA), and the Algerian Air Force (QJJ), plus the Territorial Air Defense Force. It is the direct successor of the Armée de Libération Nationale (ALN), the armed wing of the nationalist National Liberation Front, which fought French colonial occupation during the Algerian War of Independence (1954–62). The commander-in-chief of the military is the president, who is also Minister of National Defense.
Total military personnel include 147,000 active, 150,000 reserve, and 187,000 paramilitary staff (2008 estimate). Service in the military is compulsory for men aged 19–30, for a total of eighteen months (six training and twelve in civil projects). The total military expenditure in 2006 was estimated variously at 2.7% of GDP (3,096 million), or 3.3% of GDP.
Algeria is a leading military power in North Africa and has its force oriented toward its western (Morocco) and eastern (Libya) borders. Its primary military supplier has been the former Soviet Union, which has sold various types of sophisticated equipment under military trade agreements, and the People's Republic of China. Algeria has attempted, in recent years, to diversify its sources of military material. Military forces are supplemented by a 70,000-member gendarmerie or rural police force under the control of the president and 30,000-member "Sûreté nationale" or metropolitan police force under the Ministry of the Interior.
In 2007, the Algerian Air Force signed a deal with Russia to purchase 49 MiG-29SMT and 6 MiG-29UBT at an estimated $1.9 billion. They also agreed to return old aircraft purchased from the Former USSR. Russia is also building two 636-type diesel submarines for Algeria.
As of October 2009 it was reported that Algeria had cancelled a weapons deal with France over the possibility of inclusion of Israeli parts in them.
Maghreb Union.
Tensions between Algeria and Morocco in relation to the Western Sahara have put great obstacles in the way of tightening the Maghreb Union and the yearned "Great Maghreb Sultanate", which was nominally established in 1989 but carried little practical weight with its coastal neighbors.
Provinces and districts.
Algeria is divided into 48 provinces ("wilayas"), 553 districts ("daïras") and 1,541 municipalities ("baladiyahs"). Each province, district, and municipality is named after its seat, which is usually the largest city.
According to the Algerian constitution, a province is "a territorial collectivity enjoying some economic freedom".
The People's Provincial Assembly is the political entity governing a province, which has a "president", who is elected by the members of the assembly. They are in turn elected on universal suffrage every five years. The "Wali" (Prefect or governor) directs each province. This person is chosen by the Algerian President to handle the PPA's decisions.
Economy.
The fossil fuels energy sector is the backbone of Algeria's economy, accounting for roughly 60% of budget revenues, 30% of GDP, and over 95% of export earnings. The country ranks fourteenth in petroleum reserves, containing of proven oil reserves with estimates suggesting that the actual amount is even more. The U.S. Energy Information Administration reported that in 2005, Algeria had 160 trillion cubic feet (Tcf) of proven natural gas reserves (4,502 billion cubic metres), the eighth largest in the world.
Algeria’s financial and economic indicators improved during the mid-1990s, in part because of policy reforms supported by the International Monetary Fund (IMF) and debt rescheduling from the Paris Club. Algeria's finances in 2000 and 2001 benefited from an increase in oil prices and the government’s tight fiscal policy, leading to a large increase in the trade surplus, record highs in foreign exchange reserves, and reduction in foreign debt.
The government's continued efforts to diversify the economy by attracting foreign and domestic investment outside the energy sector have had little success in reducing high unemployment and improving living standards, however. In 2001, the government signed an Association Treaty with the European Union that will eventually lower tariffs and increase trade. In March 2006, Russia agreed to erase $4.74 billion of Algeria's Soviet-era debt during a visit by President Vladimir Putin to the country, the first by a Russian leader in half a century. In return, president Bouteflika agreed to buy $7.5 billion worth of combat planes, air-defense systems and other arms from Russia, according to the head of Russia's state arms exporter Rosoboronexport.
Algeria also decided in 2006 to pay off its full $8bn (£4.3bn) debt to the Paris Club group of rich creditor nations before schedule. This will reduce the Algerian foreign debt to less than $5bn in the end of 2006. The Paris Club said the move reflected Algeria's economic recovery in recent years.
Agriculture.
Algeria has always been noted for the fertility of its soil. 25% of Algerians are employed in the agricultural sector.
A considerable amount of cotton was grown at the time of the United States' Civil War, but the industry declined afterwards. In the early years of the twentieth century efforts to extend the cultivation of the plant were renewed. A small amount of cotton is also grown in the southern oases. Large quantities of dwarf palm are cultivated for the leaves, the fibers of which resemble horsehair. The olive (both for its fruit and oil) and tobacco are cultivated with great success.
More than are devoted to the cultivation of cereal grains. The Tell Atlas is the grain-growing land. During the time of French rule its productivity was increased substantially by the sinking of artesian wells in districts which only required water to make them fertile. Of the crops raised, wheat, barley and oats are the principal cereals. A great variety of vegetables and fruits, especially citrus products, are exported. Algeria also exports figs, dates, esparto grass, and cork. It is the largest oat market in Africa.
Algeria is known for Bertolli's olive oil spread, although the spread has an Italian background.
Demographics.
The population of Algeria is 35,190,000 (January 2009 est.), with 99% classified ethnically as Berber/Arab.
About 70% of Algerians live in the northern, coastal area; the minority who inhabit the Sahara are mainly concentrated in oases, although some 1.5 million remain nomadic or partly nomadic. Almost 30% of Algerians are under 15. Algeria has the fourth lowest fertility rate in the Greater Middle East, after those of Cyprus, Tunisia, and Turkey.
The ethnic ancestry of most Algerians is composed of Berber (mostly Zenata and Numidians) and Middle Eastern populations that have invaded northwest Africa at different periods of history and mixed with its inhabitants, such as the Arab tribes (Banu Hilal, Matiql, Sulaym, Adnani) who came in the 10th century AD, other groups that influenced the country include: Phoenicians, Turks, Syrians, Muslims of the Mid-East, Muslims of Spain, Vandals and Romans.
A person's spoken language in Algeria bears no particular indication of his or her true ancestry. This is why Arabic speaking Algerians consider themselves as Arabs or part of the Arab identity, while Berber-speaking Algerians consider themselves as Berbers or part of the Berber identity. Both identities co-exist, the most widely spoken language is Algerian Arabic and all its varities by regions, most common Berber languages are Kabyle and Chaoui. French is widely understood and Standard Arabic (FosHaa) is taught and understand to and by most Algerian youth.
Europeans account for less than 1% of the population, inhabiting almost exclusively the largest metropolitan areas. However, during the colonial period there was a large (15.2% in 1962) European population, consisting primarily of French people, in addition to Spaniards in the west of the country, Italians and Maltese in the east, and other Europeans in smaller numbers. Known as "pieds-noirs", European colonists were concentrated on the coast and formed a majority of the population of Oran (60%) and important proportions in other large cities like Algiers and Annaba. Almost all of this population left during or immediately after the country's independence from France.
Shortages of housing and medicine continue to be pressing problems in Algeria. Failing infrastructure and the continued influx of people from rural to urban areas has overtaxed both systems. According to the UNDP, Algeria has one of the world's highest per housing unit occupancy rates for housing, and government officials have publicly stated that the country has an immediate shortfall of 1.5 million housing units.
Women make up 70 percent of Algeria's lawyers and 60 percent of its judges, and also dominate the field of medicine. Increasingly, women are contributing more to household income than men. Sixty percent of university students are women, according to university researchers.
It is estimated that 95,700 refugees and asylum-seekers have sought refuge in Algeria. This includes roughly 90,000 from Morocco and 4,100 from Palestine. An estimated 90,000 to 160,000 Sahrawis – people from the disputed territory of Western Sahara – live in refugee camps in the Algerian part of the Sahara Desert. There are currently around 35,000 Chinese migrant workers in Algeria.
Ethnic groups.
The ethnic composition of Algeria is mixed Arab and Berber origin. No official figures can be given, because Algerian law forbids population censuses based on ethnic, religious and linguistic criteria. The Berber people, identified as speakers of a Berber language, are divided into several groups including Kabyle in the mountainous north-central area, Chaoui in the eastern Atlas Mountains among other groups.
Languages.
Algerian colloquial Arabic is spoken as a native or as a second language language by more than 83% of the population; of these, over 65% speak Algerian Arabic and around 10% Hassaniya. Algerian Arabic is spoken as a second language by many Berbers. However, in the media and on official occasions the spoken language is Standard Arabic.
The Berbers (or Imazighen) speak one of the various dialects of Tamazight, which add up to around 28% of the population. Arabic remains Algeria's only official language, although Tamazight has recently been recognized as a national language.
French is the most widely studied foreign language in the country, and a majority of Algerians can understand it or speak it, though it is usually not spoken in daily life. Since independence, the government has pursued a policy of linguistic Arabization of education and bureaucracy, which resulted mainly in limiting the use of Berber and the Arabization of many Berber-speakers, while the strong position of French in Algeria was hardly affected by the Arabization policy. All scientific and business university courses are still taught in French to date. Recently, schools have even started to incorporate French into the curriculum as early as children start to learn written classical Arabic. French is also used in media and business. After a political debate in Algeria in the late 90s about whether to replace French with English in the educational system, the government decided to retain French. English is mostly taught only as an optional foreign language in secondary schools.
Religion.
Islam is the predominant religion, followed by more than 99 percent of the country's population. This figure includes all these born in families considered of Muslim descent.
Officially, nearly 100% of all Algerians are Muslims, but atheists and other kinds of non-believers are not counted in the statistics. Nearly all Algerians follow Sunni Islam, with the exception of some 200,000 ibadis in the M'zab Valley in the region of Ghardaia.
There are also some 150,000 Christians in the country, including about 10,000 Roman Catholics and 50,000 to 100,000 evangelical Protestants (mainly Pentecostal), according to the Protestant Church of Algeria's leader Mustapha Krim.
Algeria had an important Jewish community until the 1960s. Nearly all of this community emigrated following the country's independence, although a very small number of Jews continue to live in Algiers.
Health.
In 2002 Algeria had inadequate numbers of physicians (1.13 per 1,000 people), nurses (2.23 per 1,000 people), and dentists (0.31 per 1,000 people). Access to “improved water sources” was limited to 92 percent of the population in urban areas and 80 percent of the population in rural areas. Some 99 percent of Algerians living in urban areas, but only 82 percent of those living in rural areas, had access to “improved sanitation.” According to the World Bank, Algeria is making progress toward its goal of “reducing by half the number of people without sustainable access to improved drinking water and basic sanitation by 2015.” Given Algeria’s young population, policy favors preventive health care and clinics over hospitals. In keeping with this policy, the government maintains an immunization program. However, poor sanitation and unclean water still cause tuberculosis, hepatitis, measles, typhoid fever, cholera, and dysentery. The poor generally receive health care free of charge.
Education.
Education is officially compulsory for children between the ages of six and fifteen. In the year 1997, there was an outstanding amount of teachers and students in primary schools. About 30% of the adult population of the country are illiterate.
Culture.
Modern Algerian literature, split between Arabic and French, has been strongly influenced by the country's recent history. Famous novelists of the twentieth century include Mohammed Dib, Albert Camus, and Kateb Yacine, while Assia Djebar is widely translated. Among the important novelists of the 1980s were Rachid Mimouni, later vice-president of Amnesty International, and Tahar Djaout, murdered by an Islamist group in 1993 for his secularist views.
In philosophy and the humanities, Jacques Derrida, the father of deconstruction, was born in El Biar in Algiers; Malek Bennabi and Frantz Fanon are noted for their thoughts on decolonization; Augustine of Hippo was born in Tagaste (modern-day Souk Ahras); and Ibn Khaldun, though born in Tunis, wrote the Muqaddima while staying in Algeria.
Algerian culture has been strongly influenced by Islam, the main religion. The works of the Sanusi family in pre-colonial times, and of Emir Abdelkader and Sheikh Ben Badis in colonial times, are widely noted. The Latin author Apuleius was born in Madaurus (Mdaourouch), in what later became Algeria.
In painting, Mohammed Khadda and M'Hamed Issiakhem have been notable in recent years.
UNESCO World Heritage Sites in Algeria.
There are several UNESCO World Heritage Sites in Algeria including Al Qal'a of Beni Hammad, the first capital of the Hammadid empire; Tipasa, a Phoenician and later Roman town; and Djémila and Timgad, both Roman ruins; M'Zab Valley, a limestone valley containing a large urbanized oasis; also the Casbah of Algiers is an important citadel. The only natural World Heritage Sites is the Tassili n'Ajjer, a mountain range.
---END.OF.DOCUMENT---

List of Atlas Shrugged characters.
This is a list of characters in Ayn Rand's novel, "Atlas Shrugged."
Major characters.
The following are major characters from the novel.
Ragnar Danneskjöld.
One of the original strikers. He is now world famous as a pirate. Ragnar was from Norway, the son of a bishop and the scion of one of Norway's most ancient, noble families. He attended Patrick Henry University and became friends with John Galt and Francisco d'Anconia, while studying under Hugh Akston and Robert Stadler. When he became a pirate, he was disowned and excommunicated. There is a price on his head in Norway, Portugal, and Turkey; at one point, a policeman remarks that the worldwide rewards offered for his capture total $3 million.
Danneskjöld seizes relief ships that are being sent from the United States to The People's States of Europe. As the novel progresses, Ragnar begins, for the first time, to become active in American waters, and is even spotted in Delaware Bay. Reportedly, his ship is better than any available in the fleets of the world's navies. People assume that as a pirate he simply takes the seized goods for himself. However, while many other protagonists take pride in making a personal profit from the proceeds of their creativity, Danneskjöld's motivation is to restore to other creative people the money which was unjustly taken away from them - specifically, their income tax payments.
For that purpose, Danneskjöld maintains a network of informants who provide him with detailed copies of the tax receipts; among other talents, he is mentioned as being a skilled accountant. The proceeds from the goods he seizes are deposited in accounts opened in Midas Mulligan's bank in the names of various industrialists, to the amounts of the income tax taken from them - which are handed to them (in gold) upon their joining the strikers.
Kept in the background for much of the book, Danneskjöld makes a personal appearance when he risks his life to meet "Hank Rearden" in the night and hand him a bar of gold as an "advance payment", to encourage Rearden to persevere in his increasingly difficult situation.
As a robber with ideological principles, Danneskjöld might be compared with Robin Hood, but he considers himself as the opposite of that what Robin Hood is remembered for, and indeed he considers Robin Hood as an arch-enemy which he had sworn to pursue and destroy. Rather, not Robin Hood the person, who is long dead, or even what Robin Hood stood for, giving back what was stolen by corrupt officials to those it was stolen from, but what Robin Hood has come to be remembered as the principle that it is permissible to rob the productive rich and give to the poor, a principle which in Danneskjöld's (and Rand's) view is highly pernicious.
In the conversation with Rearden, Danneskjöld claims to limit himself to attacks on government property and never touch private property. This contradicts previous chapters where there is mention of Danneskjöld sinking ships belonging to d'Anconia Copper and destroying Orren Boyle's plant on the coast of Maine, where Boyle attempted to produce Rearden Metal. However, the first does not truly constitute robbery, since it was done with the consent of and in collusion with the owner, Danneskjöld's old friend Francisco d'Anconia, and was aimed at helping Francisco's efforts to destroy his own company. And the second was in reaction to Boyle having violated, with government sanction, Rearden's intellectual property.
Danneskjöld is married to the actress Kay Ludlow - a relationship kept hidden from the outside world, which only knows of Ludlow as a former famous film star who retired and dropped out of sight. It is mentioned that some of the strikers have strong reservations about his way of "conducting the common struggle".
Members of Danneskjöld's crew, other than himself, are never named nor appear in the book. In the end of the book, Danneskjöld's crew are mentioned as preparing to form a new community, while his ship would be converted into "a modest ocean liner". Danneskjöld himself refreshes his knowledge of Aristotle and prepares to become a full-time philosopher, and it is hinted that posterity might remember him mainly as Hugh Akston's disciple rather than as a pirate.
According to Barbara Branden, who was closely associated with Rand at the time the book was written, there were sections written describing directly Danneskjöld's adventures at sea which were cut out from the final published text. In the published book, Danneskjöld is always seen through the eyes of others (Dagny Taggart or Hank Rearden) except for a brief paragraph at the very last chapter.
Francisco d'Anconia.
One of the central characters in "Atlas Shrugged". Owner by inheritance of the world's largest copper mining empire, the man behind the San Sebastián Mines, and a childhood friend and first love of Dagny Taggart.
Francisco began working on the sly as a teenager in order to learn all he could about business. While still a student at Patrick Henry University, a classmate of John Galt and Ragnar Danneskjöld and student of both Hugh Akston and Robert Stadler, he began working at a copper foundry, and investing in the stock market. By the time he was twenty he had made enough to purchase the foundry. He began working for d'Anconia Copper as assistant superintendent of a mine in Montana, but was quickly promoted to head of the New York office. In this way he proved that, though unlike other characters he was born to wealth and power, he could have made a successful career all by himself. He took over d'Anconia Copper at age 23, after the death of his father.
When he was 26, Francisco secretly joined the strikers and began to slowly destroy the d'Anconia empire so the looters could not get it. His actions were also specifically designed both to "trap" looters into relying upon, or seizing, his worthless ventures to disrupt their schemes and to try to show them and the rest of the world the inevitable consequences of looting. In the latter he failed, becoming a rather tragic, Cassandra-esque figure. He adopted the persona of a worthless playboy, by which he is known to the world, as an effective cover.
He was the childhood friend of Dagny Taggart and Eddie Willers and later became Dagny's lover. Giving her up—since, knowing her intimately, he knew she would not be ready to join the strikers—was the hardest part for him. He remains deeply in love with her to the end of the book, while also being a good and loyal friend of her other two lovers, John Galt and Hank Rearden.
His full name is Francisco Domingo Carlos Andres Sebastián d'Anconia, and he was born in Argentina.
Dr. Floyd Ferris.
Ferris is a biologist who works as "co-ordinator" at the State Science Institute. He uses his position there to deride reason and productive achievement. The Institute publishes his book, "Why Do You Think You Think?", in which he calls reason "an irrational idea" that is "incapable of dealing with the nature of the universe." He clashes on several occasions with Hank Rearden. When Rearden Metal is first produced, Ferris has the Institute put out a statement raising doubts about it. He twice attempts to blackmail Rearden. The first attempt, which fails, is to get him to sell Rearden Metal to the Institute for use on Project X. The second attempt, which succeeds, is to get Rearden to sign the rights to Rearden Metal over to the government. He is also one of the group of looters who tries to get Rearden to agree to the Steel Unification Plan.
Ferris hosts the demonstration of the Project X weapon, and gets Dr. Robert Stadler to publicly endorse it. When John Galt is captured by the looters, Ferris tries to convince him to help them by suggesting that one-third of children and the elderly will be executed if Galt refuses. Later he uses a device called the "Ferris Persuader" to torture Galt, but it breaks down before extracting the information Ferris wants from Galt.
John Galt.
The enigmatic John Galt is the primary male hero of "Atlas Shrugged". He initially appears as an unnamed menial worker for Taggart Transcontinental who often dines with Eddie Willers in the employee's cafeteria. Eddie finds him very easy to talk to, and the unnamed worker leads him on so that Eddie reveals important information about Dagny Taggart and Taggart Transcontinental; only Eddie's side of each conversation is given in the novel. Eddie tells him which suppliers and contractors Dagny is most dependent on, and with remarkable consistency, those are the next men to disappear mysteriously. Later in the novel the reader discovers the true identity of this worker is John Galt.
Wesley Mouch.
The incompetent lobbyist whom Hank Rearden reluctantly employs in Washington. Later in the novel, he becomes the country's economic dictator.
Henry Rearden.
Henry (also known as "Hank") is one of the central characters in "Atlas Shrugged". Like many of Rand's capitalist characters, he is a self-made man who started as an ordinary worker, showed talent, founded Rearden Steel and made it the most important steel company of the US (and one of the most important businesses of any kind). Later, he conceived of and invented the Rearden Metal, a form of metal stronger than steel (it stands to steel as steel stands to ordinary iron).
He is a demanding employer, intolerant of sloppy work, but pays his workers salaries "above any union scale". He arouses a strong feeling of loyalty among the workers, and was never faced with a strike.
He lives in Philadelphia with his wife Lillian, his brother Philip, and his elderly mother (whose name never appears in the book), all of whom he supports. Gwen Ives is his secretary.
The character of Hank Rearden has two important roles to play in the novel. First, he is aware that there is something wrong with the world but is unsure of what it is. Rearden is guided toward an understanding of the solution through his friendship with Francisco d'Anconia, who does know the secret, and by this mechanism the reader is also prepared to understand the secret when it is revealed explicitly in Galt's Speech.
Second, Rearden is used to illustrate Rand's theory of sex. Lillian Rearden cannot appreciate Hank Rearden's virtues, and she is portrayed as being disgusted by sex. Dagny Taggart clearly does appreciate Rearden's virtues, and this appreciation evolves into a sexual desire. Rearden is torn by a contradiction because he accepts the premises of the traditional view of sex as a lower instinct, while responding sexually to Dagny, who represents his highest values. Rearden struggles to resolve this internal conflict and in doing so illustrates Rand's sexual theory.
Lillian Rearden.
The unsupportive wife of Hank Rearden. They have been married eight years as the novel begins. Lillian is a frigid moocher who seeks to destroy her husband. She compares being Rearden's wife with owning the world's most powerful horse. Since she cannot comfortably ride a horse that goes too fast, she must bridle it down to her level, even if that means it will never reach its full potential and its power will be grievously wasted.
As her motives become more clear, Lillian is found to share the sentiments of many other moochers and their worship of destruction. Her actions are explained as the desire to destroy achievement in the false belief that such an act bestows a greatness to the destroyer equal to the accomplishment destroyed. She seeks, then, to ruin Rearden in an effort to prove her own value, but fails.
Lillian tolerates sex with her husband only because she is 'realistic' enough to know he is just a brute who requires satisfaction of his brute instincts. She indicates that she abhors Francisco d'Anconia, because she believes he is a sexual adventurer.
Dagny Taggart.
Dagny Taggart is the protagonist of the novel. She is Operating Vice-President in Charge of Operations for Taggart Transcontinental, under her brother, James Taggart. However, due to James' incompetence, it is Dagny that is actually responsible for all the workings of the railroad.
Dagny encounters three romantic relationships, each with a man of ability: Francisco d'Anconia, Hank Rearden, and John Galt. Galt marks the pinnacle of everything Dagny seeks in the world and is the kind of man alluded to in her youth, whom she imagines a man, standing off in the distance, at the end of a great set of railroad tracks, at the end of all her struggles.
The essential drama of Dagny's character is her struggle to reconcile the life she lives and the railroad which she loves, with the moral code of those who wish to destroy it. She believes they simply want to heap burdens upon her, for the sake of others, which she has the ability to carry. Like Hank, she believes they basically want to live, but are too stupid and incompetent to realize how their duties and altruistic projects impede that goal. It is not until she sees the man most important to her in the world - John Galt - strapped to a torture machine, about to be killed by the looters (who recognize, too, that he is the only man who can save them from economic collapse), that she realizes that the moral code of the looters is one of death: that they recognize what is good and necessary for life, but wish to destroy it anyway.
She is a typical Randian heroine, similar to Dominique Francon ("The Fountainhead") or Kira Argounova ("We the Living").
James Taggart.
The President of Taggart Transcontinental and the book's most important antagonist. Taggart is an expert influence peddler who is, however, incapable of making operational decisions on his own. He relies on his sister Dagny Taggart to actually run the railroad, but nonetheless opposes her in almost every endeavor. In a sense, he is the antithesis of Dagny.
As the novel progresses, the moral philosophy of the looters is revealed: it is a code of stagnation. The goal of this code is to not exist, to not move forward, to become a zero. Taggart struggles to remain unaware that this is his goal. He maintains his pretense that he wants to live, and becomes horrified whenever his mind starts to grasp the truth about himself. This contradiction leads to the recurring absurdity of his life: the desire to destroy those on whom his life depends, and the horror that he will succeed at this. In the final chapters of the novel, he suffers a complete mental breakdown upon realizing that he can no longer deceive himself in this respect.
Dr. Robert Stadler.
A former professor at Patrick Henry University, mentor to Francisco d'Anconia, John Galt and Ragnar Danneskjöld. He has since become a sell-out, one who had great promise but squandered it for social approval, to the detriment of the free. He works at the State Science Institute where all his inventions are perverted for use by the military, including the instrument of his demise: Project X. The character was, in part, modeled on J. Robert Oppenheimer, whom Rand had interviewed for an earlier project, and his part in the creation of nuclear weapons.
Secondary characters.
The following secondary characters also appear in the novel.
Planned characters not in final version.
In the introduction to the 35th anniversary edition, (1991), Leonard Peikoff introduced excerpts from Rand's journals concerning the book, which she originally intended to call "The Strike". Among many other things, the journals reveal some characters which were originally planned to appear in the book and were deleted from the final version.
Peikoff says that "Father Amadeus was Taggart's priest, to whom he confessed his sins. The priest was supposed to be a positive character honestly devoted to the good but practicing consistently the morality of mercy. Miss Rand dropped him, she told me, when she found that it was impossible to make such a character convincing." The quotation from Rand's journals included a passage describing what John Galt represented to each main characters, including one about Father Amadeus: "For Father Amadeus [Galt represents] the source of the conflict. The uneasy realization that Galt is the end of his endeavors, the man of virtue, the perfect man - and that his means do not fit this end (and that he is destroying this, his ideal, for the sake of those who are evil)."
Peikoff also mentions that as originally conceived the book was going to have a character named Stacy Rearden, a sister of Hank Rearden. Not much is told of what her role was supposed to be and why she was eventually dropped. Apparently, she was going to be another parasite like Rearden's brother, mother and wife; presumably, Rand came to the conclusion that three such characters around Rearden sufficiently fulfilled her literary and philosophical purposes.
---END.OF.DOCUMENT---

Anthropology.
Anthropology is the study of humanity. Anthropology has origins in the natural sciences, the humanities, and the social sciences. The term "anthropology", is from the Greek, "anthrōpos", "human", and -λογία, "-logia", "discourse" or "study", and was first used by François Péron when discussing his encounters with Tasmanian Aborigines.
Anthropology's basic concerns are "What defines "Homo sapiens"?", "Who are the ancestors of modern "Homo sapiens"?", "What are humans' physical traits?", "How do humans behave?", "Why are there variations and differences among different groups of humans?", "How has the evolutionary past of "Homo sapiens" influenced its social organization and culture?" and so forth.
In the United States, contemporary anthropology is typically divided into four sub-fields: cultural anthropology (also called "social anthropology"), archaeology, linguistic anthropology and biological/physical anthropology. The so-called "four-field" approach to anthropology is reflected in many undergraduate textbooks as well as anthropology programs (e.g. Michigan, Berkeley, UPenn, etc.). At universities in the United Kingdom, and much of Europe, these "sub-fields" are frequently housed in separate departments and are seen as distinct disciplines.
The social and cultural sub-field has been heavily influenced by post-modern theories. During the 1970s and 1980s there was an epistemological shift away from the positivist traditions that had largely informed the discipline. During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in Cultural and Social Anthropology. In contrast, Archaeology, Biological Anthropology, and linguistic anthropology remained largely positivist. Due to this difference in epistemology, anthropology as a discipline has lacked cohesion over the last several decades. This has even led to departments diverging, for example in the 1998-9 academic year at Stanford University, where the "scientists" and "non-scientists" divided into two departments: Anthropology, and Cultural and Social Anthropology. (Anthropology at Stanford later reunified in the 2008-9 academic year)
Overview.
Anthropology is traditionally divided into four sub-fields, each with its own further branches: biological or physical anthropology, social anthropology or cultural anthropology, archaeology and anthropological linguistics. These fields frequently overlap, but tend to use different methodologies and techniques.
Biological anthropology or Physical anthropology, focuses on the study of human populations using an evolutionary framework. Biological anthropologists have theorized about how the globe has become populated with humans (e.g. the "Out of Africa" and "multi-regional evolution" debate), as well as tried to explain geographical human variation and race. Many biological anthropologists studying modern human populations identify their field as human ecology - itself linked to sociobiology. Human ecology uses evolutionary theory to understand phenomena among contemporary human populations. Another large sector of biological anthropology is primatology, where anthropologists focus on understanding other primate populations. Methodologically, primatologists borrow heavily from field biology and ecology in their research.
Cultural anthropology is also called socio-cultural anthropology or social anthropology (especially in Great Britain). It is the study of culture, and is often based on ethnography. Ethnography can refer to both a methodology and a product of research, namely a monograph or book. Ethnography is a grounded, inductive method, that heavily relies on participant-observation. Ethnology involves the systematic comparison of different cultures. In some European countries, all cultural anthropology is known as ethnology (a term coined and defined by Adam F. Kollár in 1783).
The study of kinship and social organization is a central focus of cultural anthropology, as kinship is a human universal. Cultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology).
Archaeology is the study of human material culture, including both artifacts (older pieces of human culture) carefully gathered "in situ", museum pieces and modern garbage. Archaeologists work closely with biological anthropologists, art historians, physics laboratories (for dating), and museums. They are charged with preserving the results of their excavations and are often found in museums. Typically, archaeologists are associated with "digs," or excavation of layers of ancient sites.
Archaeologists subdivide time into cultural periods based on long-lasting artifacts: the Paleolithic, the Neolithic, the Bronze Age, which are further subdivided according to artifact traditions and culture region, such as the Oldowan or the Gravettian. In this way, archaeologists provide a vast frame of reference for the places human beings have traveled, their ways of making a living, and their demographics. Archaeologists also investigate nutrition, symbolization, art, systems of writing, and other physical remnants of human cultural activity.
Linguistic anthropology (also called anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis.
Linguistic anthropology is divided into its own sub-fields: descriptive linguistics the construction of grammars and lexicons for unstudied languages; historical linguistics, including the reconstruction of past languages, from which our current languages have descended; ethnolinguistics, the study of the relationship between language and culture, and sociolinguistics, the study of the social functions of language. Anthropological linguistics is also concerned with the evolution of the parts of the brain that deal with language.
Because anthropology developed from so many different enterprises (see History of Anthropology), including but not limited to fossil-hunting, exploring, documentary film-making, paleontology, primatology, antiquity dealings and curatorship, philology, etymology, genetics, regional analysis, ethnology, history, philosophy and religious studies, it is difficult to characterize the entire field in a brief article, although attempts to write histories of the entire field have been made.
On the one hand this has led to instability in many American anthropology departments, resulting in the division or reorganization of sub-fields (e.g. at Stanford, Duke, and most recently at Harvard). However, seen in a positive light, anthropology is one of the few place in many American universities where humanities, social, and natural sciences are forced to confront one another. As such, anthropology has also been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, human-computer interaction, and various ethnic studies.
Basic trends.
There are several characteristics that tend to unite anthropological work. One of the central characteristics is that anthropology tends to provide a comparatively more holistic account of phenomena and tends to be highly empirical. The quest for holism leads most anthropologists to study a particular place or thing in detail, using a variety of methods, over a more extensive period than normal in many parts of academia.
The specific focus of social and cultural anthropology has significantly changed. Initially the sub-field was focused on the study of cultures around the world.
In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.
Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures) They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists "into the field" which means traveling to a community in its own setting, to do something called "fieldwork." On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs. Due to the interest in variation, anthropologists are drawn to the study of human extremes, aberrations and other unusual circumstances, such as headhunting, whirling dervishes, whether there were real Hobbit people, snake handling, and glossolalia (speaking in tongues), just to list a few.
At the same time, anthropologists urge, as part of their quest for scientific objectivity, cultural relativism, which has an influence on all the sub-fields of anthropology. This is the notion that particular cultures should not be judged by one culture's values or viewpoints, but that all cultures should be viewed as relative to each other. There should be no notions, in good anthropology, of one culture being better or worse than another culture.
Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation including especially circumcision and subincision, and torture. Topics like racism, slavery or human sacrifice, therefore, attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as "racism" and find thousands of anthropological references, stretching across all the major and minor sub-fields.
Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology. Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levallois help archaeologists and other anthropologists in understanding major trends in the human past. Anthropologists and geographers share approaches to Culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science.
Contemporary anthropology is an established science with academic departments at most universities and colleges. The single largest organization of Anthropologists is the American Anthropological Association, which was founded in 1903. Membership is made up of Anthropologists from around the globe. Hundreds of other organizations exist in the various sub-fields of anthropology, sometimes divided up by nation or region, and many anthropologists work with collaborators in other disciplines, such as geology, physics, zoology, paleontology, anatomy, music theory, art history, sociology and so on, belonging to professional societies in those disciplines as well.
History.
The first use of the term "anthropology" in English to refer to a natural science of humankind was apparently in 1593, the first of the "logies" to be coined. It took Immanuel Kant 25 years to write one of the first major treatises on anthropology, his "Anthropology from a Pragmatic Point of View". Kant is not generally considered to be a modern anthropologist, however, as he never left his region of Germany nor did he study any cultures besides his own, and in fact, describes the need for anthropology as a corollary field to his own primary field of philosophy. He did, however, begin teaching an annual course in anthropology in 1772. Anthropology is thus primarily an Enlightenment and post-Enlightenment endeavor.
Historians of anthropology, like Marvin Harris, indicate two major frameworks within which empirical anthropology has arisen: interest in comparisons of people over space and interest in longterm human processes or humans as viewed through time. Harris dates both to Classical Greece and Classical Rome, specifically Herodotus, often called the "father of history" and the Roman historian Tacitus, who wrote many of our only surviving contemporary accounts of several ancient Celtic and Germanic peoples. Herodotus first formulated some of the persisting problems of anthropology.
Medieval scholars may be considered forerunners of modern anthropology as well, insofar as they conducted or wrote detailed studies of the customs of peoples considered "different" from themselves in terms of geography. John of Plano Carpini reported of his stay among the Mongols. His report was unusual in its detailed depiction of a non-European culture!
Marco Polo's systematic observations of nature, anthropology, and geography are another example of studying human variation across space. Polo's travels took him across such a diverse human landscape and his accounts of the peoples he met as he journeyed were so detailed that they earned for Polo the name "the father of modern anthropology."
Another candidate for one of the first scholars to carry out comparative ethnographic-type studies in person was the medieval Persian scholar Abū Rayhān Bīrūnī in the 11th century, who wrote about the peoples, customs, and religions of the Indian subcontinent. Like modern anthropologists, he engaged in extensive participant observation with a given group of people, learnt their language and studied their primary texts, and presented his findings with objectivity and neutrality using cross-cultural comparisons. He wrote detailed comparative studies on the religions and cultures in the Middle East, Mediterranean and especially South Asia. Biruni's tradition of comparative cross-cultural study continued in the Muslim world through to Ibn Khaldun's work in the 14th century.
Most scholars consider modern anthropology as an outgrowth of the Age of Enlightenment, a period when Europeans attempted systematically to study human behavior, the known varieties of which had been increasing since the 15th century as a result of the first European colonization wave. The traditions of jurisprudence, history, philology, and sociology then evolved into something more closely resembling the modern views of these disciplines and informed the development of the social sciences, of which anthropology was a part.
Developments in the systematic study of ancient civilizations through the disciplines of Classics and Egyptology informed both archaeology and eventually social anthropology, as did the study of East and South Asian languages and cultures. At the same time, the Romantic reaction to the Enlightenment produced thinkers, such as Johann Gottfried Herder and later Wilhelm Dilthey, whose work formed the basis for the "culture concept," which is central to the discipline.
Institutionally, anthropology emerged from the development of natural history (expounded by authors such as Buffon) that occurred during the European colonization of the 17th, 18th, 19th and 20th centuries. Programs of ethnographic study originated in this era as the study of the "human primitives" overseen by colonial administrations.
There was a tendency in late 18th century Enlightenment thought to understand human society as natural phenomena that behaved according to certain principles and that could be observed empirically. In some ways, studying the language, culture, physiology, and artifacts of European colonies was not unlike studying the flora and fauna of those places.
Early anthropology was divided between proponents of unilinealism, who argued that all societies passed through a single evolutionary process, from the most primitive to the most advanced, and various forms of non-lineal theorists, who tended to subscribe to ideas such as diffusionism. Most 19th-century social theorists, including anthropologists, viewed non-European societies as windows onto the pre-industrial human past.
As academic disciplines began to differentiate over the course of the 19th century, anthropology grew increasingly distinct from the biological approach of natural history, on the one hand, and from purely historical or literary fields such as Classics, on the other. A common criticism has been that many social science scholars (such as economists, sociologists, and psychologists) in Western countries focus disproportionately on Western subjects, while anthropology focuses disproportionately on the "Other"; this has changed over the last part of the 20th century as anthropologists increasingly also study Western subjects, particularly variation across class, region, or ethnicity within Western societies, and other social scientists increasingly take a global view of their fields.
20th century.
In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The natural and biological "sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras.
The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences. In particular, social sciences often develop statistical descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains.
Anthropology as it emerged amongst the Western colonial powers (mentioned above) has generally taken a different path than that in the countries of southern and central Europe (Italy, Greece, and the successors to the Austro-Hungarian and Ottoman empires). In the former, the encounter with multiple, distinct cultures, often very different in organization and language from those of Europe, has led to a continuing emphasis on cross-cultural comparison and a receptiveness to certain kinds of cultural relativism.
In the successor states of continental Europe, on the other hand, anthropologists often joined with folklorists and linguists in building nationalist perspectives. Ethnologists in these countries tended to focus on differentiating among local ethnolinguistic groups, documenting local folk culture, and representing the prehistory of what has become a nation through various forms of public education (eg, museums of several kinds).
In this scheme, Russia occupied a middle position. On the one hand, it had a large region (largely east of the Urals) of highly distinct, pre-industrial, often non-literate peoples, similar to the situation in the Americas. On the other hand, Russia also participated to some degree in the nationalist (cultural and political) movements of Central and Eastern Europe. After the Revolution of 1917, anthropology in the USSR, and later the Soviet Bloc countries, were highly shaped by the requirement to conform to Marxist theories of social evolution.
Britain.
E. B. Tylor (2 October 1832 – 2 January 1917) and James George Frazer (1 January 1854 – 7 May 1941) are generally considered the antecedents to modern social anthropology in Britain. Though Tylor undertook a field trip to Mexico, both he and Frazer derived most of the material for their comparative studies through extensive reading, not fieldwork, mainly the Classics (literature and history of Greece and Rome), the work of the early European folklorists, and reports from missionaries, travelers, and contemporaneous ethnologists.
Tylor advocated strongly for unilinealism and a form of "uniformity of mankind". Tylor in particular laid the groundwork for theories of cultural diffusionism, stating that there are three ways that different groups can have similar cultural forms or technologies: "independent invention, inheritance from ancestors in a distant region, transmission from one race [sic] to another."
Tylor formulated one of the early and influential anthropological conceptions of culture as "that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society." However, as Stocking notes, Tylor mainly concerned himself with describing and mapping the distribution of particular elements of culture, rather than with the larger function, and generally seemed to assume a Victorian idea of progress rather than the idea of non-directional, multilineal cultural development proposed by later anthropologists.
Tylor also theorized about the origins of religious feelings in human beings, proposing a theory of animism as the earliest stage, and noting that "religion" has many components, of which he believed the most important to be belief in supernatural beings (as opposed to moral systems, cosmology, etc.). Frazer, a Scottish scholar with a broad knowledge of Classics, also concerned himself with religion, myth, and magic. His comparative studies, most influentially in the numerous editions of "The Golden Bough", analyzed similarities in religious belief and symbolism globally.
Neither Tylor nor Frazer, however, were particularly interested in fieldwork, nor were they interested in examining how the cultural elements and institutions fit together. Toward the turn of the twentieth century, a number of anthropologists became dissatisfied with this categorization of cultural elements; historical reconstructions also came to seem increasingly speculative.
Under the influence of several younger scholars, a new approach came to predominate among British anthropologists, concerned with analyzing how societies held together in the present (synchronic analysis, rather than diachronic or historical analysis), and emphasizing long-term (one to several years) immersion fieldwork. Cambridge University financed a multidisciplinary expedition to the Torres Strait Islands in 1898, organized by Alfred Court Haddon and including a physician-anthropologist, William Rivers, as well as a linguist, a botanist, other specialists. The findings of the expedition set new standards for ethnographic description.
A decade and a half later, Polish anthropology student Bronisław Malinowski (1884–1942) was beginning what he expected to be a brief period of fieldwork in the old model, collecting lists of cultural items, when the outbreak of the First World War stranded him in New Guinea. As a subject of the Austro-Hungarian Empire resident on a British colonial possession, he was effectively confined to New Guinea for several years.
He made use of the time by undertaking far more intensive fieldwork than had been done by "British" anthropologists, and his classic ethnography, "Argonauts of the Western Pacific" (1922) advocated an approach to fieldwork that became standard in the field: getting "the native's point of view" through participant observation. Theoretically, he advocated a functionalist interpretation, which examined how social institutions functioned to satisfy individual needs.
British social anthropology had an expansive moment in the Interwar period, with key contributions coming from the Polish-British Bronisław Malinowski and Meyer Fortes
A. R. Radcliffe-Brown also published a seminal work in 1922. He had carried out his initial fieldwork in the Andaman Islands in the old style of historical reconstruction. However, after reading the work of French sociologists Émile Durkheim and Marcel Mauss, Radcliffe-Brown published an account of his research (entitled simply "The Andaman Islanders") that paid close attention to the meaning and purpose of rituals and myths. Over time, he developed an approach known as structural-functionalism, which focused on how institutions in societies worked to balance out or create an equilibrium in the social system to keep it functioning harmoniously. (This contrasted with Malinowski's functionalism, and was quite different from the later French structuralism, which examined the conceptual structures in language and symbolism.)
Malinowski and Radcliffe-Brown's influence stemmed from the fact that they, like Boas, actively trained students and aggressively built up institutions that furthered their programmatic ambitions. This was particularly the case with Radcliffe-Brown, who spread his agenda for "Social Anthropology" by teaching at universities across the British Commonwealth. From the late 1930s until the postwar period appeared a string of monographs and edited volumes that cemented the paradigm of British Social Anthropology (BSA). Famous ethnographies include "The Nuer," by Edward Evan Evans-Pritchard, and "The Dynamics of Clanship Among the Tallensi," by Meyer Fortes; well-known edited volumes include "African Systems of Kinship and Marriage" and "African Political Systems."
Max Gluckman, together with many of his colleagues at the Rhodes-Livingstone Institute and students at Manchester University, collectively known as the Manchester School, took BSA in new directions through their introduction of explicitly Marxist-informed theory, their emphasis on conflicts and conflict resolution, and their attention to the ways in which individuals negotiate and make use of the social structural possibilities.
In Britain, anthropology had a great intellectual impact, it "contributed to the erosion of Christianity, the growth of cultural relativism, an awareness of the survival of the primitive in modern life, and the replacement of diachronic modes of analysis with synchronic, all of which are central to modern culture."
Later in the 1960s and 1970s, Edmund Leach and his students Mary Douglas and Nur Yalman, among others, introduced French structuralism in the style of Lévi-Strauss; while British anthropology has continued to emphasize social organization and economics over purely symbolic or literary topics, differences among British, French, and American sociocultural anthropologies have diminished with increasing dialogue and borrowing of both theory and methods. Today, social anthropology in Britain engages internationally with many other social theories and has branched in many directions.
In countries of the British Commonwealth, social anthropology has often been institutionally separate from physical anthropology and primatology, which may be connected with departments of biology or zoology; and from archaeology, which may be connected with departments of Classics, Egyptology, and the like. In other countries (and in some, particularly smaller, British and North American universities), anthropologists have also found themselves institutionally linked with scholars of folklore, museum studies, human geography, sociology, social relations, ethnic studies, cultural studies, and social work.
19th Century to 1940s.
From its beginnings in the early 19th century through the early 20th century, anthropology in the United States was influenced by the presence of Native American societies.
Cultural anthropology in the United States was influenced greatly by the ready availability of Native American societies as ethnographic subjects. The field was pioneered by staff of the Bureau of Indian Affairs and the Smithsonian Institution's Bureau of American Ethnology, men such as John Wesley Powell and Frank Hamilton Cushing.
Lewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from "savagery", to "barbarism", to "civilization". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.
Boasian anthropology.
Franz Boas established academic anthropology in the United States in opposition to this sort of evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.
Influenced by the German tradition, Boas argued that the world was full of distinct "cultures," rather than societies whose evolution could be measured by how much or how little "civilization" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.
In doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called "Four Field Approach" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.
Boas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.
The publication of Alfred Kroeber's textbook, "Anthropology," marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.
Though such works as "Coming of Age in Samoa" and "The Chrysanthemum and the Sword" remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined by Ralph Linton, and Mead was limited to her offices at the AMNH.
Canada.
Canadian anthropology began, as in other parts of the Colonial world, as ethnological data in the records of travellers and missionaries. In Canada, Jesuit missionaries such as Fathers LeClercq, Le Jeune and Sagard, in the 1600s, provide the oldest ethnographic records of native tribes in what was then the Domain of Canada.
True anthropology began with a Government department: the Geological Survey of Canada, and George Mercer Dawson (director in 1895). Dawson's support for anthropology created impetus for the profession in Canada. This was expanded upon by Prime Minister Wilfrid Laurier, who established a Division of Anthropology within the Geological Survey in 1910. Anthropologists were recruited from England and the USA, setting the foundation for the unique Canadian style of anthropology. Scholars include the linguist and Boasian Edward Sapir.
France.
Anthropology in France has a less clear genealogy than the British and American traditions, in part because many French writers influential in anthropology have been trained or held faculty positions in sociology, philosophy, or other fields rather than in anthropology.
Most commentators consider Marcel Mauss (1872–1950), nephew of the influential sociologist Émile Durkheim to be the founder of the French anthropological tradition. Mauss belonged to Durkheim's Année Sociologique group; and while Durkheim and others examined the state of modern societies, Mauss and his collaborators (such as Henri Hubert and Robert Hertz) drew on ethnography and philology to analyze societies which were not as 'differentiated' as European nation states.
Two works by Mauss in particular proved to have enduring relevance: "Essay on the Gift" a seminal analysis of exchange and reciprocity, and his Huxley lecture on the notion of the person, the first comparative study of notions of person and selfhood cross-culturally.
Throughout the interwar years, French interest in anthropology often dovetailed with wider cultural movements such as surrealism and primitivism which drew on ethnography for inspiration. Marcel Griaule and Michel Leiris are examples of people who combined anthropology with the French avant-garde. During this time most of what is known as "ethnologie" was restricted to museums, such as the Musée de l'Homme founded by Paul Rivet, and anthropology had a close relationship with studies of folklore.
Above all, however, it was Claude Lévi-Strauss who helped institutionalize anthropology in France. Along with the enormous influence his structuralism exerted across multiple disciplines, Lévi-Strauss established ties with American and British anthropologists. At the same time he established centers and laboratories within France to provide an institutional context within anthropology while training influential students such as Maurice Godelier and Françoise Héritier who would prove influential in the world of French anthropology. Much of the distinct character of France's anthropology today is a result of the fact that most anthropology is carried out in nationally funded research laboratories (CNRS) rather than academic departments in universities.
Other influential writers in the 1970s include Pierre Clastres, who explains in his books on the Guayaki tribe in Paraguay that "primitive societies" actively oppose the institution of the state. Therefore, these stateless societies are not less evolved than societies with states, but took the active choice of conjuring the institution of authority as a separate function from society. The leader is only a spokesperson for the group when it has to deal with other groups ("international relations") but has no inside authority, and may be violently removed if he attempts to abuse this position.
The most important French social theorist since Foucault and Lévi-Strauss is Pierre Bourdieu, who trained formally in philosophy and sociology and eventually held the Chair of Sociology at the Collège de France. Like Mauss and others before him, however, he worked on topics both in sociology and anthropology. His fieldwork among the Kabyles of Algeria places him solidly in anthropology, while his analysis of the function and reproduction of fashion and cultural capital in European societies places him as solidly in sociology.
Other countries.
Anthropology in Greece and Portugal is much influenced by British anthropology. In Greece, there was since the 19th century a science of the folklore called "laographia" (laography), in the form of "a science of the interior", although theoretically weak; but the connotation of the field deeply changed after World War II, when a wave of Anglo-American anthropologists introduced a science "of the outside". In Italy, the development of ethnology and related studies did not receive as much attention as other branches of learning.
Germany and Norway are the countries that showed the most division and conflict between scholars focusing on domestic socio-cultural issues and scholars focusing on "other" societies.
Post-World War II.
Before WWII British 'social anthropology' and American 'cultural anthropology' were still distinct traditions. After the war, enough British and American anthropologists borrowed ideas and methodological approaches from one another that some began to speak of them collectively as 'sociocultural' anthropology.
In the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche—an approach popularized by Marvin Harris.
Economic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated Lévi-Strauss's structuralism into their work.
Structuralism also influenced a number of developments in 1960s and 1970s, including cognitive anthropology and componential analysis. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. In keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as "Reinventing Anthropology" worried about anthropology's relevance.
Since the 1980s issues of power, such as those examined in Eric Wolf's "Europe and the People Without History", have been central to the discipline. In the 80s books like "Anthropology and the Colonial Encounter" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins (again), who drew on Lévi-Strauss and Fernand Braudel to examine the relationship between social structure and individual agency. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.
In the late 1980s and 1990s authors such as George Marcus and James Clifford pondered ethnographic authority, particularly how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by Feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theorizing and methods became "de rigueur" as part of the 'post-modern moment' in anthropology: Ethnographies became more reflexive, explicitly addressing the author's methodology, cultural, gender and racial positioning, and their influence on his or her ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.
Controversies about its history.
Anthropologists, like other researchers (especially historians and scientists engaged in field research), have over time assisted state policies and projects, especially colonialism.
Military.
Anthropologists' involvement with the U.S. government, in particular, has caused bitter controversy within the discipline. Franz Boas publicly objected to US participation in World War I, and after the war he published a brief expose and condemnation of the participation of several American archaeologists in espionage in Mexico under their cover as scientists.
But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the "Axis" (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces but others worked in intelligence (for example, Office of Strategic Services (OSS) and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies.
Attempts to accuse anthropologists of complicity with the CIA and government intelligence activities during the Vietnam War years have turned up surprisingly little (although anthropologist Hugo Nutini was active in the stillborn Project Camelot). Many anthropologists (students and teachers) were active in the antiwar movement and a great many resolutions condemning the war in all its aspects were passed overwhelmingly at the annual meetings of the American Anthropological Association (AAA).
In the decades since the Vietnam war the tone of cultural and social anthropology, at least, has been increasingly politicized, with the dominant liberal tone of earlier generations replaced with one more radical, a mix of, and varying degrees of, Marxist, feminist, anarchist, post-colonial, post-modern, Saidian, Foucauldian, identity-based, and more.
Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarships ethically dangerous. The AAA's current 'Statement of Professional Responsibility' clearly states that "in relation with their own government and with host governments... no secret research, no secret reports or debriefings of any kind should be agreed to or given."
However, anthropologists, along with other social scientists, are again being used in warfare as part of the. The Christian Science Monitor reports that "Counterinsurgency efforts focus on better grasping and meeting local needs" in Afghanistan, under the rubric of "Human Terrain Team" (HTT).
Focus on other cultures===.
Some authors argue that anthropology originated and developed as the study of "other cultures", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal "Exploring the City: Inquiries Toward an Urban Anthropology" mentions that the "Third World" had habitually received most of attention; anthropologists who traditionally specialized in "other cultures" looked for them far away and started to look "across the tracks" only in late 1960s.
Now there exist many works focusing on peoples and topics very close to the author's "home". It is also argued that other fields of study, like History and Sociology, on the contrary focus disproportionately on the West.
In France, the study of existing contemporary society has been traditionally left to sociologists, but this is increasingly changing, starting in the 1970s from scholars like Isac Chiva and journals like "Terrain" ("fieldwork"), and developing with the center founded by Marc Augé ("Le Centre d'anthropologie des mondes contemporains", the Anthropological Research Center of Contemporary Societies). The same approach of focusing on "modern world" topics by "Terrain", was also present in the British Manchester School of the 1950s.
---END.OF.DOCUMENT---

Agricultural science.
Agricultural science is a broad multidisciplinary field that encompasses the parts of exact, natural, economic and social sciences that are used in the practice and understanding of agriculture. (Veterinary science, but not animal science, is often excluded from the definition.)
Agricultural science: a local science.
With the exception of theoretical agronomy, research in agronomy, more than in any other field, is strongly related to local areas. It can be considered a science of ecoregions, because it is closely linked to soil properties and climate, which are never exactly the same from one place to another. Many people think an agricultural production system relying on local weather, soil characteristics, and specific crops has to be studied locally. Others feel a need to know and understand production systems in as many areas as possible, and the human dimension of interaction with nature.
History of agricultural science.
Agricultural science began with Gregor Mendel's genetic work, but in modern terms might be better dated from the chemical fertilizer outputs of plant physiological understanding in eighteenth century Germany. In the United States, a scientific revolution in agriculture began with the Hatch Act of 1887, which used the term "agricultural science". The Hatch Act was driven by farmers' interest in knowing the constituents of early artificial fertilizer. The Smith-Hughes Act of 1917 shifted agricultural education back to its vocational roots, but the scientific foundation had been built. After 1906, public expenditures on agricultural research in the US exceeded private expenditures for the next 44 years.
Intensification of agriculture since the 1960s in developed and developing countries, often referred to as the Green Revolution, was closely tied to progress made in selecting and improving crops and animals for high productivity, as well as to developing additional inputs such as artificial fertilizers and phytosanitary products.
As the oldest and largest human intervention in nature, the environmental impact of agriculture in general and more recently intensive agriculture, industrial development, and population growth have raised many questions among agricultural scientists and have led to the development and emergence of new fields. These include technological fields that assume the solution to technological problems lies in better technology, such as integrated pest management, waste treatment technologies, landscape architecture, genomics, and agricultural philosophy fields that include references to food production as something essentially different from non-essential economic 'goods'. In fact, the interaction between these two approaches provide a fertile field for deeper understanding in agricultural science.
New technologies, such as biotechnology and computer science (for data processing and storage), and technological advances have made it possible to develop new research fields, including genetic engineering, agrophysics, improved statistical analysis, and precision farming. Balancing these, as above, are the natural and human sciences of agricultural science that seek to understand the human-nature interactions of traditional agriculture, including interaction of religion and agriculture, and the non-material components of agricultural production systems.
Agricultural science and agriculture crisis.
Agriculture sciences seek to feed the world's population while preventing biosafety problems that may affect human health and the environment. This requires promoting good management of natural resources and respect for the environment, and increasingly concern for the psychological wellbeing of all concerned in the food production and consumption system.
Economic, environmental, and social aspects of agriculture sciences are subjects of ongoing debate. Recent crises (such as avian influenza, mad cow disease and issues such as the use of genetically modified organisms) illustrate the complexity and importance of this debate.
---END.OF.DOCUMENT---

Alchemy.
Alchemy, originally derived from the Ancient Greek word "khemia" (Χημία) meaning "art of transmuting metals", later arabicized as "al-kimia" (الكيمياء), is both a philosophy and an ancient practice focused on the attempt to change base metals into gold, investigating the preparation of the "elixir of longevity", and achieving ultimate wisdom, involving the improvement of the alchemist as well as the making of several substances described as possessing unusual properties. The practical aspect of alchemy generated the basics of modern inorganic chemistry, namely concerning procedures, equipment and the identification and use of many current substances.
Alchemy has been practiced in Mesopotamia (comprising much of today's Iraq), Egypt, Persia (today's Iran), India, China, Japan, Korea and in Classical Greece and Rome, in the Post-Islamic Persia, and then in Europe up to the 20th century, in a complex network of schools and philosophical systems spanning at least 2500 years.
Etymology.
The word alchemy derives in turn from the Old French "alkemie"; from the Medieval Latin "alchimia"; from the Arabic "al-kimia" (الكيمياء); and ultimately from the Ancient Greek "khemia" (Χημία) meaning "art of transmuting metals".
During the seventeenth century chemistry as a separate science was derived from Alchemy, with the work of Robert Boyle, sometimes known as "The father of Chemistry", who in his book "The Skeptical Chymist" attacked Paracelsus and the old Aristotelian concepts of the elements and laid down the foundations of modern chemistry.
Alchemy as a philosophical and spiritual discipline.
Alchemy became known as the "spagyric art" after Greek words meaning "to separate" and "to join together" in the 16th century, the word probably being coined by Paracelsus. Compare this with one of the dictums of Alchemy in Latin: Solve et Coagula — "Separate, and Join Together" (or "dissolve and coagulate").
The best-known goals of the alchemists were the transmutation of common metals into gold (called chrysopoeia) or silver (less well known is plant alchemy, or "spagyric"); the creation of a "panacea", or the elixir of life, a remedy that, it was supposed, would cure all diseases and prolong life indefinitely; and the discovery of a universal solvent. Although these were not the only uses for the discipline, they were the ones most documented and well-known. Certain Hermetic schools argue that the transmutation of lead into gold is analogical for the transmutation of the physical body (Saturn or lead) into (Gold) with the goal of attaining immortality. This is described as Internal Alchemy. Starting with the Middle Ages, Persian and European alchemists invested much effort in the search for the "philosopher's stone", a legendary substance that was believed to be an essential ingredient for either or both of those goals. Pope John XXII issued a bull against alchemical counterfeiting, and the Cistercians banned the practice amongst their members. In 1403, Henry IV of England banned the practice of Alchemy. In the late 14th century, Piers the Ploughman and Chaucer both painted unflattering pictures of Alchemists as thieves and liars. By contrast, Rudolf II, Holy Roman Emperor, in the late 16th century, sponsored various alchemists in their work at his court in Prague.
It is a popular belief that Alchemists made mundane contributions to the "chemical" industries of the day—ore testing and refining, metalworking, production of gunpowder, ink, dyes, paints, cosmetics, leather tanning, ceramics, glass manufacture, preparation of extracts, liquors, and so on (it seems that the preparation of "aqua vitae", the "water of life", was a fairly popular "experiment" among European alchemists). In reality, although Alchemists contributed distillation to Western Europe, they did little for any known industry. Long before Alchemists appeared, goldsmiths knew how to tell what was good gold or fake, and industrial technology grew by the work of the artisans themselves, rather than any Alchemical helpers.
The double origin of Alchemy in Greek philosophy as well as in Egyptian and Mesopotamian technology set, from the start, a double approach: the technological, operative one, which Marie-Louise von Franz call extravert, and the mystic, contemplative, psychological one, which von Franz names as introvert. These are not mutually exclusive, but complementary instead, as meditation requires practice in the real world, and conversely.
Several early alchemists, such as Zosimos of Panopolis, are recorded as viewing alchemy as a spiritual discipline, and, in the Middle Ages, metaphysical aspects, substances, physical states, and molecular material processes as mere metaphors for spiritual entities, spiritual states, and, ultimately, transformations. In this sense, the literal meanings of 'Alchemical Formulas' were a blind, hiding their true spiritual philosophy, which being at odds with the Medieval Christian Church was a necessity that could have otherwise led them to the "stake and rack" of the Inquisition under charges of heresy. Thus, both the transmutation of common metals into gold and the universal panacea symbolized evolution from an imperfect, diseased, corruptible, and ephemeral state towards a perfect, healthy, incorruptible, and everlasting state; and the philosopher's stone then represented a mystic key that would make this evolution possible. Applied to the alchemist himself, the twin goal symbolized his evolution from ignorance to enlightenment, and the stone represented a hidden spiritual truth or power that would lead to that goal. In texts that are written according to this view, the cryptic alchemical symbols, diagrams, and textual imagery of late alchemical works typically contain multiple layers of meanings, allegories, and references to other equally cryptic works; and must be laboriously "decoded" in order to discover their true meaning.
Q. When the Philosophers speak of gold and silver, from which they extract their matter, are we to suppose that they refer to the vulgar gold and silver?
A. By no means; vulgar silver and gold are dead, while those of the Philosophers are full of life.
Psychology.
Alchemical symbolism has been occasionally used by psychologists and philosophers. Carl Jung reexamined alchemical symbolism and theory and began to show the inner meaning of alchemical work as a spiritual path. Alchemical philosophy, symbols and methods have enjoyed something of a renaissance in post-modern contexts.
Jung saw alchemy as a Western proto-psychology dedicated to the achievement of individuation. In his interpretation, alchemy was the vessel by which Gnosticism survived its various purges into the Renaissance, a concept also followed by others such as Stephan A. Hoeller. In this sense, Jung viewed alchemy as comparable to a Yoga of the East, and more adequate to the Western mind than Eastern religions and philosophies. The practice of Alchemy seemed to change the mind and spirit of the Alchemist. Conversely, spontaneous changes on the mind of Western people undergoing any important stage in individuation seems to produce, on occasion, imagery known to Alchemy and relevant to the person's situation.
His interpretation of Chinese alchemical texts in terms of his analytical psychology also served the function of comparing Eastern and Western alchemical imagery and core concepts and hence its possible inner sources (archetypes).
Marie-Louise von Franz, a disciple of Jung, continued Jung's studies on Alchemy and its psychological meaning.
Magnum opus.
After the 15th century, many writers tended to compress "citrinitas" into "rubedo" and consider only three stages.
However, it is in citrinitas that the Chemical Wedding takes place, generating the Philosophical Mercury without which the Philosopher's Stone, triumph of the Work, could never be accomplished.
Within the Magnum Opus was the creation of the Sanctum Moleculae, that is the 'Sacred Masses' that were derived from the Sacrum Particulae, that is the 'Sacred Particles', needed to complete the process of achieving the Magnum Opus.
Alchemy as a subject of historical research.
The history of alchemy has become a vigorous academic field. As the obscure hermetic language of the alchemists is gradually being "deciphered", historians are becoming more aware of the intellectual connections between that discipline and other facets of Western cultural history, such as the sociology and psychology of the intellectual communities, kabbalism, spiritualism, Rosicrucianism, and other mystic movements, cryptography, witchcraft, and the evolution of science and philosophy.
History.
In a historical sense, Alchemy is the pursuit of transforming common metals into valuable gold. According to Marie-Louise von Franz, the initial basis for alchemy were Egyptian metal technology and mummification, Mesopotamian technology and astrology, and Pre-Socratic Greek philosophers such as Empedocles, Thales of Miletus and Heraclitus.
The origins of Western alchemy are traceable back to ancient Egypt. The Leyden papyrus X and the Stockholm papyrus along with the Greek magical papyri comprise the first "book" on alchemy still existent. Babylonian, Greek and Indian philosophers theorized that there were only four classical elements (rather than today's 117 chemical elements, a useful analogy is with the highly similar states of matter); Earth, Fire, Water, and Air. The Greek philosophers, in order to prove their point, burned a log: The log was the earth, the flames burning it was fire, the smoke being released was air, and the smoldering soot at the bottom was bubbling water. Because of this, the belief that these four "elements" were at the heart of everything soon spread, only later being replaced in the Middle Ages by Geber's theory of seven elements, which was then replaced by the modern theory of chemical elements during the early modern period.
Alchemy encompasses several philosophical traditions spanning four millennia and three continents. These traditions' general penchant for cryptic and symbolic language makes it hard to trace their mutual influences and "genetic" relationships. Alchemy starts becoming much clearer in the 8th century with the works of the Islamic alchemist, Jabir ibn Hayyan (known as "Geber" in Europe), who introduced a methodical and experimental approach to scientific research based in the laboratory, in contrast to the ancient Greek and Egyptian alchemists whose works were mainly allegorical.
Other famous alchemists include Rhazes, Avicenna and Imad ul-din in Persia; Wei Boyang in Chinese alchemy; and Nagarjuna in Indian alchemy; and Albertus Magnus and Pseudo-Geber in European alchemy; as well as the anonymous author of the "Mutus Liber", published in France in the late 17th century, which was a 'wordless book' that claimed to be a guide to making the philosopher's stone, using a series of 15 symbols and illustrations. The philosopher's stone was an object that was thought to be able to amplify one's power in alchemy and, if possible, grant the user ageless immortality, unless he fell victim to burnings or drowning; the common belief was that fire and water were the two greater elements that were implemented into the creation of the stone.
In the case of the Chinese and European alchemists, there was a difference between the two. The European alchemists tried to transmute lead into gold, and, no matter how futile or toxic the element, would continue trying until it was royally outlawed later into the century. The Chinese, however, paid no heed to the philosopher's stone or transmutation of lead to gold; they focused more on medicine for the greater good. During Enlightenment, these "elixirs" were a strong cure for sicknesses, unless it was a test medicine. In general, most tests were fatal, but stabilized elixirs served great purposes. On the other hand, the Islamic alchemists were interested in alchemy for a variety of reasons, whether it was for the transmutation of metals or artificial creation of life, or for practical uses such as medicine.
Modern connections to alchemy.
Persian alchemy was a forerunner of modern scientific chemistry. Alchemists used many of the same laboratory tools that are used today. These tools were not usually sturdy or in good condition, especially during the medieval period of Europe. Many transmutation attempts failed when alchemists unwittingly made unstable chemicals. This was made worse by the unsafe conditions in which the alchemists worked.
Up to the 16th century, alchemy was considered serious science in Europe; for instance, Isaac Newton devoted considerably more of his writing to the study of alchemy (see Isaac Newton's occult studies) than he did to either optics or physics, for which he is famous. Other eminent alchemists of the Western world are Roger Bacon, Saint Thomas Aquinas, Tycho Brahe, Thomas Browne, and Parmigianino. The decline of alchemy began in the 18th century with the birth of modern chemistry, which provided a more precise and reliable framework for matter transmutations and medicine, within a new grand design of the universe based on rational materialism.
Alchemy in traditional medicine.
Traditional medicines involve transmutation by alchemy, using pharmacological or a combination of pharmacological and spiritual techniques. In Chinese medicine the alchemical traditions of pao zhi will transform the nature of the temperature, taste, body part accessed or toxicity. In Ayurveda the samskaras are used to transform heavy metals and toxic herbs in a way that removes their toxicity. These processes are actively used to the present day.
Nuclear transmutation.
In 1919, Ernest Rutherford used artificial disintegration to convert nitrogen into oxygen. From then on, this sort of "scientific transmutation" has been routinely performed in many nuclear physics-related laboratories and facilities, like particle accelerators, nuclear power stations and nuclear weapons as a by-product of fission and other physical processes.
In literature.
A play by Ben Jonson, The Alchemist, is a satirical and skeptical take on the subject.
Part 2 of Goethe's Faust, is full of alchemical symbolism.
According to "Hermetic Fictions: Alchemy and Irony in the Novel" (Keele University Press, 1995), by David Meakin, alchemy is also featured in such novels and poems as those by William Godwin, Percy Bysshe Shelley, Emile Zola, Jules Verne, Marcel Proust, Thomas Mann, Hermann Hesse, James Joyce, Gustav Meyrink, Lindsay Clarke, Marguerite Yourcenar, Umberto Eco, Michel Butor, Paulo Coelho, Amanda Quick, Gabriel García Marquez and Maria Szepes.
Hilary Mantel, in her novel Fludd (1989, Penguin), mentions the spagyric art. 'After separation, drying out, moistening, dissolving, coagulating, fermenting, comes purification, recombination: the creation of substances the world until now has never beheld. This is the opus contra naturem, this is the spagyric art, this is the Alchymical Wedding'. (page 79)
In Dante's Inferno, it is placed within the Tenth ring of the 8th circle.
In Angie Sage's Septimus Heap series, Marcellus Pye is an important Alchemist that first appears in Physik, the third book.
In The Secrets of the Immortal Nicholas Flamel series, one of the main characters is a alchemist.
The manga and anime series Fullmetal Alchemist bases itself of a more fantasised version of alchemy.
In contemporary art.
In the twentieth century alchemy was a profoundly important source of inspiration for the Surrealist artist Max Ernst, who used the symbolism of alchemy to inform and guide his work. M.E. Warlick wrote his "Max Ernst and Alchemy" describing this relationship in detail.
Contemporary artists use alchemy as inspiring subject matter, like Odd Nerdrum, whose interest has been noted by Richard Vine, and the painter Michael Pearce, whose interest in alchemy dominates his work. His works "Fama" and "The Aviator's Dream" particularly express alchemical ideas in a painted allegory.
---END.OF.DOCUMENT---

Austria.
Austria (), officially the Republic of Austria (German:; Austro-Bavarian: Repubblik Östareich), is a landlocked country of roughly 8.3 million people in Central Europe. It borders Germany and the Czech Republic to the north, Slovakia and Hungary to the east, Slovenia and Italy to the south, and Switzerland and Liechtenstein to the west. The territory of Austria covers, and has a temperate and alpine climate. Austria's terrain is highly mountainous due to the presence of the Alps; only 32% of the country is below, and its highest point is. The majority of the population speaks German, which is also the country's official language. Other local official languages are Croatian, Hungarian and Slovene.
The origins of Austria date back to the time of the Roman Empire when a Celtic kingdom was conquered by the Romans in approximately 15 BC, and later became Noricum, a Roman province, in the mid 1st century AD—an area which mostly encloses today's Austria. In 788 AD, the Frankish king Charlemagne conquered the area, and introduced Christianity. Under the native Habsburg dynasty, Austria became one of the great powers of Europe. In 1867, the Austrian Empire was reformed into Austria-Hungary. The Austro-Hungarian Empire collapsed in 1918 with the end of World War I. After establishing the First Austrian Republic in 1919 Austria was de facto annexed into Greater Germany by the Nazi regime in the so-called Anschluss in 1938. This lasted until the end of World War II in 1945, after which Austria was occupied by the Allies. In 1955, the Austrian State Treaty re-established Austria as a sovereign state, ending the occupation. In the same year, the Austrian Parliament created the Declaration of Neutrality which declared that the country would become permanently neutral.
Today, Austria is a parliamentary representative democracy comprising nine federal states. The capital—and with a population exceeding 1.6 million, Austria's largest city—is Vienna. Austria is one of the richest countries in the world, with a nominal per capita GDP of $43,570. The country has developed a high standard of living, and in 2008 was ranked 14th in the world for its Human Development Index. Austria has been a member of the United Nations since 1955, joined the European Union in 1995, and is a founder of the OECD. Austria also signed the Schengen Agreement in 1995, and adopted the European currency, the euro, in 1999.
Etymology.
The German name of Austria, derives from the Old High German word Ostarrîchi "eastern realm", first attested in the famous "Ostarrîchi document" of AD 996, where the term refers to the Margraviate ruled by the Babenberg Count Henry I located mostly in what is today Lower Austria and part of Upper Austria. The name Austria is a latinisation of the same Germanic word for "east", *austrō also found in "Austrasia", the eastern part of Merovingian Francia.
German "Österreich" is readily analysable as connected to "östlich" "eastern" and "Reich" "realm, dominion, empire". The term probably originates in a vernacular translation of the Medieval Latin name for the region:, which translates as "eastern marches" or "eastern borderland", as it was situated at the eastern edge of the Holy Roman Empire.==
However, Friedrich Heer, one of the most important Austrian historians in the 20th century, stated in his book "Der Kampf um die österreichische Identität" ("The Struggle Over Austrian Identity"), that the Germanic form "Ostarrîchi" was not a translation of the Latin word, but both resulted from a much older term originating in the Celtic languages of ancient Austria: More than 2,500 years ago, the major part of the actual country was called "Norig" by the Celtic population (Hallstatt culture); "No-" or "Nor-" meant "east" or "eastern", whereas "-rig" is related to the modern German "Reich"; meaning "realm". Accordingly, "Norig" would essentially mean "Ostarrîchi" and "Österreich", thus "Austria". The Celtic name was eventually Latinised to "Noricum" after the Romans conquered the area that encloses most of modern day Austria, in approximately 15 BC. "Noricum" later became a Roman province in the mid 1st century AD.
History.
Settled in ancient times, the Central European land that is now Austria was occupied in pre-Roman times by various Celtic tribes. The Celtic kingdom of Noricum was later claimed by the Roman Empire and made a province. Present day Petronell-Carnuntum in Eastern Austria was an important army camp turned capital city in what became known as the Upper Pannonia province. Fifty thousand people called Carnuntum home for nearly 400 years.
After the fall of the Roman Empire the area was invaded by Bavarians, Slavs and Avars. The Slavic tribe of the Carantanians migrated into the Alps, and established the realm of Carantania, which covered much of eastern and central Austrian territory. Charlemagne conquered the area in 788 AD, encouraged colonisation and introduced Christianity. As part of Eastern Francia, the core areas that now encompass Austria were bequeathed to the house of Babenberg. The area was known as the "marchia Orientalis" and was given to Leopold of Babenberg in 976.
The first record showing the name Austria is from 996 where it is written as "Ostarrîchi", referring to the territory of the Babenberg March. In 1156 the Privilegium Minus elevated Austria to the status of a duchy. In 1192, the Babenbergs also acquired the Duchy of Styria. With the death of Frederick II in 1246, the line of the Babenbergs went extinct.
As a result Otakar II of Bohemia effectively assumed control of the duchies of Austria, Styria and Carinthia. His reign came to an end with his defeat at Dürnkrut at the hands of Rudolf I of Germany in 1278. Thereafter, until World War I, Austria's history was largely that of its ruling dynasty, the Habsburgs.
In the 14th and 15th centuries, the Habsburgs began to accumulate other provinces in the vicinity of the Duchy of Austria. In 1438 Duke Albert V of Austria was chosen as the successor to his father-in-law, Emperor Sigismund. Although Albert himself only reigned for a year, every emperor of the Holy Roman Empire was a Habsburg, with only one exception.
The Habsburgs began also to accumulate lands far from the hereditary lands. In 1477 Archduke Maximilian, only son of Emperor Frederick III, married the heiress Maria of Burgundy, thus acquiring most of the Netherlands for the family. His son Philip the Fair married the heiress of Castile and Aragon, and thus acquired Spain and its Italian, African and New World appendages for the Habsburgs. In 1526 following the Battle of Mohács, Bohemia and the part of Hungary not occupied by the Ottomans came under Austrian rule. Ottoman expansion into Hungary led to frequent conflicts between the two empires, particularly evident in the so-called Long War of 1593 to 1606.
During the long reign of Leopold I (1657–1705) and following the successful defense of Vienna in 1683 (under the command of the King of Poland, John III Sobieski), a series of campaigns resulted in bringing all of Hungary to Austrian control by the Treaty of Carlowitz in 1699.
Emperor Charles VI relinquished many of the fairly impressive gains the empire made in the previous years, largely due to his apprehensions at the imminent extinction of the House of Habsburg. Charles was willing to offer concrete advantages in territory and authority in exchange for other powers' worthless recognitions of the Pragmatic Sanction that made his daughter Maria Theresa his heir. With the rise of Prussia the Austrian–Prussian dualism began in Germany. Austria participated, together with Prussia and Russia, in the first and the third of the three Partitions of Poland (in 1772 and 1795).
Austria later became engaged in a war with Revolutionary France, at the beginning highly unsuccessful, with successive defeats at the hands of Napoleon meaning the end of the old Holy Roman Empire in 1806. Two years earlier, in 1804, the Empire of Austria was founded. In 1814 Austria was part of the Allied forces that invaded France and brought to an end the Napoleonic wars.
It thus emerged from the Congress of Vienna in 1815 as one of four of the continent's dominant powers and a recognised great power. The same year, the German Confederation, () was founded under the presidency of Austria. Because of unsolved social, political and national conflicts the German lands were shaken by the 1848 revolution aiming to create a unified Germany. A unified Germany would have been possible either as a Greater Germany, or a Greater Austria or just the German Confederation without Austria at all. As Austria was not willing to relinquish its German-speaking territories to what would become the German Empire of 1848, the crown of the newly-formed empire was offered to the Prussian King Friedrich Wilhelm IV. In 1864 Austria and Prussia fought together against Denmark, and successfully freed the independent duchies of Schleswig and Holstein. Nevertheless as they could not agree on a solution to the administration of the two duchies, they fought in 1866 the Austro-Prussian War. Defeated by Prussia in the Battle of Königgrätz, Austria had to leave the German Confederation and subsequently no longer took part in German politics.
The Austro-Hungarian Compromise of 1867, the "Ausgleich", provided for a dual sovereignty, the Austrian Empire and the Kingdom of Hungary, under Franz Joseph I. The Austrian-Hungarian rule of this diverse empire included various Slavic groups including Croats, Czechs, Poles, Rusyns, Serbs, Slovaks, Slovenes and Ukrainians, as well as large Italian and Romanian communities.
As a result, ruling Austria–Hungary became increasingly difficult in an age of emerging nationalist movements. Yet the government of Austria tried its best to be accommodating in some respects: The "Reichsgesetzblatt", publishing the laws and ordinances of Cisleithania, was issued in eight languages, all national groups were entitled to schools in their own language and to the use of their mothertongue at state offices, for example. The government of Hungary to the contrary tried to magyarise other ethnic entities. Thus the wishes of ethnic groups dwelling in both parts of the dual monarchy hardly could be solved.
The assassination of Archduke Franz Ferdinand in Sarajevo in 1914 by Gavrilo Princip (a member of the Serbian nationalist group the Black Hand) was used by leading Austrian and Hungarian politicians and generals to persuade the emperor to declare war on Serbia, thereby risking and prompting the outbreak of World War I which led to the dissolution of the Austro-Hungarian Empire. Over one million Austro-Hungarian soldiers died in World War I.
On October 21, 1918, the elected German members of the "Reichsrat" (parliament of Imperial Austria) met in Vienna as the Provisional National Assembly for German Austria ("Provisorische Nationalversammlung für Deutschösterreich"). On October 30 the assembly founded the State of German Austria by appointing a government, called "Staatsrat". This new government was invited by the emperor to take part in the decision on the planned armistice with Italy, but refrained from this business; this left the responsibility for the end of the war on November 3, 1918, solely to the emperor and his government. On November 11 the emperor, counseled by ministers of the old and the new government, declared he would not take part in state business any more; on November 12 German Austria, by law, declared itself to be a democratic republic and part of the new German republic. The constitution, renaming "Staatsrat" to "Bundesregierung" (federal government) and "Nationalversammlung" to "Nationalrat" (national council) was passed on November 10, 1920.
The Treaty of Saint-Germain of 1919 (for Hungary the Treaty of Trianon of 1920) confirmed and consolidated the new order of Central Europe which to a great part had been established in November 1918, creating new states and resizing others. Over 3-million German Austrians found themselves living outside of the newborn Austrian Republic in the respective states of Czechoslovakia, Yugoslavia, Hungary and Italy. Between 1918 and 1919 Austria was officially known as the State of German Austria (). Not only did the Entente powers forbid German Austria to unite with Germany, they also ignored the name German Austria in the peace treaty to be signed; it was therefore changed to Republic of Austria in late 1919.
After the war inflation began to devaluate the "Krone", still Austria's currency. In the autumn of 1922 Austria was granted an international loan supervised by the League of Nations. The purpose of the loan was to avert bankruptcy, stabilise the currency and improve its general economic condition. With the granting of the loan, Austria passed from an independent state to the control exercised by the League of Nations. In 1925 the "Schilling", replacing the "Krone" by 10,000:1, was introduced. Later it was called the Alpine dollar due to its stability. From 1925 to 1929 the economy enjoyed a short high before nearly crashing after Black Friday.
The First Austrian Republic lasted until 1933 when Chancellor Engelbert Dollfuss, gladly using what he called "self-switch-off of Parliament" (), established an autocratic regime tending toward Italian fascism. The two big parties at this time, the Social Democrats and the Conservatives, had paramilitary armies; the Social Democrats' "Schutzbund" was now declared illegal but still operative as civil war broke out.
In February 1934 several members of the "Schutzbund" were executed, the Social Democratic party was outlawed and many of its members were imprisoned or emigrated. On 1 May 1934, the Austrofascists imposed a new constitution ("Maiverfassung") which cemented Dollfuss's power but on 25 July he was assassinated in a Nazi coup attempt.
His successor, Kurt Schuschnigg, struggled to keep Austria independent as "the better German state", but on 12 March 1938, German troops occupied the country while Austrian Nazis took over government. On 13 March 1938, the "Anschluss" of Austria was officially declared. Two days later Hitler, a native of Austria, proclaimed the re-unification of his home country with the rest of Germany on Vienna's Heldenplatz. He established a plebiscite confirming union with Germany in April 1938.
Austria was incorporated into the Third Reich and ceased to exist as an independent state. The Aryanisation of the wealth of Jewish Austrians started immediately mid-March with a so called "wild" (i.e. extra-legal) phase but soon was structured legally and bureaucratically to strip Jewish citizens of any asset they may have possessed. The Nazis called Austria "Ostmark" until 1942 when it was again renamed and called "Alpen-Donau-Reichsgaue". Vienna fell on 13 April 1945, during the Soviet Vienna Offensive just before the total collapse of the Third Reich.
Karl Renner and Adolf Schärf (Socialist Party of Austria [Social Democrats and Revolutionary Socialists]), Leopold Kunschak (Austria's People's Party [former Christian Social People's Party]) and Johann Koplenig (Communist Party of Austria) declared Austria's secession from the Third Reich by the Declaration of Independence on 27 April 1945, and set up a provisional government in Vienna under state Chancellor Renner the same day, with the approval of the victorious Red Army and backed by Stalin. (The date is officially named the birthday of the second republic.) At the end of April, most of Western and Southern Austria still was under Nazi rule. On May 1, 1945, the federal constitution of 1929 was put into validity again, which had been terminated by dictator Dollfuss on May 1, 1934.
Total military deaths from 1939–1945 are estimated at 260,000. Jewish Holocaust victims totaled 65,000. About 140,000 Jewish Austrians had fled the country in 1938–39. Thousands of Austrians had taken part in serious Nazi crimes, a fact officially recognised by Chancellor Franz Vranitzky in 1992.
Much like Germany, Austria was divided into a British, a French, a Soviet and a U.S. zone and governed by the Allied Commission for Austria. As forecast in the Moscow Declaration in 1943, there was a subtle difference in the treatment of Austria by the Allies. The Austrian Government, consisting of Social Democrats, Conservatives and Communists (until 1947) and residing in Vienna, which was surrounded by the Soviet zone, was recognised by the Western Allies in October 1945 after some doubts that Renner could be Stalin's puppet. Thereby the creation of a separate Western Austrian government and the division of the country could be avoided. Austria, in general, was treated as though it had been originally invaded by Germany and liberated by the Allies.
On 15 May 1955, after talks which lasted for years and were influenced by the Cold War Austria regained full independence by concluding the Austrian State Treaty with the Four Occupying Powers. On 26 October 1955, after all occupation troops had left, Austria declared its "permanent neutrality" by an act of Parliament, which remains to this day but has been implicitly overlapped by constitutional amendments concerning Austria as member of the European Union from 1995 onward.
The political system of the Second Republic is based on the constitution of 1920 and 1929, which was reintroduced in 1945. The system came to be characterised by "Proporz", meaning that most posts of political importance were split evenly between members of the Social Democrats and the People's Party. Interest group "chambers" with mandatory membership (e.g. for workers, business people, farmers) grew to considerable importance and were usually consulted in the legislative process, so that hardly any legislation was passed that did not reflect widespread consensus. Since 1945 a single-party government took place only 1966–1970 (Conservatives) and 1970–1983 (Social Democrats). During all other legislative periods, either a grand coalition of Conservatives and Social Democrats or a "small coalition" (one of these two and a smaller party) ruled the country.
Following a referendum in 1994, at which consent reached a majority of two thirds, the country became a member of the European Union on 1 January 1995. According to its economic success, Austria is one of the "net contributors" of the union.
The major parties SPÖ and ÖVP have contrary opinions about the future status of Austria's military non-alignment: While the SPÖ in public supports a neutral role, the ÖVP argues for stronger integration into the EU's security policy; even a future NATO membership is not ruled out by some ÖVP politicians. In reality, Austria is taking part in the EU's Common Foreign and Security Policy, participates in the so-called Petersburg Agenda (including peace keeping and peace creating tasks) and has become member of NATO's "Partnership for Peace"; the constitution has been amended accordingly. The term "neutrality" is only used to tranquilise voters afraid of change. Since 2008, due to the Schengen Agreement, the only neighbouring country performing border controls towards Austria is Liechtenstein.
Political system.
The Parliament of Austria is located in Vienna, the country's largest city and capital. Austria became a federal, parliamentarian, democratic republic through the Federal Constitution of 1920. It was reintroduced in 1945 to the nine states of the Federal Republic. The head of state is the Federal President ("Bundespräsident"), who is directly elected by popular vote. The chairman of the Federal Government is the Federal Chancellor, who is appointed by the president. The government can be removed from office by either a presidential decree or by vote of no confidence in the lower chamber of parliament, the Nationalrat. Voting for the federal president and for the Parliament used to be compulsory in Austria, but this was abolished in steps from 1982 to 2004.
The Parliament of Austria consists of two chambers. The composition of the Nationalrat (183 seats) is determined every five years (or whenever the Nationalrat has been dissolved by the federal president on a motion by the federal chancellor, or by Nationalrat itself) by a general election in which every citizen over 16 years (since 2007) has voting rights. While there is a general threshold of 4 percent for all parties at federal elections (Nationalratswahlen), there remains the possibility to gain a direct seat, or, in one of the 43 regional election districts.
The Nationalrat is the dominant chamber in the formation of legislation in Austria. However, the upper house of parliament, the Bundesrat, has a limited right of veto (the Nationalrat can—in almost all cases—ultimately pass the respective bill by voting a second time. This is referred to as Beharrungsbeschluss", lit. "vote of persistence"). A convention, called the was convened in June 30, 2003 to decide upon suggestions to reform the constitution, but failed to produce a proposal that would receive the two-thirds of votes in the Nationalrat necessary for constitutional amendments and/or reform.
With legislative and executive, the courts are the third column of Austrian state powers. Notably the Constitutional Court ("Verfassungsgerichtshof") may exert considerable influence on the political system by ruling out laws and ordinances not in compliance with the constitution. Since 1995, the European Court of Justice may overrule Austrian decisions in all matters defined in laws of the European Union. Concerning human rights, Austria also is implementing the decisions of the European Court of Human Rights, since the European Convention on Human Rights is part of the Austrian constitution.
Recent developments.
After general elections held in October 2006, the Social Democrats emerged as the largest party, whereas the People's Party lost about 8% in votes. Political realities prohibited any of the two major parties from forming a coalition with smaller parties. In January 2007 the People's Party and Social Democrats formed a grand coalition with the social democrat Alfred Gusenbauer as Chancellor. This coalition broke up in June 2008. Elections in September 2008 further weakened both major parties (Social Democrats and People's Party) but together they still held more than 50% of the votes with the Social Democrats holding the majority. They formed a coalition with Werner Faymann from the Social Democrats as Chancellor. The positions of the Freedom Party and the deceased Jörg Haider's new party Alliance for the Future of Austria, both right-wing parties, were strengthened during the election.
Foreign policy.
The 1955 Austrian State Treaty ended the occupation of Austria following World War II and recognised Austria as an independent and sovereign state. On 26 October 1955, the Federal Assembly passed a constitutional article in which "Austria declares of her own free will her perpetual neutrality". The second section of this law stated that "in all future times Austria will not join any military alliances and will not permit the establishment of any foreign military bases on her territory". Since then, Austria has shaped its foreign policy on the basis of neutrality, but rather different from the neutrality of Switzerland.
Austria began to reassess its definition of neutrality following the fall of the Soviet Union, granting overflight rights for the UN-sanctioned action against Iraq in 1991, and, since 1995, it has developed participation in the EU's Common Foreign and Security Policy (CFSP). Also in 1995, it joined the Partnership for Peace and subsequently participated in peacekeeping missions in Bosnia. Meanwhile, the only part of the Constitutional Law on Neutrality of 1955 still valid fully is not to allow foreign military bases in Austria.
Austria attaches great importance to participation in the Organisation for Economic Co-operation and Development and other international economic organisations, and it has played an active role in the Organization for Security and Cooperation in Europe (OSCE).
Energy politics.
In 1972, the country began construction of a nuclear-powered electricity-generation station at Zwentendorf on the River Danube, following a unanimous vote in parliament. However, in 1978, a referendum voted approximately 50.5% against nuclear power, 49.5% for, and parliament subsequently unanimously passed a law forbidding the use of nuclear power to generate electricity.
Austria currently produces more than half of its electricity by hydropower. Together with other renewable energy sources such as wind, solar and biomass powerplants, the electricity supply from renewable energy amounts to 62.89% of total use in Austria, with the rest being produced by gas and oil powerplants.
Military.
The manpower of the Austrian Armed Forces () mainly relies on conscription. All males who have reached the age of eighteen and are found fit have to serve a six months military service, followed by an eight year reserve obligation. Both males and females at the age of sixteen are eligible for voluntary service. Conscientious objection is legally acceptable and those who claim this right are obliged to serve an institutionalised nine months civilian service instead. Since 1998, women volunteers have been allowed to become professional soldiers.
The main sectors of the Bundesheer are Joint Forces (Streitkräfteführungskommando, SKFüKdo) which consist of Land Forces (Landstreitkräfte), Air Forces (Luftstreitkräfte), International Missions (Internationale Einsätze) and Special Forces (Spezialeinsatzkräfte), next to Mission Support (Kommando Einsatzunterstützung; KdoEU) and Command Support (Kommando Führungsunterstützung; KdoFüU). Being a landlocked country, Austria has no navy.
In 2004, Austria's defence expenditures corresponded to approximately 0.9% of its GDP. The Army currently has about 45,000 soldiers, of whom about half are conscripts. As head of state, Austrian President (currently Heinz Fischer) is nominally the Commander-in-Chief of the Bundesheer. In practical reality, however, command of the Austrian Armed Forces is almost exclusively exercised by the Minister of Defense, currently Norbert Darabos.
Since the end of the Cold War, and more importantly the removal of the former heavily guarded "Iron Curtain" separating Austria and Hungary, the Austrian military has been assisting Austrian border guards in trying to prevent border crossings by illegal immigrants. This assistance came to an end when Hungary joined the EU Schengen area in 2008, for all intents and purposes abolishing "internal" border controls between treaty states. Some politicians have called for a prolongation of this mission, but the legality of this is heavily disputed. In accordance with the Austrian constitution, armed forces may only be deployed in a limited number of cases, mainly to defend the country and aid in cases of national emergency, such as in the wake of natural disasters. They may generally not be used as auxiliary police forces.
Within its self-declared status of permanent neutrality, Austria has a long and proud tradition of engaging in UN-led peacekeeping and other humanitarian missions. The Austrian Forces Disaster Relief Unit (AFDRU), in particular, an all-volunteer unit with close ties to civilian specialists (e.g. rescue dog handlers) enjoys a reputation as a quick (standard deployment time is 10 hours) and efficient SAR unit. Currently, larger contingents of Austrian forces are deployed in Bosnia, Kosovo and, since 1974, in the Golan Heights.
States.
As a federal republic, Austria is divided into nine states (). These states are then divided into districts () and statutory cities (). Districts are subdivided into municipalities (). Statutory Cities have the competencies otherwise granted to both districts and municipalities. The states are not mere administrative divisions but have some legislative authority distinct from the federal government, e.g. in matters of culture, social care, youth and nature protection, hunting, building, and zoning ordinances. In recent years, it has been discussed whether today it is appropriate for a small country to maintain ten parliaments.
Geography.
Austria is a largely mountainous country due to its location in the Alps. The Central Eastern Alps, Northern Limestone Alps and Southern Limestone Alps are all partly in Austria. Of the total area of Austria (), only about a quarter can be considered low lying, and only 32% of the country is below. The Alps of western Austria give way somewhat into low lands and plains in the eastern part of the country.
Austria can be divided into five areas, the biggest being the Eastern Alps, which constitute 62% of nation's total area. The Austrian foothills at the base of the Alps and the Carpathians account for around 12% and the foothills in the east and areas surrounding the periphery of the Pannoni low country amount to about 12% of the total landmass. The second greater mountain area (much lower than the Alps) is situated in the north. Known as the Austrian granite plateau, it is located in the central area of the Bohemian Mass, and accounts for 10% of Austria. The Austrian portion of the Vienna basin comprises the remaining 4%.
Phytogeographically, Austria belongs to the Central European province of the Circumboreal Region within the Boreal Kingdom. According to the WWF, the territory of Austria can be subdivided into four ecoregions: the Central European mixed forests, Pannonian mixed forests, Alps conifer and mixed forests and Western European broadleaf forests.
Climate.
The greater part of Austria lies in the cool/temperate climate zone in which humid westerly winds predominate. With over half of the country dominated by the Alps, the alpine climate is the predominant one. In the east—in the Pannonian Plain and along the Danube valley—the climate shows continental features with less rain than the alpine areas. Although Austria is cold in the winter, summer temperatures can be relatively warm—reaching temperatures of around 20 – 40 °C.
Economy.
Austria is one of the 12 richest countries in the world in terms of GDP (Gross domestic product) per capita, has a well-developed social market economy, and a high standard of living. Until the 1980s, many of Austria's largest industry firms were nationalised; in recent years, however, privatisation has reduced state holdings to a level comparable to other European economies. Labour movements are particularly strong in Austria and have large influence on labour politics. Next to a highly developed industry, international tourism is the most important part of the national economy.
Germany has historically been the main trading partner of Austria, making it vulnerable to rapid changes in the German economy. However, since Austria became a member state of the European Union it has gained closer ties to other European Union economies, reducing its economic dependence on Germany. In addition, membership in the EU has drawn an influx of foreign investors attracted by Austria's access to the single European market and proximity to the aspiring economies of the European Union. Growth in GDP accelerated in recent years and reached 3.3% in 2006.
Currency.
In Austria, the euro was introduced as an accounting currency on 1 January 1999, and euro coins and banknotes entered circulation on 1 January 2002. As a preparation for this date, the minting of the new euro coins started as early as 1999, however all Austrian euro coins introduced in 2002 have this year on it; unlike other countries of the Eurozone where mint year is minted in the coin. Eight different designs, one per face value, were selected for the Austrian coins. In 2007, to adopt the new common map like the rest of the Eurozone countries, Austria changed the common side of its coins.
Before adopting the Euro in 2002 Austria had maintained use of the Austrian schilling which was first established in December 1924. The Schilling was abolished in the wake of the Anschluss in 1938 and has been reintroduced after the end of the World War II in November 1945.
Austria has one of the richest collection of collectors' coins in the Eurozone, with face value ranging from 10 to 100 euro (although a 100,000 euro coin was exceptionally minted in 2004). These coins are a legacy of an old national practice of minting of silver and gold coins. Unlike normal issues, these coins are not legal tender in all the eurozone. For instance, a €5 Austrian commemorative coin cannot be used in any other country.
Education.
Responsibility for educational oversight in Austria is entrusted partly to the Austrian states (Bundesländer), and partly to the federal government. School attendance is compulsory for nine years, i.e. usually to the age of fifteen.
Kindergarten education, free in most states, is provided for all children between the ages of three and six years and, whilst optional, is considered a normal part of a child's education, due to its high takeup rate. Maximum class size is around 30, each class normally being cared for by one qualified teacher and one assistant. Standard attendance times are 8am to 12am, with extra afternoon care also frequently provided for a fee.
Primary education, or Volksschule, lasts for four years, starting at age six. Maximum class size is 30, but may be as low as 15. It is generally expected that a class will be taught by one teacher for the entire four years and the stable bond between teacher and pupil is considered important for a child's well-being. The "3Rs" dominate lesson time, with less time allotted to project work than in the UK. Children work individually and all members of a class follow the same plan of work. There is no streaming. Lessons begin at 8am and last until noon or 1pm with hourly five- or ten-minute breaks. Children are given homework daily from the first year. Historically there has been no lunch hour, children returning home to eat. However, due to a rise in the number of mothers in work, primary schools are increasingly offering pre-lesson and afternoon care.
As in Germany, secondary education consists of two main types of schools, attendance at which is based on a pupil's ability as determined by grades from the primary school. The Gymnasium caters for the more able children, in the final year of which the Matura examination is taken, which is a requirement for access to university. The Hauptschule prepares pupils for vocational education but also for various types of further education (HTL = institution of higher technical education; HAK = commercial academy; HBLA = institution of higher education for economic business; etc.). Attendance at one of these further education institutes also leads to the Matura. Some schools aim to combine the education available at the Gymnasium and the Hauptschule, and are known as Gesamtschulen. In addition, a recognition of the importance of learning English has led some Gymnasiums to offer a bilingual stream, in which pupils deemed able in languages follow a modified curriculum, a portion of the lesson time being conducted in English.
As at primary school, lessons at Gymnasium begin at 8am, and continue with short intervals until lunchtime or early afternoon, with children returning home to a late lunch. Older pupils often attend further lessons after a break for lunch, generally eaten at school. As at primary level, all pupils follow the same plan of work. Great emphasis is placed on homework and frequent testing. Satisfactory marks in the end-of-the-year report ("Zeugnis") are a prerequisite for moving up ("aufsteigen") to the next class. Pupils who do not meet the required standard re-sit their tests at the end of the summer holidays; those whose marks are still not satisfactory are required to re-sit the year ("sitzenbleiben"). It is not uncommon for a pupil to re-sit more than one year of school. After completing the first two years, pupils choose between one of two strands, known as "Gymnasium" (slightly more emphasis on arts) or "Realgymnasium" (slightly more emphasis on science). Whilst many schools offer both strands, some do not, and as a result, some children move schools for a second time at age 12. At age 14, pupils may choose to remain in one of these two strands, or to change to a vocational course, possibly with a further change of school.
The Austrian university system had been open to any student who passed the Matura examination until recently. A 2006 bill allowed the introduction of entrance exams for studies such as Medicine. In 2001, an obligatory tuition fee ("Studienbeitrag") of €363.36 per term was introduced for all public universities. Since 2008, for all EU students the studies are free of charge, as long as a certain time-limit is not exceeded (the expected duration of the study plus usually two terms tolerance). When the time-limit is exceeded, the fee of around €363.36 per term is charged. Some further exceptions to the fee apply, e.g. for students with a year's salary of more than about €5000. In all cases, an obligatory fee of €15.50 for the student union and insurance is charged.
Demographics.
Austria's population estimate in January 2009 was 8,356,707. The population of the capital, Vienna, exceeds 1.6 million (2.2 million including the suburbs), representing about a quarter of the country's population. It is known for its vast cultural offerings and high standard of living.
Vienna is by far the country's largest city. Graz is second in size, with 250,099 inhabitants, followed by Linz (188,968), Salzburg (150,000), and Innsbruck (117,346). All other cities have fewer than 100,000 inhabitants.
Language.
German, Austria's official language, is spoken natively by 88.6% of the population—followed by Turkish (2.3%), Serbian (2.2%), Croatian (1.6%), Hungarian (0.5%), and Bosnian (0.4%). The Austrian federal states of Carinthia and Styria are home to a significant indigenous Slovene-speaking minority with around 14,000 members (Austrian census; unofficial numbers of Slovene groups speak of up to 50,000). In the eastermost state, Burgenland (formerly part of the Hungarian portion of Austria–Hungary), about 20,000 Austrian citizens speak Hungarian and 30,000 speak Croatian. Of the remaining number of Austria's people that are of non-Austrian descent, many come from surrounding countries, especially from the former East Bloc nations. So-called guest workers "(Gastarbeiter)" and their descendants, as well as refugees from the Yugoslav wars and other conflicts, also form an important minority group in Austria. Since 1994 the Roma–Sinti (gypsies) are an officially recognised ethnic minority in Austria.
According to census information published by Statistik Austria for 2001 there were a total of 710,926 foreign nationals living in Austria. Of these, 124,392 speak German as their mother tongue (mainly immigrants from Germany, some from Switzerland and South Tyrol, Italy) The next largest populations of linguistic and ethnic groups are 240,863 foreign nationals from the former Yugoslavia (Serbs being the largest number of these at 135,376, followed by Croatian at 105,487); 123,417 Turkish nationals; 25,155 whose native tongue is English; 24,446 Albanian; 17,899 Polish; 14,699 Hungarian; 12,216 Romanian; 7,982 Arabs; 6,902 Slovenes (not including the autochthonous minority); 6,891 Slovaks; 6,707 Czech; 5,916 Persian; 5,677 Italian; 5,466 Russian; 5,213 French; 4,938 Chinese; 4,264 Spanish; 3,503 Bulgarian. The populations of the rest fall off sharply below 3,000. Between 200,000 and 300,000 ethnic Turks (including minority of Turkish Kurds) currently live in Austria. They are the largest single immigrant group in Austria, closely followed by the Serbs.
Austria's mountainous terrain led to the development of many distinct German dialects. All of the dialects in the country, however, belong to Austro-Bavarian groups of German dialects, with the exception of the dialect spoken in its western-most Bundesland, Vorarlberg, which belongs to the group of Alemannic dialects. There is also a distinct grammatical standard for Austrian German with a few differences to the German spoken in Germany.
As of 2006, some of the Austrian states introduced standardised tests for new citizens, to assure their language ability, cultural knowledge and accordingly their ability to integrate into the Austrian society. For the national rules, see Austrian nationality law – Naturalisation.
Ethnic groups ().
An estimated 13,000 to 40,000 Slovenes in the Austrian state of Carinthia (the Carinthian Slovenes) as well as Croats (around 30,000) and Hungarians in Burgenland were recognised as a minority and have enjoyed special rights following the Austrian State Treaty () of 1955. The Slovenes in the Austrian state of Styria (estimated at a number between 1,600 and 5,000) are not recognised as a minority and do not enjoy special rights, although the State Treaty of July 27, 1955 states otherwise.
The right for bilingual topographic signs for the regions where Slovene- and Croat-Austrians live alongside the German speaking population (as required by the 1955 State Treaty) is still to be fully implemented. Many Carinthians are afraid of Slovenian territorial claims, pointing to the fact that Yugoslav troops entered the state after each of the two World Wars and considering that some official Slovenian atlases show parts of Carinthia as Slovene cultural territory. The recently deceased governor, Jörg Haider, has made this fact a matter of public argument in autumn 2005 by refusing to increase the number of bilingual topographic signs in Carinthia. A poll by the Kärntner Humaninstitut conducted in January 2006 states that 65% of Carinthians are not in favour of an increase of bilingual topographic signs, since the original requirements set by the State Treaty of 1955 have already been fulfilled according to their point of view.
Another interesting phenomenon is the so called "Windischen-Theorie" stating that the Slovenes can be split in two groups: actual Slovenes and "Windische" (a traditional German name for Slavs), based on differences in language between Austrian Slovenes, who were taught Slovene standard language in school and those Slovenes who spoke their local Slovene dialect but went to German schools. The term "Windische" was applied to the latter group as a means of distinction. This politically influenced theory, dividing Slovene Austrians into the "loyal Windische" and the "national Slovenes", was never generally accepted and fell out of use some decades ago.
Religion.
At the end of the twentieth century, about 74% of Austria's population were registered as Roman Catholic, while about 5% considered themselves Protestants. Austrian Christians are obliged to pay a mandatory membership fee (calculated by income—about 1%) to their church; this payment is called "Kirchenbeitrag" ("Ecclesiastical/Church contribution").
Since the second half of the 20th century, the number of adherents and churchgoers has dropped. Data for the end of 2005 from the Austrian Roman Catholic church lists 5,662,782 members or 68.5% of the total Austrian population, and a Sunday church attendance of 753,701 or 9% of the total Austrian population. Data for the end of 2008 published by the Austrian Roman Catholic church shows a further reduction to 5,579,493 members or 66.8% of the total Austrian population, and a Sunday church attendance of 698,527 or 8% of the total Austrian population. The Lutheran church also recorded a large drop in adherents between 2001 and 2008.
About 12% of the population declared that they have no religion. in 2001. Of the remaining people, around 340,000 are registered as members of various Muslim communities, mainly due to the influx from Turkey, Bosnia-Herzegovina and Albania. About 180,000 are members of Eastern Orthodox Churches, more than 20,000 are active Jehovah's Witnesses and about 8,100 are Jewish.
The Austrian Jewish Community of 1938—Vienna alone counted more than 200,000—was reduced to around 4,500 during the Second World War, with approximately 65,000 Jewish Austrians killed in the Holocaust and 130,000 emigrating. The large majority of the current Jewish population are post-war immigrants, particularly from eastern Europe and central Asia (including Bukharan Jews). Buddhism was legally recognised as a religion in Austria in 1983.
According to the most recent Eurobarometer Poll 2005,
While northern and central Germany was the origin of the Reformation, Austria and Bavaria were the heart of the Counter-Reformation in the sixteenth and seventeenth centuries, when the absolute monarchy of Habsburg imposed a strict regime to restore Catholicism's power and influence among Austrians. The Habsburgs for a long time viewed themselves as the vanguard of Catholicism and all other confessions and religions were repressed.
In 1781, in the era of Austrian enlightenment, Emperor Joseph II issued a Patent of Tolerance for Austria that allowed other confessions a limited freedom of worship. Religious freedom was declared a constitutional right in Cisleithania after the Austro-Hungarian "Ausgleich" in 1867 thus paying tribute to the fact that the monarchy was home of numerous religions beside Roman Catholicism such as Greek, Serbian, Romanian, Russian, and Bulgarian Orthodox Christians (Austria neighboured the Ottoman Empire for centuries), Calvinist, Lutheran Protestants and Jews. In 1912, after the annexation of Bosnia Hercegovina in 1908, Islam was officially recognised in Austria.
Austria remained largely influenced by Catholicism. After 1918, First Republic Catholic leaders such as Theodor Innitzer and Ignaz Seipel took leading positions within or close to Austria's government and increased their influence during the time of the Austrofascism; Catholicism was treated much like a state religion by Engelbert Dollfuss and Kurt Schuschnigg. Although Catholic (and Protestant) leaders initially welcomed the Germans in 1938 during the Anschluss of Austria into Germany, Austrian Catholicism stopped its support of Nazism later on and many former religious public figures became involved with the resistance during the Third Reich. After the end of World War II in 1945, a stricter secularism was imposed in Austria, and religious influence on politics declined.
Music.
Austria's past as a European power and its cultural environment have generated a broad contribution to various forms of art, most notably among them music. Austria has been the birthplace of many famous composers such as Joseph Haydn, Franz Schubert, Anton Bruckner, Johann Strauss, Sr., Johann Strauss, Jr. and Gustav Mahler as well as members of the Second Viennese School such as Arnold Schoenberg, Anton Webern and Alban Berg. Wolfgang Amadeus Mozart was born in Salzburg, then an independent Church Principality, though one that was culturally closely connected to Austria, and much of Mozart's career was spent in Vienna.
Vienna has long been especially an important centre of musical innovation. Eighteenth and nineteenth century composers were drawn to the city due to the patronage of the Habsburgs, and made Vienna the European capital of classical music. During the Baroque period, Slavic and Hungarian folk forms influenced Austrian music.
Vienna's status began its rise as a cultural center in the early 1500s, and was focused around instruments including the lute. Ludwig van Beethoven spent the better part of his life in Vienna. Austria's current national anthem, attributed to Mozart, was chosen after World War II to replace the traditional Austrian anthem by Joseph Haydn.
Austria has also produced one notable jazz musician, keyboardist Josef Zawinul, who helped pioneer electronic influences in jazz as well as being a notable composer in his own right. The pop and rock musician Falco was internationally acclaimed during the 1980s, especially for his song "Rock Me Amadeus" dedicated to Mozart. The drummer Thomas Lang was born in Vienna in 1967 and is now world renowned for his technical ability, having played with artists such as Geri Halliwell and Robbie Williams.
Art and architecture.
Among Austrian Artists and architects one can find the painters Ferdinand Georg Waldmüller, Rudolf von Alt, Hans Makart, Gustav Klimt, Oskar Kokoschka, Egon Schiele, Carl Moll, and Friedensreich Hundertwasser, the photographers Inge Morath and Ernst Haas, and architects like Johann Bernhard Fischer von Erlach, Otto Wagner, Adolf Loos, and Hans Hollein.
Film and theater.
Austrian contributions to the worlds of film and theater have traditionally been strong. Sascha Kolowrat was an Austrian pioneer of filmmaking. Billy Wilder, Fritz Lang, Josef von Sternberg, and Fred Zinnemann originally came from Austria before establishing themselves as internationally relevant movie makers. Willi Forst, Ernst Marischka, or Franz Antel enriched the popular cinema in German language speaking countries. Michael Haneke became internationally known for his disturbing cinematic studies, before receiving a Golden Globe for his critically acclaimed film "The White Ribbon" in 2010.
The first Austrian film director receiving an Academy Award was Stefan Ruzowitzky. Many Austrian actors were able to pursue a career, the impact of which was sensed beyond national borders. Among them were Peter Lorre, Curd Jürgens, Senta Berger, Oskar Werner, and Klaus Maria Brandauer. Hedy Lamarr and Arnold Schwarzenegger became American as well as international movie stars. Christoph Waltz rose to international fame with his performance in "Inglourious Basterds", earning an Golden Globe Award in 2010. Max Reinhardt was a master of spectacular and astute theater productions. Otto Schenk not only excelled as a stage actor, but also as an opera director.
Science, philosophy and economics.
Austria was the cradle of numerous scientists with international reputation. Among them are Ludwig Boltzmann, Ernst Mach, Victor Franz Hess and Christian Doppler, prominent scientists in the nineteenth century. In the twentieth century, contributions by Lise Meitner, Erwin Schrödinger and Wolfgang Pauli to nuclear research and quantum mechanics were key to these areas' development during the 1920s and 1930s. A present-day quantum physicist is Anton Zeilinger, noted as the first scientist to demonstrate quantum teleportation.
In addition to physicists, Austria was the birthplace of two of the most noteworthy philosophers of the twentieth century, Ludwig Wittgenstein and Karl Popper. In addition to them biologists Gregor Mendel and Konrad Lorenz as well as mathematician Kurt Gödel and engineers such as Ferdinand Porsche and Siegfried Marcus were Austrians.
A focus of Austrian science has always been medicine and psychology, starting in medieval times with Paracelsus. Eminent physicians like Theodore Billroth, Clemens von Pirquet, and Anton von Eiselsberg have built upon the achievements of the 19th century Vienna School of Medicine. Austria was home to psychologists Sigmund Freud, Alfred Adler, Paul Watzlawick and Hans Asperger and psychiatrist Viktor Frankl.
The Austrian School of Economics, which is prominent as one of the main competitive directions for economic theory, is related to Austrian economists Joseph Schumpeter, Eugen von Böhm-Bawerk, Ludwig von Mises, and Friedrich Hayek. Other noteworthy Austrian-born émigrés include the management thinker Peter Drucker, scientist Sir Gustav Nossal, and the 38th Governor of California, Arnold Schwarzenegger.
Literature.
Complementing its status as a land of artists and scientists, Austria has always been a country of poets, writers, and novelists. It was the home of novelists Arthur Schnitzler, Stefan Zweig, Thomas Bernhard, Franz Kafka, and Robert Musil, of poets Georg Trakl, Franz Werfel, Franz Grillparzer, Rainer Maria Rilke, Adalbert Stifter, Karl Kraus and children's author Eva Ibbotson.
Famous contemporary playwrights and novelists are Nobel prize winner Elfriede Jelinek, Peter Handke and Daniel Kehlmann.
Cuisine.
Austria's cuisine is derived from that of the Austro-Hungarian Empire. Austrian cuisine is mainly the tradition of Royal-Cuisine ("Hofküche") delivered over centuries. It is famous for its well-balanced variations of beef and pork and countless variations of vegetables. There is also the "Mehlspeisen" Bakery, which created particular delicacies such as Sachertorte, "Krapfen" which are doughnuts usually filled with apricot marmalade or custard, and "Strudel" such as "Apfelstrudel" filled with apple and "Topfenstrudel" filled with sweetened sour cream.
In addition to native regional traditions, the cuisine has been influenced by Hungarian, Bohemia Czech, Jewish, Italian, Balkan and French cuisine, from which both dishes and methods of food preparation have often been borrowed. The Austrian cuisine is therefore one of the most multicultural and transcultural in Europe.
Typical Austrian dishes include Wiener Schnitzel, Schweinsbraten, Kaiserschmarren, Knödel, Sachertorte and Tafelspitz. There are also Kärntner Kasnudeln, a cooked filled dough-bag with a type of cottage cheese and spearmint, and Eierschwammerl dishes. The "Eierschwammerl", also known as "Pfifferling", are native yellow, tan mushrooms. The candy Pez was invented in Austria, as well as Mannerschnitten. Austria is also famous for its Mozartkugeln, and its coffee tradition.
Sports.
Due to the mountainous terrain, alpine skiing is a prominent sport in Austria. Similar sports such as snowboarding or ski-jumping are also widely popular. A popular team sport in Austria is football, which is governed by the Austrian Football Association. However, Austria rarely has international success in this discipline, going out in the first round of the 2008 UEFA European Football Championship which was co-hosted by Austria and Switzerland.
Besides football, Austria also has professional national leagues for most major team sports including the Austrian Hockey League for ice hockey, and the Österreichische Basketball Bundesliga for basketball. Bobsleigh, luge, and skeleton are also popular events with a permanent track located in Igls, which hosted bobsleigh and luge competitions for the 1964 and 1976 Winter Olympics held in Innsbruck. The first Winter Youth Olympics in 2012 will be held in Innsbruck as well.
---END.OF.DOCUMENT---

Astronomer.
An astronomer is a scientist who studies celestial bodies such as planets, stars, and galaxies.
Historically, astronomy was more concerned with the classification and description of phenomena in the sky, while astrophysics attempted to explain these phenomena and the differences between them using physical laws. Today, that distinction has mostly disappeared. Professional astronomers are highly educated individuals who typically have a PhD in physics or astronomy and are employed by research institutions or universities. They spend the majority of their time working on research, although they quite often have other duties such as teaching, building instruments, or aiding in the operation of an observatory. The number of professional astronomers in the United States is actually quite small. The American Astronomical Society, which is the major organization of professional astronomers in North America, has approximately 7,700 members. This number includes scientists from other fields such as physics, geology, and engineering, whose research interests are closely related to astronomy. The International Astronomical Union comprises almost 9,259 members from 89 different countries who are involved in astronomical research at the PhD level and beyond.
While the number of professional astronomers worldwide is not much larger than the population of a small town, there is a huge community of amateur astronomers. Most cities have amateur astronomy clubs that meet on a regular basis and often host star parties in their communities. The Astronomical Society of the Pacific is the largest general astronomical society in the world, comprising both professional and amateur astronomers as well as educators from 70 different nations. Like any hobby, most people who think of themselves as amateur astronomers may devote a few hours a month to stargazing and reading the latest developments in research. However, amateurs span the range from so-called "armchair astronomers" to the very ambitious, who own science-grade telescopes and instruments with which they are able to make their own discoveries and assist professional astronomers in research.
Calculation in astronomy.
(Arabic: زيج "astronomical tables of Sind and Hind") is a work consisting of approximately 37 chapters on calendrical and astronomical calculations and 116 tables with calendrical, astronomical and astrological data, as well as a table of sine values. This is the first of many Arabic "Zijes" based on the Indian astronomical methods known as the "sindhind". The work contains tables for the movements of the sun, the moon and the five planets known at the time. This work marked the turning point in Islamic astronomy. Hitherto, Muslim astronomers had adopted a primarily research approach to the field, translating works of others and learning already discovered knowledge. Al-Khwarizmi's work marked the beginning of non-traditional methods of study and calculations.
The original Arabic version (written c. 820) is lost, but a version by the Spanish astronomer Maslamah Ibn Ahmad al-Majriti (c. 1000) has survived in a Latin translation, presumably by Adelard of Bath (January 26, 1126). The four surviving manuscripts of the Latin translation are kept at the Bibliothèque publique (Chartres), the Bibliothèque Mazarine (Paris), the Bibliotheca Nacional (Madrid) and the Bodleian Library (Oxford).
Modern astronomers.
Contrary to the classical image of an old astronomer peering through a telescope through the dark hours of the night, it is very rare for a modern professional astronomer to use an eyepiece on a larger telescope. It is far more common to use a charge-coupled device camera to record a long, deep exposure, allowing a more sensitive image to be created because the light is added over time. Before CCDs, photographic plates were a common method of observation. Modern astronomers spend relatively little time at telescopes - most spend a few weeks per year observing, and the rest of their time reducing the data (changing it from raw data to processed images) and analyzing it. Many astronomers work entirely from astronomical survey or space observatory data. Others work with radio telescopes like the Very Large Array, which is entirely automated, although it is maintained by telescope operators.
Astronomers who serve as faculty spend much of their time teaching undergraduate and graduate classes. Most universities also have outreach programs including public telescope time and sometime planetariums as a public service and to encourage interest in the field.
---END.OF.DOCUMENT---

Amoeboid.
Amoeboids are single-celled life-forms characterized by an irregular shape.
"Amoeboid" and "amoeba" are often used interchangeably even by biologists, and especially refers to a creature moving by using pseudopods. Most references to "amoebas" or "amoebae" are to amoeboids in general rather than to the specific genus "Amoeba". The genus "Amoeba" and amoeboids in general both derive their names from the ancient Greek word for change.
Structure.
The superclass Rhizopoda has naked and shelled amoebas. They tend to ingest food and creep toward food. They move using pseudopodia, which are bulges of cytoplasm. Naked amoebas use all of their body for pseudopod movement. Usually half is used in shelled amoebas; the other half grips the shell.
Amoebas breathe using their entire cell membrane that is constantly immersed in water. Because they live in water, they have no problem keeping it wet. But it does have one drawback. Excess water often crosses into the cytosol. All have one contractile vacuole that expels excess water. This contraction is powered by contracting cytosol.
The nuclei of amoebas do not have chromosomes. They also persist during cell division. The nuclear membrane pinches in two during telophase. Amoebas also do not go through meiosis. Amoebas have one or more nucleoli.
The food scources vary in rhizopoda. Some consume bacteria. Others are detritivores and eat dead organic material. Still others eat other protists. They extend a pair of pseudopodia around food. They fuse to make a food vacuole which then fuses with a lysosome to add digestive chemicals. Undigestied food is expelled at the cell membrane.
Amoebas use pseudopodia to move and feed. They are powered by flexible microfilaments near the membrane. Microfilaments are at least 50% of the cytoskeleton. The other parts are more stiff and are composed of intermediate filaments and macrotubules. These are not used in amoeboid movement, but are stiff skeletons on which organelles are supported or can move on.
The shells of amoebas are often composed of calcium ar other materials. The proteins or materials are synthesised in the cell and exported just outside the cell membrane. The shells are stiff and flexible, but not too much of either trait.
Amoebas seem to have connections with two phyla of the lineage funguslike protists. The two phyla are myxomycota (plasmodial slime molds), and acrasiomycota (cellular slime molds). These two phyla use amoeboid movement in their feeding stage. One is basically a giant multinucleate amoeba, while the other lives solitary until food runs out; in which a colony of these functions as a unit. Myxomycotes use amoeboid gametes, as well. They definitely have a connection.
Diversity.
They have appeared in a number of different groups. Some cells in multicellular animals may be amoeboid, for instance human white blood cells, which consume pathogens. Many protists also exist as individual amoeboid cells, or take such a form at some point in their life-cycle. The most famous such organism is "Amoeba proteus"; the name amoeba is variously used to describe its close relatives, other organisms similar to it, or the amoeboids in general.
As amoebas themselves are polyphyletic and subject to some imprecision in definition, the term "Amoeboid" does not provide identification of an organism, and is better understood as description of locomotion.
When used in the broader sense, the term can include the following groups: Acanthamoeba, Acrasis, Adelphamoeba, Amoeba, Astramoeba, Balamuthia, Cashia, Chaos, Clydonella, Dactylamoeba, Dientamoeba, Dinamoeba, Discamoeba, Echinamoeba, Endamoeba, Entamoeba, Filamoeba, Flabelulla, Flagellipodium, Flamella, Gephyramoeba, Gibbodiscus, Glaeseria, Gocevia, Gruberella, Gyromitus, Hartmannella, Heteramoeba, Hollandella, Histomonas, Hyalodiscus, Hydramoeba, Hyperamoeba, Iodamoeba, Korotnevella, Labyrinthula, Learamoeba, Leptomyxa, Lingulamoeba, Macropharyngomonas, Malamoeba, Mastigamoeba, Mastigella, Mastigina, Mayorella, Metachaos, Micronuclearia, Monopylocystis, Naegleria, Neoparamoeba, Neovahlkampfia, Nollandia, Nuclearia, Oscillosignum, Paragocevia, Paramoeba, Paratetramitus, Paravahlkampfia, Parvamoeba, Pelomyxa, Pernina, Pfiesteria, Polychaos, Pontifex, Phreatamoeba, Platyamoeba, Protoacanthamoeba, Protonaegleria, Psalteriomonas, Pseudomastigamoeba, Plaesiobystra, Rhizamoeba, Rosculus, Rugipes, Saccamoeba, Sappinia, Sawyeria, Stachyamoeba, Stereomyxa, Striamoeba, Striolatus, Stygamoeba, Subulamoeba, Tetramitus, Thecamoeba, Theratromyxa, Trichamoeba, Trichosphaerium, Trienamoeba, Trimastigamoeba, Unda, Vahlkampfia, Vampyrella, Vampyrellium, Vannella, Vexillifera, and Willaertia.
Classification.
Amoeboids may be divided into several morphological categories based on the form and structure of the pseudopods. Those where the pseudopods are supported by regular arrays of microtubules are called actinopods, and forms where they are not are called rhizopods, further divided into lobose, filose, and reticulose amoebae. There is also a strange group of giant marine amoeboids, the xenophyophores, that do not fall into any of these categories.
Most amoeboid are now grouped in Amoebozoa or Rhizaria.
---END.OF.DOCUMENT---

ASCII.
The American Standard Code for Information Interchange (acronym: ASCII;,) is a character-encoding scheme based on the ordering of the English alphabet. ASCII codes represent text in computers, communications equipment, and other devices that use text. Most modern character-encoding schemes are based on ASCII, though they support many more characters than did ASCII.
US-ASCII is the IANA preferred charset name for ASCII.
Historically, ASCII developed from telegraphic codes. Its first commercial use was as a seven-bit teleprinter code promoted by Bell data services. Work on ASCII formally began October 6, 1960, with the first meeting of the American Standards Association's (ASA) X3.2 subcommittee. The first edition of the standard was published during 1963, a major revision during 1967, and the most recent update during 1986. Compared to earlier telegraph codes, the proposed Bell code and ASCII were both ordered for more convenient sorting (i.e., alphabetization) of lists, and added features for devices other than teleprinters.
ASCII includes definitions for 128 characters: 33 are non-printing control characters (now mostly obsolete) that affect how text and space is processed; 94 are printable characters, and the space is considered an invisible graphic.
The most commonly used character encoding on the World Wide Web was US-ASCII until December 2007, when it was surpassed by UTF-8.
History.
The American Standard Code for Information Interchange (ASCII) was developed under the auspices of a committee of the American Standards Association, called the X3 committee, by its X3.2 (later X3L2) subcommittee, and later by that subcommittee's X3.2.4 working group. The ASA became the United States of America Standards Institute or USASI and ultimately the American National Standards Institute.
The X3.2 subcommittee designed ASCII based on earlier teleprinter encoding systems. Like other character encodings, ASCII specifies a correspondence between digital bit patterns and character symbols (i.e. graphemes and control characters). This allows digital devices to communicate with each other and to process, store, and communicate character-oriented information such as written language. Before ASCII was developed, the encodings in use included 26 alphabetic characters, 10 numerical digits, and from 11 to 25 special graphic symbols.
To include all these, and control characters compatible with the Comité Consultatif International Téléphonique et Télégraphique standard, Fieldata, and early EBCDIC, more than 64 codes were required for ASCII.
The committee debated the possibility of a shift key function (like the Baudot code), which would allow more than 64 codes to be represented by six bits. In a shifted code, some character codes determine choices between options for the following character codes.
It allows compact encoding, but is less reliable for data transmission; an error in transmitting the shift code typically makes a long part of the transmission unreadable. The standards committee decided against shifting, and so ASCII required at least a seven-bit code.
The committee considered an eight-bit code, since eight bits would allow two four-bit patterns to efficiently encode two digits with binary coded decimal. However, it would require all data transmission to send eight bits when seven could suffice. The committee voted to use a seven-bit code to minimize costs associated with data transmission. Since perforated tape at the time could record eight bits in one position, it also allowed for a parity bit for error checking if desired. Machines with octets as the native data type that did not use parity checking typically set the eighth bit to "0".
The code itself was patterned so that most control codes were together, and all graphic codes were together. The first two columns (32 positions) were reserved for control characters. The "space" character had to come before graphics to make sorting algorithms easy, so it became position 0x20. The committee decided it was important to support upper case 64-character alphabets, and chose to pattern ASCII so it could be reduced easily to a usable 64-character set of graphic codes. Lower case letters were therefore not interleaved with upper case. To keep options available for lower case letters and other graphics, the special and numeric codes were arranged before the letters, and the letter 'A' was placed in position 0x41 to match the draft of the corresponding British standard. The digits 0–9 were arranged so they correspond to values in binary prefixed with 011, making conversion with binary-coded decimal straightforward.
Many of the non-alphanumeric characters were positioned to correspond to their shifted position on typewriters. Thus #, $ and % were placed to correspond to 3, 4, and 5 in the adjacent column. The parentheses could not correspond to 9 and 0, however, because the place corresponding to 0 was taken by the space character. Since many European typewriters placed the parentheses with 8 and 9, those corresponding positions were chosen for the parentheses. The @ symbol was not used in continental Europe and the committee expected it would be replaced by an accented À in the French variation, so the @ was placed in position 0x40 next to the letter A.
The control codes felt essential for data transmission were the start of message (SOM), end of address (EOA), end of message (EOM), end of transmission (EOT), "who are you?" (WRU), "are you?" (RU), a reserved device control (DC0), synchronous idle (SYNC), and acknowledge (ACK). These were positioned to maximize the Hamming distance between their bit patterns.
With the other special characters and control codes filled in, ASCII was published as ASA X3.4-1963, leaving 28 code positions without any assigned meaning, reserved for future standardization, and one unassigned control code. It now seems obvious that these positions should have been assigned to the lower case alphabet, but there was some debate at the time whether there should be more control characters instead. The indecision did not last long: during May 1963 the CCITT Working Party on the New Telegraph Alphabet proposed to assign lower case characters to columns 6 and 7, and International Organization for Standardization TC 97 SC 2 voted during October to incorporate the change into its draft standard. The X3.2.4 task group voted its approval for the change to ASCII at its May 1963 meeting. Locating the lowercase letters in columns 6 and 7 caused the characters to differ in bit pattern from the upper case by a single bit, which simplified case-insensitive character matching and the construction of keyboards and printers.
The X3 committee made other changes, including other new characters (the brace and vertical line characters), renaming some control characters (SOM became start of header (SOH)) and moving or removing others (RU was removed). ASCII was subsequently updated as USASI X3.4-1967, then USASI X3.4-1968, ANSI X3.4-1977, and finally, ANSI X3.4-1986 (the first two are occasionally retronamed ANSI X3.4-1967, and ANSI X3.4-1968).
The X3 committee also addressed how ASCII should be transmitted (least significant bit first), and how it should be recorded on perforated tape. They proposed a 9-track standard for magnetic tape, and attempted to deal with some forms of punched card formats.
ASCII itself was first used commercially during 1963 as a seven-bit teleprinter code for American Telephone & Telegraph's TWX (Teletype Wide-area eXchange) network. TWX originally used the earlier five-bit Baudot code, which was also used by the competing Telex teleprinter system. Bob Bemer introduced features such as the escape sequence. His British colleague Hugh McGregor Ross helped to popularize this work—according to Bemer, "so much so that the code that was to become ASCII was first called the Bemer-Ross Code in Europe". Because of his extensive work on ASCII, Bemer has been called "the father of ASCII."
I have also approved recommendations of the Secretary of Commerce regarding standards for recording the Standard Code for Information Interchange on magnetic tapes and paper tapes when they are used in computer operations.
All computers and related equipment configurations brought into the Federal Government inventory on and after July 1, 1969, must have the capability to use the Standard Code for Information Interchange and the formats prescribed by the magnetic tape and paper tape standards when these media are used.
Other international standards bodies have ratified character encodings such as IEC 646 that are identical or nearly identical to ASCII, with extensions for characters outside the English alphabet and symbols used outside the United States, such as the symbol for the United Kingdom's pound sterling (£). Almost every country needed an adapted version of ASCII since ASCII only suited the needs of the USA and a few other countries. For example, Canada had its own version that supported French characters. Other adapted encodings include ISCII (India), VISCII (Vietnam), and YUSCII (Yugoslavia). Although these encodings are sometimes referred to as ASCII, true ASCII is defined strictly only by ANSI standard.
ASCII was incorporated into the Unicode character set as the first 128 symbols, so the ASCII characters have the same numeric codes in both sets. This allows UTF-8 to be backward compatible with ASCII, a significant advantage.
ASCII control characters.
ASCII reserves the first 32 codes (numbers 0–31 decimal) for control characters: codes originally intended not to represent printable information, but rather to control devices (such as printers) that make use of ASCII, or to provide meta-information about data streams such as those stored on magnetic tape. For example, character 10 represents the "line feed" function (which causes a printer to advance its paper), and character 8 represents "backspace". RFC 2822 refers to control characters that do not include carriage return, line feed or white space as non-whitespace control characters. Except for the control characters that prescribe elementary line-oriented formatting, ASCII does not define any mechanism for describing the structure or appearance of text within a document. Other schemes, such as markup languages, address page and document layout and formatting.
The original ASCII standard used only short descriptive phrases for each control character. The ambiguity this caused was sometimes intentional (where a character would be used slightly differently on a terminal link than on a data stream) and sometimes accidental (such as what "delete" means).
Probably the most influential single device on the interpretation of these characters was the ASR-33 Teletype series, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage through the 1980s, less costly and in some ways less fragile than magnetic tape. In particular, the Teletype 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (DELete) became de facto standards. Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as "delete previous character" was also adopted by many early timesharing systems but eventually became neglected.
The use of Control-S (XOFF, an abbreviation for transmit off) as a "handshaking" signal warning a sender to stop transmission because of impending overflow, and Control-Q (XON, "transmit on") to resume sending, persists to this day in many systems as a manual output control technique. On some systems Control-S retains its meaning but Control-Q is replaced by a second Control-S to resume output.
Code 127 is officially named "delete" but the Teletype label was "rubout". Since the original standard did not give detailed interpretation for most control codes, interpretations of this code varied. The original Teletype meaning, and the intent of the standard, was to make it an ignored character, the same as NUL (all zeroes). This was useful specifically for paper tape, because punching the all-ones bit pattern on top of an existing mark would obliterate it. Tapes designed to be "hand edited" could even be produced with spaces of extra NULs (blank tape) so that a block of characters could be "rubbed out" and then replacements put into the empty space.
As video terminals began to replace printing ones, the value of the "rubout" character was lost. DEC systems, for example, interpreted "Delete" to mean "remove the character before the cursor," and this interpretation also became common in Unix systems. Most other systems used "Backspace" for that meaning and used "Delete" to mean "remove the character at the cursor". That latter interpretation is the most common now.
Many more of the control codes have been given meanings quite different from their original ones. The "escape" character (code 27), for example, was intended originally to allow sending other control characters as literals instead of invoking their meaning. This is the same meaning of "escape" encountered in URL encodings, C language strings, and other systems where certain characters have a reserved meaning. Over time this meaning has been co-opted and has eventually been changed. In modern use, an ESC sent to the terminal usually indicates the start of a command sequence, usually in the form of a so-called "ANSI escape code" (or, more properly, a "Control Sequence Introducer") beginning with ESC followed by a "[" (left-bracket) character. An ESC sent from the terminal is most often used as an out-of-band character used to terminate an operation, as in the TECO and vi text editors.
The inherent ambiguity of many control characters, combined with their historical usage, created problems when transferring "plain text" files between systems. The best example of this is the newline problem on various operating systems. Teletypes required that a line of text be terminated with both "Carriage Return" and "Linefeed". The first returns the printing carriage to the beginning of the line and the second advances to the next line without moving the carriage. However, requiring two characters to mark the end of a line introduced unnecessary complexity and questions as to how to interpret each character when encountered alone. To simplify matters, plain text files on Unix and Amiga systems use line feeds alone to separate lines. Similarly, older Macintosh systems, among others, use only carriage returns in plain text files. Various DEC operating systems used both characters to mark the end of a line, perhaps for compatibility with teletypes.
This de facto standard was copied into M and then into MS-DOS and eventually into Microsoft Windows. Transmission of text over the Internet, for protocols as E-mail and the World Wide Web, uses both characters.
The pre-VMS DEC operating systems, along with CP/M, tracked file length only in units of disk blocks and used Control-Z (SUB) to mark the end of the actual text in the file (also done for CP/M compatibility in some cases in MS-DOS, though MS-DOS 2 added the ability to record exact file lengths and this is usually relied on today). Text strings ending with the null character are known as ASCIZ or C strings.
ASCII printable characters.
Code 0x20, the space character, denotes the space between words, as produced by the space-bar of a keyboard. The space character is considered an invisible graphic rather than a control character. Codes 0x21 to 0x7E, known as the printable characters, represent letters, digits, punctuation marks, and a few miscellaneous symbols.
Aliases.
Of these, the IANA encourages use of the name "US-ASCII" for Internet uses of ASCII. One often finds this in the optional "charset" parameter in the Content-Type header of some MIME messages, in the equivalent "meta" element of some HTML documents, and in the encoding declaration part of the prologue of some XML documents.
Variants.
As computer technology spread throughout the world, different standards bodies and corporations developed many variations of ASCII to facilitate the expression of non-English languages that used Roman-based alphabets. One could class some of these variations as "ASCII extensions", although some misuse that term to represent all variants, including those that do not preserve ASCII's character-map in the 7-bit range.
The PETSCII code Commodore International used for their 8-bit systems is probably unique among post-1970 codes in being based on ASCII-1963, instead of the more common ASCII-1967, such as found on the ZX Spectrum computer. Atari and Galaksija computers also used ASCII variants.
Incompatibility vs interoperability.
From early in its development, ASCII was intended to be just one of several national variants of an international character code standard, ultimately published as IEC 646 (1972), which would share most characters in common but assign other locally-useful characters to several code points reserved for "national use." However, the four years that elapsed between the publication of ASCII-1963 and ISO's first acceptance of an international recommendation during 1967 caused ASCII's choices for the national use characters to seem to be de facto standards for the world, causing confusion and incompatibility once other countries did begin to make their own assignments to these code points.
ISO/IEC 646, like ASCII, was a 7-bit character set. It did not make any additional codes available, so the same code points encoded different characters in different countries. Escape codes were defined to indicate which national variant applied to a piece of text, but they were rarely used, so it was often impossible to know what variant to work with and therefore which character a code represented, and text-processing systems could generally cope with only one variant anyway.
Because the bracket and brace characters of ASCII were assigned to "national use" code points that were used for accented letters in other national variants of ISO/IEC 646, a German, French, or Swedish, etc., programmer had to get used to reading and writing
codice_1
codice_2
C trigraphs were created to solve this problem for ANSI C, although their late introduction and inconsistent implementation in compilers limited their use.
Eventually, as 8-, 16-, and 32-bit computers began to replace 18- and 36-bit computers as the norm, it became common to use an 8-bit byte to store each character in memory, providing an opportunity for extended, 8-bit, relatives of ASCII, with the 128 additional characters providing room to avoid most of the ambiguity that had been necessary in 7-bit codes.
For example, IBM developed 8-bit code pages, such as code page 437, which replaced the control-characters with graphic symbols such as smiley faces, and mapped additional graphic characters to the upper 128 positions. Operating systems such as DOS supported these code-pages, and manufacturers of IBM PCs supported them in hardware. Digital Equipment Corporation developed the Multinational Character Set (DEC-MCS) for use in the popular VT220 terminal.
Eight-bit standards such as IEC 8859 (derived from the DEC-MCS) and Mac OS Roman developed as true extensions of ASCII, leaving the original character-mapping intact, but adding additional character definitions after the first 128 (i.e., 7-bit) characters. This enabled representation of characters used in a broader range of languages. Because there were several competing 8-bit code standards, they continued to suffer from incompatibilities and limitations. Still, ISO-8859-1 (Latin 1), its variant Windows-1252 (often mislabeled as ISO-8859-1), and the original 7-bit ASCII remain the most common character encodings in use today.
Unicode.
Unicode and the ISO/IEC 10646 Universal Character Set (UCS) have a much wider array of characters, and their various encoding forms have begun to supplant ISO/IEC 8859 and ASCII rapidly in many environments. While ASCII is limited to 128 characters, Unicode and the UCS support more characters by separating the concepts of unique identification (using natural numbers called "code points") and encoding (to 8-, 16- or 32-bit binary formats, called UTF-8, UTF-16 and UTF-32).
To allow backward compatibility, the 128 ASCII and 256 ISO-8859-1 (Latin 1) characters are assigned Unicode/UCS code points that are the same as their codes in the earlier standards. Therefore, ASCII can be considered a 7-bit encoding scheme for a very small subset of Unicode/UCS, and, conversely, the UTF-8 encoding forms are binary-compatible with ASCII for code points below 128, meaning all ASCII is valid UTF-8. The other encoding forms resemble ASCII in how they represent the first 128 characters of Unicode, but use 16 or 32 bits per character, so they require conversion for compatibility. (similarly UCS-2 is upwards compatible with UTF-16)
Order.
The slang expression "ASCIIbetical" is sometimes used for this order. This ordering can be refined by converting uppercase letters to lowercase before comparing ASCII values, or for more sophisticated purposes, applying a collation map to bring accented characters into the correct positions.
---END.OF.DOCUMENT---

Animation.
The bouncing ball animation (below) consists of these 6 frames.
This animation moves at 10 frames per second.
Animation is the rapid display of a sequence of images of 2-D or 3-D artwork or model positions in order to create an illusion of movement. It is an optical illusion of motion due to the phenomenon of persistence of vision, and can be created and demonstrated in a number of ways. The most common method of presenting animation is as a motion picture or video program, although several other forms of presenting animation also exist.
Early examples.
Early examples of attempts to capture the phenomenon of motion drawing can be found in paleolithic cave paintings, where animals are depicted with multiple legs in superimposed positions, clearly attempting to convey the perception of motion.
A 5,200 year old earthen bowl found in Iran in Shahr-i Sokhta has five images of a goat painted along the sides. This has been claimed to be an example of early animation. However, since no equipment existed to show the images in motion, such a series of images cannot be called animation in a true sense of the word.
The phenakistoscope, praxinoscope, as well as the common flip book were early popular animation devices invented during the 1800s, while a Chinese zoetrope-type device was invented already in 180 AD. These devices produced movement from sequential drawings using technological means, but animation did not really develop much further until the advent of cinematography.
There is no single person who can be considered the "creator" of the art of film animation, as there were several people doing several projects which could be considered various types of animation all around the same time.
Georges Méliès was a creator of special-effect films; he was generally one of the first people to use animation with his technique. He discovered a technique by accident which was to stop the camera rolling to change something in the scene, and then continue rolling the film. This idea was later known as stop-motion animation. Méliès discovered this technique accidentally when his camera broke down while shooting a bus driving by. When he had fixed the camera, a hearse happened to be passing by just as Méliès restarted rolling the film, his end result was that he had managed to make a bus transform into a hearse. This was just one of the great contributors to animation in the early years.
The earliest surviving stop-motion advertising film was an English short by Arthur Melbourne-Cooper called "Matches: An Appeal" (1899). Developed for the Bryant and May Matchsticks company, it involved stop-motion animation of wired-together matches writing a patriotic call to action on a blackboard.
J. Stuart Blackton was possibly the first American filmmaker to use the techniques of stop-motion and hand-drawn animation. Introduced to filmmaking by Edison, he pioneered these concepts at the turn of the 20th century, with his first copyrighted work dated 1900. Several of his films, among them "The Enchanted Drawing" (1900) and "Humorous Phases of Funny Faces" (1906) were film versions of Blackton's "lightning artist" routine, and utilized modified versions of Méliès' early stop-motion techniques to make a series of blackboard drawings appear to move and reshape themselves. 'Humorous Phases of Funny Faces' is regularly cited as the first true animated film, and Blackton is considered the first true animator.
Another French artist, Émile Cohl, began drawing cartoon strips and created a film in 1908 called "Fantasmagorie". The film largely consisted of a stick figure moving about and encountering all manner of morphing objects, such as a wine bottle that transforms into a flower. There were also sections of live action where the animator’s hands would enter the scene. The film was created by drawing each frame on paper and then shooting each frame onto negative film, which gave the picture a blackboard look. This makes "Fantasmagorie" the first animated film created using what came to be known as traditional (hand-drawn) animation.
Following the successes of Blackton and Cohl, many other artists began experimenting with animation. One such artist was Winsor McCay, a successful newspaper cartoonist, who created detailed animations that required a team of artists and painstaking attention for detail. Each frame was drawn on paper; which invariably required backgrounds and characters to be redrawn and animated. Among McCay's most noted films are "Little Nemo" (1911), "Gertie the Dinosaur" (1914) and "The Sinking of the Lusitania" (1918).
The production of animated short films, typically referred to as "cartoons", became an industry of its own during the 1910s, and cartoon shorts were produced to be shown in movie theaters. The most successful early animation producer was John Randolph Bray, who, along with animator Earl Hurd, patented the cel animation process which dominated the animation industry for the rest of the decade.
Traditional animation.
Traditional animation (also called cel animation or hand-drawn animation) was the process used for most animated films of the 20th century. The individual frames of a traditionally animated film are photographs of drawings, which are first drawn on paper. To create the illusion of movement, each drawing differs slightly from the one before it. The animators' drawings are traced or photocopied onto transparent acetate sheets called cels, which are filled in with paints in assigned colors or tones on the side opposite the line drawings. The completed character cels are photographed one-by-one onto motion picture film against a painted background by a rostrum camera.
The traditional cel animation process became obsolete by the beginning of the 21st century. Today, animators' drawings and the backgrounds are either scanned into or drawn directly into a computer system. Various software programs are used to color the drawings and simulate camera movement and effects. The final animated piece is output to one of several delivery media, including traditional 35 mm film and newer media such as digital video. The "look" of traditional cel animation is still preserved, and the character animators' work has remained essentially the same over the past 70 years. Some animation producers have used the term "tradigital" to describe cel animation which makes extensive use of computer technology.
Examples of traditionally animated feature films include "Pinocchio" (United States, 1940), "Animal Farm" (United Kingdom, 1954), and "Akira" (Japan, 1988). Traditional animated films which were produced with the aid of computer technology include "The Lion King" (US, 1994) "Sen to Chihiro no Kamikakushi (Spirited Away)" (Japan, 2001), "Treasure Planet" (USA, 2002) and "Les Triplettes de Belleville" (2003).
Stop motion.
Stop-motion animation is used to describe animation created by physically manipulating real-world objects and photographing them one frame of film at a time to create the illusion of movement. There are many different types of stop-motion animation, usually named after the type of media used to create the animation. Computer software is widely available to create this type of animation.
Computer animation.
Computer animation encompasses a variety of techniques, the unifying factor being that the animation is created digitally on a computer.
2D animation.
2D animation figures are created and/or edited on the computer using 2D bitmap graphics or created and edited using 2D vector graphics. This includes automated computerized versions of traditional animation techniques such as of tweening, morphing, onion skinning and interpolated rotoscoping.
Examples: "Foster's Home for Imaginary Friends", "Danny Phantom", Waltz with Bashir
3D animation.
3D animation are digitally modeled and manipulated by an animator. In order to manipulate a mesh, it is given a digital skeletal structure that can be used to control the mesh. This process is called rigging. Various other techniques can be applied, such as mathematical functions (ex. gravity, particle simulations), simulated fur or hair, effects such as fire and water and the use of Motion capture to name but a few, these techniques fall under the category of 3d dynamics. Many 3D animations are very believable and are commonly used as Visual effects for recent movies.
Terms.
2D animation techniques tend to focus on image manipulation while 3D techniques usually build virtual worlds in which characters and objects move and interact. 3D animation can create images that seem real to the viewer.
---END.OF.DOCUMENT---

Apollo.
In Greek and Roman mythology, Apollo (in Greek, "Ἀπόλλων"—"Apóllōn" or "Ἀπέλλων"—"Apellōn"), is one of the most important and diverse of the Olympian deities. The ideal of the "kouros" (a beardless youth), Apollo has been variously recognized as a god of light and the sun; truth and prophecy; archery; medicin, healing and plague; music, poetry, and the arts; and more. Apollo is the son of Zeus and Leto, and has a twin sister, the chaste huntress Artemis. Apollo is known in Greek-influenced Etruscan mythology as "Apulu". Apollo was worshiped in both ancient Greek and Roman religion, as well as in the modern Greco–Roman Neopaganism.
As the patron of Delphi ("Pythian Apollo"), Apollo was an oracular god — the prophetic deity of the Delphic Oracle. Medicine and healing were associated with Apollo, whether through the god himself or mediated through his son Asclepius, yet Apollo was also seen as a god who could bring ill-health and deadly plague as well as one who had the ability to cure. Amongst the god's custodial charges, Apollo became associated with dominion over colonists, and as the patron defender of herds and flocks. As the leader of the Muses ("Apollon Musagetes") and director of their choir, Apollo functioned as the patron god of music and poetry. Hermes created the lyre for him, and the instrument became a common attribute of Apollo. Hymns sung to Apollo were called paeans.
In Hellenistic times, especially during the third century BCE, as "Apollo Helios" he became identified among Greeks with Helios, god of the sun, and his sister Artemis similarly equated with Selene, goddess of the moon. In Latin texts, on the other hand, Joseph Fontenrose declared himself unable to find any conflation of Apollo with Sol among the Augustan poets of the first century, not even in the conjurations of Aeneas and Latinus in "Aeneid" XII (161–215). Apollo and Helios/Sol remained separate beings in literary and mythological texts until the third century CE.
Etymology.
The etymology of "Apollo" is uncertain. Several instances of popular etymology are attested from ancient authors. Thus, Plato in "Cratylus" connects the name with "redeem", with "purification", and with "simple", in particular in reference to the Thessalian form of the name, and finally with "ever-shooting". Hesychius connects the name Apollo with the Doric απελλα, which means "assembly", so that Apollo would be the god of political life, and he also gives the explanation σηκος ("fold"), in which case Apollo would be the god of flocks and herds. It is also possible that "apellai" derives from an old form of Apollo which can be equated with Appaliunas, an Anatolian god whose name possibly means "father lion" or "father light". The Greeks later associated Apollo's name with the Greek verb απολλυμι (apollymi) meaning "to destroy".
It has also been suggested that Apollo comes from the Hurrian and Hittite divinity, Aplu, who was widely evoked during the "plague years". Aplu, it is suggested, comes from the Akkadian "Aplu Enlil", meaning "the son of Enlil", a title that was given to the god Nergal, who was linked to Shamash, Babylonian god of the sun.
Origins of cult.
There are generally two broad opinions on the origins of Apollo: one derives him from the East, the other connects him to the Dorians and their apellai (cf. also the month Apellaios). In any case, Walter Burkert notes that components of various origins are discernible in his worship: a Dorian Greek, a Cretan-Minoan and a Syro-Hittite. According to the first opinion, both Greek and Etruscan Apollo came to the Aegean during the Iron Age (i.e. from c.1100 BCE to c. 800 BCE) from Anatolia. Homer pictures him on the side of the Trojans, against the Achaeans, during the Trojan War and he has close affiliations with a Luwian deity, Apaliunas, who in turn seems to have traveled west from further east. The Late Bronze Age (from 1700–1200 BCE) Hittite and Hurrian "Aplu", like the Homeric Apollo, was a god of plagues, and resembles the mouse god "Apollo Smintheus". Here we have an apotropaic situation, where a god originally bringing the plague was invoked to end it, merging over time through fusion with the Mycenaean healer-god Paeon (PA-JA-WO in Linear B); Paeon, in Homer's "Iliad", was the Greek healer of the wounded gods Ares and Hades. In other writers, the word becomes a mere epithet of Apollo in his capacity as a god of healing, but it is now known from Linear B that Paeon was originally a separate deity.
Homer illustrated Paeon the god, as well as the song both of apotropaic thanksgiving or triumph, and Hesiod also separated the two; in later poetry Paeon was invoked independently as a god of healing. It is equally difficult to separate Paeon or Paean in the sense of "healer" from Paean in the sense of "song."
Such songs were originally addressed to Apollo, and afterwards to other gods, Dionysus, Helios, Asclepius. About the fourth century BCE, the paean became merely a formula of adulation; its object was either to implore protection against disease and misfortune, or to offer thanks after such protection had been rendered. It was in this way that Apollo had become recognised as the god of music. Apollo's role as the slayer of the Python led to his association with battle and victory; hence it became the Roman custom for a paean to be sung by an army on the march and before entering into battle, when a fleet left the harbour, and also after a victory had been won.
Apollo's links with oracles again seem to be associated with wishing to know the outcome of an illness. He is a god of music and the lyre. Healing belongs to his realm: he was the father of Asclepius, the god of medicine. The Muses are part of his retinue, so that music, history, poetry and dance all belong to him.
Cult sites.
Unusually among the Olympic deities, Apollo had two cult sites that had widespread influence: Delos and Delphi. In cult practice, Delian Apollo and Pythian Apollo (the Apollo of Delphi) were so distinct that they might both have shrines in the same locality. Theophoric names such as "Apollodorus" or "Apollonios" and cities named Apollonia are met with throughout the Greek world. Apollo's cult was already fully established when written sources commenced, about 650 BCE.
Oracular shrines.
Apollo had a famous oracle in Delphi, and other notable ones in Clarus and Branchidae. His oracular shrine in Abae in Phocis, where he bore the toponymic epithet "Abaeus" (, "Apollon Abaios") was important enough to be consulted by Croesus (Herodotus, 1.46).
Oracles were also given by sons of Apollo.
Festivals.
The chief Apollonian festivals were the Boedromia, Carneia, Carpiae, Daphnephoria, Delia, Hyacinthia, Metageitnia, Pyanepsia, Pythia and Thargelia.
Attributes and symbols.
Apollo's most common attributes were the bow and arrow. Other attributes of his included the kithara (an advanced version of the common lyre), the plectrum and the sword. Another common emblem was the sacrificial tripod, representing his prophetic powers. The Pythian Games were held in Apollo's honor every four years at Delphi. The bay laurel plant was used in expiatory sacrifices and in making the crown of victory at these games. The palm was also sacred to Apollo because he had been born under one in Delos. Animals sacred to Apollo included wolves, dolphins, roe deer, swans, cicadas (symbolizing music and song), hawks, ravens, crows, snakes (referencing Apollo's function as the god of prophecy), mice and griffins, mythical eagle–lion hybrids of Eastern origin.
As god of colonization, Apollo gave oracular guidance on colonies, especially during the height of colonization, 750–550 BCE. According to Greek tradition, he helped Cretan or Arcadian colonists found the city of Troy. However, this story may reflect a cultural influence which had the reverse direction: Hittite cuneiform texts mention a Minor Asian god called "Appaliunas" or "Apalunas" in connection with the city of Wilusa attested in Hittite inscriptions, which is now generally regarded as being identical with the Greek Ilion by most scholars. In this interpretation, Apollo's title of "Lykegenes" can simply be read as "born in Lycia", which effectively severs the god's supposed link with wolves (possibly a folk etymology).
In literary contexts, Apollo represents harmony, order, and reason—characteristics contrasted with those of Dionysus, god of wine, who represents ecstasy and disorder. The contrast between the roles of these gods is reflected in the adjectives Apollonian and Dionysian. However, the Greeks thought of the two qualities as complementary: the two gods are brothers, and when Apollo at winter left for Hyperborea, he would leave the Delphic oracle to Dionysus. This contrast appears to be shown on the two sides of the Borghese Vase.
Apollo is often associated with the Golden Mean. This is the Greek ideal of moderation and a virtue that opposes gluttony.
Roman Apollo.
The Roman worship of Apollo was adopted from the Greeks. As a quintessentially Greek god, Apollo had no direct Roman equivalent, although later Roman poets often referred to him as Phoebus. There was a tradition that the Delphic oracle was consulted as early as the period of the kings of Rome during the reign of Tarquinius Superbus. On the occasion of a pestilence in the 430s BC, Apollo's first temple at Rome was established in the Flaminian fields, replacing an older cult site there known as the "Apollinare". During the Second Punic War in 212 BC, the "Ludi Apollinares" ("Apollonian Games") were instituted in his honor, on the instructions of a prophecy attributed to one Marcius. In the time of Augustus, who considered himself under the special protection of Apollo and was even said to be his son, his worship developed and he became one of the chief gods of Rome. After the battle of Actium, which was fought near a sanctuary of Apollo, Augustus enlarged Apollo's temple, dedicated a portion of the spoils to him, and instituted quinquennial games in his honour. He also erected a new temple to the god on the Palatine hill. Sacrifices and prayers on the Palatine to Apollo and Diana formed the culmination of the Secular Games, held in 17 BCE to celebrate the dawn of a new era.
In art.
In art, Apollo is depicted as a handsome beardless young man, often with a kithara (as Apollo Citharoedus) or bow in his hand, or reclining on a tree (the Apollo Lykeios and Apollo Sauroctonos types). The Apollo Belvedere is a marble sculpture that was rediscovered in the late 15th century; for centuries it epitomized the ideals of Classical Antiquity for Europeans, from the Renaissance through the nineteenth century. The marble is a Hellenistic or Roman copy of a bronze original by the Greek sculptor Leochares, made between 350 and 325 BC.
The lifesize so-called "Adonis" (shown at left) found in 1780 on the site of a "villa suburbana" near the Via Labicana in the Roman suburb of Centocelle and now in the Ashmolean Museum, Oxford, is identified as an Apollo by modern scholars. It was probably never intended as a cult object, but was a pastiche of several fourth-century and later Hellenistic model types, intended to please a Roman connoisseur of the second century AD, and to be displayed in his villa.
In the late second century CE floor mosaic from El Djem, Roman "Thysdrus" (right), he is identifiable as Apollo Helios by his effulgent halo, though now even a god's divine nakedness is concealed by his cloak, a mark of increasing conventions of modesty in the later Empire. Another haloed Apollo in mosaic, from Hadrumentum, is in the museum at Sousse. The conventions of this representation, head tilted, lips slightly parted, large-eyed, curling hair cut in locks grazing the neck, were developed in the third century BCE to depict Alexander the Great (Bieber 1964, Yalouris 1980). Some time after this mosaic was executed, the earliest depictions of Christ will be beardless and haloed.
Birth.
When Hera discovered that Leto was pregnant and that Zeus was the father, she banned Leto from giving birth on "terra firma", or the mainland, or any island. In her wanderings, Leto found the newly created floating island of Delos, which was neither mainland nor a real island, so she gave birth there. The island was surrounded by swans. Afterwards, Zeus secured Delos to the bottom of the ocean. This island later became sacred to Apollo.
It is also stated that Hera kidnapped Ilithyia, the goddess of childbirth, to prevent Leto from going into labor. The other gods tricked Hera into letting her go by offering her a necklace, nine yards (8 m) long, of amber. Mythographers agree that Artemis was born first and then assisted with the birth of Apollo, or that Artemis was born one day before Apollo, on the island of Ortygia and that she helped Leto cross the sea to Delos the next day to give birth to Apollo. Apollo was born on the seventh day () of the month Thargelion —according to Delian tradition— or of the month Bysios— according to Delphian tradition. The seventh and twentieth, the days of the new and full moon, were ever afterwards held sacred to him.
Youth.
Four days after his birth, Apollo killed the chthonic dragon Python, which lived in Delphi beside the Castalian Spring. This was the spring which emitted vapors that caused the oracle at Delphi to give her prophesies. Hera sent the serpent to hunt Leto to her death across the world. In order to protect his mother, Apollo begged Hephaestus for a bow and arrows. After receiving them, Apollo cornered Python in the sacred cave at Delphi. Apollo killed Python but had to be punished for it, since Python was a child of Gaia.
Hera then sent the giant Tityos to kill Leto. This time Apollo was aided by his sister Artemis in protecting their mother. During the battle Zeus finally relented his aid and hurled Tityos down to Tartarus. There he was pegged to the rock floor, covering an area of, where a pair of vultures feasted daily on his liver.
Admetus.
When Zeus struck down Apollo's son Asclepius with a lightning bolt for resurrecting Hippolytus from the dead (transgressing Themis by stealing Hades's subjects), Apollo in revenge killed the Cyclopes, who had fashioned the bolt for Zeus. Apollo would have been banished to Tartarus forever, but was instead sentenced to one year of hard labor as punishment, thanks to the intercession of his mother, Leto. During this time he served as shepherd for King Admetus of Pherae in Thessaly. Admetus treated Apollo well, and, in return, the god conferred great benefits on Admetus.
Apollo helped Admetus win Alcestis, the daughter of King Pelias and later convinced the Fates to let Admetus live past his time, if another took his place. But when it came time for Admetus to die, his parents, whom he had assumed would gladly die for him, refused to cooperate. Instead, Alcestis took his place, but Heracles managed to "persuade" Thanatos, the god of death, to return her to the world of the living.
Trojan War.
Apollo shot arrows infected with the plague into the Greek encampment during the Trojan War in retribution for Agamemnon's insult to Chryses, a priest of Apollo whose daughter Chryseis had been captured. He demanded her return, and the Achaeans complied, indirectly causing the anger of Achilles, which is the theme of the "Iliad".
When Diomedes injured Aeneas ("Iliad"), Apollo rescued him. First, Aphrodite tried to rescue Aeneas but Diomedes injured her as well. Aeneas was then enveloped in a cloud by Apollo, who took him to Pergamos, a sacred spot in Troy.
Apollo aided Paris in the killing of Achilles by guiding the arrow of his bow into Achilles' heel. One interpretation of his motive is that it was in revenge for Achilles' sacrilege in murdering Troilus, the god's own son by Hecuba, on the very altar of the god's own temple.
Niobe.
The queen of Thebes and wife of Amphion, Niobe boasted of her superiority to Leto because she had fourteen children (Niobids), seven male and seven female, while Leto had only two. Apollo killed her sons as they practiced athletics, with the last begging for his life, and Artemis her daughters. Apollo and Artemis used poisoned arrows to kill them, though according to some versions of the myth, a number of the Niobids were spared (Chloris, usually). Amphion, at the sight of his dead sons, either killed himself or was killed by Apollo after swearing revenge. A devastated Niobe fled to Mount Sipylos in Asia Minor and turned into stone as she wept. Her tears formed the river Achelous. Zeus had turned all the people of Thebes to stone and so no one buried the Niobids until the ninth day after their death, when the gods themselves entombed them.
Consorts and children.
Love affairs ascribed to Apollo are a late development in Greek mythology. Their vivid anecdotal qualities have made favorites some of them of painters since the Renaissance, so that they stand out more prominently in the modern imagination.
Female lovers.
In explanation of the connection of Apollon with "daphne", the Laurel whose leaves his priestess employed at Delphi, it was told by Libanius, a fourth-century CE teacher of rhetoric, that Apollo chased a nymph, Daphne, daughter of Peneus, who had scorned him. In Ovid's telling for a Roman audience, Phoebus Apollo chaffs Cupid for toying with a man's weapon suited to a man, whereupon Cupid wounds him with an arrow with a golden dart; simultaneously, however, Eros had shot a leaden arrow into Daphne, causing her to be repulsed by Apollo. Following a spirited chase by Apollo, Daphne prayed to Mother Earth, or, alternatively, her father — a river god — to help her and he changed her into the Laurel tree, sacred to Apollo.
Apollo had an affair with a human princess named Leucothea, daughter of Orchamus and sister of Clytia. Leucothea loved Apollo who disguised himself as Leucothea's mother to gain entrance to her chambers. Clytia, jealous of her sister because she wanted Apollo for herself, told Orchamus the truth, betraying her sister's trust and confidence in her. Enraged, Orchamus ordered Leucothea to be buried alive. Apollo refused to forgive Clytia for betraying his beloved, and a grieving Clytia wilted and slowly died. Apollo changed her into an incense plant, either heliotrope or sunflower, which follows the sun every day.
Marpessa was kidnapped by Idas but was loved by Apollo as well. Zeus made her choose between them, and she chose Idas on the grounds that Apollo, being immortal, would tire of her when she grew old.
Castalia was a nymph whom Apollo loved. She fled from him and dived into the spring at Delphi, at the base of Mt. Parnassos, which was then named after her. Water from this spring was sacred; it was used to clean the Delphian temples and inspire poets.
By Cyrene, Apollo had a son named Aristaeus, who became the patron god of cattle, fruit trees, hunting, husbandry and bee-keeping. He was also a culture-hero and taught humanity dairy skills and the use of nets and traps in hunting, as well as how to cultivate olives.
With Hecuba, wife of King Priam of Troy, Apollo had a son named Troilus. An oracle prophesied that Troy would not be defeated as long as Troilus reached the age of twenty alive. He was ambushed and killed by Achilles.
Apollo also fell in love with Cassandra, daughter of Hecuba and Priam, and Troilus' half-sister. He promised Cassandra the gift of prophecy to seduce her, but she rejected him afterwards. Enraged, Apollo indeed gifted her with the ability to know the future, with a curse that she could only see the future tragedies and that no one would ever believe her.
Coronis, daughter of Phlegyas, King of the Lapiths, was another of Apollo's liaisons. Pregnant with Asclepius, Coronis fell in love with Ischys, son of Elatus. A crow informed Apollo of the affair. When first informed he disbelieved the crow and turned all crows black (where they were previously white) as a punishment for spreading untruths. When he found out the truth he sent his sister, Artemis, to kill Coronis (in other stories, Apollo himself had killed Coronis). As a result he also made the crow sacred and gave them the task of announcing important deaths. Apollo rescued the baby and gave it to the centaur Chiron to raise. Phlegyas was irate after the death of his daughter and burned the Temple of Apollo at Delphi. Apollo then killed him for what he did.
In Euripides' play "Ion", Apollo fathered Ion by Creusa, wife of Xuthus. Creusa left Ion to die in the wild, but Apollo asked Hermes to save the child and bring him to the oracle at Delphi, where he was raised by a priestess.
One of his other liaisons was with Acantha, the spirit of the acanthus tree. Upon her death, Apollo transformed her into a sun-loving herb.
According to the "Biblioteca", or "library" of mythology mis-attributed to Apollodorus, he fathered the Corybantes on the Muse Thalia.
Male lovers.
Hyacinth (or Hyacinthus) was one of his male lovers. Hyacinthus was a Spartan prince, beautiful and athletic. The pair were practicing throwing the discus when a discus thrown by Apollo was blown off course by the jealous Zephyrus and struck Hyacinthus in the head, killing him instantly. Apollo is said to be filled with grief: out of Hyacinthus' blood, Apollo created a flower named after him as a memorial to his death, and his tears stained the flower petals with "άί" "άί", meaning alas. The Festival of Hyacinthus was a celebration of Sparta.
Another male lover was Cyparissus, a descendant of Heracles. Apollo gave him a tame deer as a companion but Cyparissus accidentally killed it with a javelin as it lay asleep in the undergrowth. Cyparissus asked Apollo to let his tears fall forever. Apollo granted the request by turning him into the Cypress named after him, which was said to be a sad tree because the sap forms droplets like tears on the trunk.
Birth of Hermes.
Hermes was born on Mount Cyllene in Arcadia. The story is told in the Homeric Hymn to Hermes. His mother, Maia, had been secretly impregnated by Zeus. Maia wrapped the infant in blankets but Hermes escaped while she was asleep. Hermes ran to Thessaly, where Apollo was grazing his cattle. The infant Hermes stole a number of his cows and took them to a cave in the woods near Pylos, covering their tracks. In the cave, he found a tortoise and killed it, then removed the insides. He used one of the cow's intestines and the tortoise shell and made the first lyre. Apollo complained to Maia that her son had stolen his cattle, but Hermes had already replaced himself in the blankets she had wrapped him in, so Maia refused to believe Apollo's claim. Zeus intervened and, claiming to have seen the events, sided with Apollo. Hermes then began to play music on the lyre he had invented. Apollo, a god of music, fell in love with the instrument and offered to allow exchange of the cattle for the lyre. Hence, Apollo became a master of the lyre.
Other stories.
Apollo gave the order through the Oracle at Delphi, for Orestes to kill his mother, Clytemnestra, and her lover, Aegisthus. Orestes was punished fiercely by the Erinyes (the Furies, female personifications of vengeance) for this crime. Relentlessly pursued by the Furies, Orestes asked for the intercession of Athena, who decreed that he be tried by a jury of his peers, with Apollo acting as his advocate.
In the Odyssey, Odysseus and his surviving crew landed on an island sacred to Helios the sun god, where he kept sacred cattle. Though Odysseus warned his men not to (as Tiresias and Circe had told him), they killed and ate some of the cattle and Helios had Zeus destroy the ship and all the men, except Odysseus.
Apollo also had a lyre-playing contest with Cinyras, his son, who committed suicide when he lost.
Apollo killed the Aloadae when they attempted to storm Mt. Olympus.
Callimachus sang that Apollo rode on the back of a swan to the land of the Hyperboreans during the winter months.
Apollo turned Cephissus into a sea monster.
Pan.
Once Pan had the audacity to compare his music with that of Apollo, and to challenge Apollo, the god of the kithara, to a trial of skill. Tmolus, the mountain-god, was chosen to umpire. Pan blew on his pipes, and with his rustic melody gave great satisfaction to himself and his faithful follower, Midas, who happened to be present. Then Apollo struck the strings of his lyre. Tmolus at once awarded the victory to Apollo, and all but Midas agreed with the judgment. He dissented, and questioned the justice of the award. Apollo would not suffer such a depraved pair of ears any longer, and caused them to become the ears of a donkey.
Marsyas.
Apollo has ominous aspects aside from his plague-bringing, death-dealing arrows: Marsyas was a satyr who challenged Apollo to a contest of music. He had found an aulos on the ground, tossed away after being invented by Athena because it made her cheeks puffy. The contest was judged by the Muses. After they each performed, both were deemed equal until Apollo decreed they play and sing at the same time. As Apollo played the lyre, this was easy to do. Marsyas could not do this as he only knew how to use the flute and could not sing at the same time. Apollo was declared the winner because of this. Apollo flayed Marsyas alive in a cave near Celaenae in Phrygia for his hubris to challenge a god. He then nailed Marsyas' shaggy skin to a nearby pine-tree. Marsyas' blood turned into the river Marsyas.
Another variation is that Apollo played his instrument (the lyre) upside down. Marsyas could not do this with his instrument (the flute), and so Apollo hung him from a tree and flayed him alive.
Graeco–Roman epithets and cult titles.
Apollo, like other Greek deities, had a number of epithets applied to him, reflecting the variety of roles, duties, and aspects ascribed to the god. However, while Apollo has a great number of appellations in Greek myth, only a few occur in Latin literature, chief among them Phoebus ("shining one"), which was very commonly used by both the Greeks and Romans in Apollo's role as the god of light.
In Apollo's role as healer, his appellations included Akesios, Iatros, and Acestor meaning "healer". He was also called Alexicacus ("restrainer of evil") and Apotropaeus ("he who averts evil"), and was referred to by the Romans as Averruncus ("averter of evils"). As a plague god and defender against rats and locusts, Apollo was known as Smintheus ("mouse-catcher") and Parnopius ("grasshopper"). The Romans also called Apollo Culicarius ("driving away midges"). In his healing aspect, the Romans referred to Apollo as Medicus ("the Physician"), and a temple was dedicated to "Apollo Medicus" at Rome, probably next to the temple of Bellona. As a sun-god he was worshiped as Aegletes, the radiant god.
As a god of archery, Apollo was known as Aphetoros ("god of the bow") and Argurotoxos ("with the silver bow"). The Romans referred to Apollo as Articenens ("carrying the bow") as well. As a pastoral shepherd-god, Apollo was known as Nomios ("wandering"). As the protector of roads and homes he was Agyieus.
Apollo was also known as Archegetes ("director of the foundation"), who oversaw colonies. He was known as Klarios, from the Doric "klaros" ("allotment of land"), for his supervision over cities and colonies.
He was known as Delphinios ("Delphinian"), meaning "of the womb", in his association with "Delphoi" (Delphi). At Delphi, he was also known as Pythios ("Pythian"). An aitiology in the Homeric hymns connects the epitheton to dolphins. Kynthios, another common epithet, stemmed from his birth on Mt. Cynthus. He was also known as Lyceios or Lykegenes, which either meant "wolfish" or "of Lycia", Lycia being the place where some postulate that his cult originated.
Specifically as god of prophecy, Apollo was known as Loxias ("the obscure"). He was also known as Coelispex ("he who watches the heavens") to the Romans. Apollo was attributed the epithet Musagetes as the leader of the muses, and Nymphegetes as "nymph-leader".
Acesius was the epithet of Apollo worshipped in Elis, where he had a temple in the agora. This surname, which has the same meaning as "akestor" and "alexikakos", characterized the god as the averter of evil. Acraephius or Acraephiaeus was his epithet worshipped in the Boeotian town of Acraephia, reputedly founded by his son, Acraepheus. Actiacus was his epithet in Actium, one of the principal places of his worship.
Celtic epithets and cult titles.
Apollo was worshipped throughout the Roman Empire. In the traditionally Celtic lands he was most often seen as a healing and sun god. He was often equated with Celtic gods of similar character.
Reception.
Apollo has often featured in postclassical art and literature. Percy Bysshe Shelley composed a "Hymn of Apollo" (1820), and the god's instruction of the Muses formed the subject of Igor Stravinsky's "Apollon musagète" (1927–1928). The name "Apollo" was given to NASA's Apollo Lunar program in the 1960s.
The statue of Apollo from the west pediment of the Temple of Zeus at Olympia (currently in the Archaeological Museum of Olympia) was depicted on the obverse of the Greek 1000 drachmas banknote of 1987–2001.
---END.OF.DOCUMENT---

Andre Agassi.
Andre Kirk Agassi (; born April 29, 1970) is an American former World No. 1 professional tennis player who won eight Grand Slam singles tournaments and an Olympic gold medal in singles. Generally considered by critics and fellow players to be one of the greatest tennis players of all time, he has been called the best service returner in the history of tennis. Known for his unorthodox apparel and attitude, Agassi is often cited as one of the most charismatic players in the history of the game, and is credited for helping revive the popularity of tennis during the 1990s. He is married to fellow retired professional tennis player and multiple Grand Slam champion Steffi Graf.
Agassi is, with Rod Laver, Don Budge, Fred Perry, Roy Emerson, and Roger Federer, one of only six men to have achieved a Career Grand Slam, one of only three (with Laver and Federer) since the beginning of the Open Era, and the only male player to have achieved a Career Golden Slam. In addition to his Grand Slam and Olympic singles titles, he won the Tennis Masters Cup and was part of a winning Davis Cup team. He won 17 ATP Masters Series tournaments, more than any other player. Agassi's Grand Slam composition is (4 Australian Open, 1 French Open, 1 Wimbledon, 2 US Open) for his career.
After suffering from sciatica caused by two bulging discs in his back, a spondylolisthesis (vertebral displacement) and a bone spur that interferes with the nerve, Agassi retired from professional tennis on September 3, 2006, after losing in the third round of the US Open. He is the founder of the Andre Agassi Charitable Foundation, which has raised over $60 million for at-risk children in Southern Nevada. In 2001, the Foundation opened the Andre Agassi College Preparatory Academy in Las Vegas, a K-12 public charter school for at-risk children.
1970–1985: Early life.
Agassi was born in Las Vegas, Nevada, to Emmanuel "Mike" Aghassian and Elizabeth "Betty" Agassi (née Dudley). His father is an Iranian of Armenian and Assyrian ethnicity who represented Iran in boxing at the 1948 and 1952 Olympic Games before emigrating to the United States. Andre Agassi's mother, Betty, is a breast cancer survivor.
Mike Agassi was renowned for his domineering nature, reportedly taking a hammer to matches and banging on the fences in disgust when Andre lost a point. He sometimes screamed at officials and was ejected more than once. At age 13, Andre was sent to Nick Bollettieri's Tennis Academy in Florida. He was meant to stay for only 3 months because that was all his father could afford. However, after ten minutes of watching Agassi rally, Bollettieri called Mike and said: "Take your check back. He's here for free," claiming that Agassi had more natural talent than anyone else he had seen.
1986–1993.
He turned professional at the age of 16 and his first tournament was in La Quinta, California. He won his first match against John Austin 6–4, 6–2 but then lost his second match to Mats Wilander 6–1, 6–1. By the end of the year, Agassi was ranked World No. 91. Agassi won his first top-level singles title in 1987 at the Sul American Open in Itaparica. He ended the year ranked World No. 25. He won six additional tournaments in 1988 (Memphis, U.S. Men's Clay Court Championships, Forest Hills WCT, Stuttgart Outdoor, Volvo International and Livingston Open), and, by December of that year, he had surpassed US$2 million in career prize money after playing in just 43 tournaments – the fastest anyone in history had reached that level. His year-end ranking was World No. 3, behind second-ranked Ivan Lendl and top-ranked Mats Wilander. Both the Association of Tennis Professionals and "Tennis" magazine named Agassi the Most Improved Player of the Year for 1988.
In addition to not playing the Australian Open (which would later become his best Grand Slam event) for the first eight years of his career, Agassi chose not to play at Wimbledon from 1988 through 1990 and publicly stated that he did not wish to play there because of the event's traditionalism, particularly its "predominantly white" dress code to which players at the event are required to conform.
Strong performances on the tour meant that Agassi was quickly tipped as a future Grand Slam champion. While still a teenager, he reached the semi-finals of both the French Open and the US Open in 1988, and made the US Open semifinals in 1989. He began the 1990s, however, with a series of near-misses. He reached his first Grand Slam final in 1990 at the French Open, where he was favored before losing in four sets to Andrés Gómez. He reached his second Grand Slam final of the year at the US Open, defeating defending champion Boris Becker in the semifinals. His opponent in the final was Pete Sampras; a year earlier, Agassi had beaten Sampras 6-2, 6-1 after which he told his coach that he felt bad for Sampras because he was never going to make it as a pro. Agassi lost the US Open final to Sampras 6–4, 6–3, 6–2. The rivalry between these two American players became the dominant rivalry in tennis over the rest of the decade. Also in 1990, Agassi helped the United States win its first Davis Cup in 8 years and won his only Tennis Masters Cup, beating reigning Wimbledon champion Stefan Edberg in the final.
In 1991, Agassi reached his second consecutive French Open final, where he faced fellow Bollettieri Academy alumnus Jim Courier. Courier emerged the victor in a five set final. Agassi decided to play at Wimbledon in 1991, leading to weeks of speculation in the media about the clothes he would wear. He eventually emerged for the first round in a completely white outfit. He went on to reach the quarter-finals on that occasion, losing in five sets to David Wheaton.
Agassi's Grand Slam tournament breakthrough came at Wimbledon, not at the French Open or the US Open where he had previously enjoyed success. In 1992, he defeated Goran Ivanišević in a five set final. Along the way, Agassi overcame two former Wimbledon champions in Boris Becker and John McEnroe. No other baseliner would triumph at Wimbledon until Lleyton Hewitt ten years later. Agassi was named the BBC Overseas Sports Personality of the Year in 1992. Agassi once again played on the United States' Davis Cup winning team in 1992. It was their second Davis cup title in three years.
1993 saw Agassi win the only doubles title of his career, at the Cincinnati Masters, partnered with Petr Korda. Agassi missed much of the early part of that year with injuries. Although he made the quarterfinals in his Wimbledon title defense, he lost to eventual champion and World number one Pete Sampras in five-sets. Agassi lost in the first-round at the US Open to Thomas Enqvist and required wrist surgery late in the year.
1994–1997.
With new coach Brad Gilbert on board, Agassi began to employ more of a tactical, consistent approach, which fueled his resurgence. Agassi started slowly in 1994, losing in the first week at the French Open and Wimbledon. Nevertheless, Agassi emerged during the hard court season, winning the Canadian Open. His comeback culminated at the 1994 US Open with a 5-set fourth-round victory against compatriot Michael Chang and then becoming the first man to capture the US Open as an unseeded player, beating Michael Stich in the final.
In 1995, Agassi shaved his balding head, breaking with his old "image is everything" style. He competed in the 1995 Australian Open (his first appearance at the event) and won, beating Sampras in a four set final. Agassi and Sampras met in five tournament finals in 1995, all on hardcourt, with Agassi winning three. Agassi won three Masters Series events in 1995 (Cincinnati, Key Biscayne, and the Canadian Open) and seven titles total. He compiled a career-best 26-match winning streak during the summer hardcourt circuit, which ended when he lost the US Open final to Sampras.
Agassi reached the World No. 1 ranking for the first time in April 1995. He held that ranking until November, for a total of 30 weeks. In terms of win/loss record, 1995 was Agassi's best year. He won 73 matches and lost only 9. Agassi was also once again a key player on the United States' Davis Cup winning team - the third and final Davis Cup title of Agassi's career.
1996 was a less successful year for Agassi, as he failed to reach any Grand Slam final. He suffered two early round losses at the hands of compatriots Chris Woodruff and Doug Flach at the French Open and Wimbledon, respectively, and lost to Chang in straight sets in the Australian and US Open semifinals. At the time, Agassi blamed the loss on the windy conditions but later admitted in his biography that he had tanked (lost on purpose) this match as he bore a grudge against Boris Becker whom he would have faced in the final. The high point for Agassi was winning the men's singles gold medal at the Olympic Games in Atlanta, beating Sergi Bruguera of Spain in the final 6–2, 6–3, 6–1. Agassi also successfully defended his singles titles in Cincinnati and Key Biscayne.
1997 was the low point of Agassi's career. His wrist injury resurfaced, and he played only 24 matches during the year. He would later confess that he started using crystal methamphetamine at that time, allegedly on the urging of a friend. He failed an ATP drug test, but wrote a letter claiming the same friend spiked a drink. The ATP dropped the failed drug test as a warning. He stated upon admitting to his drug use that the letter was a lie. He quit the drug soon after. He won no top-level titles and his ranking sank to World No. 141 on November 10, 1997.
1998–2003.
In 1998, Agassi began a rigorous conditioning program and worked his way back up the rankings by playing in Challenger Series tournaments (a circuit for professional players ranked outside the world's top 50). He played some classic matches in this period, most notably against his rival Pete Sampras and popular Australian Patrick Rafter.
In 1998, Agassi won five titles and leapt from World No. 122 at the start of the year to World No. 6 at the end of it, making it the highest jump into the top 10 made by any player during a single calendar year. At Wimbledon that year, he had an early loss in the second round to ATP player Tommy Haas. He won five titles in ten finals and was runner-up at the Masters Series tournament in Key Biscayne, losing to Marcelo Ríos, who became World No. 1 as a result of winning that tournament.
Agassi entered the history books in 1999 when he came back from two sets to love down to beat Andrei Medvedev in a five-set French Open final, thereby becoming, at the time, only the fifth male player (joining Rod Laver, Fred Perry, Roy Emerson and Don Budge-these have since been joined by a sixth, Roger Federer) to have won all four Grand Slam singles titles during his career. This win also made him the first (of only two, the second being Roger Federer) male players in history to have won all four Grand Slam titles on three different surfaces (clay, grass, and hard courts), a tribute to his adaptability, as the other four men had won their Grand Slam titles on clay and grass courts. Agassi also became the first male player to win the Career Golden Slam, consisting of all four Grand Slam tournaments plus an Olympic gold medal.
Agassi followed his 1999 French Open victory by reaching the Wimbledon final, where he lost to Sampras in straight sets. He rebounded from his Wimbledon defeat by winning the US Open, beating Todd Martin in five sets (rallying from a 2 sets to 1 deficit) in the final. Agassi ended 1999 as the World No. 1, ending Sampras's record of six consecutive year-ending top rankings (1993–1998). This was the only time Agassi ended the year at number one.
Agassi began the next year by capturing his second Australian Open title, beating Sampras in a five-set semifinal and Yevgeny Kafelnikov in a four-set final. He was the first male player to have reached four consecutive Grand Slam finals since Rod Laver achieved the Grand Slam in 1969. At the time, Agassi was also only the fourth player since Laver to be the reigning champion of three of four Grand Slam events, missing only the Wimbledon title.
2000 also saw Agassi reach the semifinals at Wimbledon, where he lost in five sets to Rafter in a match considered by many to be one of the best ever played at Wimbledon. At the inaugural Tennis Masters Cup in Lisbon, Agassi reached the final after defeating Marat Safin 6–3, 6–3 in the semifinals to end the Russian's hopes to become the youngest World No. 1 in the history of tennis. Agassi then lost to Gustavo Kuerten in the final, allowing Kuerten to be crowned year-end World No. 1.
Agassi opened 2001 by successfully defending his Australian Open title with a straight-sets final win over Arnaud Clément. Enroute, he beat a cramping Rafter (7–5, 2–6, 6–7, 6–2, 6–3) in front of a sell-out crowd in what turned out to be the Aussie's last Australian Open. At Wimbledon, they met again in the semifinals, where Agassi lost another close match to Rafter, 8–6 in the fifth set. In the quarterfinals at the US Open, Agassi lost a 3 hour, 33 minute epic match with Sampras 6–7(7), 7–6(7), 7–6(2), 7–6(5), with no breaks of serve during the 48-game match. Despite the setback, Agassi finished 2001 ranked World No. 3, becoming the only male tennis player to finish a year ranked in the top 10 in three different decades (1980s - finishing World No. 3 in 1988 and No. 7 in 1989; 1990s - finishing World No. 4 in 1990, No. 10 in 1991, No. 9 in 1992, No. 2 in 1994 and 1995, No. 8 in 1996, No. 6 in 1998 and No. 1 in 1999; 2000s - finishing World No. 6 in 2000, No. 3 in 2001, No. 2 in 2002, No. 4 in 2003, No. 8 in 2004 and No. 7 in 2005). He also was the oldest player (age 31) to finish in the top three since 32-year old Connors finished at World No. 2 in 1984.
2002 opened with disappointment for Agassi, as injury forced him to skip the Australian Open, where he was a two-time defending champion. The last duel between Agassi and Sampras came in the final of the US Open, which Sampras won in four sets and left Sampras with a 20–14 edge in their 34 career meetings. The match proved to be the last of Sampras's career. Agassi's US Open finish, along with his Masters Series victories in Key Biscayne, Rome, and Madrid, helped him finish 2002 as the oldest year-end World No. 2 at 32 years and 8 months.
In 2003, Agassi won the eighth (and final) Grand Slam title of his career at the Australian Open, where he beat Rainer Schüttler in straight sets in the final. In March, he won his sixth career and third consecutive Key Biscayne title, in the process surpassing his wife, Steffi Graf, who was a 5-time winner of the event. The final was his 18th straight win in that tournament, which broke the previous record of 17 set by Sampras from 1993–1995. (Agassi's winning streak continued to 20 after winning his first two matches at the 2004 edition of that tournament before bowing to Agustín Calleri.) With the victory, Agassi became the youngest (19 years old) and oldest (32) winner of the Key Biscayne tournament. On April 28, 2003, he recaptured the World No. 1 ranking after a quarterfinal victory over Xavier Malisse at the Queen's Club Championships to become the oldest top ranked male player since the ATP rankings began at 33 years and 13 days. He held the World No. 1 ranking for two weeks when Lleyton Hewitt took it back on May 12, 2003. Agassi then recaptured the World No. 1 ranking once again on June 16, 2003, which he held for 12 weeks until September 7, 2003. During his career, Agassi held the World No. 1 ranking for a total of 101 weeks. Agassi's ranking slipped when injuries forced him to withdraw from many events. He did manage to reach the US Open semifinals, where he lost to Juan Carlos Ferrero and surrendered his World No. 1 ranking to Ferrero. At the year-ending Tennis Masters Cup, Agassi lost in the final to Federer and finished the year ranked World No. 4. At age 33, he was the oldest player to rank in the top five since Connors, at age 35, was World No. 4 in 1987.
2004–2006.
In 2004, Agassi won the Masters series event in Cincinnati to bring his career total to 59 top-level singles titles and a record 17 ATP Masters Series titles, having already won seven of the nine ATP Masters tournament—all except the tournaments in Monte Carlo and Hamburg. At 34, he became the second-oldest singles champion in Cincinnati tournament history (the tournament began in 1899), surpassed only by Ken Rosewall who won the title in 1970 at age 35. He finished the year ranked World No. 8, the oldest player to finish in the top 10 since the 36-year-old Connors was World No. 7 in 1988. Agassi also became only the sixth male player during the open era to reach 800 career wins with his first round victory over Alex Bogomolov in Countrywide Classic in Los Angeles.
Agassi's 2005 began with a quarterfinal loss to Federer at the Australian Open. Agassi had several other deep runs at tournaments but had to withdraw from several events due to injury. He lost to Jarkko Nieminen in the first round of the French Open. He won his fourth title in Los Angeles and reached the final of the Rogers Cup before falling to World No. 2 Rafael Nadal. Agassi's 2005 was defined by an improbable run to the US Open final. After beating Răzvan Sabău and Ivo Karlović in straight sets and Tomáš Berdych in four sets, Agassi won three consecutive five-set matches to advance to the final. The most notable of these matches was his quarterfinal victory over James Blake, where he rallied from two sets down to win 3–6, 3–6, 6–3, 6–3, 7–6(6). His other five-set victims were Xavier Malisse in the fourth round and Robby Ginepri in the semifinals. In the final, Agassi faced Federer, who was seeking his second consecutive US Open title and his sixth Grand Slam title in two years. Federer defeated Agassi in four sets, although Agassi gave him a scare when Agassi was up a break in the third set after splitting the first two sets.
Before the 2005 Tennis Masters Cup in Shanghai, Agassi rolled his ankle in a racquetball accident and tore several ligaments. He was unable to walk for weeks. He nevertheless committed to the tournament, in which he was seeded third, and played Nikolay Davydenko in his first round robin match. Agassi's movement was noticeably hindered, particularly on his backhand return of serve, and he lost in straight sets. He then withdrew from the tournament.
Agassi finished 2005 ranked World No. 7, his 16th time in the year-end top 10 rankings, which tied Connors for the most times ranked in the top 10 at year's end. In 2005, Agassi left Nike after 17 years and signed an endorsement deal with Adidas. A major reason for Agassi leaving Nike was because Nike refused to donate to Agassi's charities and Adidas was more than happy to do so.
Agassi had a poor start to 2006. He was still recovering from an ankle injury and also suffering from back and leg pain and lack of match play. Agassi withdrew from the Australian Open because of the ankle injury, and his back injury and other pains forced him to withdraw from several other events, eventually skipping the entire clay court season, including the French Open. This caused his ranking to drop out of the top 10 for the last time.
Agassi returned for the grass court season, playing a tune-up and then Wimbledon. He was defeated in the third round by World No. 2 (and eventual runner-up) Rafael Nadal 7–6(5), 6–2, 6–4. Against conventions, Agassi, the losing player, was interviewed on court after the match. At Wimbledon, Agassi announced his plans to retire following the US Open.
Agassi played only two events during the summer hardcourt season, with his best result being a quarterfinal loss at the Countrywide Classic in Los Angeles to Fernando González of Chile 6–4, 3–6, 7–5. As a result, he was unseeded at the US Open.
Agassi had a short but dramatic run in his final US Open. Because of extreme back pain, Agassi was forced to receive anti-inflammatory injections after every match. After a tough four-set win against Andrei Pavel, Agassi faced eighth-seeded Marcos Baghdatis in the second round, who had earlier advanced to the 2006 Australian Open final and Wimbledon semifinals. Agassi won 6–4, 6–4, 3–6, 5–7, 7–5 as the younger Baghdatis succumbed to muscle cramping in the final set. In his last match, Agassi fell to 112th ranked big-serving Benjamin Becker of Germany in four sets. Agassi received an eight minute standing ovation from the crowd after the match and delivered a memorable retirement speech.
Earnings.
Agassi earned more than US$ 30 million in prize-money during his career, third only to Sampras and Federer to date. He also earned more than US $25 million a year through endorsements, during his career and fourth in all sports at the time.
Post retirement.
Since retiring after the 2006 US Open, Agassi has participated in a series of charity tournaments and continues his work with his own charity. On September 5, 2007, Agassi was a surprise guest commentator for the Andy Roddick/Roger Federer US Open quarterfinal. He played an exhibition match at Wimbledon, teamed with his wife, Steffi Graf, to play with Tim Henman and Kim Clijsters. He will play World Team Tennis for the Philadelphia Freedoms in the summer of 2009 and played at the Outback Champions Series event for the first time. He played the Cancer Treatment Centers of America Tennis Championships at Surprise, Arizona where he reached the final before bowing to eventual champion Todd Martin who captured his fourth career Outback Champions Series win. On the way to the finals, Agassi beat Mikael Pernfors in the quarterfinals and Wayne Ferreira in the semifinals. However, he clarified that he will not be playing the tour on a full-time basis as he only played the tournament as a favor to long-time friend Jim Courier.
Playing style.
Early on in his career, Agassi would look to end points quickly, typically by inducing a weak return with a deep, hard shot, and then playing a winner at an extreme angle. His return of serve, baseline game, and keen sense of anticipation were among the best in the game, and helped him win the Wimbledon title in 1992. On the rare occasion that he charged the net, Agassi liked to take the ball in the air and hit a swinging volley for the winner.
Agassi continually put pressure on opponents with a preference to taking the ball early and was famously known for swinging deep angles like a smoking backhand up the line. His strength was dictating play from the back of the court. While growing up his father and Nick Bollettieri trained him in this way. He was never known for a strong serve, net work or volleying. When in control of a point, Agassi would often pass up an opportunity to attempt a winner and hit a slightly more conservative shot, both to minimize his errors and to make his opponent run more. His penchant for running players around point after point has earned him the nickname "The Punisher".
Agassi's serve was never the strength of his game, but it improved steadily over the course of his career, and went from being a liability to being above average. He often used his hard slice serve in the deuce service box to seek to send his opponent off the court, followed by a shot to the opponent's opposite corner. Agassi's service speed when hitting a flat first serve would often range between to. His second serve however was usually only in the mid 80's. He relied on a heavy kick serve for his second serve.
Personal and family life.
Agassi married actress Brooke Shields on April 19, 1997. In February 1998, they filed suit against "The National Enquirer" claiming it printed "false and fabricated" statements about the couple, but the case was dismissed. The couple later filed for divorce, which was granted on April 9, 1999.
At the 1999 French Open, Agassi and Steffi Graf were the surprise champions, since he had not won a Grand Slam title since 1995 and she since 1996. At the winners' ball, they met each other for the second time. Shortly after, they started dating. Graf retired after they both reached the Wimbledon final in July. They were married on October 22, 2001. Their son, Jaden Gil, was born four days later, October 26. Their daughter, Jaz Elle, was born on October 3, 2003. The couple live in the Las Vegas area and own several vacation homes.
Agassi's older sister, Rita, was married to tennis player Pancho Gonzales. In 1995, when Gonzales died in Las Vegas, Agassi paid for the funeral. Long-time trainer Gil Reyes has been called one of Agassi's closest friends; some have described him as being a "father figure". Andre Agassi's other sister, like their mother, Betty, is a breast cancer survivor.
In December 2008, Agassi's childhood friend and former business manager Perry Rogers sued Graf for $50,000 in management fees he claimed that she owed him.
Agassi's autobiography, "Open" (written with assistance from J. R. Moehringer) was published in November 2009. In it, Agassi admitted to using and testing positive for methamphetamine in 1997, and that his then-distinctive long hair was actually a wig; in Agassi's opinion, his defeat in the 1990 French Open final was partly due to issues with the wearing of the wig. He also revealed that he had always hated tennis during his career, because of the constant pressure it exerted on him. He also revealed he thought Pete Sampras was "robotic". The book reached #1 on the New York Times Best Seller list and received favorable reviews.
Politics.
Agassi is a registered Democrat and has donated more than $100,000 to Democratic candidates.
Philanthropy.
Agassi has participated in many charity organizations and founded the Andre Agassi Charitable Association in 1994, which assists Las Vegas' young people. Agassi was awarded the ATP Arthur Ashe Humanitarian award in 1995 for his efforts to help disadvantaged youth. He is regularly cited as the most charitable and socially involved player in professional tennis. It has also been surmised that he may be the most charitable athlete of his generation, which includes Lance Armstrong.
Andre Agassi's charities help in assisting children reach their athletic potential. His Boys & Girls Club sees 2,000 children throughout the year and boasts a world class junior tennis team. It also has a basketball program (the Agassi Stars) and a rigorous system that encourages a mix of academics and athletics.
In 2001, Agassi opened up the Andre Agassi College Preparatory Academy in Las Vegas, a tuition-free charter school for at-risk children in the area. In 2009, the graduating class had 100 percent graduation rate and a 100 percent college acceptance rate. Among other child-related programs that Agassi supports through his Andre Agassi Charitable Foundation is Clark County's only residential facility for abused and neglected children called Child Haven. In 1997, Agassi donated funding to Child Haven for a six-room classroom building now named the Agassi Center for Education. His foundation also provided $720,000 to assist in the building of the Andre Agassi Cottage for Medically Fragile Children. This facility opened in December 2001 and accommodates developmentally delayed or handicapped children and children quarantined for infectious diseases. It houses approximately 20 beds and gives children with special needs the special attention needed to make them feel comfortable in their new surroundings."
In 2007, Agassi, Muhammad Ali, Lance Armstrong, Warrick Dunn, Jeff Gordon, Mia Hamm, Tony Hawk, Andrea Jaeger, Jackie Joyner-Kersee, Mario Lemieux, Alonzo Mourning and Cal Ripken, Jr. founded the charity Athletes for Hope, which helps professional athletes get involved in charitable causes and inspires millions of non-athletes to volunteer and support the community.
Recognition.
"Tennis" magazine named him the 7th greatest male player—and 12th greatest player overall—for the period 1965 through 2005.
Records.
Most ATP World Tour Masters 1000 (formerly ATP Masters Series) titles: 17
Oldest top ranked male player in the ATP Entry Rankings: 33 years 4 months.
---END.OF.DOCUMENT---

Austro-Asiatic languages.
The Austro-Asiatic languages are a large language family of Southeast Asia, and also scattered throughout India and Bangladesh. The name comes from the Latin word for "south" and the Greek name of Asia, hence "South Asia." Among these languages, only Khmer, Vietnamese, and Mon have a long established recorded history, and only Vietnamese and Khmer have official status (in Vietnam and Cambodia, respectively). The rest of the languages are spoken by minority groups. Ethnologue identifies 168 Austro-Asiatic languages. These are traditionally divided into two families, Mon-Khmer and Munda, but two recent classifications have abandoned Mon-Khmer as a valid node, although this is tentative and not generally accepted.
Austro-Asiatic languages have a disjunct distribution across India, Bangladesh and Southeast Asia, separated by regions where other languages are spoken. It is widely believed that the Austro-Asiatic languages are the autochthonous languages of Southeast Asia and the eastern Indian subcontinent, and that the other languages of the region, including the Indo-European, Kradai, Dravidian and Sino-Tibetan languages, are the result of later migrations of people.
The Austro-Asiatic languages are well known for having a "sesqui-syllabic" pattern, with basic nouns and verbs consisting of a reduced minor syllable plus a full syllable. Many of them also have infixes.
Classification.
Linguists traditionally recognize two primary divisions of Austro-Asiatic: the Mon-Khmer languages of Southeast Asia, Northeast India and the Nicobar Islands, and the Munda languages of East and Central India and parts of Bangladesh. However, no evidence for this classification has ever been published, and it is possible that the linguistic classification has been influenced by researchers' subjective perception of a racial dichotomy between the speakers of languages that have traditionally been classified as Mon-Khmer and those that have traditionally been classified as Munda.
Each of the families that is written in boldface type below is accepted as a valid clade. However, the relationships between these families within Austro-Asiatic is debated; in addition to the traditional classification, two recent proposals are given, neither of which accept traditional Mon-Khmer as a valid unit. It should be noted that little of the data used for competing classifications has ever been published, and therefore cannot be evaluated by peer review.
Ilia Peiros (2004).
Peiros is a lexicostatistic classification, based on percentages of shared vocabulary. This means that a language may appear to be more distantly related than it actually is due to language contact, so it is only a starting point for a proper genealogical classification.
Diffloth (1974).
Diffloth's widely cited original classification, now abandoned by Diffloth himself, is used in "Encyclopædia Britannica" and—except for the breakup of Southern Mon-Khmer—in "Ethnologue."
Protolanguage.
This is identical to earlier reconstructions except for, which is better preserved in the Katuic languages which Sidwell specializes in than in other branches of Austro-Asiatic.
---END.OF.DOCUMENT---

Afroasiatic languages.
The Afroasiatic languages constitute a language family with about 375 living languages and more than 350 million speakers spread throughout North Africa, the Horn of Africa, and Southwest Asia, as well as parts of the Sahel, West Africa and East Africa. The most widely spoken Afroasiatic language is Arabic, with 230 million speakers (all the colloquial varieties). In addition to languages now spoken, Afroasiatic includes several ancient languages, such as Ancient Egyptian, Biblical Hebrew, and Akkadian.
The term "Afroasiatic" (often now spelled as Afro-Asiatic) was coined by Maurice Delafosse (1914). It did not come into general use until it was adopted by Joseph Greenberg (1950) to replace the earlier term "Hamito-Semitic", following his demonstration that Hamitic is not a valid language family. The term "Hamito-Semitic" remains in use in the academic traditions of some European countries. Some authors now replace "Afro-Asiatic" with "Afrasian", or, reflecting an opinion that it is more African than Asian, "Afrasan". Individual scholars have called the family "Erythraean" (Tucker 1966) and "Lisramic" (Hodge 1972).
Classification history==.
In the 9th century, the Hebrew grammarian Judah ibn Quraysh of Tiaret in Algeria was the first to link two branches of Afroasiatic together; he perceived a relationship between Berber and Semitic. He knew of Semitic through Arabic, Hebrew, and Aramaic.
In the course of the 19th century, Europeans also began suggesting such relationships. In 1844, Theodor Benfey suggested a language family consisting of Semitic, Berber, and Cushitic (calling the latter "Ethiopic"). In the same year, T.N. Newman suggested a relationship between Semitic and Hausa, but this would long remain a topic of dispute and uncertainty.
Friedrich Müller named the traditional "Hamito-Semitic" family in 1876 in his "Grundriss der Sprachwissenschaft". He defined it as consisting of a Semitic group plus a "Hamitic" group containing Egyptian, Berber, and Cushitic; he excluded the Chadic group. These classifications relied in part on non-linguistic anthropological and racial arguments (see Hamitic hypothesis).
Leo Reinisch (1909) proposed linking Cushitic and Chadic, while urging a more distant affinity to Egyptian and Semitic, thus foreshadowing Greenberg, but his suggestion found little resonance.
Marcel Cohen (1924) rejected the idea of a distinct Hamitic subgroup and included Hausa (a Chadic language) in his comparative Hamito-Semitic vocabulary.
Joseph Greenberg (1950) strongly confirmed Cohen's rejection of "Hamitic", added (and sub-classified) the Chadic branch, and proposed the new name "Afroasiatic" for the family. Nearly all scholars have accepted Greenberg's classification.
In 1969, Harold Fleming proposed that what had previously been known as Western Cushitic is an independent branch of Afroasiatic, suggesting for it the new name Omotic. This proposal and name have met with widespread acceptance.
Several scholars, including Harold Fleming and Robert Hetzron, have since questioned the traditional inclusion of Beja in Cushitic.
Subgrouping.
Little agreement exists on the subgrouping of the five or six branches of Afroasiatic: Semitic, Egyptian, Berber, Chadic, Cushitic, and Omotic (if Omotic is not included in Cushitic). However, Christopher Ehret (1979), Harold Fleming (1981), and Joseph Greenberg (1981) all agree that the Omotic branch split from the rest first.
Position among the world's languages.
Afroasiatic is one of the four language families of Africa identified by Joseph Greenberg in his book "The Languages of Africa" (1963). It is the only one that extends outside of Africa, via the Semitic branch.
Origins and common features.
All Afroasiatic subfamilies show evidence of a causative affix "s", but a similar suffix also appears in other groups, such as the Niger-Congo languages.
Semitic, Berber, Cushitic (including Beja), and Chadic support possessive suffixes.
Tonal languages appear in the Omotic, Chadic, and Cushitic branches of Afroasiatic, according to Ehret (1996). The Semitic, Berber, and Egyptian branches do not use tones phonemically.
---END.OF.DOCUMENT---

Andorra.
Andorra, officially the Principality of Andorra (), also called the Principality of the Valleys of Andorra, is a small country in southwestern Europe, located in the eastern Pyrenees mountains and bordered by Spain and France. It is the sixth smallest nation in Europe having an area of and an estimated population of 83,888 in 2009. Its capital, Andorra la Vella, is the highest capital city in Europe, being at an elevation of 1023 metres. The official language is Catalan, although Spanish, French, and Portuguese are also commonly spoken.
The Principality was formed in 1278. The role of monarch is shared between the President of the French Republic and the Bishop of Urgell, Catalonia, Spain. It is a prosperous country mainly because of its tourism industry, which services an estimated 10.2 million visitors annually, and also because of its status as a tax haven. It is not a member of the European Union, but the euro is the "de facto" currency. The people of Andorra have the 2nd highest human life expectancy in the world — 82 years at birth.
History.
Tradition holds that Charles the Great (Charlemagne) granted a charter to the Andorran people in return for fighting against the Moors. Overlordship of the territory was by the Count of Urgell and eventually by the bishop of the Diocese of Urgell. In 988, Borrell II, Count of Urgell, gave the Andorran valleys to the Diocese of Urgell in exchange for land in Cerdanya. Since then the Bishop of Urgell, based in Seu d'Urgell, has owned Andorra.
Before 1095, Andorra did not have any type of military protection and the Bishop of Urgell, who knew that the Count of Urgell wanted to reclaim the Andorran valleys, asked for help and protection from the Lord of Caboet. In 1095, the Lord of Caboet and the Bishop of Urgell signed under oath a declaration of their co-sovereignty over Andorra. Arnalda, daughter of Arnau of Caboet, married the Viscount of Castellbò and both became Viscounts of Castellbò and Cerdanya. Years later their daughter, Ermessenda, married Roger Bernat II, the French Count of Foix. They became Roger Bernat II and Ermessenda I, Counts of Foix, Viscounts of Castellbò and Cerdanya, and also co-sovereigns of Andorra (shared with the Bishop of Urgell).
In the eleventh century, a dispute arose between the Bishop of Urgell and the Count of Foix. The conflict was resolved in 1278 with the mediation of Aragon by the signing of the first paréage which provided that Andorra's sovereignty be shared between the count of Foix (whose title would ultimately transfer to the French head of state) and the Bishop of Urgell, in Catalonia. This gave the principality its territory and political form.
Over the years, the French co-title to Andorra passed to the kings of Navarre. After Henry of Navarre became King Henry IV of France, he issued an edict in 1607 that established the head of the French state and the Bishop of Urgell as co-princes of Andorra. In 1812–13, the First French Empire annexed Catalonia and divided it in four départements, with Andorra being made part of the district of Puigcerdà (département of Sègre).
20th century.
Andorra declared war on Imperial Germany during World War I, but did not actually take part in the fighting. It remained in an official state of belligerency until 1957 as it was not included in the Treaty of Versailles.
In 1933, France occupied Andorra as a result of social unrest before elections. On July 12, 1934, adventurer Boris Skossyreff issued a proclamation in Urgell, declaring himself Boris I, sovereign prince of Andorra, simultaneously declaring war on the Bishop of Urgell. He was arrested by Spanish authorities on July 20 and ultimately expelled from Spain. From 1936 to 1940, a French detachment was garrisoned in Andorra to prevent influences of the Spanish Civil War and Franco's Spain. Francoist troops reached the Andorran border in the later stages of the war. During World War II, Andorra remained neutral and was an important smuggling route between Vichy France and Spain.
Given its relative isolation, Andorra has existed outside the mainstream of European history, with few ties to countries other than France and Spain. In recent times, however, its thriving tourist industry along with developments in transport and communications have removed the country from its isolation. Its political system was thoroughly modernised in 1993, the year in which it became a member of the United Nations and the Council of Europe.
Politics.
Andorra is a parliamentary co-principality with the President of France and the Bishop of Urgell (Catalonia, Spain), as co-princes, in a duumvirate. The politics of Andorra take place in a framework of a parliamentary representative democracy, whereby the Prime Minister of Andorra is the head of government, and of a pluriform multi-party system.
The current Prime Minister is Jaume Bartumeu of the Social Democratic Party (PS). Executive power is exercised by the government. Legislative power is vested in both the government and parliament.
The Parliament of Andorra is known as the General Council. The General Council consists of between 28 and 42 Councilors, as the members of the legislative branch are called. The Councilors serve for four-year terms and elections are held between the thirtieth and fortieth days following the dissolution of the previous Council. The Councilors can be elected on two equal constituencies.
Half are elected in equal number from each of the seven administrative parishes and the other half of the Councilors are elected from a single national constituency. Fifteen days after the election, the Councilors hold their inauguration. During this session, the Syndic General, who is the head of the General Council, and the Subsyndic General, his assistant, are elected. Eight days later, the Council convenes once more. During this session the Head of Government, the Prime Minister of Andorra, is chosen from among the Councilors.
Candidates for the prime-ministerial nomination can be proposed by a minimum of one-fifth of the Councilors. The Council then elects the candidate with the absolute majority of votes to be Head of Government. The Syndic General then notifies the Co-princes who in turn appoint the elected candidate as the Prime Minister of Andorra. The General Council is also responsible for proposing and passing laws. Bills may be presented to the Council as Private Members' Bills by three of the Local Parish Councils jointly or by at least one tenth of the citizens of Andorra.
The Council also approves the annual budget of the principality. The government must submit the proposed budget for parliamentary approval at least two months before the previous budget expires. If the budget is not approved by the first day of the next year, the previous budget is extended until a new one is approved. Once any bill is approved, the Syndic General is responsible for presenting it to the Co-princes so that they may sign and enact it.
If the Head of Government is not satisfied with the Council, he may request that the Co-princes dissolve the Council and order new elections. In turn, the Councilors have the power to remove the Head of Government from office. After a motion of censure is approved by at least one-fifth of the Councilors, the Council will vote and if it receives the absolute majority of votes, the Prime Minister is removed.
Law and criminal justice.
The judiciary is composed of the Magistrates Court, the Criminal Law Court, the High Court of Andorra, and the Constitutional Court. The High Court of Justice is composed of five judges: one appointed by the Head of Government, one each by the Coprinces, one by the Syndic General, and one by the Judges and Magistrates. It is presided over by the member appointed by the Syndic General and the judges hold office for six-year terms.
The Magistrates and Judges are appointed by the High Court, and so is the President of the Criminal Law Court. The High Court also appoints members of the Office of the Attorney General. The Constitutional Court is responsible for interpreting the Constitution and reviewing all appeals of unconstitutionality against laws and treaties. It is composed of four judges, one appointed by each of the Coprinces and two by the General Council. They serve eight-year terms. The Court is presided over by one of the Judges on a two-year rotation so that each judge at one point will be the leader of the Court.
Foreign relations and defence.
Responsibility for defending Andorra rests with Spain and France.
Geography.
File:Andorramap.png|thumb|250px|Map of Andorra with its seven parishes labeled ()
rect 34 350 121 404 Andorra la Vella
rect 443 148 519 172 Canillo
rect 437 363 520 389 Encamp
rect 352 443 464 498 Escaldes-Engordany
rect 36 223 155 247 La Massana
rect 284 78 354 103 Ordino
rect 208 567 304 618 Sant Julià de Lòria
rect 651 54 745 83 France
rect 484 583 560 619 Spain
Physical geography.
Due to its location in the eastern Pyrenees mountain range, Andorra consists predominantly of rugged mountains, the highest being the Coma Pedrosa at, and the average elevation of Andorra is. These are dissected by three narrow valleys in a Y shape that combine into one as the main stream, the Gran Valira river, leaves the country for Spain (at Andorra's lowest point of). Andorra's surface area is.
Phytogeographically, Andorra belongs to the Atlantic European province of the Circumboreal Region within the Boreal Kingdom. According to the WWF, the territory of Andorra belongs to the ecoregion of Pyrenees conifer and mixed forests.
Climate.
Andorra has a temperate climate similar to that of its neighbours, but its higher elevation means there is, on average, more snow in winter, lower humidity, and it is slightly cooler in summer. There are, on average, 300 days per year of sunshine.
Economy.
Tourism, the mainstay of Andorra's tiny, well-to-do economy, accounts for roughly 80% of GDP. An estimated 10.2 million tourists visit annually, attracted by Andorra's duty-free status and by its summer and winter resorts. Andorra's comparative advantage has recently eroded as the economies of adjoining France and Spain have been opened up, providing broader availability of goods and lower tariffs.
The banking sector, with its tax haven status, also contributes substantially to the economy. Agricultural production is limited—only 2% of the land is arable—and most food has to be imported. Some tobacco is grown locally. The principal livestock activity is domestic sheep raising. Manufacturing output consists mainly of cigarettes, cigars, and furniture. Andorra's natural resources include hydroelectric power, mineral water, timber, iron ore, and lead.
Andorra is not a member of the European Union, but enjoys a special relationship with it, such as being treated as an EU member for trade in manufactured goods (no tariffs) and as a non-EU member for agricultural products. Andorra lacks a currency of its own and uses that of its two surrounding nations. Andorra used the French franc and the Spanish peseta until 1999 when both currencies were replaced by the EU's single currency, the euro. Coins and notes of both the franc and the peseta, however, remained legal tender in Andorra until 2002. Andorra is negotiating to issue its own euro coins.
Population.
The population of Andorra is estimated to be 83,888 (July 2009). The population has grown from 5,000 in 1900, and reached a peak of 84,484 (estimated) in July 2008.
Andorrans are a minority in their own country (31,363); other nationalities including Spaniards (27,300), Portuguese (13,794), French (5,213), Britons (1,085) and Italians altogether make up a total of 67.7% of Andorra's population.
Languages.
The historic and official language is Catalan, a Romance language. Because of immigration, historical links, and close geographic proximity, other languages such as Spanish, French and Portuguese are also commonly spoken. Most Andorrans also speak Spanish (Castilian), French or both. Andorra is one of only four European countries (together with France, Monaco, and Turkey) that have never signed the Council of Europe Framework Convention on National Minorities.
Religion.
The population of Andorra is predominantly (90%) Roman Catholic. Their patron saint is Our Lady of Meritxell. Though it is not an official state religion, the constitution acknowledges a special relationship with the Roman Catholic Church, offering some special privileges to that group. The Muslim community is primarily made up of North African immigrants. Other Christian denominations include the Anglican Church, Jehovah’s Witnesses, the Reunification Church, the New Apostolic Church, and The Church of Jesus Christ of Latter-day Saints. There is a small community of Hindus.
Schools.
Children between the ages of 6 and 16 are required by law to have full-time education. Education up to secondary level is provided free of charge by the government.
There are three systems of schools – Andorran, French and Spanish – which use Catalan, French and Spanish, respectively, as the main language of instruction. Parents may choose which system their children attend. All schools are built and maintained by Andorran authorities, but teachers in the French and Spanish schools are paid for the most part by France and Spain. About 50% of Andorran children attend the French primary schools, and the rest attend Spanish or Andorran schools.
University of Andorra.
The University of Andorra (UdA) is the state public university and is the only university in Andorra. It was established in 1997. The University provides first-level degrees in nursing, computer science, business administration, and educational sciences, in addition to higher professional education courses. The only two graduate schools in Andorra are the Nursing School and the School of Computer Science, the latter having a PhD programme.
Virtual Studies Centre.
The geographical complexity of the country as well as the small number of students prevents the University of Andorra from developing a full academic programme, and it serves principally as a centre for virtual studies, connected to Spanish and French universities. The Virtual Studies Centre ("Centre d’Estudis Virtuals") at the University runs in the region of twenty degrees at both undergraduate and postgraduate levels in fields including tourism, law, Catalan philology, humanities, psychology, political sciences, audiovisual communication, telecommunications engineering, and East Asia studies. The Centre also runs various postgraduate programmes and continuing-education courses for professionals.
Healthcare.
Healthcare in Andorra is provided to all employed persons and their families by the government-run social security system, CASS (Caixa Andorrana de Seguretat Social), which is funded by employer and employee contributions in respect of salaries. The cost of healthcare is covered by CASS at rates of 75% for out-patient expenses such as medicines and hospital visits, 90% for hospitalisation, and 100% for work-related accidents. The remainder of the costs may be covered by private health insurance. Other residents and tourists require full private health insurance.
The main hospital, Meritxell, is in Escaldes-Engordany. Its services include 24-hour accident and emergency, anatomy, angiology, pathology, anesthesiology, clinical cardiology, clinical biochemistry, clinical neurology, dermatology, endocrinology, gastroenterology, genetics, geriatrics, hematology, immunology, intensive care, internal medicine, medical oncology, microbiology, nephrology and dialysis, neurophysiology, obstetrics and gynecology, oncology, ophthalmology, otorhinolaringology, orthopedics, pediatrics, physiotherapy, neonatology, plastic surgery, general surgery, oral and maxillofacial surgery, neurosurgery, pediatric surgery, thoracic surgery, trauma surgery, cardiovascular surgery, parasitology, psychiatry, radiodiagnostics, radiotherapy, urology, and venerealogy.
There are also 12 primary health care centres in various locations around the Principality.
Transport.
Andorra has a road network of, of which is unpaved. The two main roads out of Andorra la Vella are the CG-1 to the Spanish border, and the CG-2 to the French border via the Envalira Tunnel near Pas de la Casa. In winter, the main roads in Andorra are usually quickly cleared of snow and remain accessible, but the main road out of Andorra on the French side (RN-20/22) is less frequently cleared and is sometimes closed by avalanches. Other main roads out of Andorra la Vella are the CG-3 and CG-4 to Arcalis and Pal, respectively.
Bus services cover all metropolitan areas and many rural communities, with services on most major routes running half-hourly or more frequently during peak travel times. There are frequent long-distance bus services from Andorra to Barcelona and Barcelona Airport, and also to Toulouse and Toulouse Airport, in each case taking approximately 3 hours. Bus routes also serve Girona Airport and Portugal via Lleida. Bus services are mostly run by private companies, but some local ones are operated by the Government. The private bus companies are Autocars Nadal, Camino Bus, Cooperativa Interurbana Andorrana, Eurolines, Hispano Andorrana, and Novatel.
There are no railways, ports, or airports for fixed-wing aircraft in Andorra. There are, however, heliports in La Massana, Arinsal and Escaldes-Engordany with commercial helicopter services. Nearby airports are located in Barcelona, Toulouse, Perpignan, Reus, and Girona. The closest public airport is Perpignan - Rivesaltes Airport, which is away and has short-haul services to several destinations in the United Kingdom and France. La Seu d'Urgell Airport, a small airfield south of Andorra currently used only by private aeroplanes, is being studied by the Catalan government as a possible future airport for public aviation services.
The nearest railway station is L'Hospitalet-près-l'Andorre east of Andorra which is on the -gauge line from Latour-de-Carol, () southeast of Andorra, to Toulouse and on to Paris by the French high-speed trains. This line is operated by the SNCF. Latour-de-Carol has a scenic metre-gauge trainline to Villefranche-de-Conflent, as well as the SNCF's -gauge line connecting to Perpignan, and the RENFE's -gauge line to Barcelona.
Media and telecommunications.
In Andorra, mobile and fixed telephony and internet services are operated exclusively by the Andorran national telecommunications company, SOM, also known as "Servei de Telecomunicacions d'Andorra" (STA). The same company also manages the technical infrastructure for national broadcasting of digital television and radio.
By the end of 2010, it is planned that every home in the country will have Fibre-Optic to the Home for internet access at a minimum speed of 100 Mbps.
There is only one Andorran television station, "Ràdio i Televisió d'Andorra" (RTVA). "Radio Nacional d’Andorra" operates two radio stations, "Radio Andorra" and "Andorra Música". There are three national newspapers, "Diari D'Andorra", "El Periòdic", and "Bon Dia" as well as several local newspapers.
Culture.
The official and historic language is Catalan. Thus, its culture is Catalan with some own specificity.
Andorra is home to folk dances like the contrapàs and marratxa, which survive in Sant Julià de Lòria especially. Andorran folk music has similarities to the music of its neighbours, but is especially Catalan in character, especially in the presence of dances such as the sardana. Other Andorran folk dances include contrapàs in Andorra la Vella and Saint Anne's dance in Escaldes-Engordany. Andorra's national holiday is Our Lady of Meritxell Day, September 8.
---END.OF.DOCUMENT---

Arithmetic mean.
In mathematics and statistics, the arithmetic mean (or simply the mean) of a list of numbers is the sum of all of the list divided by the number of items in the list. If the list is a statistical population, then the mean of that population is called a population mean. If the list is a statistical sample, we call the resulting statistic a sample mean.
The mean is the most commonly-used type of average and is often referred to simply as the "average". The term "mean" or "arithmetic mean" is preferred in mathematics and statistics to distinguish it from other averages such as the median and the mode.
Introduction.
If we denote a set of data by "X" = ("x"1, "x"2..., "x'n"), then the sample mean is typically denoted with a horizontal bar over the variable (formula_1, enunciated "x" bar").
The Greek letter μ is used to denote the arithmetic mean of an entire population. Or, for a random variable that has a defined mean, μ is the "probabilistic mean" or expected value of the random number. If the set "X" is a collection of random numbers with probabilistic mean of μ, then for any individual sample, "x'i", from that collection, μ = E is the expected value of that sample.
In practice, the difference between μ and formula_1 is that μ is typically unobservable because one observes only a sample rather than the whole population, and if the sample is drawn randomly, then one may treat formula_1, but not μ, as a random variable, attributing a probability distribution to it (the sampling distribution of the mean).
It is a U-statistic for the function formula_5 meaning that it is obtained by averaging a 1-sample statistic over the population.
If "X" is a random variable, then the expected value of "X" can be seen as the long-term arithmetic mean that occurs on repeated measurements of "X". This is the content of the law of large numbers. As a result, the sample mean is used to estimate unknown expected values.
Simple algebra will prove that a mean of "n" + 1 numbers is larger than the mean of "n" numbers if and only if the new number is larger than the old mean, smaller if and only if it is smaller, and remains stable if and only if it is equal to the old mean. The larger "n" is, the smaller is the magnitude of the change in the mean relative to the distance between the old mean and the new number.
Note that several other "means" have been defined, including the generalized mean, the generalized f-mean, the harmonic mean, the arithmetic-geometric mean, and various weighted means.
Not robust.
While the mean is often used to report central tendency, it is not a robust statistic, meaning that it is greatly influenced by outliers. Notably, for skewed distributions, the arithmetic mean may not accord with one's notion of "middle", and robust statistics such as the median may be a better description of central tendency.
A classic example is average income. The arithmetic mean may be misinterpreted as the median to imply that most people's incomes are higher than is in fact the case. When presented with an "average" one may be led to believe that "most" people's incomes are near this number. This "average" (arithmetic mean) income "is" higher than most people's incomes, because high income outliers skew the result higher (in contrast, the median income "resists" such skew). However, this "average" says nothing about the number of people near the median income (nor does it say anything about the modal income that most people are near). Nevertheless, because one might carelessly relate "average" and "most people" one might incorrectly assume that most people's incomes would be higher (nearer this inflated "average") than they are. For instance, reporting the "average" net worth in Medina, Washington as the arithmetic mean of all annual net worths would yield a surprisingly high number because of Bill Gates. Consider the scores (1, 2, 2, 2, 3, 9). The arithmetic mean is 3.17, but five out of six scores are below this.
Compounding.
If numbers "multiply" instead of "add," one should average using the geometric mean, not the arithmetic mean. This most often happens when computing the rate of return, as in finance.
For example, if a stock fell 10 % in the first year, and rose 30 % in the second year, then it would be incorrect to report its "average" increase per year over this two year period as the arithmetic mean (−10 % + 30 %)/2 = 10 %; the correct average in this case is the compound annual growth rate, which yields an annualized increase per year of only 8.2 %.
The reason for this is that each of those percents have different starting points: the 30% is 30% "of a smaller number". If the stock starts at $30 and falls 10 %, it is now at $27. If the stock then rises 30 %, it is now $35.1. The arithmetic mean of those rises is 10 %, but since the stock rose by $5.1 in 2 years, an average of 8.2 % would result in the final $35.1 figure [$30(1-10 %)(1+30 %) = $30(1+8.2 %)(1+8.2 %) = $35.1]. If one used the arithmetic mean 10 % in the same way, one would not get the actual increase [$30(1+10 %)(1+10 %) = $36.3].
Stated generally, compounding yields 90% * 130% = 117% overall growth, and annualizing yields formula_8, so 8.2% per year.
Directions.
Particular care must be taken when using cyclic data such as phases or angles. Naïvely taking the arithmetic mean of 1° and 359° yields a result of 180°.
In general application such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (viz, define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).
---END.OF.DOCUMENT---

American Football Conference.
The American Football Conference (AFC) is one of the two conferences of the National Football League (NFL). This conference and its counterpart, the National Football Conference (NFC), currently contain 16 teams each, making up the 32 teams of the NFL.
Current teams.
Since 2002, the AFC has comprised 16 teams, organized into four divisions: North, South, East, West.
Season structure.
Each AFC team plays the other teams in their division twice (home and away) during the regular season, in addition to 10 other games assigned to their schedule by the NFL the previous May. Two of these games are assigned on the basis of the team's final division standing in the previous season. The remaining 8 games are split between the roster of two other NFL divisions. This assignment shifts each year. For instance, in the 2007 regular season, each team in the AFC West played one game against each team in both the AFC South and the NFC North. In this way division competition consists of common opponents, with the exception of the 2 games assigned on the strength of each team's prior division standing. (i.e. the division winner will face the other two division winners in the AFC divisions that they are not scheduled to play) The NFC operates according to the same system.
At the end of each football season, there are playoff games involving the top six teams in the AFC (the four division champions by place standing and the top two remaining non-division-champion teams ("wild cards") by record). The last two teams remaining play in the AFC Championship game with the winner receiving the Lamar Hunt Trophy. The AFC champion plays the NFC champion in the Super Bowl. After Super Bowl XLIII the AFC has won 19 Super Bowls to the 21 won by the NFC. Since losing 13 consecutive Super Bowls in the 1980s and 1990s (XIX–XXXI), the AFC has won nine of the last twelve. The losing coach of the AFC Championship game is the coach of the Pro Bowl the week after the Super Bowl.
History.
The AFC was created after the NFL merged with the American Football League (AFL) in 1970. All of the 10 former AFL teams along with the NFL's Cleveland Browns, Pittsburgh Steelers, and the then-Baltimore Colts joined the AFC.
Since the merger, five expansion teams have joined the AFC and two have left, thus making the current total 16. When the Seattle Seahawks and the Tampa Bay Buccaneers joined the league in 1976, they were temporarily placed in the NFC and AFC respectively. This arrangement lasted for one season only before the two teams switched conferences. The Seahawks eventually returned to the NFC as a result of the 2002 realignment. The expansion Jacksonville Jaguars joined the AFC in 1995.
Due to the relocation controversy of the Cleveland Browns, a new AFC franchise called the Baltimore Ravens was officially established in 1996 while the Browns were reactivated in 1999.
The Houston Texans were then added to the league in 2002, joining the AFC.
Logo.
The merged league created a new logo for the AFC that took elements of the old AFL logo, specifically the "A" and the six stars surrounding it. The AFC logo basically remained unchanged from 1970 to 2009. The 2010 NFL season introduced an updated AFC logo, with the most notable revision being the addition of a fourth star (representing the four divisions of the AFC), and moving the stars inside the letter, similar to the NFC logo.
---END.OF.DOCUMENT---

Animal Farm.
"Animal Farm" is a dystopian allegorical novella by George Orwell. Published in England on 17 August 1945, the book reflects events leading up to and during the Stalin era before World War II. Orwell, a democratic socialist and a member of the Independent Labour Party for many years, was a critic of Joseph Stalin and was suspicious of Moscow-directed Stalinism after his experiences with the NKVD during the Spanish Civil War. In a letter to Yvonne Davet, Orwell described "Animal Farm" as his novel "contre Stalin".
The original title was "Animal Farm: A Fairy Story", but "A Fairy Story" was dropped by the US publishers for its 1946 publication. Of all the translations during Orwell's lifetime, only Telugu kept the original title. Other variations in the title include: "A Satire" and "A Contemporary Satire". Orwell suggested for the French translation the title "Union des républiques socialistes animales", recalling the French name of the Soviet Union, "Union des républiques socialistes soviétiques", and which abbreviates URSA, which means "bear", a symbol of Russia, in Latin.
"Time" Magazine chose the book as one of the 100 best English-language novels (1923 to 2005); it also places at number 31 on the Modern Library List of Best 20th-Century Novels. It won a Retrospective Hugo Award in 1996 and is also included in the Great Books of the Western World.
Overview.
The novel addresses not only the corruption of the revolution by its leaders but also how wickedness, indifference, ignorance, greed and myopia destroy any possibility of a Utopia. While this novel portrays corrupt leadership as the flaw in revolution (and not the act of revolution itself), it also shows how potential ignorance and indifference to problems within a revolution could allow horrors to happen if smooth transition to a people's government isn't satisfied.
Plot summary.
Old Major, the old boar on the Manor Farm, calls the animals on the farm for a meeting, where he compares the humans to parasites and teaches the animals a revolutionary song, "Beasts of England."
When Major dies three days later, two young pigs, Snowball and Napoleon, assume command and turn his dream into a philosophy. The animals revolt and drive the drunken and irresponsible Mr. Jones from the farm, renaming it "Animal Farm."
The Seven Commandments of Animalism are written on the wall of a barn. The most important is the seventh, "All animals are equal." All the animals work, but the workhorse, Boxer, does more than others and adopts the maxim — "I will work harder."
Snowball attempts to teach the animals reading and writing; food is plentiful; and the farm runs smoothly. The pigs elevate themselves to positions of leadership and set aside special food items ostensibly for their personal health. Napoleon takes the pups from the farm dogs and trains them privately. When Mr. Jones tries retaking the farm, the animals defeat him at what they call the "Battle of the Cowshed." Napoleon and Snowball struggle for leadership. When Snowball announces his idea for a windmill, Napoleon opposes it. Snowball makes a speech in favour of the windmill, whereupon Napoleon has his dogs chase Snowball away. In Snowball's absence, Napoleon declares himself leader and makes changes. Meetings will no longer be held and instead a committee of pigs will run the farm.
Using a young pig named Squealer as a mouthpiece, Napoleon announces that Snowball stole the idea for the windmill from him. The animals work harder with the promise of easier lives with the windmill. After a violent storm, the animals find the windmill annihilated. Napoleon and Squealer convince the animals that Snowball destroyed the windmill, although the scorn of the neighbouring farmers suggests the windmill's walls were too thin. Once Snowball becomes a scapegoat, Napoleon begins purging the farm, killing animals he accuses of consorting with Snowball. Meanwhile, Boxer takes up a second maxim: "Napoleon is always right."
Napoleon abuses his powers, making life harder for the animals; the pigs impose more control while reserving privileges for themselves. The pigs rewrite history, villainizing Snowball and glorifying Napoleon. Squealer justifies every statement Napoleon makes, even the pigs' alteration of the Seven Commandments of Animalism. "No animal shall drink alcohol" is changed to "No animal shall drink alcohol "to excess" when the pigs discover the farmer's whisky. "Beasts of England" is banned as inappropriate, as according to Napoleon the dream of Animal Farm has been realized. It is replaced by an anthem glorifying Napoleon, who appears to be adopting the lifestyle of a man. The animals, though cold, starving, and overworked, remain convinced through psychological conditioning that they are better off than they were when ruled by Mr. Jones. Squealer abuses the animals' poor memories and invents numbers to show their improvement.
Mr. Frederick, one of the neighbouring farmers, swindles Napoleon by buying old wood with forged money, and then attacks the farm, using blasting powder to blow up the restored windmill. Though the animals win the battle, they do so at great cost, as many, including Boxer, are wounded. Boxer continues working harder and harder, until he collapses while working on the windmill. Napoleon sends for a van to take Boxer to the veterinarian, explaining that better care can be given there. Benjamin the donkey, who "could read as well as any pig", notices that the van belongs to "Alfred Simmonds, Horse Slaughterer and Glue Boiler", and attempts to mount a rescue; but the animals' attempts are futile. Squealer reports that the van was purchased by the hospital and the writing from the previous owner had not been repainted. He recounts a tale of Boxer's death in the hands of the best medical care. In reality, the pigs sent Boxer to his death in exchange for money to buy more whisky.
Years pass, and the pigs learn to walk upright, carry whips, and wear clothes. The Seven Commandments are reduced to a single phrase: "All animals are equal, but some animals are more equal than others." Napoleon holds a dinner party for the pigs and the humans of the area, who congratulate Napoleon on having the hardest-working animals in the country on the least feed. Napoleon announces an alliance with the humans, against the labouring classes of both "worlds". He abolishes practices and traditions related to the Revolution, and reverts the name of the farm to "Manor Farm".
The animals, overhearing the conversation, notice that the faces of the pigs have begun changing. During a poker match, an argument breaks out between Napoleon and Mr. Pilkington when they both play the Ace of Spades, and the animals realize that the faces of the pigs look like the faces of humans and no one can tell the difference between them.
Animalism.
Animalism is an allegorical mirror of the Soviet Union, particularly between the 1910s and the 1940s, as well as the evolution of the view of the Russian revolutionaries and government of how to practice it. It is invented by the highly respected pig Old Major. The pigs Snowball, Napoleon, and Squealer adapt Old Major's ideas into an actual philosophy, which they formally name Animalism. Soon after, Napoleon and Squealer indulge in the vices of humans (drinking alcohol, sleeping in beds, trading). Squealer is employed to alter the Seven Commandments to account for his humanization, which represents the Soviet government's tweaking of communist theory to make it more a reformation of capitalism than a replacement.
Later, Napoleon and his pigs are corrupted by the absolute power they hold over the farm. To maintain their popularity with the other animals, Squealer secretly paints additions to some commandments to benefit the pigs while keeping them free of accusations of breaking the laws (such as "No animal shall drink alcohol" having "to excess" appended to it and "No animal shall sleep in a bed" with "with sheets" added to it). Eventually the laws are replaced with
"All animals are equal, "but some animals are more equal than others", and "Four legs good, two legs "better!" as the pigs become more human.
Characters.
The events and characters in Animal Farm satirise Communism ("Animalism"), authoritarian government and human gullibility generally; Snowball is seen as Leon Trotsky and the head pig, Napoleon, is Stalin.
Equines.
There are four main equine characters: Boxer, Clover, and Mollie, who are horses, and Benjamin, who is a donkey. Boxer is a loyal, kind, dedicated, and respectful worker. He is physically the strongest animal on the farm, but naive and slow, which leaves him constantly stating "I will work harder" and "Napoleon is always right" despite the corruption. Clover is Boxer's companion, who constantly cares for him, and she also acts as the matriarch for the other horses, and other animals in general (such as the ducklings she shelters with her fore-legs and hooves during Old Major's speech). Mollie is a self-centred, self-indulgent and vain young white mare who likes wearing ribbons in her mane, eating sugar cubes, and being pampered and groomed by humans. She quickly leaves for another farm and is only once mentioned again. Benjamin is one of the longest-lived animals, has the worst temper and one of the few who can read. Benjamin is a very dedicated friend to Boxer, and does nothing to warn the other animals of the pigs' corruption, which he secretly realizes is steadily unfolding. When asked if he was happier post-Revolution than before the Revolution, Benjamin remarks, "Donkeys live a long time. None of you has ever seen a dead donkey." He is cynical and pessimistic, his most often made statement being "Life will go on as it has always gone on — that is, badly". But he is also one of the wisest animals on the farm, and is able to "read as well as any pig".
Origin.
George Orwell wrote the manuscript in 1943 and 1944 following his experiences during the Spanish Civil War, which he described in his 1938 "Homage to Catalonia".
In the preface of a 1947 Ukrainian edition of Animal Farm he explained how escaping the communist purges in Spain taught him "how easily totalitarian propaganda can control the opinion of enlightened people in democratic countries." This motivated Orwell to expose and strongly condemn what he saw as the Stalinist corruption of the original socialist ideals.
Orwell encountered great difficulty getting the manuscript published. Four publishers refused; one had initially accepted the work but declined after consulting with the Ministry of Information. Eventually Secker and Warburg published the first edition in 1945.
Significance.
In the Eastern Bloc both "Animal Farm" and later, also "Nineteen Eighty-Four" were on the list of forbidden books up until "die Wende" in 1989, and were only available via clandestine Samizdat networks.
The novel's "Battle of the Windmill" is referred to by Sant Singh Bal as one "of the important episodes which constitute the essence of the plot of the novel." Harold Bloom writes that the "Battle of the Windmill rings a special bell: the repulse of the Duke of Brunswick in 1792, following the Prussian bombardment that made the windmill of Valmy famous." By contrast, Peter Edgerly Firchow and Peter Hobley Davison consider that in real life, with events in "Animal Farm" mirroring those in the Soviet Union, this fictional battle represents the Great Patriotic War (World War II), especially the Battle of Stalingrad and the Battle of Moscow. Prestwick House's "Activity Pack" for "Animal Farm" also identifies the Battle of the Windmill as an allegory for World War II, while noting that the "catalyst for the Battle of the Windmill, though, is less clear." During the battle, Fredrick drills a hole and places explosives inside, and it is followed by "All the animals, except Napoleon" took cover; Orwell had the publisher alter this from "All the animals, including Napoleon" in recognition of Joseph Stalin's decision to remain in Moscow during the German advance.
The "Battle of the Cowshed" represents the allied invasion of the Soviet Russia in 1918, and the defeat of the White Russians in the Russian Civil War.
Efforts to find a publisher.
During World War II it became apparent to Orwell that anti-Soviet literature was not something which most major publishing houses would touch — including his regular publisher Gollancz. He also submitted the manuscript to Faber and Faber, where the poet T. S. Eliot (who was a director of the firm) also rejected it; Eliot wrote back to Orwell praising its "good writing" and "fundamental integrity" but declaring that they would only accept it for publication if they had some sympathy for the viewpoint "which I take to be generally Trotskyite". Eliot said he found the view "not convincing", and contended that the pigs were made out to be the best to run the farm; he posited that someone might argue "what was needed.. was not more communism but more public-spirited pigs".
Although it was written in 1943, "Animal Farm" was not published until 1945 due to paper rationing and fear of damaging the Anglo-Soviet alliance.
"The Freedom of the Press".
Orwell originally wrote a preface which complains about self-imposed British self-censorship and how the British people were suppressing criticism of the USSR, their World War II ally. "The sinister fact about literary censorship in England is that it is largely voluntary. ... Things are kept right out of the British press, not because the Government intervenes but because of a general tacit agreement that 'it wouldn't do' to mention that particular fact." The preface itself was censored and as of June 2009 has not been published with most editions of the book. His wife Eileen Blair had worked during the war at the Ministry of Information censoring newspapers.
Secker and Warburg published the first edition of Animal Farm in 1945 without any introduction. However, the publisher had provided space for a preface in the author's proof composited from the manuscript. For reasons unknown, no preface was supplied and all the page numbers needed to be redone at the last minute.
Years later, in 1972, Ian Angus found the original typescript titled "The Freedom of the Press", and Bernard Crick published it, together with his own introduction in The Times Literary Supplement on 15 September 1972 as "How the essay came to be written". Orwell's essay criticized British self-censorship by the press, specifically the suppression of unflattering descriptions of Stalin and the Soviet government. The same essay also appeared in the Italian 1976 Animal Farm edition, with another introduction by Crick, claiming to be the first edition with the preface. Other publishers were still declining to publish it.
Cultural references.
References to the novella are frequent in other works of popular culture, particularly in popular music and television series.
Adaptations.
"Animal Farm" has been adapted to film twice. The 1954 "Animal Farm" film was an animated feature and the 1999 "Animal Farm" film was a TV live action version, both differ from the novel. In the 1954 film Napoleon is overthrown in a second revolution while the 1999 film shows Napoleon's regime collapsing in on itself, as happened in the Soviet Union.
Editions.
On July 17, 2009, Amazon.com withdrew certain Amazon Kindle titles, including "Animal Farm" and "Nineteen Eighty-Four" by George Orwell, from sale, refunded buyers, and remotely deleted items from purchasers' devices after discovering that the publisher lacked rights to publish the titles in question. Notes and annotations for the books made by users on their devices were also deleted. After the move prompted outcry and comparisons to "Nineteen Eighty-Four" itself, Amazon spokesman Drew Herdener stated that the company is "… changing our systems so that in the future we will not remove books from customers' devices in these circumstances."
---END.OF.DOCUMENT---

Amphibian.
Amphibians (class Amphibia), such as frogs, toads, salamanders, newts, and caecilians, are ectothermic (or cold-blooded) animals that either metamorphose from a juvenile water-breathing form, to an adult air-breathing form, or paedomorph and retain some juvenile characteristics. Mudpuppies and waterdogs are good examples of paedomorphic species. Though amphibians typically have four limbs, the caecilians are notable for being limbless. Unlike other land vertebrates (amniotes), amphibians lay eggs in water. Amphibians are superficially similar to reptiles.
Amphibians are ecological indicators, and in recent decades there has been a dramatic decline in amphibian populations around the globe. Many species are now threatened or extinct.
Amphibians evolved in the Devonian Period and were top predators in the Carboniferous and Permian Periods, but many lineages were wiped out during the Permian–Triassic extinction. One group, the metoposaurs, remained important predators during the Triassic, but as the world became drier during the Early Jurassic they died out, leaving a handful of relict temnospondyls like "Koolasuchus" and the modern orders of Lissamphibia.
Etymology.
Amphibian is derived from the Ancient Greek term ἀμφίβιος "amphíbios" which means both kinds of life, "amphi" meaning “both” and "bio" meaning life. The term was initially used for all kinds of combined natures. Eventually it was used to refer to animals that live both in the water and on land.
Evolutionary history.
The first major groups of amphibians developed in the Devonian Period from fish similar to the modern coelacanth and lungfish which had evolved multi-jointed leg-like fins that enabled them to crawl along the sea bottom. These amphibians were as much as one to five meters in length. However, amphibians never developed the ability to live their entire lives on land, having to return to water to lay their shell-less eggs.
In the Carboniferous Period, the amphibians moved up in the food chain and began to occupy the ecological position currently occupied by crocodiles. These amphibians were notable for eating the mega insects on land and many types of fishes in the water. During the Triassic Period, the better land-adapted proto-crocodiles began to compete with amphibians, leading to their reduction in size and importance in the biosphere.
Taxonomy.
Of these only the last subclass includes recent species.
With the phylogenetic revolution, this classification has been modified, or changed, and the Labyrinthodontia discarded as being a paraphyletic group without unique defining features apart from shared primitive characteristics. Classification varies according to the preferred phylogeny of the author, whether they use a stem-based or node-based classification. Generally amphibians are defined as the group that includes the common ancestors of all living amphibians (frogs, salamanders, etc.) and all their descendants. This may also include extinct groups like the temnospondyls (traditionally placed in the disbanded subclass “labyrinthodontia”), and the Lepospondyls. This means that there are a now large number of basal Devonian and Carboniferous tetrapod groups, described as “amphibians” in earlier books, that are no longer placed in the formal Amphibia.
All recent amphibians are included in the subclass Lissamphibia, superorder Salientia, which is usually considered a clade (which means that it is thought that they evolved from a common ancestor apart from other extinct groups), although it has also been suggested that salamanders arose separately from a temnospondyl-like ancestor.
Authorities also disagree on whether Salientia is a Superorder that includes the order Anura, or whether Anura is a sub-order of the order Salientia. Practical considerations seem to favor using the former arrangement now.
The Lissamphibia, superorder Salientia, are traditionally divided into three orders, but an extinct salamander-like family, the Albanerpetontidae, is now considered part of the Lissamphibia, besides the superorder Salientia. Furthermore, Salientia includes all three recent orders plus a single Triassic proto-frog, "Triadobatrachus".
The actual number of species partly also depends on the taxonomic classification followed, the two most common classifications being the classification of the website AmphibiaWeb, University of California (Berkeley) and the classification by herpetologist Darrel Frost and The American Museum of Natural History, available as the online reference database Amphibian Species of the World. The numbers of species cited above follow Frost.
Reproductive system.
For the purpose of reproduction most amphibians require fresh water. A few (e.g. "Fejervarya raja") can inhabit brackish water and even survive (though not thrive) in seawater, but there are no true marine amphibians. Several hundred frog species in adaptive radiations (e.g., "Eleutherodactylus", the Pacific Platymantines, the Australo-Papuan microhylids, and many other tropical frogs), however, do not need any water for breeding in the wild. They reproduce via direct development, an ecological and evolutionary adaptation that has allowed them to be completely independent from free-standing water. Almost all of these frogs live in wet tropical rainforests and their eggs hatch directly into miniature versions of the adult, passing through the tadpole stage within the egg. Several species have also adapted to arid and semi-arid environments, but most of them still need water to lay their eggs. Symbiosis with single celled algae that lives in the jelly-like layer of the eggs has evolved several times. The larvae (tadpoles or polliwogs) breathe with exterior gills. After hatching, they start to transform gradually into the adult's appearance. This process is called metamorphosis. Typically, the animals then leave the water and become terrestrial adults, but there are many interesting exceptions to this general way of reproduction.
Conservation.
Dramatic declines in amphibian populations, including population crashes and mass localized extinction, have been noted in the past two decades from locations all over the world, and amphibian declines are thus perceived as one of the most critical threats to global biodiversity. A number of causes are believed to be involved, including habitat destruction and modification, over-exploitation, pollution, introduced species, climate change, endocrine-disrupting pollutants, destruction of the ozone layer (ultraviolet radiation has shown to be especially damaging to the skin, eyes, and eggs of amphibians), and diseases like chytridiomycosis. However, many of the causes of amphibian declines are still poorly understood, and are a topic of ongoing discussion. A global strategy to stem the crisis has been released in the form of the Amphibian Conservation Action Plan (available at http://www.amphibians.org). Developed by over 80 leading experts in the field, this call to action details what would be required to curtail amphibian declines and extinctions over the next 5 years - and how much this would cost. The Amphibian Specialist Group of the World Conservation Union (IUCN) is spearheading efforts to implement a comprehensive global strategy for amphibian conservation.
On January 21, 2008, Evolutionarily Distinct and Globally Endangered (EDGE), as given by chief Helen Meredith, identified nature's most endangered species: "The EDGE amphibians are amongst the most remarkable and unusual species on the planet and yet an alarming 85% of the top 100 are receiving little or no conservation attention." The top 10 endangered species (in the List of endangered animal species)
include: the Chinese giant salamander, a distant relative of the newt, the tiny Gardiner's Seychelles, the limbless Sagalla caecilian, South African ghost frogs, lungless Mexican salamanders, the Malagasy rainbow frog, Chile's Darwin frog (Rhinoderma rufum) and the Betic Midwife Toad.
---END.OF.DOCUMENT---

Alaska.
Alaska () is the largest state of the United States by area; it is situated in the northwest extremity of the North American continent, with Canada to the east, the Arctic Ocean to the north, and the Pacific Ocean to the west and south, with Russia further west across the Bering Strait. Approximately half of Alaska's 698,473 residents live within the Anchorage metropolitan area. As of 2009, Alaska remains the least densely populated state of the U.S.
The U.S. Senate approved the purchase of Alaska from the Russian Empire on March 30, 1867, for $7.2 million at about two cents per acre ($4.74/km2). The land went through several administrative changes before becoming an organized territory on May 11, 1912, and the 49th state of the U.S. on January 3, 1959. The name "Alaska" (Аляска) was already introduced in the Russian colonial time, when it was used only for the peninsula and is derived from the Aleut "alaxsxaq", meaning "the mainland" or more literally, "the object towards which the action of the sea is directed". It is also known as Alyeska, the "great land", an Aleut word derived from the same root.
Geography.
Alaska has a longer coastline than all the other U.S. states combined. It is the only non-contiguous U.S. state on continental North America; about of British Columbia (Canada) separate Alaska from Washington state. Alaska is thus an exclave of the United States. It is technically part of the continental U.S., but is often not included in colloquial use; Alaska is not part of the contiguous U.S., often called "the Lower 48." The capital city, Juneau, is situated on the mainland of the North American continent, but is not connected by road to the rest of the North American highway system.
The state is bordered by the Yukon Territory and British Columbia in Canada, to the east, the Gulf of Alaska and the Pacific Ocean to the south, the Bering Sea, Bering Strait, and Chukchi Sea to the west and the Arctic Ocean to the north. Alaska's territorial waters touch Russia's territorial waters in the Bering Strait, as the Russian and Alaskan islands are only apart. As it extends into the eastern hemisphere, it is technically both the westernmost and easternmost state in the United States, as well as also being the northernmost.
Alaska is the largest state in the United States in land area at, over twice the size of Texas, the next largest state. Alaska is larger than all but 18 sovereign countries.
Counting territorial waters, Alaska is larger than the combined area of the next three largest states: Texas, California, and Montana. It is also larger than the combined area of the 22 smallest U.S. states.
The International Date Line was drawn west of 180° to keep the whole state, and thus the entire North American continent, within the same legal day.
Natural features.
With its myriad islands, Alaska has nearly of tidal shoreline. The Aleutian Islands chain extends west from the southern tip of the Alaska Peninsula. Many active volcanoes are found in the Aleutians. Unimak Island, for example, is home to Mount Shishaldin, which is an occasionally smoldering volcano that rises to above the North Pacific. It is the most perfect volcanic cone on Earth, even more symmetrical than Japan's Mount Fuji. The chain of volcanoes extends to Mount Spurr, west of Anchorage on the mainland. Alaska has more volcanoes than any other state. Geologists have identified Alaska as part of Wrangellia, a large region consisting of multiple states and Canadian provinces in the Pacific Northwest which is actively undergoing continent building.
One of the world's largest tides occurs in Turnagain Arm, just south of Anchorage – tidal differences can be more than. (Many sources say Turnagain has the second-greatest tides in North America, but several areas in Canada have larger tides.)
Alaska has more than three million lakes. Marshlands and wetland permafrost cover (mostly in northern, western and southwest flatlands). Glacier ice covers some of land and of tidal zone. The Bering Glacier complex near the southeastern border with Yukon covers alone. With over 100,000 of them, Alaska has half of the world's glaciers.
Land ownership.
According to an October 1998 report by the United States Bureau of Land Management, approximately 65% of Alaska is owned and managed by the U.S. federal government as public lands, including a multitude of national forests, national parks, and national wildlife refuges. Of these, the Bureau of Land Management manages 87 million acres (350,000 km²), or 23.8% of the state. The Arctic National Wildlife Refuge is managed by the United States Fish and Wildlife Service. It is the world's largest wildlife refuge, comprising.
Of the remaining land area, the State of Alaska owns; another are owned by 12 regional and dozens of local Native corporations created under the Alaska Native Claims Settlement Act. Thus, indirectly, the 84,000 Eskimo, Aleut and American Indian inhabitants of Alaska own one-ninth of the state. Various private interests own the remaining land, totaling about one percent of the state.
Climate.
The climate in Juneau and the southeast panhandle is a mid-latitude oceanic climate (Köppen climate classification "Cfb") in the southern sections and a subarctic oceanic climate (Köppen "Cfc") in the northern parts. On an annual basis, the panhandle is both the wettest and warmest part of Alaska with milder temperatures in the winter and high precipitation throughout the year. Juneau averages over of precipitation a year, while other areas receive over. This is also the only region in Alaska in which the average daytime high temperature is above freezing during the winter months.
The climate of Anchorage and south central Alaska is mild by Alaskan standards due to the region's proximity to the seacoast. While the area gets less rain than southeast Alaska, it gets more snow, and days tend to be clearer. On average, Anchorage receives of precipitation a year, with around of snow, although there are areas in the south central which receive far more snow. It is a subarctic climate (Köppen "Dfc") due to its brief, cool summers.
The climate of Western Alaska is determined in large part by the Bering Sea and the Gulf of Alaska. It is a subarctic oceanic climate in the southwest and a continental subarctic climate farther north. The temperature is somewhat moderate considering how far north the area is. This area has a tremendous amount of variety in precipitation. The northern side of the Seward Peninsula is technically a desert with less than of precipitation annually, while some locations between Dillingham and Bethel average around of precipitation.
The climate of the interior of Alaska is subarctic. Some of the highest and lowest temperatures in Alaska occur around the area near Fairbanks. The summers may have temperatures reaching into the 90s°F (the low to mid 30s °C), while in the winter, the temperature can fall below −60 °F (-52 °C). Precipitation is sparse in the Interior, often less than a year, but what precipitation falls in the winter tends to stay the entire winter.
The highest and lowest recorded temperatures in Alaska are both in the Interior. The highest is 100 °F (38 °C) in Fort Yukon (which is just inside the arctic circle) on June 27, 1915, tied with Pahala, Hawaii as the lowest high temperature in the United States. The lowest official Alaska temperature is −80 °F (-62 °C) in Prospect Creek on January 23, 1971, one degree above the lowest temperature recorded in continental North America (in Snag, Yukon, Canada).
The climate in the extreme north of Alaska is Arctic (Köppen "ET") with long, very cold winters and short, cool summers. Even in July, the average low temperature in Barrow is 34 °F (1 °C). Precipitation is light in this part of Alaska, with many places averaging less than per year, mostly as snow which stays on the ground almost the entire year.
History.
The first European contact with Alaska occurred in 1741, when Vitus Bering led an expedition for the Russian Navy aboard the "St. Peter". After his crew returned to Russia bearing sea otter pelts judged to be the finest fur in the world, small associations of fur traders began to sail from the shores of Siberia towards the Aleutian islands. The first permanent European settlement was founded in 1784, and the Russian-American Company carried out an expanded colonization program during the early to mid-1800s. New Archangel on Kodiak Island was Alaska's first capital, but for a century under both Russia and the U.S. Sitka was the capital. The Russians never fully colonized Alaska, and the colony was never very profitable. William H. Seward, the U.S. Secretary of State, negotiated the Alaskan purchase with the Russians in 1867 for $7.2 million. Alaska was loosely governed by the military initially, and was unofficially a territory of the United States from 1884 on.
In the 1890s, gold rushes in Alaska and the nearby Yukon Territory brought thousands of miners and settlers to Alaska. Alaska was granted official territorial status in 1912. At this time the capital was moved to Juneau.
During World War II, the Aleutian Islands Campaign focused on the three outer Aleutian Islands – Attu, Agattu and Kiska – that were invaded by Japanese troops and occupied between June 1942 and August 1943. Unalaska/Dutch Harbor became a significant base for the U.S. Army Air Corps and Navy submariners.
The U.S. Lend-Lease program involved the flying of American warplanes through Canada to Fairbanks and thence Nome; Soviet pilots took possession of these aircraft, ferrying them to fight the German invasion of the Soviet Union. The construction of military bases contributed to the population growth of some Alaskan cities.
Statehood was approved on July 7, 1958. Alaska was officially proclaimed a state on January 3, 1959.
In 1964, the massive "Good Friday Earthquake" killed 131 people and destroyed several villages, mainly by the resultant tsunamis. It was the third most powerful earthquake in the recorded history of the world, with a moment magnitude of 9.2. It was over one thousand times more powerful than the 1989 San Francisco earthquake. Luckily, the epicenter was in an unpopulated area or thousands more would have been killed.
The 1968 discovery of oil at Prudhoe Bay and the 1977 completion of the Trans-Alaska Pipeline led to an oil boom. In 1989, the "Exxon Valdez" hit a reef in the Prince William Sound, spilling over 11 million gallons of crude oil over 1,100 miles (1,600 km) of coastline. Today, the battle between philosophies of development and conservation is seen in the contentious debate over oil drilling in the Arctic National Wildlife Refuge.
Demographics.
The United States Census Bureau, as of July 1, 2008, estimated Alaska's population at 686,293, which represents an increase of 59,361, or 9.5%, since the last census in 2000. This includes a natural increase since the last census of 60,994 people (that is 86,062 births minus 25,068 deaths) and a decrease due to net migration of 5,469 people out of the state. Immigration from outside the U.S. resulted in a net increase of 4,418 people, and migration within the country produced a net loss of 9,887 people. In 2000 Alaska ranked the 48th state by population, ahead of Vermont and Wyoming (and Washington D.C.). Alaska is the least densely populated state, and one of the most sparsely populated areas in the world, at 1.0 person per square mile (0.42/km²), with the next state, Wyoming, at 5.1 per square mile (1.97/km²). Alaska is the largest U.S. state by area, and the sixth wealthiest (per capita income).
Race and ancestry.
According to the 2000 U.S. Census, White Americans made up 69.3% of Alaska's population. African Americans made up 3.5% of Alaska's population. In addition, American Indians and Alaska Natives were the largest minority group; they made up 15.6% of Alaska's population. Asian Americans made up 4.0% of Alaska's population. Pacific Islander Americans made up 0.5% of Alaska's population. Individuals from some other race made up 1.6% of Alaska's population while individuals from two or more races made up 5.4% of the state's population. In addition, Hispanics and Latinos made up 4.1% of Alaska's population.
In terms of ancestry, German Americans were the largest single ethnic group in Alaska; they made up 16.6% of Alaska's population and they were the only ethnic group in the state to number over 100,000 members. Irish Americans made up 10.8% of Alaska's population while English Americans made up 9.6% of the state's population. Norwegian Americans made up 4.2% of Alaska's population and French Americans made up 3.2% of the state's population.
As of the 2005–2007 American Community Survey conducted by the U.S. Census Bureau, White Americans made up 68.5% of Alaska's population. Blacks or African Americans made up 3.8% of Alaska's population. American Indians and Alaska Natives made up 13.4% of Alaska's population; still remaining the largest minority group. Asian Americans made up 4.6% of Alaska's population. Pacific Islander Americans remained at 0.5% of the state's population. Individuals from some other race made up 1.9% of Alaska's population while individuals from two or more races made up 7.2% of the state's population. Hispanics or Latinos made up 5.5% of Alaska's population.
In terms of ancestry, German Americans remained the largest single ethnic group in Alaska; they made up 19.3% of Alaska's population and were still the only ethnic group in the state with over 100,000 members. Irish Americans made up 12.5% of Alaska's population while English Americans made up 10.8% of the state's population. Norwegian Americans remained at 4.2% of Alaska's population and French Americans made up 3.6% of the state's population.
Languages.
According to the 2005–2007 American Community Survey, 84.7% of people over the age of five speak only English at home. About 3.5% speak Spanish at home. About 2.2% speak another Indo-European language at home and about 4.3% speak an Asian language at home. And about 5.3% speak other languages at home.
A total of 5.2% of Alaskans speak one of the state's 22 indigenous languages, known locally as "native languages". These languages belong to two major language families: Eskimo-Aleut and Na-Dene. As the homeland of these two major language families of North America, Alaska has been described as the crossroads of the continent, providing evidence for the recent settlement of North America by way of the Bering land bridge.
Religion.
Alaska has been identified, along with Pacific Northwest states Washington and Oregon, as being the least religious in the U.S. According to statistics collected by the Association of Religion Data Archives, about 39% of Alaska residents were members of religious congregations. Evangelical Protestants had 78,070 members, Roman Catholics had 54,359, and mainline Protestants had 37,156. After Catholicism, the largest single denominations are The Church of Jesus Christ of Latter Day Saints (Mormons/LDS) with 29,460, Southern Baptists with 22,959, and Orthodox with 20,000. The large Eastern Orthodox (with 49 parishes and up to 50,000 followers) population is a result of early Russian colonization and missionary work among Alaska Natives. In 1795, the First Russian Orthodox Church was established in Kodiak. Intermarriage with Alaskan Natives helped the Russian immigrants integrate into society. As a result, an increasing number of Russian Orthodox churches gradually became established within Alaska. Alaska also has the largest Quaker population (by percentage) of any state. In 2003 there were 3,000 Jews in Alaska (for whom observance of the mitzvah may pose special problems). Estimates for the number of Alaskan Muslims range from 2,000 to 5,000. Alaskan Hindus often share venues and celebrations with members of other religious communities including Sikhs and Jains.
Economy.
The 2007 gross state product was $44.9 billion, 45th in the nation. Its per capita personal income for 2007 was $40,042, ranking 15th in the nation. The oil and gas industry dominates the Alaskan economy, with more than 80% of the state's revenues derived from petroleum extraction. Alaska's main export product (excluding oil and natural gas) is seafood, primarily salmon, cod, Pollock and crab. Agriculture represents only a fraction of the Alaskan economy. Agricultural production is primarily for consumption within the state and includes nursery stock, dairy products, vegetables, and livestock. Manufacturing is limited, with most foodstuffs and general goods imported from elsewhere. Employment is primarily in government and industries such as natural resource extraction, shipping, and transportation. Military bases are a significant component of the economy in both Fairbanks and Anchorage. Federal subsidies are also an important part of the economy, allowing the state to keep taxes low. Its industrial outputs are crude petroleum, natural gas, coal, gold, precious metals, zinc and other mining, seafood processing, timber and wood products. There is also a growing service and tourism sector. Tourists have contributed to the economy by supporting local lodging.
Energy.
Alaska has vast energy resources. Major oil and gas reserves are found in the Alaska North Slope (ANS) and Cook Inlet basins. According to the Energy Information Administration, Alaska ranks second in the nation in crude oil production. Prudhoe Bay on Alaska's North Slope is the highest yielding oil field in the United States and on North America, typically producing about. The Trans-Alaska Pipeline can pump up to of crude oil per day, more than any other crude oil pipeline in the United States. Additionally, substantial coal deposits are found in Alaska's bituminous, sub-bituminous, and lignite coal basins. The United States Geological Survey estimates that there are of undiscovered, technically recoverable gas from natural gas hydrates on the Alaskan North Slope. Alaska also offers some of the highest hydroelectric power potential in the country from its numerous rivers. Large swaths of the Alaskan coastline offer wind and geothermal energy potential as well.
Alaska's economy depends heavily on increasingly expensive diesel fuel for heating, transportation, electric power and light. Though wind and hydroelectric power are abundant and underdeveloped, proposals for state-wide energy systems (e.g. with special low-cost electric interties) were judged uneconomical (at the time of the report, 2001) due to low (<$0.50/Gal) fuel prices, long distances and low population. The cost of a gallon of gas in urban Alaska today is usually $0.30-$0.60 higher than the national average; prices in rural areas are generally significantly higher but vary widely depending on transportation costs, seasonal usage peaks, nearby petroleum development infrastructure and many other factors.
Alaska accounts for one-fifth (20 percent) of domestically produced United States oil production. Prudhoe Bay (North America's largest oil field) alone accounts for 8% of the U.S. domestic oil production.
Permanent Fund.
The Alaska Permanent Fund is a legislatively controlled appropriation established in 1976 to manage a surplus in state petroleum revenues from the recently constructed Trans-Alaska Pipeline System. From its initial principal of $734,000, the fund has grown to $40 billion as a result of oil royalties and capital investment programs. Starting in 1982, dividends from the fund's annual growth have been paid out each year to eligible Alaskans, ranging from $331.29 in 1984 to $3,269.00 in 2008 (which included a one-time $1200 "Resource Rebate"). Every year, the state legislature takes out 8 percent from the earnings, puts 3 percent back into the principal for inflation proofing, and the remaining 5 percent is distributed to all qualifying Alaskans. To qualify for the Alaska State Permanent Fund one must have lived in the state for a minimum of 12 months, and maintain constant residency.
Cost of living.
The cost of goods in Alaska has long been higher than in the contiguous 48 states. This has changed for the most part in Anchorage and to a lesser extent in Fairbanks, where the cost of living has dropped somewhat in the past five years. Federal government employees, particularly United States Postal Service (USPS) workers and active-duty military members, receive a Cost of Living Allowance usually set at 25% of base pay because, while the cost of living has gone down, it is still one of the highest in the country.
The introduction of big-box stores in Anchorage, Fairbanks (Wal-Mart in March 2004), and Juneau also did much to lower prices. However, rural Alaska suffers from extremely high prices for food and consumer goods, compared to the rest of the country due to the relatively limited transportation infrastructure. Many rural residents come into these cities and purchase food and goods in bulk from warehouse clubs like Costco and Sam's Club. Some have embraced the free shipping offers of some online retailers to purchase items much more cheaply than they could in their own communities, if they are available at all.
Agriculture.
Due to the northern climate and steep terrain, relatively little farming occurs in Alaska. Most farms are in either the Matanuska Valley, about northeast of Anchorage, or on the Kenai Peninsula, about southwest of Anchorage. The short 100-day growing season limits the crops that can be grown, but the long sunny summer days make for productive growing seasons. The primary crops are potatoes, carrots, lettuce, and cabbage. Farmers exhibit produce at the Alaska State Fair. "Alaska Grown" is used as an agricultural slogan.
Alaska has an abundance of seafood, with the primary fisheries in the Bering Sea and the North Pacific, and seafood is one of the few food items that is often cheaper within the state than outside it. Many Alaskans fish the rivers during salmon season to gather significant quantities of their household diet while fishing for subsistence, sport, or both.
Hunting for subsistence, primarily caribou, moose, and Dall sheep is still common in the state, particularly in remote Bush communities. An example of a traditional native food is Akutaq, the Eskimo ice cream, which can consist of reindeer fat, seal oil, dried fish meat and local berries.
Most food in Alaska is transported into the state from "outside", and shipping costs make food in the cities relatively expensive. In rural areas, subsistence hunting and gathering is an essential activity because imported food is prohibitively expensive. The cost of importing food to villages begins at 7¢ per pound (15¢/kg) and rises rapidly to 50¢ per pound ($1.10/kg) or more. The cost of delivering a seven-pound gallon of milk is about $3.50 in many villages where per capita income can be $20,000 or less. Fuel for snow machines and boats that consume a couple of gallons per hour can exceed $8.00 per gallon.
Roads.
Alaska has few road connections compared to the rest of the U.S. The state's road system covers a relatively small area of the state, linking the central population centers and the Alaska Highway, the principal route out of the state through Canada. The state capital, Juneau, is not accessible by road, only a car ferry, which has spurred several debates over the decades about moving the capital to a city on the road system, or building a road connection from Haines. The western part of Alaska has no road system connecting the communities with the rest of Alaska.
One unique feature of the Alaska Highway system is the Anton Anderson Memorial Tunnel, an active Alaska Railroad tunnel recently upgraded to provide a paved roadway link with the isolated community of Whittier on Prince William Sound to the Seward Highway about southeast of Anchorage. At the tunnel was the longest road tunnel in North America until 2007. The tunnel is the longest combination road and rail tunnel in North America.
Rail.
Built around 1915, the Alaska Railroad (ARR) played a key role in the development of Alaska through the 20th century. It links north Pacific shipping through providing critical infrastructure with tracks that run from Seward to Interior Alaska by way of South Central Alaska, passing through Anchorage, Eklutna, Wasilla, Talkeetna, Denali, and Fairbanks, with spurs to Whittier, Palmer and North Pole. The cities, towns, villages, and region served by ARR tracks are known statewide as "The Railbelt". In recent years, the ever-improving paved highway system began to eclipse the railroad's importance in Alaska's economy.
The railroad, though famed for its summertime tour passenger service, played a vital role in Alaska's development, moving freight into Alaska while transporting natural resources southward (i.e., coal from the Usibelli coal mine near Healy to Seward and gravel from the Matanuska Valley to Anchorage).
The Alaska Railroad was one of the last railroads in North America to use cabooses in regular service and still uses them on some gravel trains. It continues to offer one of the last flag stop routes in the country. A stretch of about of track along an area north of Talkeetna remains inaccessible by road; the railroad provides the only transportation to rural homes and cabins in the area; until construction of the Parks Highway in the 1970s, the railroad provided the only land access to most of the region along its entire route.
In northern Southeast Alaska, the White Pass and Yukon Route also partly runs through the State from Skagway northwards into Canada (British Columbia and Yukon Territory), crossing the border at White Pass Summit. This line is now mainly used by tourists, often arriving by cruise liner at Skagway. It featured in the 1983 BBC television series Great Little Railways.
Marine transport.
Most cities, towns and villages in the state do not have road or highway access; the only modes of access involve travel by air, river, or the sea.
Alaska's well-developed state-owned ferry system (known as the Alaska Marine Highway) serves the cities of Alaska Panhandle, the Gulf Coast and the Alaska Peninsula. The system also operates a ferry service from Bellingham, Washington and Prince Rupert, British Columbia in Canada through the Inside Passage to Skagway. The Inter-Island Ferry Authority also serves as an important marine link for many communities in the Prince of Wales Island region of Southeast and works in concert with the Alaska Marine Highway.
In recent years, large cruise ships began creating a summertime tourism market, mainly connecting the Pacific Northwest to Southeast Alaska and, to a lesser degree, towns along the north gulf coast. Several times each summer, the population of Ketchikan sharply rises for a few hours when two ships dock to debark more than a thousand passengers each while four other ships lie at anchor nearby, waiting their turn at the dock.
Air transport.
Cities not served by road, sea, or river can be reached only by air, foot, dogsled, or snowmachine accounting for Alaska's extremely well-developed bush air services—an Alaskan novelty. Anchorage itself, and to a lesser extent Fairbanks, are served by many major airlines. Because of limited highway access, air travel remains the most efficient form of transportation in and out of the state. Anchorage recently completed extensive remodeling and construction at Ted Stevens Anchorage International Airport to help accommodate the upsurge in tourism (in 2000–2001, the latest year for which data is available, 2.4 million total arrivals to Alaska were counted, 1.7 million by air travel; 1.4 million were visitors).
Regular flights to most villages and towns within the state that are commercially viable are challenging to provide, so they are heavily subsidized by the federal government through the Essential Air Service program. Alaska Airlines is the only major airline offering in-state travel with jet service (sometimes in combination cargo and passenger Boeing 737-400s) from Anchorage and Fairbanks to regional hubs like Bethel, Nome, Kotzebue, Dillingham, Kodiak, and other larger communities as well as to major Southeast and Alaska Peninsula communities. The bulk of remaining commercial flight offerings come from small regional commuter airlines such as Era Aviation, PenAir, and Frontier Flying Service. The smallest towns and villages must rely on scheduled or chartered bush flying services using general aviation aircraft such as the Cessna Caravan, the most popular aircraft in use in the state. Much of this service can be attributed to the Alaska bypass mail program which subsidizes bulk mail delivery to Alaskan rural communities. The program requires 70% of that subsidy to go to carriers who offer passenger service to the communities. Many communities have small air taxi services, such as Hudson's Air Service, Kantishna Air Taxi, and Talkeetna Air Taxi. These operations, though now catering primarily to tourists, originated from the demand for customized transport to remote areas. Perhaps the most quintessentially Alaskan plane is the bush seaplane. The world's busiest seaplane base is Lake Hood, located next to Ted Stevens Anchorage International Airport, where flights bound for remote villages without an airstrip carry passengers, cargo, and many items from stores and warehouse clubs. Alaska has the highest number of pilots per capita of any U.S. state: out of the estimated 663,661 residents, 8,550 are pilots, or about one in 78.
Other transport.
Another Alaskan transportation method is the dogsled. In modern times (that is, any time after the mid-late 1920s), dog mushing is more of a sport than a true means of transportation. Various races are held around the state, but the best known is the Iditarod Trail Sled Dog Race, a 1150-mile (1850 km) trail from Anchorage to Nome (although the mileage varies from year to year, the official distance is set at 1049 miles). The race commemorates the famous 1925 serum run to Nome in which mushers and dogs like Togo and Balto took much-needed medicine to the diphtheria-stricken community of Nome when all other means of transportation had failed. Mushers from all over the world come to Anchorage each March to compete for cash, prizes, and prestige. The "Serum Run" is another sled dog race that more accurately follows the route of the famous 1925 relay, leaving from the community of Nenana (southwest of Fairbanks) to Nome.
In areas not served by road or rail, primary transportation in summer is by all-terrain vehicle and in winter by snowmobile or "snow machine," as it is commonly referred to in Alaska.
State government.
Like all other U.S. states, Alaska is governed as a republic, with three branches of government: an executive branch consisting of the Governor of Alaska and the other independently elected constitutional officers; a legislative branch consisting of the Alaska House of Representatives and Alaska Senate; and a judicial branch consisting of the Alaska Supreme Court and lower courts.
The State of Alaska employs approximately 15,000 employees statewide.
The Alaska Legislature consists of a 40-member House of Representatives and a 20-member Senate. Senators serve four year terms and House members two. The Governor of Alaska serves four-year terms. The lieutenant governor runs separately from the governor in the primaries, but during the general election, the nominee for governor and nominee for lieutenant governor run together on the same ticket.
Alaska's court system has four levels: the Alaska Supreme Court, the court of appeals, the superior courts and the district courts. The superior and district courts are trial courts. Superior courts are courts of general jurisdiction, while district courts only hear certain types of cases, including misdemeanor criminal cases and civil cases valued up to $100,000. The Supreme Court and the Court Of Appeals are appellate courts. The Court Of Appeals is required to hear appeals from certain lower-court decisions, including those regarding criminal prosecutions, juvenile delinquency, and habeas corpus. The Supreme Court hears civil appeals and may in its discretion hear criminal appeals.
State politics.
Although Alaska entered the union as a Democratic state, since the early 1970s Alaska has been characterized as a Republican-leaning state. Local political communities have often worked on issues related to land use development, fishing, tourism, and individual rights. Alaska Natives, while organized in and around their communities, have been active within the Native corporations. These have been given ownership over large tracts of land, which require stewardship.
Alaska is the only state in which possession of one ounce or less of marijuana in one's home is completely legal under state law, though the federal law remains in force.
The state has an independence movement favoring a vote on secession from the United States, with the Alaska Independence Party labeled as one of "the most significant state-level third parties operating in the 20th century".
Six Republicans and four Democrats have served as governor of Alaska. In addition, Republican Governor Wally Hickel was elected to the office for a second term in 1990 after leaving the Republican party and briefly joining the Alaskan Independence Party ticket just long enough to be reelected. He subsequently officially rejoined the Republican party in 1994.
Taxes.
To finance state government operations, Alaska depends primarily on petroleum revenues and federal subsidies. This allows it to have the lowest individual tax burden in the United States, and be one of only five states with no state sales tax, one of seven states that do not levy an individual income tax, and one of two states that has neither. The Department of Revenue Tax Division reports regularly on the state's revenue sources. The Department also issues an annual summary of its operations, including new state laws that directly affect the tax division.
While Alaska has no state sales tax, 89 municipalities collect a local sales tax, from 1–7.5%, typically 3–5%. Other local taxes levied include raw fish taxes, hotel, motel, and bed-and-breakfast 'bed' taxes, severance taxes, liquor and tobacco taxes, gaming (pull tabs) taxes, tire taxes and fuel transfer taxes. A part of the revenue collected from certain state taxes and license fees (such as petroleum, aviation motor fuel, telephone cooperative) is shared with municipalities in Alaska.
Fairbanks has one of the highest property taxes in the state as no sales or income taxes are assessed in the Fairbanks North Star Borough (FNSB). A sales tax for the FNSB has been voted on many times, but has yet to be approved, leading law makers to increase taxes dramatically on other goods such as liquor and tobacco.
In 2008 the Tax Foundation ranked Alaska as having the 4th most "business friendly" tax policy. More "friendly" states were Wyoming, Nevada, and South Dakota.
Federal politics.
In presidential elections, the state's electoral college votes have been won by the Republican nominee in every election since statehood, except for 1964. No state has voted for a Democratic presidential candidate fewer times. Alaska supported Democratic nominee Lyndon B. Johnson in the landslide year of 1964, although the 1960 and 1968 elections were close. Republican John McCain defeated Democrat Barack Obama in Alaska, 59.49% to 37.83%. McCain's running mate was Sarah Palin, the state's governor and the first Alaskan on a major party ticket. The Alaska Bush, the city of Juneau and midtown and downtown Anchorage have been strongholds of the Democratic party. Matanuska-Susitna Borough and South Anchorage typically have the strongest Republican showing. As of 2004, well over half of all registered voters have chosen "Non-Partisan" or "Undeclared" as their affiliation, despite recent attempts to close primaries.
Because of its population relative to other U.S. states, Alaska has only one member in the U.S. House of Representatives. This seat is currently being held by Republican Don Young, who was re-elected to his 19th consecutive term in 2008.
On November 19, 2008, Democrat Mark Begich, mayor of Anchorage, defeated long-time Republican senator Ted Stevens. Stevens had been convicted on seven felony counts of failing to report gifts on Senate financial discloser forms one week before the election. The conviction was set aside in April 2009 after evidence of prosecutorial misconduct emerged.
Republican Frank Murkowski held the state's other senatorial position. After being elected governor in 2002, he resigned from the Senate and appointed his daughter, State Representative Lisa Murkowski as his successor. In response to a subsequent ballot initiative, the state legislature attempted to amend the law to limit the length of gubernatorial appointments. She won a full six-year term in 2004. In 2006 Frank Murkowski was defeated in the Republican primary by Sarah Palin, who in 2008 became the Republican nominee for Vice President of the United States.
Cities, towns and boroughs.
Alaska is not divided into counties, as most of the other U.S. states, but it is divided into "boroughs". Many of the more densely populated parts of the state are part of Alaska's sixteen boroughs, which function somewhat similarly to counties in other states. However, unlike county-equivalents in the other 49 states, the boroughs do not cover the entire land area of the state. The area not part of any borough is referred to as the Unorganized Borough. The Unorganized Borough has no government of its own, but the U.S. Census Bureau in cooperation with the state divided the Unorganized Borough into 11 census areas solely for the purposes of statistical analysis and presentation. A recording district is a mechanism for administration of the public record in Alaska. The state is divided into 34 recording districts which are centrally administered under a State Recorder. All recording districts use the same acceptance criteria, fee schedule, etc., for accepting documents into the public record.
Whereas many U.S. states use a three-tiered system of decentralization—state/county/township—most of Alaska uses only two tiers—state/borough. Owing to the low population density, most of the land is located in the Unorganized Borough which, as the name implies, has no intermediate borough government of its own, but is administered directly by the state government. Currently (2000 census) 57.71% of Alaska's area has this status, with 13.05% of the population. For statistical purposes the United States Census Bureau divides this territory into census areas. Anchorage merged the city government with the Greater Anchorage Area Borough in 1975 to form the Municipality of Anchorage, containing the city proper and the communities of Eagle River, Chugiak, Peters Creek, Girdwood, Bird, and Indian. Fairbanks has a separate borough (the Fairbanks North Star Borough) and municipality (the City of Fairbanks).
The state's most populous city is Anchorage, home to 278,700 people in 2006, 225,744 of whom live in the urbanized area. The richest location in Alaska by per capita income is Halibut Cove ($89,895). Yakutat City, Sitka, Juneau, and Anchorage are the four largest cities in the U.S. by area.
Education.
The Alaska Department of Education and Early Development administers many school districts in Alaska. In addition, the state operates a boarding school, Mt. Edgecumbe High School in Sitka; and provides partial funding for other boarding schools including, Nenana Student Living Center in Nenana, and The Galena Interior Learning Academy in Galena.
There are more than a dozen colleges and universities in Alaska. Accredited universities in Alaska include the University of Alaska Anchorage, University of Alaska Fairbanks, University of Alaska Southeast, and Alaska Pacific University. 43% of the population attends or attended college.
Alaska has had a problem with a "brain drain". Many of its young people, including most of the highest academic achievers, leave the state after high school graduation and do not return. The University of Alaska has attempted to combat this by offering partial four-year scholarships to the top 10% of Alaska high school graduates, via the Alaska Scholars Program.
Public health and public safety.
Alaska residents have long had a problem with alcohol use and abuse. Many rural communities in Alaska have outlawed its import. This problem directly relates to Alaska's high rate of Fetal alcohol syndrome (FAS) as well as contributing to the high rate of suicides and teenage pregnancies. Suicide rates for rural residents are higher than urban.
Domestic abuse and other violent crimes are also at high levels in the state; this is in part linked to alcohol abuse.
Culture.
Some of Alaska's popular annual events are the Iditarod Trail Sled Dog Race that starts in Anchorage and ends in Nome, World Ice Art Championships in Fairbanks, the Alaska Hummingbird Festival in Ketchikan, the Sitka Whale Fest, and the Stikine River Garnet Fest in Wrangell. The Stikine River features the largest springtime concentration of American Bald Eagles in the world.
The Alaska Native Heritage Center celebrates the rich heritage of Alaska's 11 cultural groups. Their purpose is to enhance self-esteem among Native people and to encourage cross-cultural exchanges among all people. The Alaska Native Arts Foundation promotes and markets Native art from all regions and cultures in the State, both on the internet; at its gallery in Anchorage, 500 West Sixth Avenue, and at the Alaska House New York, 109 Mercer Street in SoHo.
Alaska Natives – Inuit, Inupiaq or Yupik drummers and dancers – give informal performances in the lobby of the Alaska Native Medical Center in Anchorage on weekday evenings.
Libraries.
The four main libraries in the state are the Alaska State Library in Juneau, the Elmer E. Rasmuson Library in Fairbanks, the Z. J. Loussac Library in Anchorage, and the APU Consortium Library, also in Anchorage. Alaska is one of three states (the others are Delaware and Rhode Island) that does not have a Carnegie library.
Music.
Influences on music in Alaska include the traditional music of Alaska Natives as well as folk music brought by later immigrants from Russia and Europe. Prominent musicians from Alaska include singer Jewel, traditional Aleut flautist Mary Youngblood, folk singer-songwriter Libby Roderick, Christian music singer/songwriter Lincoln Brewster, metal/post hardcore band 36 Crazyfists and the groups Pamyua and Portugal. The Man.
There are many established music festivals in Alaska, including the Alaska Folk Festival, the Fairbanks Summer Arts Festival the Anchorage Folk Festival, the Athabascan Old-Time Fiddling Festival, the Sitka Jazz Festival, and the Sitka Summer Music Festival. The most prominent symphony in Alaska is the Anchorage Symphony Orchestra, though the Fairbanks Symphony Orchestra and Juneau Symphony are also notable. The Anchorage Opera is currently the state's only professional opera company, though there are several volunteer and semi-professional organizations in the state as well.
The official state song of Alaska is "Alaska's Flag", which was adopted in 1955; it celebrates the flag of Alaska.
Movies filmed in Alaska.
Alaska's first independent picture all made on place was in the silent years. The Chechahcos, was released in 1924 by the Alaska Moving Picture Corp. It was the only film the company made.
One of the most prominent movies filmed in Alaska is MGM's Academy Award winning classic "Mala The Magnificent" starring Alaska's own Ray Mala. In 1932 an expedition set out from MGM's studios in Hollywood to Alaska to film what was then billed as "The Biggest Picture Ever Made." Upon arriving in Alaska, they set up "Camp Hollywood" in Northwest Alaska, where they lived during the duration of the filming. Louis B. Mayer spared no expense in making sure they had everything they needed during their stay—he even sent the famous chef from the Hotel Roosevelt on Hollywood Blvd (the site of the first Oscars) with them to Alaska to cook for them. When "Eskimo" premiered at the famed Astor Theatre in Times Square, New York, the studio received the largest amount of feedback in the history of the studio up to that time. "Eskimo" was critically acclaimed and released worldwide; as a result Inupiat Eskimo actor Ray Mala became an international movie star. "Eskimo" is significant for the following: winning the very first Oscar for Best Film Editing at the Academy Awards, for forever preserving Inupiat culture on film, and for being the first motion picture to be filmed in an all native language (Inupiat).
The psychological thriller "Insomnia", starring Al Pacino and Robin Williams was shot in Canada, but was set in Alaska. The 2007 horror feature "30 Days of Night" is set in Barrow, Alaska but was filmed in New Zealand. Most films and television shows set in Alaska are not filmed there; for example, "Northern Exposure", set in the fictional town of Cicely, Alaska, was actually filmed in Roslyn, Washington.
The 1983 Disney movie "Never Cry Wolf" was at least partially shot in Alaska. The 1991 film "White Fang", starring Ethan Hawke, was filmed in and around Haines, Alaska. The 1999 John Sayles film "Limbo", starring David Strathairn, Mary Elizabeth Mastrantonio and Kris Kristofferson, was filmed in Juneau.
The 2007 film directed by Sean Penn, "Into The Wild" was partially filmed and set in Alaska. The film, which is based on the novel of the same name, follows the adventures of Christopher McCandless, who died in a remote abandoned bus in Alaska in 1992.
---END.OF.DOCUMENT---

Agriculture.
Agriculture is the production of food and goods through farming. Agriculture was the key development that led to the rise of human civilization, with the husbandry of domesticated animals and plants (i.e. crops) creating food surpluses that enabled the development of more densely populated and stratified societies. The study of agriculture is known as agricultural science. Central to human society, agriculture is also observed in certain species of ant and termite.
Agriculture encompasses a wide variety of specialties and techniques, including ways to expand the lands suitable for plant raising, by digging water-channels and other forms of irrigation. Cultivation of crops on arable land and the pastoral herding of livestock on rangeland remain at the foundation of agriculture. In the past century there has been increasing concern to identify and quantify various forms of agriculture. In the developed world the range usually extends between sustainable agriculture (e.g. permaculture or organic agriculture) and intensive farming (e.g. industrial agriculture).
Modern agronomy, plant breeding, pesticides and fertilizers, and technological improvements have sharply increased yields from cultivation, and at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry such as intensive pig farming (and similar practices applied to the chicken) have similarly increased the output of meat, but have raised concerns about animal cruelty and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production.
The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. In the 2000s, plants have been used to grow biofuels, biopharmaceuticals, bioplastics, and pharmaceuticals. Specific foods include cereals, vegetables, fruits, and meat. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are produced by plants, such as resins. Biofuels include methane from biomass, ethanol, and biodiesel. Cut flowers, nursery plants, tropical fish and birds for the pet trade are some of the ornamental products.
In 2007, about one third of the world's workers were employed in agriculture. The services sector has overtaken agriculture as the economic sector employing the most people worldwide. Despite the size of its workforce, agricultural production accounts for less than five percent of the gross world product (an aggregate of all gross domestic products).
Etymology.
The word "agriculture" is the English adaptation of Latin "agricultūra", from "ager", "a field", and "cultūra", "cultivation" in the strict sense of "tillage of the soil". Thus, a literal reading of the word yields "tillage of a field / of fields"...
Overview.
Agriculture has played a key role in the development of human civilization. Until the Industrial Revolution, the vast majority of the human population labored in agriculture. Development of agricultural techniques has steadily increased agricultural productivity, and the widespread diffusion of these techniques during a time period is often called an agricultural revolution. A remarkable shift in agricultural practices has occurred over the past century in response to new technologies. In particular, the Haber-Bosch method for synthesizing ammonium nitrate made the traditional practice of recycling nutrients with crop rotation and animal manure less necessary.
Synthetic nitrogen, along with mined rock phosphate, pesticides and mechanization, have greatly increased crop yields in the early 20th century. Increased supply of grains has led to cheaper livestock as well. Further, global yield increases were experienced later in the 20th century when high-yield varieties of common staple grains such as rice, wheat, and corn (maize) were introduced as a part of the Green Revolution. The Green Revolution exported the technologies (including pesticides and synthetic nitrogen) of the developed world to the developing world. Thomas Malthus famously predicted that the Earth would not be able to support its growing population, but technologies such as the Green Revolution have allowed the world to produce a surplus of food.
Many governments have subsidized agriculture to ensure an adequate food supply. These agricultural subsidies are often linked to the production of certain commodities such as wheat, corn (maize), rice, soybeans, and milk. These subsidies, especially when instituted by developed countries have been noted as protectionist, inefficient, and environmentally damaging. In the past century agriculture has been characterized by enhanced productivity, the use of synthetic fertilizers and pesticides, selective breeding, mechanization, water contamination, and farm subsidies. Proponents of organic farming such as Sir Albert Howard argued in the early 1900s that the overuse of pesticides and synthetic fertilizers damages the long-term fertility of the soil. While this feeling lay dormant for decades, as environmental awareness has increased in the 2000s there has been a movement towards sustainable agriculture by some farmers, consumers, and policymakers. In recent years there has been a backlash against perceived external environmental effects of mainstream agriculture, particularly regarding water pollution, resulting in the organic movement. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food.
In late 2007, several factors pushed up the price of grains consumed by humans as well as used to feed poultry and dairy cows and other cattle, causing higher prices of wheat (up 58%), soybean (up 32%), and maize (up 11%) over the year. Food riots took place in several countries across the world. Contributing factors included drought in Australia and elsewhere, increasing demand for grain-fed animal products from the growing middle classes of countries such as China and India, diversion of foodgrain to biofuel production and trade restrictions imposed by several countries.
An epidemic of stem rust on wheat caused by race Ug99 is currently spreading across Africa and into Asia and is causing major concern. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.
History.
Since its development roughly 10,000 years ago, agriculture has expanded vastly in geographical coverage and yields. Throughout this expansion, new technologies and new crops were integrated. Even then crops were modified through cross-breeding for better yields. Agricultural practices such as irrigation, crop rotation, fertilizers, and pesticides were developed long ago, but have made great strides in the past century. The history of agriculture has played a major role in human history, as agricultural progress has been a crucial factor in worldwide socio-economic change. Wealth-concentration and militaristic specializations rarely seen in hunter-gatherer cultures are commonplace in societies which practice agriculture. So, too, are arts such as epic literature and monumental architecture, as well as codified legal systems. When farmers became capable of producing food beyond the needs of their own families, others in their society were freed to devote themselves to projects other than food acquisition. Historians and anthropologists have long argued that the development of agriculture made civilization possible.
Ancient origins.
The Fertile Crescent of Western Asia, Egypt, and India were sites of the earliest planned sowing and harvesting of plants that had previously been gathered in the wild. Independent development of agriculture occurred in northern and southern China, Africa's Sahel, New Guinea and several regions of the Americas. The eight so-called Neolithic founder crops of agriculture appear: first emmer wheat and einkorn wheat, then hulled barley, peas, lentils, bitter vetch, chick peas and flax.
By 7000 BC, small-scale agriculture reached Egypt. From at least 7000 BC the Indian subcontinent saw farming of wheat and barley, as attested by archaeological excavation at Mehrgarh in Balochistan. By 6000 BC, mid-scale farming was entrenched on the banks of the Nile. About this time, agriculture was developed independently in the Far East, with rice, rather than wheat, as the primary crop. Chinese and Indonesian farmers went on to domesticate taro and beans including mung, soy and azuki. To complement these new sources of carbohydrates, highly organized net fishing of rivers, lakes and ocean shores in these areas brought in great volumes of essential protein. Collectively, these new methods of farming and fishing inaugurated a human population boom that dwarfed all previous expansions and continues today.
By 5000 BC, the Sumerians had developed core agricultural techniques including large-scale intensive cultivation of land, mono-cropping, organized irrigation, and the use of a specialized labor force, particularly along the waterway now known as the Shatt al-Arab, from its Persian Gulf delta to the confluence of the Tigris and Euphrates. Domestication of wild aurochs and mouflon into cattle and sheep, respectively, ushered in the large-scale use of animals for food/fiber and as beasts of burden. The shepherd joined the farmer as an essential provider for sedentary and semi-nomadic societies. Maize, manioc, and arrowroot were first domesticated in the Americas as far back as 5200 BC. The potato, tomato, pepper, squash, several varieties of bean, tobacco, and several other plants were also developed in the New World, as was extensive terracing of steep hillsides in much of Andean South America. The Greeks and Romans built on techniques pioneered by the Sumerians but made few fundamentally new advances. Southern Greeks struggled with very poor soils, yet managed to become a dominant society for years. The Romans were noted for an emphasis on the cultivation of crops for trade.
Middle Ages.
During the Middle Ages, farmers in North Africa, the Near East, and Europe began making use of agricultural technologies including irrigation systems based on hydraulic and hydrostatic principles, machines such as norias, water-raising machines, dams, and reservoirs. This combined with the invention of a three-field system of crop rotation and the moldboard plow greatly improved agricultural efficiency.
Modern era.
After 1492, a global exchange of previously local crops and livestock breeds occurred. Key crops involved in this exchange included the tomato, maize, potato, manioc, cocoa bean and tobacco going from the New World to the Old, and several varieties of wheat, spices, coffee, and sugar cane going from the Old World to the New. The most important animal exportation from the Old World to the New were those of the horse and dog (dogs were already present in the pre-Columbian Americas but not in the numbers and breeds suited to farm work). Although not usually food animals, the horse (including donkeys and ponies) and dog quickly filled essential production roles on western-hemisphere farms.
The potato became an important staple crop in northern Europe. Since being introduced by Portuguese in the 16th century, maize and manioc have replaced traditional African crops as the continent's most important staple food crops.
By the early 1800s, agricultural techniques, implements, seed stocks and cultivated plants selected and given a unique name because of its decorative or useful characteristics had so improved that yield per land unit was many times that seen in the Middle Ages. With the rapid rise of mechanization in the late 19th and 20th centuries, particularly in the form of the tractor, farming tasks could be done with a speed and on a scale previously impossible. These advances have led to efficiencies enabling certain modern farms in the United States, Argentina, Israel, Germany, and a few other nations to output volumes of high-quality produce per land unit at what may be the practical limit.
The Haber-Bosch method for synthesizing ammonium nitrate represented a major breakthrough and allowed crop yields to overcome previous constraints. In the past century agriculture has been characterized by enhanced productivity, the substitution of labor for synthetic fertilizers and pesticides, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic movement.
The cereals rice, corn, and wheat provide 60% of human food supply. Between 1700 and 1980, "the total area of cultivated land worldwide increased 466%" and yields increased dramatically, particularly because of selectively-bred high-yielding varieties, fertilizers, pesticides, irrigation, and machinery. For example, irrigation increased corn yields in eastern Colorado by 400 to 500% from 1940 to 1997.
However, concerns have been raised over the sustainability of intensive agriculture. Intensive agriculture has become associated with decreased soil quality in India and Asia, and there has been increased concern over the effects of fertilizers and pesticides on the environment, particularly as population increases and food demand expands. The monocultures typically used in intensive agriculture increase the number of pests, which are controlled through pesticides. Integrated pest management (IPM), which "has been promoted for decades and has had some notable successes" has not significantly affected the use of pesticides because policies encourage the use of pesticides and IPM is knowledge-intensive. Although the "Green Revolution" significantly increased rice yields in Asia, yield increases have not occurred in the past 15–20 years. The genetic "yield potential" has increased for wheat, but the yield potential for rice has not increased since 1966, and the yield potential for maize has "barely increased in 35 years". It takes a decade or two for herbicide-resistant weeds to emerge, and insects become resistant to insecticides within about a decade. Crop rotation helps to prevent resistances.
Agricultural exploration expeditions, since the late nineteenth century, have been mounted to find new species and new agricultural practices in different areas of the world. Two early examples of expeditions include Frank N. Meyer's fruit- and nut-collecting trip to China and Japan from 1916-1918
and the Dorsett-Morse Oriental Agricultural Exploration Expedition to China, Japan, and Korea from 1929-1931 to collect soybean germplasm to support the rise in soybean agriculture in the United States.
In 2005, the agricultural output of China was the largest in the world, accounting for almost one-sixth of world share, followed by the EU, India and the USA, according to the International Monetary Fund. More than 40 million Chinese farmers have been displaced from their land in recent years, usually for economic development, contributing to the 87,000 demonstrations and riots across China in 2005. Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 2.6 times more productive than it was in 1948.
Six countries - the US, Canada, France, Australia, Argentina and Thailand - supply 90% of grain exports. The United States controls almost half of world grain exports. Water deficits, which are already spurring heavy grain imports in numerous middle-sized countries, including Algeria, Iran, Egypt, and Mexico, may soon do the same in larger countries, such as China or India.
Crop production systems.
Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer. Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10-20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs. Further industrialization lead to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time are other kinds of annual cropping systems known as polycultures.
In tropical environments, all of these cropping systems are practiced. In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual cropping is the dominant farming system.
The last century has seen the intensification, concentration and specialization of agriculture, relying upon new technologies of agricultural chemicals (fertilizers and pesticides), mechanization, and plant breeding (hybrids and GMO's). In the past few decades, a move towards sustainability in agriculture has also developed, integrating ideas of socio-economic justice and conservation of resources and the environment within a farming system. This has led to the development of many responses to the conventional agriculture approach, including organic agriculture, urban agriculture, community supported agriculture, ecological or biological agriculture, integrated farming and holistic management, as well as an increased trend towards agricultural diversification.
Crop statistics.
Important categories of crops include grains and pseudograins, pulses (legumes), forage, and fruits and vegetables. Specific crops are cultivated in distinct growing regions throughout the world. In millions of metric tons, based on FAO estimate.
Livestock production systems.
Animals, including horses, mules, oxen, camels, llamas, alpacas, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship.
Livestock production systems can be defined based on feed source, as grassland - based, mixed, and landless. Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30-40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastic (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops. Approximately 68% of all agricultural land is permanent pastures used in the production of livestock. Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in OECD member countries. In the U.S., 70% of the grain grown is fed to animals on feedlots. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution.
Production practices.
Tillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.
Pest control includes the management of weeds, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.
Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.
Water management is where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.
Processing, distribution, and marketing.
In the United States, food costs attributed to processing, distribution, and marketing have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. From 1960 to 1980 the farm share was around 40%, but by 1990 it had declined to 30% and by 1998, 22.2%. Market concentration has increased in the sector as well, with the top 20 food manufacturers accounting for half the food-processing value in 1995, over double that produced in 1954. As of 2000 the top six US supermarket groups had 50% of sales compared to 32% in 1992. Although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.
Crop alteration and biotechnology.
Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles gave plant breeders a better understanding of genetics and brought great insights to the techniques utilized by plant breeders. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.
Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray an ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.
The green revolution popularized the use of conventional hybridization to increase yield many folds by creating "high-yielding varieties". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging)..
Genetic Engineering.
Genetically Modified Organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. After mechanical tomato-harvesters were developed in the early 1960s, agricultural scientists genetically modified tomatoes to be more resistant to mechanical handling. More recently, genetic engineering is being employed in various parts of the world, to create crops with other beneficial traits.
Herbicide-tolerant GMO Crops.
Roundup-Ready seed has a herbicide resistant gene implanted into its genome that allows the plants to tolerate exposure to glyphosate. Roundup is a trade name for a glyphosate based product, which is a systemic, non-selective herbicide used to kill weeds. Roundup-Ready seeds allow the farmer to grow a crop that can be sprayed with glyphosate to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. Today, 92% of soybean acreage in the US is planted with genetically-modified herbicide-tolerant plants. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.
Insect-Resistant GMO Crops.
Other GMO crops utilized by growers include insect-resistant crops, which have a gene from the soil bacterium "Bacillus thuringiensis" (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects; one such crop is Starlink. Another is cotton, which accounts for 63% of US cotton acreage.
Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least nineteen diseases did so through crossing with wild populations of tomatoes.
Costs and Benefits of GMOs.
Genetic engineers may someday develop transgenic plants which would allow for irrigation, drainage, conservation, sanitary engineering, and maintaining or increasing yields while requiring fewer fossil fuel derived inputs than conventional crops. Such developments would be particularly important in areas which are normally arid and rely upon constant irrigation, and on large scale farms.
However, genetic engineering of plants has proven to be controversial. Many issues surrounding food security and environmental impacts have risen regarding GMO practices. For example, GMOs are questioned by some ecologists and economists concerned with GMO practices such as terminator seeds, which is a genetic modification that creates sterile seeds. Terminator seeds are currently under strong international opposition and face continual efforts of global bans.
Another controversial issue is the patent protection given to companies that develop new types of seed using genetic engineering. Since companies have intellectual ownership of their seeds, they have the power to dictate terms and conditions of their patented product. Currently, ten seed companies control over two-thirds of the global seed sales. Vandana Shiva argues that these companies are guilty of biopiracy by patenting life and exploiting organisms for profit Farmers using patented seed are restricted from saving seed for subsequent plantings, which forces farmers to buy new seed every year. Since seed saving is a traditional practice for many farmers in both developing and developed countries, GMO seeds legally bind farmers to change their seed saving practices to buying new seed every year.
Locally adapted seeds are an essential hertitage that has the potential to be lost with current hybridized crops and GMOs. Locally adapted seeds, also called land races or crop eco-types, are important because they have adapted over time to the specific microclimates, soils, other environmental conditions, field designs, and ethnic preference indigenous to the exact area of cultivation. Introducing GMOs and hybridized commercial seed to an area brings the risk of cross-pollination with local land races Therefore, GMOs pose a threat to the sustainability of land races and the ethnic heritage of cultures. Once seed contains transgenic material, it becomes subject to the conditions of the seed company that owns the patent of the transgenic material.
There is also concern that GMOs will cross-pollinate with wild species and permanently alter native populations’ genetic integrity; there are already identified populations of wild plants with transgenic genes. GMO gene flow to related weed species is a concern, as well as cross-pollination with non-transgenic crops. Since many GMO crops are harvested for their seed, such as rapeseed, seed spillage in is problematic for volunteer plants in rotated fields, as well as seed-spillage during transportation.
Food safety and labeling.
Food security issues also coincide with food safety and food labeling concerns. Currently a global treaty, the BioSafety Protocol, regulates the trade of GMOs. The EU currently requires all GMO foods to be labeled, whereas the US does not require transparent labeling of GMO foods. Since there are still questions regarding the safety and risks associated with GMO foods, some believe the public should have the freedom to choose and know what they are eating and require all GMO products to be labeled.
Environmental impact.
Agriculture imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies concluded that more should be done to internalize external costs, and neither included subsidies in their analysis, but noted that subsidies also influence the cost of agriculture to society. Both focused on purely fiscal impacts. The 2000 review included reported pesticide poisonings but did not include speculative chronic effects of pesticides, and the 2004 review relied on a 1992 estimate of the total impact of pesticides.
Livestock issues.
A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2,) and 37% of all human-induced methane (which is 23 times as warming as CO2. It also generates 64% of the ammonia, which contributes to acid rain and acidification of ecosystems. Livestock expansion is cited as a key factor driving deforestation, in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity.
Land transformation and degradation.
Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39–50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).
Eutrophication.
Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.
Pesticides.
Pesticide use has increased since 1950 to 2.5 million tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that 3 million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the 'pesticide treadmill' in which pest resistance warrants the development of a new pesticide. An alternative argument is that the way to 'save the environment' and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation.
Climate Change.
Climate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO2, solar radiation and the interaction of these elements. Agriculture can both mitigate or worsen global warming. Some of the increase in CO2 in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddies. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gas nitric oxide. Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO2 in the atmosphere.
Distortions in modern global agriculture.
Differences in economic development, population density and culture mean that the farmers of the world operate under very different conditions.
A US cotton farmer may receive US$230 in government subsidies per acre planted (in 2003), while farmers in Mali and other third-world countries do without. When prices decline, the heavily subsidised US farmer is not forced to reduce his output, making it difficult for cotton prices to rebound, but his Mali counterpart may go broke in the meantime.
A livestock farmer in South Korea can calculate with a (highly subsidized) sales price of US$1300 for a calf produced. A South American Mercosur country rancher calculates with a calf's sales price of US$120–200 (both 2008 figures).
With the former, scarcity and high cost of land is compensated with public subsidies, the latter compensates absence of subsidies with economics of scale and low cost of land.
In the Peoples Republic of China, a rural household's productive asset may be one hectare of farmland.
In Brazil, Paraguay and other countries where local legislature allows such purchases, international investors buy thousands of hectares of farmland or raw land at prices of a few hundred US$ per hectare.
Energy and Agriculture.
Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between 1950 and 1984, the Green Revolution transformed agriculture around the globe, with world grain production increasing by 250% as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages.
Modern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1) direct consumption on the farm and 2) indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gas, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1 percent of the nation's total energy. Indirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002. The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption, but it is not included in USDA estimates of U.S. agricultural energy use. Together, direct and indirect consumption by U.S. farms accounts for about 2 percent of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years.
Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for approximately one-fifth of food system energy use in the United States.
Oil shortages could impact this food supply. Some farmers using modern organic-farming methods have reported yields as high as those available from conventional farming without the use of synthetic fertilizers and pesticides. However, the reconditioning of soil to restore nutrients lost during the use of monoculture agriculture techniques made possible by petroleum-based technology takes time.
In 2007, higher incentives for farmers to grow non-food biofuel crops combined with other factors (such as over-development of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth) to cause food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes.
The biggest fossil fuel input to agriculture is the use of natural gas as a hydrogen source for the Haber-Bosch fertilizer-creation process. Natural gas is used because it is the cheapest currently available source of hydrogen. When oil production becomes so scarce that natural gas is used as a partial stopgap replacement, and hydrogen use in transportation increases, natural gas will become much more expensive. If the Haber Process is unable to be commercialized using renewable energy (such as by electrolysis) or if other sources of hydrogen are not available to replace the Haber Process, in amounts sufficient to supply transportation and agricultural needs, this major source of fertilizer would either become extremely expensive or unavailable. This would either cause food shortages or dramatic rises in food prices.
Mitigation of effects of petroleum shortages.
One effect oil shortages could have on agriculture is a full return to organic agriculture. In light of peak-oil concerns, organic methods are more sustainable than contemporary practices because they use no petroleum-based pesticides, herbicides, or fertilizers. Some farmers using modern organic-farming methods have reported yields as high as those available from conventional farming. Organic farming may however be more labor-intensive and would require a shift of the workforce from urban to rural areas.
It has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural "waste" to provide charcoal fertilizer, some fuel "and" food, instead of the normal food vs fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion.
It has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops. The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds, and a January 2008 report shows that GMO practices "fail to deliver environmental,
social and economic benefits." While there has been some research on sustainability using GMO crops, at least one hyped and prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop. Additionally, a survey by the bio-tech industry of subsistence farmers in Africa to discover what GMO research would most benefit sustainable agriculture only identified non-transgenic issues as areas needing to be addressed.
Nevertheless, some governments in Africa continue to view investments in new transgenic technologies as an essential component of efforts to improve sustainability.
---END.OF.DOCUMENT---

Aldous Huxley.
Aldous Leonard Huxley (26 July 1894 – 22 November 1963) was an English writer and one of the most prominent members of the famous Huxley family. He spent the later part of his life in the United States, living in Los Angeles from 1937 until his death in 1963. Best known for his novels including "Brave New World" and wide-ranging output of essays, Huxley also edited the magazine "Oxford Poetry", and published short stories, poetry, travel writing, and film stories and scripts.
Aldous Huxley was a humanist and pacifist, and he was latterly interested in spiritual subjects such as parapsychology and philosophical mysticism. He is also well known for advocating and taking psychedelics.
By the end of his life Huxley was considered, in some academic circles, a leader of modern thought and an intellectual of the highest rank, and highly regarded as one of the most prominent explorers of visual communication and sight-related theories as well.
Early years.
Aldous Huxley was born in Godalming, Surrey, UK in 1894. He was the third son of the writer and school-master Leonard Huxley and first wife, Julia Arnold who founded Prior's Field School. Julia was the niece of Matthew Arnold and the sister of Mrs. Humphrey Ward. Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic and controversialist ("Darwin's Bulldog"). His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists. Huxley had another brother Noel Trevenen (1891–1914) who committed suicide after a period of clinical depression.
Huxley began his learning in his father's well-equipped botanical laboratory, then continued in a school named Hillside. His teacher was his mother who supervised him for several years until she became terminally ill. After Hillside, he was educated at Eton College. Huxley's mother died in 1908, when he was fourteen. In 1911, he suffered an illness (keratitis punctata) which "left [him] practically blind for two to three years". Aldous's near-blindness disqualified him from service in the First World War. Once his eyesight recovered sufficiently, he was able to study English literature at Balliol College, Oxford. In 1916 he edited "Oxford Poetry" and later graduated with first class honours.
Following his education at Balliol, Huxley was financially indebted to his father and had to earn a living. He taught French for a year at Eton, where Eric Blair (later known by the pen name George Orwell) and Stephen Runciman were among his pupils, but was remembered as an incompetent and hopeless teacher who couldn’t keep discipline. Nevertheless, Blair and others were impressed by his use of words. For a short while in 1918, he was employed acquiring provisions at the Air Ministry.
Significantly, Huxley also worked for a time in the 1920s at the technologically-advanced Brunner and Mond chemical plant in Billingham, Teesside, and the most recent introduction to his famous science fiction novel "Brave New World" (1932) states that this experience of "an ordered universe in a world of planless incoherence" was one source for the novel.
Huxley completed his first (unpublished) novel at the age of seventeen and began writing seriously in his early twenties. His earlier work includes important novels on the dehumanizing aspects of scientific progress, most famously "Brave New World", and on pacifist themes (for example, "Eyeless in Gaza"). In "Brave New World" Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning. Huxley was strongly influenced by F. Matthias Alexander and included him as a character in "Eyeless in Gaza".
Middle years.
During the First World War, Huxley spent much of his time at Garsington Manor, home of Lady Ottoline Morrell, working as a farm labourer. Here he met several Bloomsbury figures including Bertrand Russell and Clive Bell. Later, in "Crome Yellow" (1921) he caricatured the Garsington lifestyle. In 1919 he married Maria Nys (10 September 1899 - 12 February 1955), a Belgian woman he met at Garsington. They had one child, Matthew Huxley (19 April 1920 - 10 February 2005), who had a career as an epidemiologist. The family lived in Italy part of the time in the 1920s, where Huxley would visit his friend D. H. Lawrence. Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1933).
In 1937, Huxley moved to Hollywood, California with his wife Maria, son Matthew, and friend Gerald Heard. He lived in the U.S., mainly in southern California, until his death, but also for a time in Taos, New Mexico, where he wrote "Ends and Means" (published in 1937). In this work he examines the fact that although most people in modern civilization agree that they want a world of "liberty, peace, justice, and brotherly love", they have not been able to agree on how to achieve it.
Heard introduced Huxley to Vedanta (Veda-Centric Hinduism), meditation, and vegetarianism through the principle of ahimsa. In 1938 Huxley befriended J. Krishnamurti, whose teachings he greatly admired. He also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle. Not long after, Huxley wrote his book on widely held spiritual values and ideas, "The Perennial Philosophy", which discussed the teachings of renowned mystics of the world.
Huxley became a close friend of Remsen Bird, president of Occidental College. He spent much time at the college, which is in the Eagle Rock neighborhood of Los Angeles. The college appears as "Tarzana College" in his satirical novel "After Many a Summer" (1939). The novel won Huxley that year's James Tait Black Memorial Prize for fiction. Huxley also incorporated Bird into the novel.
During this period Huxley earned some Hollywood income as a writer. In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for "Madame Curie" which was originally to star Greta Garbo and be directed by George Cukor. (The film was eventually filmed by MGM in 1943 with a different director and stars.) Huxley received screen credit for "Pride and Prejudice" (1940) and was paid for his work on a number of other films, including "Jane Eyre" (1944).
However, his experience in Hollywood was not a success. When he wrote a synopsis of "Alice in Wonderland", Walt Disney rejected it on the grounds that "he could only understand every third word". Huxley's leisurely development of ideas, it seemed, was not suitable for the movie moguls, who demanded fast, dynamic dialogue above all else.
On 21 October 1949, Huxley wrote to George Orwell, author of "Nineteen Eighty-Four", congratulating Orwell on "how fine and how profoundly important the book is". In his letter to Orwell, he predicted: "Within the next generation I believe that the world's leaders will discover that infant conditioning and narco-hypnosis are more efficient, as instruments of government, than clubs and prisons, and that the lust for power can be just as completely satisfied by suggesting people into loving their servitude as by flogging them and kicking them into obedience."
Post-war.
After the Second World War Huxley applied for United States citizenship, but his application was continuously deferred on the grounds that he would not say he would take up arms to defend the U.S., so he withdrew it. Nevertheless, he remained in the country, and in 1959 he turned down an offer of a Knight Bachelor by the Macmillan government. During the 1950s Huxley's interest in the field of psychical research grew keener, and his later works are strongly influenced by both mysticism and his experiences with psychedelic drugs.
In October 1930, the occultist Aleister Crowley dined with Huxley in Berlin, and to this day rumours persist that Crowley introduced Huxley to peyote on that occasion. He was introduced to mescaline (considered to be the key active ingredient of peyote) by the psychiatrist Humphry Osmond in 1953. Through Dr. Osmond, Huxley met millionaire Alfred Matthew Hubbard who would deal with LSD on a wholesale basis. On 24 December 1955, Huxley took his first dose of LSD. Indeed, Huxley was a pioneer of self-directed psychedelic drug use "in a search for enlightenment", famously taking 100 micrograms of LSD as he lay dying. His psychedelic drug experiences are described in the essays "The Doors of Perception" (the title deriving from some lines in the book "The Marriage of Heaven and Hell" by William Blake), and "Heaven and Hell". Some of his writings on psychedelics became frequent reading among early hippies. While living in Los Angeles, Huxley was a friend of Ray Bradbury. According to Sam Weller's biography of Bradbury, the latter was dissatisfied with Huxley, especially after Huxley encouraged Bradbury to take psychedelic drugs.
In 1955, Huxley's wife, Maria, died of breast cancer. In 1956 he married Laura Archera (1911–2007), also an author. She wrote "This Timeless Moment", a biography of Huxley. In 1960 Huxley himself was diagnosed with cancer, and in the years that followed, with his health deteriorating, he wrote the Utopian novel "Island", and gave lectures on "Human Potentialities" at the Esalen institute, which were fundamental to the forming of the Human Potential Movement.
On his deathbed, unable to speak, Huxley made a written request to his wife for "LSD, 100 µg, intramuscular". According to her account of his death, in "This Timeless Moment", she obliged with an injection at 11:45 am and another a couple of hours later. He died at 5:21 pm on 22 November 1963, aged 69. Huxley's ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, a village near Guildford, Surrey, England.
Media coverage of his death was overshadowed by the assassination of President John F. Kennedy, on the same day, as was the death of the Irish author C. S. Lewis. This coincidence was the inspiration for Peter Kreeft's book '.
Huxley's only child, Matthew Huxley, was also an author, as well as an educator, anthropologist, and prominent epidemiologist. Aldous Huxley is also survived by two grandchildren.
Association with Vedanta.
Beginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda. Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices.
In 1944 Huxley wrote the introduction to the "Bhagavad Gita: The Song of God", translated by Swami Prabhavanada and Christopher Isherwood, which was published by The Vedanta Society of Southern California.
From 1941 through 1960 Huxley contributed 48 articles to "Vedanta and the West", published by the Society. He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962.
Huxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples. Two of those lectures have been released on CD: "Knowledge and Understanding" and "Who Are We" from 1955.
After the publication of "The Doors of Perception", Huxley and the Swami disagreed about the meaning and importance of the LSD drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the Society's journal, lecture at the temple, and attend social functions.
Literary themes.
"Crome Yellow" (1921) attacks Victorian and Edwardian social principles which led to World War I and its terrible aftermath. Together with Huxley's second novel, "Antic Hay" (1923), the book expresses much of the mood of disenchantment of the early 1920s. It was intended to reflect, as Huxley stated in a letter to his father, "the life and opinions of an age which has seen the violent disruption of almost all the standards, conventions and values current in the present epoch."
Huxley's reputation for iconoclasm and emancipation grew. He was condemned for his explicit discussion of sex and free thought in his fiction. "Antic Hay", for example, was burned in Cairo and in the years that followed many of Huxley's books were received with disapproval or banned at one time or another. The exclusion of "Brave New World", "Point Counter Point" and "Island" from "Time" magazine's Best 100 novels list in 2006 created an uproar.
Huxley, however, said that a novel should be full of interesting opinions and arresting ideas, describing his aim as a novelist as being 'to arrive, technically, at a perfect fusion of the novel and the essay'; and with "Point Counter Point" (1928), Huxley wrote his first true 'novel of ideas', the type of thought-provoking fiction with which he is now associated.
One of his main ideas was pessimism about the cultural future of society, a pessimism which sprang largely from his visit to the United States between September 1925 and June 1926. He recounted his experiences in "Jesting Pilate" (1926): "The thing which is happening in America is a reevaluation of values, a radical alteration (for the worse) of established standards", and it was soon after this visit that he conceived the idea of writing a satire of what he had encountered.
"Brave New World" (1932) as well as "Island" (1962) form the cornerstone of Huxley's damning indictment of commercialism based upon goods generally manufactured from other countries. Indeed also, "Brave New World" (along with Orwell's "Nineteen Eighty-Four" and Yevgeni Zamyatin's "We") helped form the anti-utopian or dystopian tradition in literature and has become synonymous with a future world in which the human spirit is subject to conditioning and control. "Island" acts as an antonym to "Brave New World"; it is described as "one of the truly great philosophical novels".
He devoted his time at his small house at Llano in the Mojave Desert to a life of contemplation, mysticism, and experimentation with hallucinogenic drugs. His suggestions in The Doors of Perception (1954) that mescaline and lysergic acid were 'drugs of unique distinction' which should be exploited for the 'supernaturally brilliant' visionary experience they offered provoked even more outrage than his passionate defense of the Bates method in "The Art of Seeing" (1942). However, the book went on to become a cult text in the psychedelic 1960s, and inspire the name of the rock band The Doors (although it was originally derived from William Blake's "Marriage of Heaven and Hell"). Huxley also appears on the sleeve of The Beatles' landmark 1967 album "Sgt. Pepper's Lonely Hearts Club Band".
Eyesight.
With respect to details about the true quality of Huxley’s eyesight at specific points in his life, there are differing accounts. Around 1939, Huxley encountered the Bates Method for better eyesight, and a teacher, Margaret Corbett, who was able to teach him in the method. In 1940, Huxley relocated from Hollywood to a "ranchito" in the high desert hamlet of Llano, California in northernmost Los Angeles County. Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert. He reported that for the first time in over 25 years, he was able to read without glasses and without strain. He even tried driving a car along the dirt road beside the ranch. He wrote a book about his successes with the Bates Method, "The Art of Seeing" which was published in 1942 (US), 1943 (UK). It was from this period, with the publication of the generally disputed theories contained in the latter book, that a growing degree of popular controversy arose over the subject of Huxley’s eyesight.
"Then suddenly he faltered—and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonizing moment."
"...Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision. For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens..."
"The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable."
Nevertheless, the topic of Huxley’s eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.
Awards.
In 1959 Aldous Huxley received the American Academy of Arts and Letters Award of Merit for the novel "Brave New World". He received the James Tait Black Memorial Prize in 1939 for "After Many a Summer Dies the Swan".
In 1962, Huxley was awarded the Companion of Literature by the Royal Society of Literature.
Films.
Notable works include the original screenplay for Disney's animated "Alice in Wonderland" (which was rejected because it was too literary), two productions of "Brave New World", one of "Point Counter Point", one of "Eyeless in Gaza", and one of "Ape and Essence". He was a credited screenwriter for "Pride and Prejudice" (1940), co-authored the screenplay for "Jane Eyre" (1944) with John Houseman, "A Woman's Vengeance" (1947), and contributed to the screenplays of "Madame Curie" (1943) and "Alice in Wonderland" (1951) without credit.
Director Ken Russell's 1971 film "The Devils", starring Vanessa Redgrave and Oliver Reed, was adapted from Huxley's "The Devils of Loudun". A made-for-television adaptation of "Brave New World" was made in 1990.
---END.OF.DOCUMENT---

Aberdeen (disambiguation).
Aberdeen is a city in Scotland, United Kingdom.
---END.OF.DOCUMENT---

Algae.
Algae (or; singular "alga", Latin for "seaweed") are a large and diverse group of simple, typically autotrophic organisms, ranging from unicellular to multicellular forms. The largest and most complex marine forms are called seaweeds. They are photosynthetic, like plants, and "simple" because they lack the many distinct organs found in land plants.
Though the prokaryotic "Cyanobacteria" (commonly referred to as blue-green algae) were traditionally included as "algae" in older textbooks, many modern sources regard this as outdated as they are now considered to be closely related to bacteria. The term "algae" is now restricted to eukaryotic organisms. All true algae therefore have a nucleus enclosed within a membrane and chloroplasts bound in one or more membranes. Algae constitute a paraphyletic and polyphyletic group, as they do not include all the descendants of the last universal ancestor nor do they all descend from a common algal ancestor, although their chloroplasts seem to have a single origin. Diatoms are also examples of algae.
Algae lack the various structures that characterize land plants, such as phyllids (leaves) and rhizoids in nonvascular plants, or leaves, roots, and other organs that are found in tracheophytes (vascular plants). Many are photoautotrophic, although some groups contain members that are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy. Some unicellular species rely entirely on external energy sources and have limited or no photosynthetic apparatus.
Nearly all algae have photosynthetic machinery ultimately derived from the Cyanobacteria, and so produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria. Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago.
The first alga to have its genome sequenced was "Cyanidioschyzon merolae".
Etymology and study.
The singular "alga" is the Latin word for a particular seaweed and retains that meaning in English. The etymology is obscure. Although some speculate that it is related to Latin "algēre", "be cold", there is no known reason to associate seaweed with temperature. A more likely source is "alliga", "binding, entwining." Since Algae has become a biological classification, alga can also mean one classification under Algae, parallel to a fungus being a species of fungi, a plant being a species of plant, and so on.
The ancient Greek word for seaweed was "φῦκος" (fūkos or phykos), which could mean either the seaweed, probably Red Algae, or a red dye derived from it. The Latinization, "fūcus", meant primarily the cosmetic rouge. The etymology is uncertain, but a strong candidate has long been some word related to the Biblical "פוך" (pūk), "paint" (if not that word itself), a cosmetic eye-shadow used by the ancient Egyptians and other inhabitants of the eastern Mediterranean. It could be any color: black, red, green, blue.
Accordingly the modern study of marine and freshwater algae is called either phycology or algology. The name Fucus appears in a number of taxa.
Classification.
While "Cyanobacteria" have been traditionally included among the Algae, recent works usually exclude them due to large differences such as the lack of membrane-bound organelles, the presence of a single circular chromosome, the presence of peptidoglycan in the cell walls, and ribosomes different in size and content from those of the Eukaryotes. Rather than in chloroplasts, they conduct photosynthesis on specialized infolded cytoplasmic membranes called thylakoid membranes. Therefore, they differ significantly from the Algae despite occupying similar ecological niches.
By modern definitions Algae are Eukaryotes and conduct photosynthesis within membrane-bound organelles called chloroplasts. Chloroplasts contain circular DNA and are similar in structure to Cyanobacteria, presumably representing reduced cyanobacterial endosymbionts. The exact nature of the chloroplasts is different among the different lines of Algae, reflecting different endosymbiotic events. The table below describes the composition of the three major groups of Algae. Their lineage relationships are shown in the figure in the upper right. Many of these groups contain some members that are no longer photosynthetic. Some retain plastids, but not chloroplasts, while others have lost plastids entirely.
W.H.Harvey (1811—1866) was the first to divide the Algae into four divisions based on their pigmentation. This is the first use of a biochemical criterion in plant systematics. Harvey's four divisions are: Red Algae (Rhodophyta), Brown Algae (Heteromontophyta), Green Algae (Chlorophyta) and Diatomaceae.
Relationship to higher plants.
The first plants on earth evolved from shallow freshwater algae much like "Chara" some 400 million years ago. These probably had an isomorphic alternation of generations and were probably filamentous. Fossils of isolated land plant spores suggest land plants may have been around as long as 475 million years ago.
Morphology.
A range of algal morphologies are exhibited, and convergence of features in unrelated groups is common. The only groups to exhibit three dimensional multicellular thalli are the reds and browns, and some chlorophytes. Apical growth is constrained to subsets of these groups: the florideophyte reds, various browns, and the charophytes. The form of charophytes is quite different to those of reds and browns, because have distinct nodes, separated by internode 'stems'; whorls of branches reminiscent of the horsetails occur at the nodes. Conceptacles are another polyphyletic trait; they appear in the coralline algae and the Hildenbrandiales, as well as the browns.
Most of the simpler algae are unicellular flagellates or amoeboids, but colonial and non-motile forms have developed independently among several of the groups. Some of the more common organizational levels, more than one of which may occur in the life cycle of a species, are
In three lines even higher levels of organization have been reached, with full tissue differentiation. These are the brown algae,—some of which may reach 50 m in length (kelps)—the red algae, and the green algae. The most complex forms are found among the green algae (see Charales and Charophyta), in a lineage that eventually led to the higher land plants. The point where these non-algal plants begin and algae stop is usually taken to be the presence of reproductive organs with protective cell layers, a characteristic not found in the other alga groups.
Symbiotic algae.
Some species of algae form symbiotic relationships with other organisms. In these symbioses, the algae supply photosynthates (organic substances) to the host organism providing protection to the algal cells. The host organism derives some or all of its energy requirements from the algae. Examples are as follows.
Lichens.
"Lichens" are defined by the International Association for Lichenology to be "an association of a fungus and a photosynthetic symbiont resulting in a stable vegetative body having a specific structure." The fungi, or mycobionts, are from the Ascomycota with a few from the Basidiomycota. They are not found alone in nature but when they began to associate is not known. One mycobiont associates with the same phycobiont species, rarely two, from the Green Algae, except that alternatively the mycobiont may associate with the same species of Cyanobacteria (hence "photobiont" is the more accurate term). A photobiont may be associated with many specific mycobionts or live independently; accordingly, lichens are named and classified as fungal species. The association is termed a morphogenesis because the lichen has a form and capabilities not possessed by the symbiont species alone (they can be experimentally isolated). It is possible that the photobiont triggers otherwise latent genes in the mycobiont.
Coral reefs.
Coral reefs are accumulated from the calcareous exoskeletons of marine invertebrates of the Scleractinia order; i.e., the Stony Corals. As animals they metabolize sugar and oxygen to obtain energy for their cell-building processes, including secretion of the exoskeleton, with water and carbon dioxide as byproducts. As the reef is the result of a favorable equilibrium between construction by the corals and destruction by marine erosion, the rate at which metabolism can proceed determines the growth or deterioration of the reef.
Algae of the Dinoflagellate phylum are often endosymbionts in the cells of marine invertebrates, where they accelerate host-cell metabolism by generating immediately available sugar and oxygen through photosynthesis using incident light and the carbon dioxide produced in the host. Endosymbiont algae in the Stony Corals are described by the term zooxanthellae, with the host Stony Corals called on that account hermatypic corals, which although not a taxon are not in healthy condition without their endosymbionts. Zooxanthellae belong almost entirely to the genus "Symbiodinium". The loss of "Symbiodinium" from the host is known as coral bleaching, a condition which unless corrected leads to the deterioration and loss of the reef.
Sea sponges.
Green Algae live close to the surface of some sponges, for example, breadcrumb sponge ("Halichondria panicea"). The alga is thus protected from predators; the sponge is provided with oxygen and sugars which can account for 50 to 80% of sponge growth in some species.
Life-cycle.
Rhodophyta, Chlorophyta and Heterokontophyta, the three main algal Phyla, have life-cycles which show tremendous variation with considerable complexity. In general there is an asexual phase where the seaweed's cells are diploid, a sexual phase where the cells are haploid followed by fusion of the male and female gametes. Asexual reproduction is advantageous in that it permits efficient population increases, but less variation is possible. Sexual reproduction allows more variation, but is more costly. Often there is no strict alternation between the sporophyte and also because there is often an asexual phase, which could include the fragmentation of the thallus.
Numbers.
The "Algal Collection of the U.S. National Herbarium" (located in the National Museum of Natural History) consists of approximately 320500 dried specimens, which, although not exhaustive (no exhaustive collection exists), gives an idea of the order of magnitude of the number of algal species (that number remains unknown). Estimates vary widely. For example, according to one standard textbook, in the British Isles the "UK Biodiversity Steering Group Report" estimated there to be 20000 algal species in the UK. Another checklist reports only about 5000 species. Regarding the difference of about 15000 species, the text concludes: "It will require many detailed field surveys before it is possible to provide a reliable estimate of the total number of species..."
Regional and group estimates have been made as well: 5000—5500 species of Red Algae worldwide, "some 1300 in Australian Seas," 400 seaweed species for the western coastline of South Africa, 669 marine species from California (U.S.A.), 642 in the check-list of Britain and Ireland, and so on, but lacking any scientific basis or reliable sources, these numbers have no more credibility than the British ones mentioned above. Most estimates also omit the microscopic Algae, such as the phytoplankta, entirely.
Distribution.
The topic of distribution of algal species has been fairly well studied since the founding of phytogeography in the mid-19th century AD. Algae spread mainly by the dispersal of spores analogously to the dispersal of Plantae by seeds and spores. Spores are everywhere in all parts of the Earth: the waters fresh and marine, the atmosphere, free-floating and in precipitation or mixed with dust, the humus and in other organisms, such as humans. Whether a spore is to grow into an organism depends on the combination of the species and the environmental conditions.
The spores of fresh-water Algae are dispersed mainly by running water and wind, as well as by living carriers. The bodies of water into which they are transported are chemically selective. Marine spores are spread by currents. Ocean water is temperature selective, resulting in phytogeographic zones, regions and provinces.
To some degree the distribution of Algae is subject to floristic discontinuities caused by geographical features, such as Antarctica, long distances of ocean or general land masses. It is therefore possible to identify species occurring by locality, such as "Pacific Algae" or "North Sea Algae". When they occur out of their localities, it is usually possible to hypothesize a transport mechanism, such as the hulls of ships. For example, "Ulva reticulata" and "Ulva fasciata" travelled from the mainland to Hawaii in this manner.
Mapping is possible for select species only: "there are many valid examples of confined distribution patterns." For example, "Clathromorphum" is an arctic genus and is not mapped far south of there. On the other hand, scientists regard the overall data as insufficient due to the "difficulties of undertaking such studies."
Locations.
Algae are prominent in bodies of water, common in terrestrial environments and are found in unusual environments, such as on snow and on ice. Seaweeds grow mostly in shallow marine waters, under; however some have been recorded to a depth of
The various sorts of algae play significant roles in aquatic ecology. Microscopic forms that live suspended in the water column (phytoplankton) provide the food base for most marine food chains. In very high densities (algal blooms) these algae may discolor the water and outcompete, poison, or asphyxiate other life forms.
Algae are variously sensitive to different factors, which has made them useful as biological indicators in the Ballantine Scale and its modification.
Agar.
Agar, an Algae derivative, has a number of commercial uses.
Alginates.
Between 100,000 and 170,000 wet tons of "Macrocystis" are harvested annually in California for alginate extraction and abalone feed.
Energy source.
To be competitive and independent from fluctuating support from (local) policy on the long run, biofuels should equal or beat the cost level of fossil fuels. Here, algae based fuels hold great promise, directly related to the potential to produce more biomass per unit area in a year than any other form of biomass. The break-even point for algae-based biofuels should be within reach in about ten years.
Fertilizer.
For centuries seaweed has been used as a fertilizer; George Owen of Henllys writing in the 16th century referring to drift weed in South Wales:This kind of ore they often gather and lay on great heapes, where it heteth and rotteth, and will have a strong and loathsome smell; when being so rotten they cast on the land, as they do their muck, and thereof springeth good corn, especially barley... After spring-tydes or great rigs of the sea, they fetch it in sacks on horse backes, and carie the same three, four, or five miles, and cast it on the lande, which doth very much better the ground for corn and grass.
Today Algae are used by humans in many ways; for example, as fertilizers, soil conditioners and livestock feed. Aquatic and microscopic species are cultured in clear tanks or ponds and are either harvested or used to treat effluents pumped through the ponds. Algaculture on a large scale is an important type of aquaculture in some places. Maerl is commonly used as a soil conditioner.
Nutrition.
Naturally growing seaweeds are an important source of food, especially in Asia. They provide many vitamins including: A, B1, B2, B6, niacin and C, and are rich in iodine, potassium, iron, magnesium and calcium. In addition commercially cultivated microalgae, including both Algae and Cyanobacteria, are marketed as nutritional supplements, such as Spirulina, Chlorella and the Vitamin-C supplement, Dunaliella, high in beta-carotene.
Algae are national foods of many nations: China consumes more than 70 species, including "fat choy", a cyanobacterium considered a vegetable; Japan, over 20 species; Ireland, dulse; Chile, cochayuyo. Laver is used to make "laver bread" in Wales where it is known as "bara lawr"; in Korea, gim; in Japan, nori and aonori. It is also used along the west coast of North America from California to British Columbia, in Hawaii and by the Māori of New Zealand. Sea lettuce and badderlocks are a salad ingredient in Scotland, Ireland, Greenland and Iceland.
The oils from some Algae have high levels of unsaturated fatty acids. For example, "Parietochloris incisa" is very high in arachidonic acid, where it reaches up to 47% of the triglyceride pool. Some varieties of Algae favored by vegetarianism and veganism contain the long-chain, essential omega-3 fatty acids, Docosahexaenoic acid (DHA) and Eicosapentaenoic acid (EPA), in addition to vitamin B12. The vitamin B12 in algae is not biologically active. Fish oil contains the omega-3 fatty acids, but the original source is algae (microalgae in particular), which are eaten by marine life such as copepods and are passed up the food chain. Algae has emerged in recent years as a popular source of omega-3 fatty acids for vegetarians who cannot get long-chain EPA and DHA from other vegetarian sources such as flaxseed oil, which only contains the short-chain Alpha-Linolenic acid (ALA).
Pigments.
The natural pigments produced by algae can be used as an alternative to chemical dyes and coloring agents.
Stabilizing substances.
Carrageenan, from the red alga "Chondrus crispus", is used as a stabiliser in milk products.
---END.OF.DOCUMENT---

Analysis of variance.
In statistics, analysis of variance (ANOVA) is a collection of statistical models, and their associated procedures, in which the observed variance is partitioned into components due to different explanatory variables. In its simplest form ANOVA gives a statistical test of whether the means of several groups are all equal, and therefore generalizes Student's two-sample "t"-test to more than two groups. ANOVAs are helpful because they possess a certain advantage over a two-sample t-test. Doing multiple two-sample t-tests would result in a largely increased chance of committing a type I error. For this reason, ANOVAs are useful in comparing three or more means.
Fixed-effects models (Model 1).
The fixed-effects model of analysis of variance applies to situations in which the experimenter applies several treatments to the subjects of the experiment to see if the response variable values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.
Random-effects models (Model 2).
Random effects models are used when the treatments are not fixed. This occurs when the various treatments (also known as factor levels) are sampled from a larger population. Because the treatments themselves are random variables, some assumptions and the method of contrasting the treatments differ from ANOVA model 1.
Most random-effects or mixed-effects models are not concerned with making inferences concerning the particular sampled factors. For example, consider a large manufacturing plant in which many machines produce the same product. The statistician studying this plant would have very little interest in comparing the three particular machines to each other. Rather, inferences that can be made for "all" machines are of interest, such as their variability and the mean.
Assumptions of anova.
There are several approaches to the analysis of variance.
A model often presented in textbooks.
Levene's test for homogeneity of variances is typically used to examine the plausibility of homoscedasticity. The Kolmogorov–Smirnov or the Shapiro–Wilk test may be used to examine normality.
When used in the analysis of variance to test the hypothesis that all treatments have exactly the same effect, the F-test is robust (Ferguson & Takane, 2005, pp. 261–2).
The Kruskal–Wallis test is a nonparametric alternative which does not rely on an assumption of normality. And the Friedman test is the nonparametric alternative for a one way repeated measures ANOVA.
The separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors are independent and
Randomization-based analysis.
In a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce and Ronald A. Fisher. This design-based analysis was advocated and developed by Oscar Kempthorne at Iowa State University. Kempthorne and his students make an assumption of "unit treatment additivity", which is discussed in the books of Kempthorne and David R. Cox.
Unit treatment additivity.
In its simplest form, the assumption of treatment unit additivity states that the observed response formula_2 from experimental unit formula_3 when receiving treatment formula_4 can be written as the sum formula_5. The assumption of unit treatment addivity implies that every treatment have exactly the same effect on every experiment unit. The assumption of unit treatment additivity is a hypothesis which is not directly falsifiable, according to Cox and Kempthorne.
However, many consequences of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of treatment additivity implies that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit treatment additivity is that the variance is constant.
The property of unit treatment additivity is not invariant under a change of scale, so statisticians often use transformations to achieve unit treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be tranformed to stabilize the variance. Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.
The assumption of unit treatment additivity was enunciated in experimental design by Kempthorne and Cox. Kempthorne's use of unit treatment additivity and randomization is similar to the design-based analysis of finite population survey sampling.
Derived linear model.
Kempthorne uses the randomization-distribution and the assumption of "unit treatment additivity" to produce a "derived linear model", very similar to the textbook model discussed previously.
The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies by Kempthorne and his students (Hinkelmann and Kempthorne). However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations (Hinkelmann and Kempthorne, volume one, chapter 7; Bailey chapter 1.14). In the randomization-based analysis, there is "no assumption" of a "normal" distribution and certainly "no assumption" of "independence". On the contrary, "the observations are dependent"!
The randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time. Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.
Statistical models for observational data.
However, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization. For observational data, the derivation of confidence intervals must use "subjective" models, as emphasized by Ronald A. Fisher and his followers. In practice, the estimates of treatment-effects from observational studies generally are often inconsistent (Freedman). In practice, "statistical models" and observational data are useful for suggesting hypothesis that should be treated very cautiously by the public (Freedman).
Partitioning of the sum of squares.
The fundamental technique is a partitioning of the total sum of squares (abbreviated SS) into components related to the effects used in the model. For example, we show the model for a simplified ANOVA with one type of treatment at different levels.
So, the number of degrees of freedom (abbreviated df) can be partitioned in a similar way and specifies the chi-square distribution which describes the associated sums of squares.
See also Lack-of-fit sum of squares.
The F-test.
The F-test is used for comparisons of the components of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic
to the F-distribution with "I" − 1,"n"T − "I" degrees of freedom. Using the F-distribution is a natural candidate because the test statistic is the quotient of two mean sums of squares which have a chi-square distribution.
ANOVA on ranks.
When the data do not meet the assumptions of normality, the suggestion has arisen to replace each original data value by its rank (from 1 for the smallest to N for the largest), then run a standard ANOVA calculation on the rank-transformed data. Conover and Iman (1981) provided a review of the four main types of rank transformations. Commercial statistical software packages (e.g., SAS, 1985, 1987, 2008) followed with recommendations to data analysts to run their data sets through a ranking procedure (e.g., PROC RANK) prior to conducting standard analyses using parametric procedures.
This rank-based procedure has been recommended as being robust to non-normal errors, resistant to outliers, and highly efficient for many distributions. It may result in a known statistic (e.g., Wilcoxon Rank-Sum / Mann-Whitney U), and indeed provide the desired robustness and increased statistical power that is sought. For example, Monte Carlo studies have shown that the rank transformation in the two independent samples t test layout can be successfully extended to the one-way independent samples ANOVA, as well as the two independent samples multivariate Hotelling's T2 layouts (Nanna, 2002).
Conducting factorial ANOVA on the ranks of original scores has also been suggested (Conover & Iman, 1976, Iman, 1974, and Iman & Conover, 1976). However, Monte Carlo studies by Sawilowsky (1985a; 1989 et al.; 1990) and Blair, Sawilowsky, and Higgins (1987), and subsequent asymptotic studies (e.g. Thompson & Ammann, 1989; "there exist values for the main effects such that, under the null hypothesis of no interaction, the expected value of the rank transform test statistic goes to infinity as the sample size increases," Thompson, 1991, p. 697), found that the rank transformation is inappropriate for testing interaction effects in a 4x3 and a 2x2x2 factorial design. As the number of effects (i.e., main, interaction) become non-null, and as the magnitude of the non-null effects increase, there is an increase in Type I error, resulting in a complete failure of the statistic with as high as a 100% probability of making a false positive decision. Similarly, Blair and Higgins (1985) found that the rank transformation increasingly fails in the two dependent samples layout as the correlation between pretest and posttest scores increase. Headrick (1997) discovered the Type I error rate problem was exacerbated in the context of Analysis of Covariance, particularly as the correlation between the covariate and the dependent variable increased. For a review of the properties of the rank transformation in designed experiments see Sawilowsky (2000).
A variant of rank-transformation is 'quantile normalization' in which a further transformation is applied to the ranks such that the resulting values have some defined distribution (often a normal distribution with a specified mean and variance). Further analyses of quantile-normalized data may then assume that distribution to compute significance values. However, two specific types of secondary transformations, the random normal scores and expected normal scores transformation, have been shown to greatly inflate Type I errors and severely reduce statistical power (Sawilowsky, 1985a, 1985b).
Effect size measures.
Several standardized measures of effect are used within the context of ANOVA to describe the degree of relationship between a predictor or set of predictors and the dependent variable. Effect size estimates are reported to allow researchers to compare findings in studies and across disciplines. Common effect size estimates reported in bivariate (e.g. ANOVA) and multivariate (MANOVA, ANCOVA, Multiple Discriminant Analysis) statistical analysis includes eta-squared, partial eta-squared, omega, and intercorrelation (Strang, 2009).
Eta-squared describes the ratio of variance explained in the dependent variable by a predictor while controlling for other predictors. Eta-squared is a biased estimator of the variance explained by the model in the population (it only estimates effect size in the sample). On average it overestimates the variance explained in the population. As the sample size gets larger the amount of bias gets smaller. It is, however, an easily calculated estimator of the proportion of the variance in a population explained by the treatment. Note that earlier versions of statistical software (such as SPSS) incorrectly reports Partial eta squared under the misleading title "Eta squared".
Partial eta-squared describes the "proportion of total variation attributable to the factor, partialling out (excluding) other factors from the total nonerror variation" (Pierce, Block & Aguinis, 2004, p. 918). Partial eta squared is normally higher than eta squared (except in simple one-factor models).
The generally accepted regression benchmark for effect size comes from (Cohen, 1992; 1988): 0.20 is a minimal solution (but significant in social science research); 0.50 is a medium effect; anything equal to or greater than 0.80 is a large effect size (Keppel & Wickens, 2004; Cohen, 1992).
Because this common interpretation of effect size has been repeated from Cohen (1988) over the years with no change or comment to validity for contemporary experimental research, it is questionable outside of psychological/behavioural studies, and more so questionable even then without a full understanding of the limitations ascribed by Cohen. Note: The use of specific partial eta-square values for large medium or small as a "rule of thumb" should be avoided.
Nevertheless, alternative rules of thumb have emerged in certain disciplines: Small = 0.01; medium = 0.06; large = 0.14 (Kittler, Menard & Phillips, 2007).
This measure of effect size is frequently encountered when performing power analysis calculations. Conceptually it represents the square root of variance explained over variance not explained.
Follow up tests.
A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses.
Follow up tests are often distinguished in terms of whether they are planned (a priori) or post hoc. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.
Post hoc tests such as Tukey's range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors.
Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has at two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.
Power analysis.
Power analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and alpha level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis.
Examples.
In a first experiment, Group A is given vodka, Group B is given gin, and Group C is given a placebo. All groups are then tested with a memory task. A one-way ANOVA can be used to assess the effect of the various treatments (that is, the vodka, gin, and placebo).
In a second experiment, Group A is given vodka and tested on a memory task. The same group is allowed a rest period of five days and then the experiment is repeated with gin. The procedure is repeated using a placebo. A one-way ANOVA with repeated measures can be used to assess the effect of the vodka versus the impact of the placebo.
Each group is then tested on a memory task. The advantage of this design is that multiple variables can be tested at the same time instead of running two different experiments. Also, the experiment can determine whether one variable affects the other variable (known as interaction effects). A factorial ANOVA (2×2) can be used to assess the effect of expecting vodka or the placebo and the actual reception of either.
History.
The analysis of variance was used informally by researchers in the 1800s using least squares. In physics and psychology, researchers included a term for the operator-effect, the influence of a particular person on measurements, according to Stephen Stigler's histories.
In its modern form, the analysis of variance was one of the many important statistical innovations of Ronald A. Fisher. Fisher proposed a formal analysis of variance in his 1918 paper The Correlation Between Relatives on the Supposition of Mendelian Inheritance. His first application of the analysis of variance was published in 1921. Analysis of variance became widely known after being included in Fisher's 1925 book "Statistical Methods for Research Workers".
---END.OF.DOCUMENT---

Alkane.
Alkanes, also known as paraffins, are chemical compounds that consist only of the elements carbon (C) and hydrogen (H) (i.e., hydrocarbons), wherein these atoms are linked together exclusively by single bonds (i.e., they are saturated compounds) without any cyclic structure (i.e. loops). Alkanes belong to a homologous series of organic compounds in which the members differ by a constant relative molecular mass of 14.
Each carbon atom must have 4 bonds (either C-H or C-C bonds), and each hydrogen atom must be joined to a carbon atom (H-C bonds). A series of linked carbon atoms is known as the carbon skeleton or carbon backbone. In general, the number of carbon atoms is often used to define the size of the alkane (e.g., C2-alkane).
An alkyl group is a functional group or side-chain that, like an alkane, consists solely of singly-bonded carbon and hydrogen atoms, for example a methyl or ethyl group.
Saturated hydrocarbons can be linear (general formula) wherein the carbon atoms are joined in a snake-like structure, branched (general formula, "n" > 3) wherein the carbon backbone splits off in one or more directions, or cyclic (general formula, "n" > 2) wherein the carbon backbone is linked so as to form a loop. According to the definition by IUPAC, the former two are alkanes, whereas the third group is called cycloalkanes. Saturated hydrocarbons can also combine any of the linear, cyclic (e.g., polycyclic) and branching structures, and they are still alkanes (no general formula) as long as they are acyclic (i.e., having no loops).
The simplest possible alkane (the parent molecule) is methane, CH4. There is no limit to the number of carbon atoms that can be linked together, the only limitation being that the molecule is acyclic, is saturated, and is a hydrocarbon. Saturated oils and waxes are examples of larger alkanes where the number of carbons in the carbon backbone tends to be greater than 10.
Alkanes are not very reactive and have little biological activity. Alkanes can be viewed as a molecular tree upon which can be hung the interesting biologically active/reactive portions (functional groups) of the molecule.
Isomerism.
butane is the only C4H6 compound and has no isomer; tetrahedrane (not shown) is the only C4H4 compound and has also no isomer.
Branched alkanes can be chiral: 3-methylhexane and its higher homologues are chiral due to their stereogenic center at carbon atom number 3. Chiral alkanes are of certain importance in biochemistry, as they occur as sidechains in chlorophyll and tocopherol (vitamin E). Chiral alkanes can be resolved into their enantiomers by enantioselective chromatography.
In addition to these isomers, the chain of carbon atoms may form one or more loops. Such compounds are called cycloalkanes.
Nomenclature.
The IUPAC nomenclature (systematic way of naming compounds) for alkanes is based on identifying hydrocarbon chains. Unbranched, saturated hydrocarbon chains are named systematically with a Greek numerical prefix denoting the number of carbons and the suffix "-ane".
August Wilhelm von Hofmann suggested systematizing nomenclature by using the whole sequence of vowels a, e, i, o and u to create suffixes -ane, -ene, -ine (or -yne), -one, -une, for the hydrocarbons. The first three name hydrocarbons with single, double and triple bonds; "-one" represents a ketone; "-ol" represents an alcohol or OH group; "-oxy-" means an ether and refers to oxygen between two carbons, so that methoxy-methane is the IUPAC name for dimethyl ether.
It is difficult or impossible to find compounds with more than one IUPAC name. This is because shorter chains attached to longer chains are prefixes and the convention includes brackets. Numbers in the name, referring to which carbon a group is attached to, should be as low as possible, so that 1- is implied and usually omitted from names of organic compounds with only one side-group; "1-" is implied in Nitro-octane. Symmetric compounds will have two ways of arriving at the same name.
Linear alkanes.
Straight-chain alkanes are sometimes indicated by the prefix "n-" (for "normal") where a non-linear isomer exists. Although this is not strictly necessary, the usage is still common in cases where there is an important difference in properties between the straight-chain and branched-chain isomers, e.g., "n"-hexane or 2- or 3-methylpentane.
These names were derived from methanol, ether, propionic acid and butyric acid, respectively. Alkanes with five or more carbon atoms are named by adding the suffix -ane to the appropriate numerical multiplier prefix
with elision of any terminal vowel ("-a" or "-o") from the basic numerical term. Hence, pentane, C5H12; hexane, C6H14; heptane, C7H16; octane, C8H18; etc. The prefix is generally Greek, with the exceptions of nonane which has a Latin prefix, and undecane and tridecane which have mixed-language prefixes. For a more complete list, see List of alkanes.
Branched alkanes.
Simple branched alkanes often have a common name using a prefix to distinguish them from linear alkanes, for example "n"-pentane, isopentane, and neopentane.
IUPAC naming conventions can be used to produce a systematic name.
Cyclic alkanes.
So-called cyclic alkanes are, in the technical sense, "not" alkanes, but cycloalkanes. They are hydrocarbons just like alkanes, but contain one or more rings.
Simple cycloalkanes have a prefix "cyclo-" to distinguish them from alkanes. Cycloalkanes are named as per their acyclic counterparts with respect to the number of carbon atoms, e.g., cyclopentane (C5H10) is a cycloalkane with 5 carbon atoms just like pentane (C5H12), but they are joined up in a five-membered ring. In a similar manner, propane and cyclopropane, butane and cyclobutane, etc.
Substituted cycloalkanes are named similar to substituted alkanes — the cycloalkane ring is stated, and the substituents are according to their position on the ring, with the numbering decided by Cahn-Ingold-Prelog rules.
Trivial names.
The trivial (non-systematic) name for alkanes is "paraffins." Together, alkanes are known as the "paraffin series". Trivial names for compounds are usually historical artifacts. They were coined before the development of systematic names, and have been retained due to familiar usage in industry. Cycloalkanes are also called naphthenes.
It is almost certain that the term paraffin stems from the petrochemical industry. Branched-chain alkanes are called "isoparaffins". The use of the term "paraffin" is a general term and often does not distinguish between a pure compounds and mixtures of isomers with the same chemical formula (i.e., like a chemical anagram), e.g., pentane and isopentane.
Boiling point.
Alkanes experience inter-molecular van der Waals forces. Stronger inter-molecular van der Waals forces give rise to greater boiling points of alkanes.
Under standard conditions, from CH4 to C4H10 alkanes are gaseous; from C5H12 to C17H36 they are liquids; and after C18H38 they are solids. As the boiling point of alkanes is primarily determined by weight, it should not be a surprise that the boiling point has almost a linear relationship with the size (molecular weight) of the molecule. As a rule of thumb, the boiling point rises 20 - 30 °C for each carbon added to the chain; this rule applies to other homologous series.
A straight-chain alkane will have a boiling point higher than a branched-chain alkane due to the greater surface area in contact, thus the greater van der Waals forces, between adjacent molecules. For example, compare isobutane (2-methylpropane) and n-butane (butane), which boil at -12 and 0 °C, and 2,2-dimethylbutane and 2,3-dimethylbutane which boil at 50 and 58 °C, respectively. For the latter case, two molecules 2,3-dimethylbutane can "lock" into each other better than the cross-shaped 2,2-dimethylbutane, hence the greater van der Waals forces.
On the other hand, cycloalkanes tend to have higher boiling points than their linear counterparts due to the locked conformations of the molecules, which give a plane of intermolecular contact.
Melting point.
The melting points of the alkanes follow a similar trend to boiling points for the same reason as outlined above. That is, (all other things being equal) the larger the molecule the higher the melting point. There is one significant difference between boiling points and melting points. Solids have more rigid and fixed structure than liquids. This rigid structure requires energy to break down. Thus the stronger better put together solid structures will require more energy to break apart. For alkanes, this can be seen from the graph above (i.e., the blue line). The odd-numbered alkanes have a lower trend in melting points than even numbered alkanes. This is because even numbered alkanes pack well in the solid phase, forming a well-organised structure, which requires more energy to break apart. The odd-number alkanes pack less well and so the "looser" organised solid packing structure requires less energy to break apart.
The melting points of branched-chain alkanes can be either higher or lower than those of the corresponding straight-chain alkanes, again depending on the ability of the alkane in question to packing well in the solid phase: This is particularly true for isoalkanes (2-methyl isomers), which often have melting points higher than those of the linear analogues.
Conductivity.
Alkanes do not conduct electricity, nor are they substantially polarized by an electric field. For this reason they do not form hydrogen bonds and are insoluble in polar solvents such as water. Since the hydrogen bonds between individual water molecules are aligned away from an alkane molecule, the coexistence of an alkane and water leads to an increase in molecular order (a reduction in entropy). As there is no significant bonding between water molecules and alkane molecules, the second law of thermodynamics suggests that this reduction in entropy should be minimised by minimising the contact between alkane and water: Alkanes are said to be hydrophobic in that they repel water.
Their solubility in nonpolar solvents is relatively good, a property that is called lipophilicity. Different alkanes are, for example, miscible in all proportions among themselves.
The density of the alkanes usually increases with increasing number of carbon atoms, but remains less than that of water. Hence, alkanes form the upper layer in an alkane-water mixture.
Molecular geometry===.
The molecular structure of the alkanes directly affects their physical and chemical characteristics. It is derived from the electron configuration of carbon, which has four valence electrons. The carbon atoms in alkanes are always sp3 hybridised, that is to say that the valence electrons are said to be in four equivalent orbitals derived from the combination of the 2s orbital and the three 2p orbitals. These orbitals, which have identical energies, are arranged spatially in the form of a tetrahedron, the angle of cos−1(−⅓) ≈ 109.47° between them.
Bond lengths and bond angles.
An alkane molecule has only C – H and C – C single bonds. The former result from the overlap of a sp³-orbital of carbon with the 1s-orbital of a hydrogen; the latter by the overlap of two sp³-orbitals on different carbon atoms. The bond lengths amount to 1.09×10−10 m for a C – H bond and 1.54×10−10 m for a C – C bond.
The spatial arrangement of the bonds is similar to that of the four sp³-orbitals—they are tetrahedrally arranged, with an angle of 109.47° between them. Structural formulae that represent the bonds as being at right angles to one another, while both common and useful, do not correspond with the reality.
Conformation.
The structural formula and the bond angles are not usually sufficient to completely describe the geometry of a molecule. There is a further degree of freedom for each carbon – carbon bond: the torsion angle between the atoms or groups bound to the atoms at each end of the bond. The spatial arrangement described by the torsion angles of the molecule is known as its conformation.
Ethane forms the simplest case for studying the conformation of alkanes, as there is only one C – C bond. If one looks down the axis of the C – C bond, one will see the so-called Newman projection. The hydrogen atoms on both the front and rear carbon atoms have an angle of 120° between them, resulting from the projection of the base of the tetrahedron onto a flat plane. However, the torsion angle between a given hydrogen atom attached to the front carbon and a given hydrogen atom attached to the rear carbon can vary freely between 0° and 360°. This is a consequence of the free rotation about a carbon – carbon single bond. Despite this apparent freedom, only two limiting conformations are important: eclipsed conformation and staggered conformation.
The two conformations, also known as rotamers, differ in energy: The staggered conformation is 12.6 kJ/mol lower in energy (more stable) than the eclipsed conformation (the least stable).
This difference in energy between the two conformations, known as the torsion energy, is low compared to the thermal energy of an ethane molecule at ambient temperature. There is constant rotation about the C-C bond. The time taken for an ethane molecule to pass from one staggered conformation to the next, equivalent to the rotation of one CH3-group by 120° relative to the other, is of the order of 10−11 seconds.
The case of higher alkanes is more complex but based on similar principles, with the antiperiplanar conformation always being the most favoured around each carbon-carbon bond. For this reason, alkanes are usually shown in a zigzag arrangement in diagrams or in models. The actual structure will always differ somewhat from these idealised forms, as the differences in energy between the conformations are small compared to the thermal energy of the molecules: Alkane molecules have no fixed structural form, whatever the models may suggest.
Spectroscopic properties.
Virtually all organic compounds contain carbon – carbon and carbon – hydrogen bonds, and so show some of the features of alkanes in their spectra. Alkanes are notable for having no other groups, and therefore for the "absence" of other characteristic spectroscopic features.
Infrared spectroscopy.
The carbon–hydrogen stretching mode gives a strong absorption between 2850 and 2960 cm−1, while the carbon–carbon stretching mode absorbs between 800 and 1300 cm−1. The carbon–hydrogen bending modes depend on the nature of the group: methyl groups show bands at 1450 cm−1 and 1375 cm−1, while methylene groups show bands at 1465 cm−1 and 1450 cm−1. Carbon chains with more than four carbon atoms show a weak absorption at around 725 cm−1.
NMR spectroscopy.
The proton resonances of alkanes are usually found at δH = 0.5 – 1.5. The carbon-13 resonances depend on the number of hydrogen atoms attached to the carbon: δC = 8 – 30 (primary, methyl, -CH3), 15 – 55 (secondary, methylene, -CH2-), 20 – 60 (tertiary, methyne, C-H) and quaternary. The carbon-13 resonance of quaternary carbon atoms is characteristically weak, due to the lack of Nuclear Overhauser effect and the long relaxation time, and can be missed in weak samples, or sample that have not been run for a sufficiently long time.
Mass spectrometry.
Alkanes have a high ionisation energy, and the molecular ion is usually weak. The fragmentation pattern can be difficult to interpret, but, in the case of branched chain alkanes, the carbon chain is preferentially cleaved at tertiary or quaternary carbons due to the relative stability of the resulting free radicals. The fragment resulting from the loss of a single methyl group (M−15) is often absent, and other fragment are often spaced by intervals of fourteen mass units, corresponding to sequential loss of CH2-groups.
Chemical properties.
In general, alkanes show a relatively low reactivity, because their C bonds are relatively stable and cannot be easily broken. Unlike most other organic compounds, they possess no functional groups.
They react only very poorly with ionic or other polar substances. The acid dissociation constant (pKa) values of all alkanes are above 60, hence they are practically inert to acids and bases (see: carbon acids). This inertness is the source of the term "paraffins" (with the meaning here of "lacking affinity"). In crude oil the alkane molecules have remained chemically unchanged for millions of years.
However redox reactions of alkanes, in particular with oxygen and the halogens, are possible as the carbon atoms are in a strongly reduced condition; in the case of methane, the lowest possible oxidation state for carbon (−4) is reached. Reaction with oxygen leads to combustion without any smoke; with halogens, substitution. In addition, alkanes have been shown to interact with, and bind to, certain transition metal complexes in (See: carbon-hydrogen bond activation).
Free radicals, molecules with unpaired electrons, play a large role in most reactions of alkanes, such as cracking and reformation where long-chain alkanes are converted into shorter-chain alkanes and straight-chain alkanes into branched-chain isomers.
In highly branched alkanes, the bond angle may differ significantly from the optimal value (109.5°) in order to allow the different groups sufficient space. This causes a tension in the molecule, known as steric hindrance, and can substantially increase the reactivity.
Reactions with oxygen (combustion reaction).
See the alkane heat of formation table for detailed data.
The standard enthalpy change of combustion, Δc"H"o, for alkanes increases by about 650 kJ/mol per CH2 group. Branched-chain alkanes have lower values of Δc"H"o than straight-chain alkanes of the same number of carbon atoms, and so can be seen to be somewhat more stable.
Reactions with halogens.
Alkanes react with halogens in a so-called "free radical halogenation" reaction. The hydrogen atoms of the alkane are progressively replaced by halogen atoms. Free-radicals are the reactive species that participate in the reaction, which usually leads to a mixture of products. The reaction is highly exothermic, and can lead to an explosion.
Cracking.
Cracking breaks larger molecules into smaller ones. This can be done with a thermal or catalytic method. The thermal cracking process follows a homolytic mechanism with formation of free-radicals. The catalytic cracking process involves the presence of acid catalysts (usually solid acids such as silica-alumina and zeolites), which promote a heterolytic (asymmetric) breakage of bonds yielding pairs of ions of opposite charges, usually a carbocation and the very unstable hydride anion. Carbon-localized free-radicals and cations are both highly unstable and undergo processes of chain rearrangement, C-C scission in position beta (i.e., cracking) and intra- and intermolecular hydrogen transfer or hydride transfer. In both types of processes, the corresponding reactive intermediates (radicals, ions) are permanently regenerated, and thus they proceed by a self-propagating chain mechanism. The chain of reactions is eventually terminated by radical or ion recombination.
Isomerization and reformation.
Isomerization and reformation are processes in which straight-chain alkanes are heated in the presence of a platinum catalyst. In isomerization, the alkanes become branched-chain isomers. In reformation, the alkanes become cycloalkanes or aromatic hydrocarbons, giving off hydrogen as a by-product. Both of these processes raise the octane number of the substance.
Other reactions.
Alkanes will react with steam in the presence of a nickel catalyst to give hydrogen. Alkanes can be chlorosulfonated and nitrated, although both reactions require special conditions. The fermentation of alkanes to carboxylic acids is of some technical importance. In the Reed reaction, sulfur dioxide, chlorine and light convert hydrocarbons to sulfonyl chlorides.
Occurrence of alkanes in the Universe.
Alkanes form a small portion of the atmospheres of the outer gas planets such as Jupiter (0.1% methane, 0.0002% ethane), Saturn (0.2% methane, 0.0005% ethane), Uranus (1.99% methane, 0.00025% ethane) and Neptune (1.5% methane, 1.5 ppm ethane). Titan (1.6% methane), a satellite of Saturn, was examined by the "Huygens" probe, which indicate that Titan's atmosphere periodically rains liquid methane onto the moon's surface. Also on Titan, a methane-spewing volcano was spotted and this volcanism is believed to be a significant source of the methane in the atmosphere. There also appear to be Methane/Ethane lakes near the north polar regions of Titan, as discovered by Cassini's radar imaging. Methane and ethane have also been detected in the tail of the comet Hyakutake. Chemical analysis showed that the abundances of ethane and methane were roughly equal, which is thought to imply that its ices formed in interstellar space, away from the Sun, which would have evaporated these volatile molecules. Alkanes have also been detected in meteorites such as carbonaceous chondrites.
Occurrence of alkanes on Earth.
Traces of methane gas (about 0.0001% or 1 ppm) occur in the Earth's atmosphere, produced primarily by organisms such as Archaea, found for example in the gut of cows.
These hydrocarbons collected in porous rocks, located beneath an impermeable cap rock and so are trapped. Unlike methane, which is constantly reformed in large quantities, higher alkanes (alkanes with 9 or more carbon atoms) rarely develop to a considerable extent in nature. These deposits, e.g., oil fields, have formed over millions of years and once exhausted cannot be readily replaced. The depletion of these hydrocarbons is the basis for what is known as the energy crisis.
Solid alkanes are known as tars and are formed when more volatile alkanes such as gases and oil evaporate from hydrocarbon deposits. One of the largest natural deposits of solid alkanes is in the asphalt lake known as the Pitch Lake in Trinidad and Tobago.
Methane is also present in what is called biogas, produced by animals and decaying matter, which is a possible renewable energy source.
Alkanes have a low solubility in water, so the content in the oceans is negligible; however, at high pressures and low temperatures (such as at the bottom of the oceans), methane can co-crystallize with water to form a solid methane hydrate. Although this cannot be commercially exploited at the present time, the amount of combustible energy of the known methane hydrate fields exceeds the energy content of all the natural gas and oil deposits put together;methane extracted from methane hydrate is considered therefore a candidate for future fuels.
Biological occurrence.
Although alkanes occur in nature in various way, they do not rank biologically among the essential materials. Cycloalkanes with 14 to 18 carbon atoms occur in musk, extracted from deer of the family Moschidae. All further information refers to (acyclic) alkanes.
Certain types of bacteria can metabolise alkanes: they prefer even-numbered carbon chains as they are easier to degrade than odd-numbered chains.
Methanogens are also the producers of marsh gas in wetlands, and release about two billion tonnes of methane per year—the atmospheric content of this gas is produced nearly exclusively by them. The methane output of cattle and other herbivores, which can release up to 150 litres per day, and of termites, is also due to methanogens. They also produce this simplest of all alkanes in the intestines of humans. Methanogenic archaea are, hence, at the end of the carbon cycle, with carbon being released back into the atmosphere after having been fixed by photosynthesis. It is probable that our current deposits of natural gas were formed in a similar way.
Alkanes also play a role, if a minor role, in the biology of the three eukaryotic groups of organisms: fungi, plants and animals. Some specialised yeasts, e.g., "Candida tropicale", "Pichia" sp., "Rhodotorula" sp., can use alkanes as a source of carbon and/or energy. The fungus "Amorphotheca resinae" prefers the longer-chain alkanes in aviation fuel, and can cause serious problems for aircraft in tropical regions.
In plants, the solid long-chain alkanes are found in the plant cuticle and epicuticular wax of many species, but are only rarely major constituents. They protect the plant against water loss, prevent the leaching of important minerals by the rain, and protect against bacteria, fungi, and harmful insects. The carbon chains in plant alkanes are usually odd-numbered, between twenty-seven and thirty-three carbon atoms in length and are made by the plants by decarboxylation of even-numbered fatty acids. The exact composition of the layer of wax is not only species-dependent, but changes also with the season and such environmental factors as lighting conditions, temperature or humidity.
Alkanes are found in animal products, although they are less important than unsaturated hydrocarbons. One example is the shark liver oil, which is approximately 14% pristane (2,6,10,14-tetramethylpentadecane, C19H40). Their occurrence is more important in pheromones, chemical messenger materials, on which above all insects are dependent for communication. With some kinds, as the support beetle "Xylotrechus colonus", primarily pentacosane (C25H52), 3-methylpentaicosane (C26H54) and 9-methylpentaicosane (C26H54), they are transferred by body contact. With others like the tsetse fly "Glossina morsitans morsitans", the pheromone contains the four alkanes 2-methylheptadecane (C18H38), 17,21-dimethylheptatriacontane (C39H80), 15,19-dimethylheptatriacontane (C39H80) and 15,19,23-trimethylheptatriacontane (C40H82), and acts by smell over longer distances, a useful characteristic for pest control. Waggle-dancing honeybees produce and release two alkanes, tricosane and pentacosane.
Ecological relations.
One example, in which both plant and animal alkanes play a role, is the ecological relationship between the sand bee ("Andrena nigroaenea") and the early spider orchid ("Ophrys sphegodes"); the latter is dependent for pollination on the former. Sand bees use pheromones in order to identify a mate; in the case of "A. nigroaenea", the females emit a mixture of tricosane (C23H48), pentacosane (C25H52) and heptacosane (C27H56) in the ratio 3:3:1, and males are attracted by specifically this odour. The orchid takes advantage of this mating arrangement to get the male bee to collect and disseminate its pollen; parts of its flower not only resemble the appearance of sand bees, but also produce large quantities of the three alkanes in the same ratio as female sand bees. As a result numerous males are lured to the blooms and attempt to copulate with their imaginary partner: although this endeavour is not crowned with success for the bee, it allows the orchid to transfer its pollen,
which will be dispersed after the departure of the frustrated male to different blooms.
Petroleum refining.
As stated earlier, the most important source of alkanes is natural gas and crude oil. Alkanes are separated in an oil refinery by fractional distillation and processed into many different products.
Fischer-Tropsch.
The Fischer-Tropsch process is a method to synthesize liquid hydrocarbons, including alkanes, from carbon monoxide and hydrogen. This method is used to produce substitutes for petroleum distillates.
Laboratory preparation.
Alkanes or alkyl groups can also be prepared directly from alkyl halides in the Corey-House-Posner-Whitesides reaction. The Barton-McCombie deoxygenation removes hydroxyl groups from alcohols e.g.
Applications.
The applications of a certain alkane can be determined quite well according to the number of carbon atoms. The first four alkanes are used mainly for heating and cooking purposes, and in some countries for electricity generation. Methane and ethane are the main components of natural gas; they are normally stored as gases under pressure. It is, however, easier to transport them as liquids: This requires both compression and cooling of the gas.
Propane and butane can be liquefied at fairly low pressures, and are well known as liquified petroleum gas (LPG). Propane, for example, is used in the propane gas burner, butane in disposable cigarette lighters. The two alkanes are used as propellants in aerosol sprays.
From pentane to octane the alkanes are reasonably volatile liquids. They are used as fuels in internal combustion engines, as they vaporise easily on entry into the combustion chamber without forming droplets, which would impair the uniformity of the combustion. Branched-chain alkanes are preferred as they are much less prone to premature ignition, which causes knocking, than their straight-chain homologues. This propensity to premature ignition is measured by the octane rating of the fuel, where 2,2,4-trimethylpentane ("isooctane") has an arbitrary value of 100, and heptane has a value of zero. Apart from their use as fuels, the middle alkanes are also good solvents for nonpolar substances.
Alkanes from nonane to, for instance, hexadecane (an alkane with sixteen carbon atoms) are liquids of higher viscosity, less and less suitable for use in gasoline. They form instead the major part of diesel and aviation fuel. Diesel fuels are characterised by their cetane number, cetane being an old name for hexadecane. However, the higher melting points of these alkanes can cause problems at low temperatures and in polar regions, where the fuel becomes too thick to flow correctly.
Alkanes from hexadecane upwards form the most important components of fuel oil and lubricating oil. In latter function, they work at the same time as anti-corrosive agents, as their hydrophobic nature means that water cannot reach the metal surface. Many solid alkanes find use as paraffin wax, for example, in candles. This should not be confused however with true wax, which consists primarily of esters.
Alkanes with a chain length of approximately 35 or more carbon atoms are found in bitumen, used, for example, in road surfacing. However, the higher alkanes have little value and are usually split into lower alkanes by cracking.
Some synthetic polymers such as polyethylene and polypropylene are alkanes with chains containing hundreds of thousands of carbon atoms. These materials are used in innumerable applications, and billions of kilograms of these materials are made and used each year.
Environmental transformations.
When released in the environment, alkanes don't undergo rapid biodegradation, because they haven't functional groups (like hydroxyl or carbonyl) that are needed by most organismsm in order to metabolize the compound.
However, some bacteria can metabolize some alkanes (expecially those linear and short), by oxidizing the terminal carbon atom. The product is an alcohol, that could be next oxidized to an aldehyde, and finally to a carboxylic acid. The resulting fatty acid could be metabolized through the fatty acid degradation pathway.
Hazards.
Methane is explosive when mixed with air (1 – 8% CH4) and is a strong greenhouse gas: Other lower alkanes can also form explosive mixtures with air. The lighter liquid alkanes are highly flammable, although this risk decreases with the length of the carbon chain. Pentane, hexane, heptane, and octane are classed as "dangerous for the environment" and "harmful". The straight-chain isomer of hexane is a neurotoxin. Halogen-rich alkanes, like chloroform, can be carcinogenic as well.
---END.OF.DOCUMENT---

Appeal.
In law, an appeal is a process for requesting a formal change to an official decision.
The specific procedures for appealing, including even whether there is a right of appeal from a particular type of decision, can vary greatly from country to country. Even within a jurisdiction, the nature of an appeal can vary greatly depending on the type of case.
An appellate court is a court that hears cases on appeal from another court. Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds. These grounds typically could include errors of law, fact, or procedure (in the United States, due process).
In different jurisdictions, appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts.
Who can appeal.
A party who files an appeal is called an "appellant" or "petitioner", and a party on the other side is called a "respondent" (in most common-law countries) or an "appellee" (in the United States). A "cross-appeal" is an appeal brought by the respondent. For example, suppose at trial the judge found for the plaintiff and ordered the defendant to pay $50,000. If the defendant files an appeal arguing that he should not have to pay any money, then the plaintiff might file a cross-appeal arguing that the defendant should have to pay $200,000 instead of $50,000.
The appellant is the party who, having lost part or all their claim in a lower court decision, is appealing to a higher court to have their case reconsidered. This is usually done on the basis that the lower court judge erred in the application of law, but it may also be possible to appeal on the basis of court misconduct, or that a finding of fact was entirely unreasonable to make on the evidence.
The appellant in the new case can be either the plaintiff (or "claimant"), defendant, or respondent (appellee) from the lower case, depending on who was the losing party. The winning party from the lower court, however, is now the respondent. In unusual cases the appellant can be the victor in the court below, but still appeal. For example, in "Doyle v Olby (Ironmongers) Ltd" [1969] 2 QB 158, the claimant appealed (successfully) on the basis that, although he won in the court below, the lower court had applied the wrong measure of damages and he had not been fully recompensed.
An appellee is the party to an appeal in which the lower court judgment was in its favor. The appellee is required to respond to the petition, oral arguments, and legal briefs of the appellant. In general, the appellee takes the procedural posture that the lower court's decision should be affirmed.
Ability to appeal.
An appeal "as of right" is one that is guaranteed by statute or some underlying constitutional or legal principle. The appellate court cannot refuse to listen to the appeal. An appeal "by leave" or "permission" requires the appellant to move for leave to appeal; in such a situation either or both of the lower court and the appellate court may have the discretion to grant or refuse the appellant's demand to appeal the lower court's decision. A good example of this is the U.S. Supreme Court in which at least three justices must agree to hear the case if there is a constitutional issue.
In tort, equity, or other civil matters either party to a previous case may file an appeal. In criminal matters, however, the state or prosecution generally has no appeal "as of right". And due to the double jeopardy principle, in the United States the state or prosecution may never appeal a jury or bench verdict of acquittal. But in some jurisdictions, the state or prosecution may appeal "as of right" from a trial court's dismissal of an indictment in whole or in part or from a trial court's granting of a defendant's suppression motion. Likewise, in some jurisdictions, the state or prosecution may appeal an issue of law "by leave" from the trial court and/or the appellate court. The ability of the prosecution to appeal a decision in favor of a defendant varies significantly internationally. All parties must present grounds to appeal, or it will not be heard.
By convention in some law reports, the appellant is named first. This can mean that where it is the defendant who appeals, the name of the case in the law reports reverses (in some cases twice) as the appeals work their way up the court hierarchy. This is not always true, however. In the United States federal courts, the parties' names always stay in the same order as the lower court when an appeal is taken to the circuit courts of appeals, and are re-ordered only if the appeal reaches the United States Supreme Court.
Direct or collateral.
Many jurisdictions recognize two types of appeals, particularly in the criminal context. The first is the traditional "direct" appeal in which the appellant files an appeal with the next higher court of review. The second is the collateral appeal or post-conviction petition, in which the petitioner-appellant files the appeal in a court of first instance—usually the court that tried the case.
The key distinguishing factor between direct and collateral appeals is that the former only reviews evidence that was presented in the trial court, but the latter allows review of evidence dehors the record: depositions, affidavits, and witness statements that did not come in at trial. The standard for post-conviction relief is high, typically requiring the petitioner to demonstrate that the evidence presented was not available in the usual course of trial discovery.
Relief in post-conviction is rare and is most often found in capital or violent felony cases. The typical scenario involves an incarcerated defendant locating DNA evidence demonstrating the defendant's actual innocence.
Types of appeal.
There are a number of appeal actions, their differences being potentially confusing, thus bearing some explanation. Three of the most common are an appeal to which the defendant has as a right, a writ of certiorari and a writ of habeas corpus.
An appeal to which the defendant has a right cannot be abridged by the court which is, by designation of its jurisdiction, obligated to hear the appeal. In such an appeal, the appellant feels that some error has been made in his trial, necessitating an appeal. A matter of importance is the basis on which such an appeal might be filed: generally appeals as a matter of right may only address issues which were originally raised in trial (as evidenced by documentation in the official record). Any issue not raised in the original trial may not be considered on appeal and will be considered estoppel. A convenient test for whether a petition is likely to succeed on the grounds of error is confirming that (1) a mistake was indeed made (2) an objection to that mistake was presented by counsel and (3) that mistake negatively affected the defendant’s trial.
A writ of certiorari, otherwise know as simply as cert, is an order by a higher court directing a lower court to send record of a case for review, and is the next logical step in post-trial procedure. While states may have similar processes, a writ of cert is usually only issued, in the United States, by the Supreme Court, although some states retain this procedure. Unlike the aforementioned appeal, a writ of cert is not a matter of right. A writ of cert will have to be petitioned for, the higher court issuing such writs on limited bases according to constraints such as time. In another sense, a writ of cert is like an appeal in its constraints; it too may only seek relief on grounds raised in the original trial.
A writ of habeas corpus is the last opportunity for the defendant to find relief against his guilty conviction. Habeas corpus may be pursued if a defendant is unsatisfied with the outcome of his appeal and has been refused (or did not pursue) a writ of cert, at which point he may petition one of several courts for a writ of habeas corpus. Again, these are granted at the discretion of the court and require a petition. Like appeals or writs of cert, a writ of habeas corpus may overturn a defendant's guilty conviction by finding some error in the original trial. The major difference is that writs of habeas corpus may, and often, focus on issues that lay outside the original premises of the trial, i.e., issues that could not be raised by appeal or writs of cert. These often fall in two logical categories: (1) that the trial lawyer was ineffectual or incompetent or (2) that some constitutional right has been violated.
Notice of appeal.
A notice of appeal is a form or document that in many cases is required to begin an appeal. The form is completed by the appellant or by the appellant's legal representative. The nature of this form can vary greatly from country to country and from court to court within a country.
The specific rules of the legal system will dictate exactly how the appeal is officially begun. For example, the appellant might have to file the notice of appeal with the appellate court, or with the court from which the appeal is taken, or both.
Some courts have samples of a notice of appeal on the court's own web site.
The deadline for beginning an appeal can often be very short: traditionally, it is measured in days, not years. This can vary from country to country, as well as within a country, depending on the specific rules in force.
How an appeal is processed.
Generally speaking the appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether that decision was legally sound or not. The appellate court will typically be deferential to the lower court's findings of fact (such as whether a defendant committed a particular act), unless clearly erroneous, and so will focus on the court's application of the law to those facts (such as whether the act found by the court to have occurred fits a legal definition at issue).
If the appellate court finds no defect, it "affirms" the judgment. If the appellate court does find a legal defect in the decision "below" (i.e., in the lower court), it may "modify" the ruling to correct the defect, or it may nullify ("reverse" or "vacate") the whole decision or any part of it. It may, in addition, send the case back ("remand" or "remit") to the lower court for further proceedings to remedy the defect.
In some cases, an appellate court may review a lower court decision "de novo" (or completely), challenging even the lower court's findings of fact. This might be the proper standard of review, for example, if the lower court resolved the case by granting a pre-trial motion to dismiss or motion for summary judgment which is usually based only upon written submissions to the trial court and not on any trial testimony.
Another situation is where appeal is by way of "re-hearing". Certain jurisdictions permit certain appeals to cause the trial to be heard afresh in the appellate court. An example would be an appeal from a magistrates' court to the Crown Court in England and Wales.
Sometimes, the appellate court finds a defect in the procedure the parties used in filing the appeal and dismisses the appeal without considering its merits, which has the same effect as affirming the judgment below. (This would happen, for example, if the appellant waited too long, under the appellate court's rules, to file the appeal.) In England and many other jurisdictions, however, the phrase "appeal dismissed" is equivalent to the U.S. term "affirmed"; and the phrase "appeal allowed" is equivalent to the U.S. term "reversed".
Generally, there is no trial in an appellate court, only consideration of the record of the evidence presented to the trial court and all the pre-trial and trial court proceedings are reviewed—unless the appeal is by way of re-hearing, new evidence will usually only be considered on appeal in "very" rare instances, for example if that material evidence was unavailable to a party for some very significant reason such as prosecutorial misconduct.
In some systems, an appellate court will only consider the written decision of the lower court, together with any written evidence that was before that court and is relevant to the appeal. In other systems, the appellate court will normally consider the record of the lower court. In those cases the record will first be certified by the lower court.
The appellant has the opportunity to present arguments for the granting of the appeal and the appellee (or respondent) can present arguments against it. Arguments of the parties to the appeal are presented through their appellate lawyers, if represented, or "pro se" if the party has not engaged legal representation. Those arguments are presented in written briefs and sometimes in oral argument to the court at a hearing. At such hearings each party is allowed a brief presentation at which the appellate judges ask questions based on their review of the record below and the submitted briefs.
It is important to note that in an adversarial system appellate courts do not have the power to review lower court decisions unless a party appeals it. Therefore if a lower court has ruled in an improper manner or against legal precedent that judgment will stand even if it might have been overturned on appeal.
United States.
The United States legal system generally recognizes two types of appeals: a trial "de novo" or an appeal on the record.
A trial de novo is usually available for review of informal proceedings conducted by some minor judicial tribunals in proceedings that do not provide all the procedural attributes of a formal judicial trial. If unchallenged, these decisions have the power to settle more minor legal disputes once and for all. If a party is dissatisfied with the finding of such a tribunal, one generally has the power to request a trial "de novo" by a court of record. In such a proceeding, all issues and evidence may be developed newly, as though never heard before, and one is not restricted to the evidence heard in the lower proceeding. Sometimes, however, the decision of the lower proceeding is itself admissible as evidence, thus helping to curb frivolous appeals.
In an appeal on the record from a decision in a judicial proceeding, both appellant and respondent are bound to base their arguments wholly on the proceedings and body of evidence as they were presented in the lower tribunal. Each seeks to prove to the higher court that the result they desired was the just result. Precedent and case law figure prominently in the arguments. In order for the appeal to succeed, the appellant must prove that the lower court committed reversible error, that is, an impermissible action by the court acted to cause a result that was unjust, and which would not have resulted had the court acted properly. Some examples of reversible error would be erroneously instructing the jury on the law applicable to the case, permitting seriously improper argument by an attorney, admitting or excluding evidence improperly, acting outside the court's jurisdiction, injecting bias into the proceeding or appearing to do so, juror misconduct, etc. The failure to formally object at the time, to what one views as improper action in the lower court, may result in the affirmance of the lower court's judgment on the grounds that one did not "preserve the issue for appeal" by objecting.
In cases where a judge rather than a jury decided issues of fact, an appellate court will apply an "abuse of discretion" standard of review. Under this standard, the appellate court gives deference to the lower court's view of the evidence, and reverses its decision only if it were a clear abuse of discretion. This is usually defined as a decision outside the bounds of reasonableness. On the other hand, the appellate court normally gives less deference to a lower court's decision on issues of law, and may reverse if it finds that the lower court applied the wrong legal standard.
In some rare cases, an appellant may successfully argue that the law under which the lower decision was rendered was unconstitutional or otherwise invalid, or may convince the higher court to order a new trial on the basis that evidence earlier sought was concealed or only recently discovered. In the case of new evidence, there must be a high probability that its presence or absence would have made a material difference in the trial. Another issue suitable for appeal in criminal cases is effective assistance of counsel. If a defendant has been convicted and can prove that his lawyer did not adequately handle his case "and" that there is a reasonable probability that the result of the trial would have been different had the lawyer given competent representation, he is entitled to a new trial.
In the United States, a lawyer traditionally starts an oral argument to any appellate court with the words "May it please the court."
After an appeal is heard, the "mandate" is a formal notice of a decision by a court of appeal; this notice is transmitted to the trial court and, when filed by the clerk of the trial court, constitutes the final judgment on the case, unless the appeal court has directed further proceedings in the trial court. The mandate is distinguished from the appeal court's opinion, which sets out the legal reasoning for its decision. In some U.S. jurisdictions the mandate is known as the "remittitur".
Appellate review.
Appellate review is the general term for the process by which courts with appellate jurisdiction take jurisdiction of matters decided by lower courts. It is distinguished from judicial review, which refers to the court's overriding constitutional or statutory right to determine if a legislative act or administrative decision is defective for jurisdictional or other reasons (which may vary by jurisdiction).
In most jurisdictions the normal and preferred way of seeking appellate review is by filing an appeal of the final judgment. Generally, an appeal of the judgment will also allow appeal of all other orders or rulings made by the trial court in the course of the case. This is because such orders cannot be appealed "as of right". However, certain critical interlocutory court orders, such as the denial of a request for an interim injunction, or an order holding a person in contempt of court, can be appealed immediately although the case may otherwise not have been fully disposed of.
In American law, there are two distinct forms of appellate review, "direct" and "collateral". For example, a criminal defendant may be convicted in state court, and lose on "direct appeal" to higher state appellate courts, and if unsuccessful, mount a "collateral" action such as filing for a writ of habeas corpus in the federal courts. Generally speaking, "[d]irect appeal statutes afford defendants the opportunity to challenge the merits of a judgment and allege errors of law or fact.... [Collateral review], on the other hand, provide[s] an independent and civil inquiry into the validity of a conviction and sentence, and as such are generally limited to challenges to constitutional, jurisdictional, or other fundamental violations that occurred at trial." "Graham v. Borgen", __ F 3d. __ (7th Cir. 2007) (no. 04-4103) (slip op. at 7) (citation omitted).
In Anglo-American common law courts, appellate review of lower court decisions may also be obtained by filing a petition for review by prerogative writ in certain cases. There is no corresponding right to a writ in any pure or continental civil law legal systems, though some mixed systems such as Quebec recognize these prerogative writs.
---END.OF.DOCUMENT---

Answer.
Generally, an answer is a reply to a question or is a solution, a retaliation, or a response that is relevant to the said question.
In law, an answer was originally a solemn assertion in opposition to some one or something, and thus generally any counter-statement or defense, a reply to a question or objection, or a correct solution of a problem.
In the common law, an answer is the first pleading by a defendant, usually filed and served upon the plaintiff within a certain strict time limit after a civil complaint or criminal information or indictment has been served upon the defendant. It may have been preceded by an "optional" "pre-answer" motion to dismiss or demurrer; if such a motion is unsuccessful, the defendant "must" file an answer to the complaint or risk an adverse default judgment.
The "answer" establishes which allegations (cause of action in civil matters) set forth by the complaining party will be contested by the defendant, and states all the defendant's defenses, thus establishing the nature and parameters of the controversy to be decided by the court.
In a criminal case, there is usually an arraignment or some other kind of appearance before defendant comes to court. The pleading in the criminal case, which is entered on the record in open court, is usually either guilty or not guilty. Generally speaking in private, civil cases there is no plea entered of guilt or innocence. There is only a judgment that grants money damages or some other kind of equitable remedy such as restitution or a permanent injunction. Criminal cases may lead to fines or other punishment, such as imprisonment.
The famous Latin "Responsa Prudentium" ("answers of the learned ones") were the accumulated views of many successive generations of Roman lawyers, a body of legal opinion which gradually became authoritative.
In music an "answer" (also known as countersubject) is the technical name in counterpoint for the repetition or modification by one part or instrument of a theme proposed by another.
---END.OF.DOCUMENT---

Appellate court.
An appellate court is any court of law that is empowered to hear an appeal of a trial court or other lower tribunal. In most jurisdictions, the court system is divided into at least three levels: the trial court, which initially hears cases and reviews evidence and testimony to determine the facts of the case; at least one intermediate appellate court; and a supreme court (or court of last resort) which primarily reviews the decisions of the intermediate courts. A supreme court is therefore itself a kind of appellate court. Appellate courts worldwide can operate by varying rules. For example, the Isle of Man's traditional local appellate court is the Staff of Government Division which has only two Justices, titled "Deemsters," whose decisions are joined to the original trial decision. They almost always have a majority, if either Deemster agrees with the trial Judge.
Institutional titles.
Many US jurisdictions title their appellate court a Court of Appeal or Court of Appeals. Historically, others have titled their appellate court a Court of Errors (or Court of Errors and Appeals), on the premise that it was intended to correct errors made by lower courts. Examples of such courts include the New Jersey Court of Errors and Appeals (which existed from 1844 to 1947), the Connecticut Supreme Court of Errors (which has been renamed the Connecticut Supreme Court), the Kentucky Court of Errors (since renamed the Kentucky Supreme Court), and the Mississippi High Court of Errors and Appeals (since renamed the Supreme Court of Mississippi). In some jurisdictions, courts able to hear appeals are known as an Appellate Division.
Depending on the system, certain courts may serve as both trial courts and appellate courts, hearing appeals of decisions made by courts with more limited jurisdiction. Some jurisdictions have specialized appellate courts, such as the Texas Court of Criminal Appeals, which only hears appeals raised in criminal cases, and the United States Court of Appeals for the Federal Circuit, which has general jurisdiction but derives most of its caseload from patent cases, on the one hand, and appeals from the Court of Federal Claims on the other.
Authority to review.
The authority of appellate courts to review a decisions of lower courts varies widely from one jurisdiction to another. In some places, the appellate court has limited powers of review. For example, in the United States, both state and federal appellate courts are usually restricted to examining whether the court below made the correct legal determinations, rather than hearing direct evidence and determining what the facts of the case were. Furthermore, U.S. appellate courts are usually restricted to hearing appeals based on matters that were originally brought up before the trial court. Hence, such an appellate court will not consider an appellant's argument if it is based on a theory that is raised for the first time in the appeal.
In most U.S. states, and in U.S. federal courts, parties before the court are allowed one appeal as of right. This means that a party who is unsatisfied with the outcome of a trial may bring an appeal to contest that outcome. However, appeals may be costly, and the appellate court must find an error on the part of the court below that justifies upsetting the verdict. Therefore, only a small proportion of trial court decisions result in appeals. Some appellate courts, particularly supreme courts, have the power of discretionary review, meaning that they can decide whether they will hear an appeal brought in a particular case.
---END.OF.DOCUMENT---

Arraignment.
Arraignment is a formal reading of a criminal complaint in the presence of the defendant to inform the defendant of the charges against him or her. In response to arraignment, the accused is expected to enter a plea. Acceptable pleas vary among jurisdictions, but they generally include "guilty", "not guilty", and the peremptory pleas (or pleas in bar) setting out reasons why a trial cannot proceed. Pleas of "nolo contendere" (no contest) and the "Alford plea" are allowed in some circumstances.
In England, Wales, and Northern Ireland, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment. The defendant is asked whether he or she pleads guilty or not guilty to each individual charge. This process is the same in Australian jurisdictions.
In the U.S. District Court, Central District of California, arraignment takes place in two stages. The first is called the initial arraignment and must take place within 48 hours of an individual's arrest. During this arraignment the defendant is informed of any pending legal charges and is informed of his or her right to retain counsel. The presiding judge will also decide whether or not to set bail, and, if so, for how much money. The second arraignment is called a post-indictment arraignment or PIA. It is during this second arraignment that a defendant will be allowed to enter a plea.
Guilty and not guilty pleas.
If the defendant pleads guilty, an evidentiary hearing usually follows. The court is not required to accept a guilty plea. During the hearing, the judge will assess the offense, mitigating factors, and the defendant's character, and pass sentence. If the defendant pleads not guilty, a date will be set for a preliminary hearing or a trial.
In the past, a defendant who refused to plead (or "stood mute") would be subject to peine forte et dure (Law French for "strong and hard punishment"). Today in common law jurisdictions, defendants who refuse to enter a plea will have a plea of not guilty entered for them by the court. The rationale for this is the defendant's right to silence.
Pre-trial Release.
This is also often the stage at which arguments in favor or against pre-trial release and bail are made, depending on the alleged crime and jurisdiction.
United States Federal Rules of Criminal Procedure.
Under the Federal Rules of Criminal Procedure, "arraignment shall...[consist of an] open...reading [of] the indictment...to the defendant...and calling on him to plead thereto. He shall be given a copy of the indictment...before he is called upon to plead."
---END.OF.DOCUMENT---

America the Beautiful.
"America the Beautiful" is an American patriotic song. The lyrics were written by Katharine Lee Bates and the music composed by church organist and choirmaster Samuel A. Ward. Bates originally wrote the words as a poem, "Pikes Peak", first published in the July 4th edition of the church periodical "The Congregationalist" in 1895. The poem was titled "America" for publication. Ward had originally written the music, "Materna", for the 1600s hymn "O Mother dear, Jerusalem" in 1882. Ward's music combined with the Bates poem was first published in 1910 and titled "America the Beautiful". The song is one of the most beloved and popular of the many American patriotic songs. From time to time it has been proposed as a replacement for "The Star-Spangled Banner" as the National Anthem.
History.
In 1893, at the age of thirty-three Katharine Lee Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its alabaster buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 4; and the majestic view of the Great Plains from high atop Zebulon's Pikes Peak.
On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in "The Congregationalist," to commemorate the Fourth of July. It quickly caught the public's fancy. Amended versions were published in 1904 and 1913.
Several existing pieces of music were adapted to the poem. A hymn tune composed by Samuel A. Ward was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward too was inspired to compose his tune. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City, after a leisurely summer day in 1882, and he immediately wrote it down. He was so anxious to capture the tune in his head, he asked fellow passenger friend Harry Martin for his shirt cuff to write the tune on, thus perhaps the "off the cuff" analogy. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates' poem were first published together in 1910 and titled, "America the Beautiful".
Ward died in 1903, not knowing the national stature his music would attain, as the music was only first applied to the song in 1904. Miss Bates was more fortunate, as the song's popularity was well-established by her death in 1929.
At various times in the more than 100 years that have elapsed since the song as we know it was born, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn, or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner." Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery. (Others prefer "The Star-Spangled Banner" for the same reason.) While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans.
Popularity of the song increased greatly following the September 11, 2001 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the "Late Show with David Letterman" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.
Ray Charles is credited with the song's most well known rendition in current times (although Elvis Presley had good success with it in the 1970s). His recording is very commonly played at major sporting events, such as the Super Bowl; Charles gave a live performance of the song prior to Super Bowl XXXV, the last Super Bowl played before the September 11 terrorist attacks. His unique take on it places the third verse first, after which he sings the usual first verse. In the third verse (see below), the author scolds the materialistic and self-serving robber barons of her day, and urges America to live up to its noble ideals and to honor, with both word and deed, the memory of those who died for their country. Symbolically, Marian Anderson (a noted opera singer of her day) sang a rendition of America on the steps of the Lincoln Memorial in 1939 after being refused use of Constitution Hall by the Daughters of the American Revolution because of her skin color.
An all-star version of "America the Beautiful" performed by country music singers Trace Adkins, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Martina McBride, Jamie O'Neal, Kenny Rogers and Keith Urban reached #58 on the "Billboard" Hot Country Singles & Tracks chart in July 2001. The song re-entered the chart following the September 11 terrorist attacks.
When Richard Nixon visited the People's Republic of China in 1972, this song was played by Chinese as the welcome music. Interestingly, the Chinese characters for United States literally mean "Beautiful Country."
The song is often included in songbooks in a wide variety of religious congregations in the United States.
Idioms.
"From sea to shining sea" is an American idiom meaning from the Pacific Ocean to the Atlantic Ocean (or vice versa). Many songs have used this term, including the American patriotic songs "America, The Beautiful" and "God Bless the USA". In addition to these, it is also featured in Schoolhouse Rock's "Elbow Room". A term similar to this is the Canadian motto "A Mari Usque Ad Mare" ("From sea to sea.")
Lyrics.
God shed his grace on thee
And crown thy good with brotherhood
Who more than self their country loved
God shed his grace on thee
And crown thy good with brotherhood
God shed his grace on thee
Till souls wax fair as earth and air
God shed his grace on thee
God shed his grace on thee
Till selfish gain no longer stain
God shed his grace on thee
Till nobler men keep once again
---END.OF.DOCUMENT---

Assistive technology.
Assistive technology (AT) is a generic term that includes assistive, adaptive, and rehabilitative devices for people with disabilities and includes the process used in selecting, locating, and using them.
The Technology-Related Assistance for Individuals with Disabilities Act of 1988 (US Public Law 100-407) states that it is "technology designed to be utilized in an assistive technology device or assistive technology service."
AT promotes greater independence by enabling people to perform tasks that they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to or changed methods of interacting with the technology needed to accomplish such tasks.
Likewise, disability advocates point out that technology is often created without regard to people with disabilities, creating unnecessary barriers to hundreds of millions of people.
Assistive technology and universal accessibility.
Universal (or broadened) accessibility, or universal design means greater usability, particularly for people with disabilities.
Universally accessible technology yields great rewards to the typical user as well; good accessible design "is" universal design. One example is the "curb cuts" (or dropped curbs) in the sidewalk at street crossings. While these curb cuts enable pedestrians with mobility impairments to cross the street, they also aid parents with carriages and strollers, shoppers with carts, and travellers and workers with pull-type bags.
As an example, the modern telephone is inaccessible to people who are deaf or hard of hearing. Combined with a text telephone (also known as a TDD Telecommunications device for the deaf and in the USA generally called a TTY[TeleTYpewriter]), which converts typed characters into tones that may be sent over the telephone line, a deaf person is able to communicate immediately at a distance. Together with "relay" services, in which an operator reads what the deaf person types and types what a hearing person says, the deaf person is then given access to everyone's telephone, not just those of people who possess text telephones. Many telephones now have volume controls, which are primarily intended for the benefit of people who are hard of hearing, but can be useful for all users at times and places where there is significant background noise. Some have larger keys well-spaced to facilitate accurate dialing.
Also, a person with a mobility impairment can have difficulty using calculators. Speech recognition software recognizes short commands and makes use of calculators easier.
People with learning disabilities like dyslexia or dysgraphia are using text-to-speech (TTS) software for reading and spelling programs for assistance in writing texts.
Computers with their peripheral devices, editing, spellchecking and speech synthesis software are becoming the core-stones of the assistive technologies coming for relief to the people with learning disabilities and to the people with visual impairments. The assisting spelling programs and voice facilities are bringing better and more convenient text reading and writing experience to the general public.
Toys which have been adapted to be used by children with disabilities may have advantages for non-disabled children as well. The Lekotek movement assists parents by lending assistive technology toys and expertise to families.
The following professionals may be certified by RESNA (RESNA.org) to serve the assistive technology needs of individuals: occupational therapists, physical therapists, speech language pathologists/audiologists, orthotists and prosthetists, educators, and a variety of other rehabilitation and health professionals.
Personal Emergency Response Systems.
Personal Emergency Response Systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer. An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia). Notably, these alerts can be customized to the particular person's risks. When the alert is triggered, a message is sent to a carer or contact centre who can respond appropriately.
Technology similar to PERS can also be used to act within a person's home rather than just to respond to a detected crisis. Using one of the examples above, gas sensors for people with dementia can be used to trigger a device that turns off the gas and tells someone what has happened.
Designing for people with dementia is a good example of how the design of the interface of a piece of AT is critical to its usefulness. People with dementia or any other identified user group must be involved in the design process to make sure that the design is accessible and usable. In the example above, a voice message could be used to remind the person with dementia to turn off the gas himself, but whose voice should be used, and what should the message say? Questions like these must be answered through user consultation, involvement and evaluation.
Accessible computer input.
Sitting at a desk with a QWERTY keyboard and a mouse remains the dominant way of interacting with a personal computer. Some Assistive Technology reduces the strain of this way of work through ergonomic accessories with height-adjustable furniture, footrests, wrist rests, and arm supports to ensure correct posture. Keyguards fit over the keyboard to help prevent unintentional keypresses.
More ambitiously, and quite crucially when keyboard or mouse prove unusable, AT can also replace the keyboard and mouse with alternative devices such as the LOMAK keyboard, trackballs, joysticks, graphics tablets, touchpads, touch screens, foot mice, a microphone with speech recognition software, sip-and-puff input, switch access, and vision-based input devices, such as eye trackers which allow the user to control the mouse with their eyes.
Visual impairment.
Choice of appropriate hardware and software will depend on the user's level of functional vision.
Augmentative and Alternative Communication (AAC).
Augmentative and alternative communication is a well defined specialty within AT. It involves ways of communication that either enhance or replace verbal language. When combined with Applied Behavior Analysis (ABA) teaching methods, AAC has improved communication skills in children with Autism.
---END.OF.DOCUMENT---

Abacus.
The abacus, also called a counting frame, is a calculating tool used primarily in parts of Asia for performing arithmetic processes. Today, abacuses are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal. The abacus was in use centuries before the adoption of the written modern numeral system and is still widely used by merchants, traders and clerks in Asia, Africa, and elsewhere. The user of an abacus is called an abacist.
Etymology.
The use of the word "abacus" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus. The Latin word came from "abakos", the Greek genitive form of "abax" ("calculating-table"), from Hebrew "ābāq" (אבק), "dust". The preferred plural of "abacus" is a subject of disagreement, with both "abacuses" and "abaci" in use.
Mesopotamian abacus.
The period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.
Some scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Carruccio (and other Old Babylonian scholars) that Babylonians "may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations".
Egyptian abacus.
The use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the manner of this disk's usage by the Egyptians was opposite in direction when compared with the Greek method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered, casting some doubt over the extent to which this instrument was used.
Iranian Persian abacus.
During the Achaemenid Persian Empire, around 600 BC, Iranians first began to use the abacus. Under Parthian and Sassanian Iranian empires, scholars concentrated on exchanging knowledge and inventions by the countries around them – India, China, and the Roman Empire, when it is thought to be expanded over the other countries.
Greek abacus.
The earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.
A tablet found on the Greek island Salamis in 1846 AD dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line.
Roman abacus.
The normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles, calculi, were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century.
Writing in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.
One example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, obviously related to the Roman numerals. The short grooves on the right may have been used for marking Roman ounces.
Chinese abacus.
The earliest known written documentation of the Chinese abacus dates to the 2nd century BC.
The Chinese abacus, known as the "suànpán"(算盤, lit. "Counting tray"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom for both decimal and hexadecimal computation. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam. If you move them toward the beam, you count their value. If you move away, you don't count their value. The suanpan can be reset to the starting position instantly by a quick jerk along the horizontal axis to spin all the beads away from the horizontal beam at the center.
Suanpans can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it.
In the famous long scroll "Along the River During the Qingming Festival" painted by Zhang Zeduan (1085–1145 AD) during the Song Dynasty (960–1297 AD), a suanpan is clearly seen lying beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).
The similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abaci may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Japanese) has 4 plus 1 bead per decimal place, the standard suanpan has 5 plus 2, allowing use with a hexadecimal numeral system. Instead of running on wires as in the Chinese and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.
Another possible source of the suanpan is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang Dynasty (618-907 AD) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.
Indian abacus.
First century sources, such as the "Abhidharmakosa" describe the knowledge and use of abacus in India. Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus. Hindu texts used the term "shunya" (zero) to indicate the empty column on the abacus..
Japanese abacus.
In Japanese, the abacus is called "soroban" (, lit. "Counting tray"), imported from China around 1600. The 1/4 abacus appeared circa 1930, and it is preferred and still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as a part of mathematics.
Korean abacus.
The Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it "jupan" (주판), "supan" (수판) or "jusan" (주산).
Native American abaci.
Some sources mention the use of an abacus called a "nepohualtzintzin" in ancient Mayan culture. This Mesoamerican abacus used a 5-digit base-20 system.
The word Nepohualtzintzin comes from the Nahuatl and it is formed by the roots; Ne - personal -; pohual or pohualli - the account -; and tzintzin - small similar elements. And its complete meaning is taken as: counting with small similar elements by somebody. Its use was taught in the "Kalmekak" to the "temalpouhkeh", who were students dedicated to take the accounts of skies, from childhood. Unfortunately the Nepohualtzintzin and its teaching were among the victims of the conquering destruction, when a diabolic origin was attributed to them after observing the tremendous properties of representation, precision and speed of calculations..
This arithmetic tool is based on the vigesimal system (base 20). For the aztec the count by 20s was completely natural, since the use of "huaraches" (native sandals) allowed them to also use the toes for their calculations. In this way, the amount of 20 meant to them a complete human being. The Nepohualtzintzin is divided in two main parts separated by a bar or intermediate cord. In the left part there are four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.
Altogether, there are 13 rows with 7 beads in each one, which makes up 91 beads in each Nepohualtzintzin. This is a basic number to understand the close relation conceived between the exact accounts and the natural phenomena. This is so that one Nepohualtzintzin (91) represents the number of days that a season of the year lasts, two Nepohualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepohualtzintzin (273) is the number of days of a baby's gestation, and four Nepohualtzintzin (364) complete a cycle and approximate a year (1 1/4 days short). It is worth to mention that in the Nepohualtzintzin, amounts in the rank from 10 to the 18 can be calculated, with floating point, which allows calculating stellar as well as infinitesimal amounts with absolute precision.
The rediscovering of the Nepohualtzintzin is due to the teacher David Esparza Hidalgo, who in his wandering by all Mexico has found diverse engravings and paintings of this instrument and has reconstructed several of them made in gold, jade, incrustations of shell, etc. There have also been found very old Nepohualtzintzin attributed to the Olmeca culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.
The quipu of the Incas was a system of knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (quechua for "counting tool"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at minimum.
Russian abacus.
The Russian abacus, the "schety" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire which has four beads, for quarter-ruble fractions. This wire is usually near the user). (Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916.) The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different colour from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.
As a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. On Today it is regarded as an archaism and replaced by microcalculator.
The Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid.
School abacus.
Around the world, abaci have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.
In Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy.
The type of abacus shown here is often used to represent numbers without the use of place value. Each bead and each wire has the same value and used in this way it can represent numbers up to 100.
Uses by the blind.
An adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cubic root.
Although blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.
Binary Abacus.
An abacus that explains how computers manipulate numbers.
The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII.
The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an 'on' or 'off' position.
In 1985, Dr. Robert C. Good, Jr. of the Widener University School of Engineering published on the binary abacus.
---END.OF.DOCUMENT---

Acid.
An acid (...from the Latin "acidus" meaning "sour") is traditionally considered any chemical compound that, when dissolved in water, gives a solution with a hydrogen ion activity greater than in pure water, i.e. a pH less than 7.0 in its standard state. That approximates the modern definition of Johannes Nicolaus Brønsted and Martin Lowry, who independently defined an acid as a compound which donates a hydrogen ion (H+) to another compound (called a base). Common examples include acetic acid (in vinegar) and sulfuric acid (used in car batteries). Acid/base systems are different from redox reactions in that there is no change in oxidation state. Acids can occur in solid, liquid or gaseous form, depending on the temperature. They can exist as pure substances or in solution.
Chemicals or substances having the property of an acid are said to be acidic.
Arrhenius acids.
In pure water the majority of molecules exist as H2O, but a small number of molecules are constantly dissociating and re-associating. Pure water is neutral with respect to acidity or basicity because the concentration of hydroxide ions is always equal to the concentration of hydronium ions. An Arrhenius base is a molecule which increases the concentration of the hydroxide ion when dissolved in water. Note that chemists often write H+("aq") and refer to the hydrogen ion when describing acid-base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion, H3O+.
Brønsted acids.
As with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed. The next two reactions do not involve the formation of ions but can still be viewed as proton transfer reactions. In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH3 combine to form the solid.
Lewis acids.
A third concept was proposed by Gilbert N. Lewis which includes reactions with acid-base characteristics that do not involve a proton transfer. A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor. Brønsted acid-base reactions are proton transfer reactions while Lewis acid-base reactions are electron pair transfers. All Brønsted acids are also Lewis acids, but not all Lewis acids are Brønsted acids. Contrast the following reactions which could be described in terms of acid-base chemistry.
In the first reaction a fluoride ion, F-, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate. Fluoride "loses" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion. BF3 is a Lewis acid because it accepts the electron pair from fluoride. This reaction cannot be described in terms of Brønsted theory because there is no proton transfer. The second reaction can be described using either theory. A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion. The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in H3O+ gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen. Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile.
The Brønsted-Lowry definition is the most widely used definition; unless otherwise specified acid-base reactions are assumed to involve the transfer of a proton (H+) from an acid to a base.
Dissociation and equilibrium.
Reactions of acids are often generalized in the form HA H+ + A-, where HA represents the acid and A- is the conjugate base. Acid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA+ H+ + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant "K" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [H2O] means "the concentration of H2O". The acid dissociation constant "K"a is generally used in the context of acid-base reactions. The numerical value of "K"a is equal to the concentration of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H+.
The stronger of two acids will have a higher "K"a than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton. Because the range of possible values for "K"a spans many orders of magnitude, a more manageable constant, p"K"a is more frequently used, where p"K"a = -log10 "K"a. Stronger acids have a smaller p"K"a than weaker acids. Experimentally determined p"K"a at 25°C in aqueous solution are often quoted in textbooks and reference material.
Nomenclature.
In the classical naming system, acids are named according to their anions. That ionic suffix is dropped and replaced with a new suffix (and sometimes prefix), according to the table below.
For example, HCl has chloride as its anion, so the -ide suffix makes it take the form hydrochloric acid. In the IUPAC naming system, "aqueous" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, the IUPAC name would be aqueous hydrogen chloride. The prefix "hydro-" is added only if the acid is made up of just hydrogen and one other element.
Acid strength.
The strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H+ and one mole of the conjugate base, A-, and none of the protonated acid HA. In contrast a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H+. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base.
Stronger acids have a higher "K"a and a lower p"K"a than weaker acids.
Sulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable.
Superacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. Superacids can permanently protonate water to give ionic, crystalline hydronium "salts". They can also quantitatively stabilize carbocations.
Polarity and the inductive effect.
The electronegative element need not be directly bonded to the acidic hydrogen to increase its acidity. An electronegative atom can pull electron density out of an acidic bond through the inductive effect. The electron-withdrawing ability diminishes quickly as the electronegative atom moves away from the acidic bond. The effect is illustrated by the following series of halogenated butanoic acids. Chlorine is more electronegative than bromine and therefore has a stronger effect. The hydrogen atom bonded to the oxygen is the acidic hydrogen. Butanoic acid is a carboxylic acid.
As the chlorine atom moves further away from the acidic O—H bond, its effect diminishes. When the chlorine atom is just one carbon removed from the carboxylic acid group the acidity of the compound increases significantly, compared to butanoic acid (a.k.a. butyric acid). However, when the chlorine atom is separated by several bonds the effect is much smaller. Bromine is much more electronegative than either carbon or hydrogen, but not as electronegative as chlorine, so the p"K"a of 2-bromobutanoic acid is slightly greater than the p"K"a of 2-chlorobutanoic acid.
The number of electronegative atoms adjacent an acidic bond also affects acid strength. Oxoacids have the general formula HOX where X can be any atom and may or may not share bonds to other atoms. Increasing the number of electronegative atoms or groups on atom X decreases the electron density in the acidic bond, making the loss of the proton easier. Perchloric acid is a very strong acid (p"K"a ≈ -8) and completely dissociates in water. Its chemical formula is HClO4 and it comprises a central chlorine atom with three chlorine-oxygen double bonds (Cl=O) and one chlorine-oxygen single bond (Cl—O). The singly bonded oxygen bears an extremely acidic hydrogen atom which is easily abstracted. In contrast, chloric acid (HClO3) is a weaker acid, though still quite strong (p"K"a = -1.0), while chlorous acid (HClO2, p"K"a = +2.0) and hypochlorous acid (HClO, p"K"a = +7.53) acids are weak acids.
Carboxylic acids are organic acids that contain an acidic hydroxyl group and a carbonyl (C=O bond). Carboxylic acids can be reduced to the corresponding alcohol; the replacement of an electronegative oxygen atom with two electropositive hydrogens yields a product which is essentially non-acidic. The reduction of acetic acid to ethanol using LiAlH4 (lithium aluminium hydride or LAH) and ether is an example of such a reaction.
The p"K"a for ethanol is 16, compared to 4.76 for acetic acid.
Atomic radius and bond strength.
Another factor that contributes to the ability of an acid to lose a proton is the strength of the bond between the acidic hydrogen and the atom that bears it. This, in turn, is dependent on the size of the atoms sharing the bond. For an acid HA, as the size of atom A increases, the strength of the bond decreases, meaning that it is more easily broken, and the strength of the acid increases. Bond strength is a measure of how much energy it takes to break a bond. In other words, it takes less energy to break the bond as atom A grows larger, and the proton is more easily removed by a base. This partially explains why hydrofluoric acid is considered a weak acid while the other hydrohalic acids (HCl, HBr, HI) are strong acids. Although fluorine is more electronegative than the other halogens, its atomic radius is also much smaller, so it shares a stronger bond with hydrogen. Moving down a column on the periodic table atoms become less electronegative but also significantly larger, and the size of the atom tends to dominate its acidity when sharing a bond to hydrogen. Hydrogen sulfide, H2S, is a stronger acid than water, even though oxygen is more electronegative than sulfur. Just as with the halogens, this is because sulfur is larger than oxygen and the H—S bond is more easily broken than the H—O bond.
Monoprotic acids.
Common examples of monoprotic acids in mineral acids include hydrochloric acid (HCl) and nitric acid (HNO3). On the other hand, for organic acids the term mainly indicates the presence of one carboxyl group and sometimes these acids are known as monocarboxylic acid. Examples in organic acids include formic acid (HCOOH), acetic acid (CH3COOH) and benzoic acid (C6H5COOH).
Polyprotic acids.
Polyprotic acids are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule. Specific types of polyprotic acids have more specific names, such as diprotic acid (two potential protons to donate) and triprotic acid (three potential protons to donate).
A diprotic acid (here symbolized by H2A) can undergo one or two dissociations depending on the pH. Each dissociation has its own dissociation constant, Ka1 and Ka2.
The first dissociation constant is typically greater than the second; i.e., "K"a1 > "K"a2. For example, sulfuric acid (H2SO4) can donate one proton to form the bisulfate anion (HSO4-), for which "K"a1 is very large; then it can donate a second proton to form the sulfate anion (SO42-), wherein the "K"a2 is intermediate strength. The large "K"a1 for the first dissociation makes sulfuric a strong acid. In a similar manner, the weak unstable carbonic acid (H2CO3) can lose one proton to form bicarbonate anion (HCO3-) and lose a second to form carbonate anion (CO32-). Both "K"a values are small, but "K"a1 > "K"a2.
A triprotic acid (H3A) can undergo one, two, or three dissociations and has three dissociation constants, where "K"a1 > "K"a2 > "K"a3.
An inorganic example of a triprotic acid is orthophosphoric acid (H3PO4), usually just called phosphoric acid. All three protons can be successively lost to yield H2PO4-, then HPO42-, and finally PO43-, the orthophosphate ion, usually just called phosphate. An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion. Even though the positions of the protons on the original molecule may be equivalent, the successive "K"a values will differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged.
Neutralization.
Neutralization is the basis of titration, where a pH indicator shows equivalence point when the equivalent number of moles of a base have been added to an acid. It is often wrongly assumed that neutralization should result in a solution with pH 7.0, which is only the case with similar acid and base strengths during a reaction.
Neutralization with a base weaker than the acid results in a weakly acidic salt. An example is the weakly acidic ammonium chloride, which is produced from the strong acid hydrogen chloride and the weak base ammonia. Conversely, neutralizing a weak acid with a strong base gives a weakly basic salt, e.g. sodium fluoride from hydrogen fluoride and sodium hydroxide.
Weak acid/weak base equilibria.
In order to lose a proton, it is necessary that the pH of the system rise above the p"K"a of the protonated acid. The decreased concentration of H+ in that basic solution shifts the equilibrium towards the conjugate base form (the deprotonated form of the acid). In lower-pH (more acidic) solutions, there is a high enough H+ concentration in the solution to cause the acid to remain in its protonated form, or to protonate its conjugate base (the deprotonated form).
Solutions of weak acids and salts of their conjugate bases form buffer solutions.
Applications of acids.
There are numerous uses for acids. Acids are often used to remove rust and other corrosion from metals in a process known as pickling. They may be used as an electrolyte in a wet cell battery, such as sulfuric acid in a car battery.
Strong acids, sulfuric acid in particular, are widely used in mineral processing. For example, phosphate minerals react with sulfuric acid to produce phosphoric acid for the production of phosphate fertilizers, and zinc is produced by dissolving zinc oxide into sulfuric acid, purifying the solution and electrowinning.
In the chemical industry, acids react in neutralization reactions to produce salts. For example, nitric acid reacts with ammonia to produce ammonium nitrate, a fertilizer. Additionally, carboxylic acids can be esterified with alcohols, to produce esters.
Acids are used as catalysts; for example, sulfuric acid is used in very large quantities in the alkylation process to produce gasoline. Strong acids, such as sulfuric, phosphoric and hydrochloric acids also effect dehydration and condensation reactions.
Acids are used as additives to drinks and foods, as they alter their taste and serve as preservatives. Phosphoric acid, for example, is a component of cola drinks.
Biological occurrence.
Many biologically important molecules are acids. Nucleic acids, including DNA and RNA contain the genetic code that determines much of an organism's characteristics, and is passed from parents to offspring. DNA contains the chemical blueprint for the synthesis of proteins which are made up of amino acid subunits. Cell membranes contain fatty acid esters such as phospholipids.
An α-amino acid has a central carbon (the α or "alpha" carbon) which is covalently bonded to a carboxyl group (thus they are carboxylic acids), an amino group, a hydrogen atom and a variable group. The variable group, also called the R group or side chain, determines the identity and many of the properties of the a specific amino acid. In glycine, the simplest amino acid, the R group is a hydrogen atom, but in all other amino acids it is contains one or more carbon atoms bonded to hydrogens, and may contain other elements such as sulfur, oxygen or nitrogen. With the exception of glycine, naturally occurring amino acids are chiral and almost invariably occur in the L-configuration. Peptidoglycan, found in some bacterial cell walls contains some D-amino acids. At physiologic pH, typically around 7, free fatty acids exist in a charged form, where the acidic carboxyl group (-COOH) loses a proton (-COO-) and the basic amine group (-NH2) gains a proton (-NH3+). The entire molecule has a net neutral charge and is a zwitterion.
Fatty acids and fatty acid derivatives are another group of carboxylic acids that play a significant role in biology. These contain long hydrocarbon chains and a carboxylic acid group on one end. The cell membrane of nearly all organisms is primarily made up of a phospholipid bilayer, a micelle of hydrophobic fatty acid esters with polar, hydrophilic phosphate "head" groups. Membranes contain additional components, some of which can participate in acid-base reactions.
In humans and many other animals, hydrochloric acid is a part of the gastric acid secreted within the stomach to help hydrolyze proteins and polysaccharides, as well as converting the inactive pro-enzyme, pepsinogen into the enzyme, pepsin. Some organisms produce acids for defense; for example, ants produce formic acid.
Acid-base equilibrium plays a critical role in regulating mammalian breathing. Oxygen gas (O2) drives cellular respiration, the process by which animals release the chemical potential energy stored in food, producing carbon dioxide (CO2) as a byproduct. Oxygen and carbon dioxide are exchanged in the lungs, and the body responds to changing energy demands by adjusting the rate of ventilation. For example, during periods of exertion the body rapidly breaks down stored carbohydrates and fat, releasing CO2 into the blood stream. In aqueous solutions such as blood CO2 exists in equilibrium with carbonic acid and bicarbonate ion.
It is the decrease in pH that signals the brain to breath faster and deeper, expelling the excess CO2 and resupplying the cells with O2.
Cell membranes are generally impermeable to charged or large, polar molecules because of the lipophilic fatty acyl chains comprising their interior. Many biologically important molecules, including a number of pharmaceutical agents, are organic weak acids which can cross the membrane in their protonated, uncharged form but not in their charged form (i.e. as the conjugate base). For this reason the activity of many drugs can be enhanced or inhibited by the use of antacids or acidic foods. The charged form, however, is often more soluble in blood and cytosol, both aqueous environments. When the extracellular environment is more acidic than the neutral pH within the cell, certain acids will exist in their neutral form and will be membrane soluble, allowing them to cross the phospholipid bilayer. Acids that lose a proton at the intracellular pH will exist in their soluble, charged form and are thus able to diffuse through the cytosol to their target. Ibuprofen, aspirin and penicillin are examples of drugs that are weak acids.
---END.OF.DOCUMENT---

Asphalt.
Asphalt () is a sticky, black and highly viscous liquid or semi-solid that is present in most crude petroleums and in some natural deposits sometimes termed asphaltum. It is most commonly modelled as a colloid, with "asphaltenes" as the dispersed phase and ' as the continuous phase (though there is some disagreement amongst chemists regarding its structure). One writer states that although a "considerable amount of work has been done on the composition of asphalt, it is exceedingly difficult to separate individual hydrocarbon in pure form", and "it is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large".
In U.S. and Polish terminology, asphalt (or asphalt cement) is the carefully refined residue from the distillation process of selected crude oils. Outside these countries, the product is often called bitumen.
The primary use of asphalt is in road construction, where it is used as the glue or binder for the aggregate particles. The road surfacing material is usually called 'asphaltic concrete', AC in North America, or 'asphalt' elsewhere. Within North America the apparent interchangeability of the words asphalt and 'bitumen' causes confusion outside the road construction industry despite quite clear definitions within industry circles.
Etymology.
The word asphalt is derived from the late Middle English: from French asphalte, based on Late Latin asphalton, asphaltum, from the Greek ásphalton, ásphaltos ("άσφαλτος"), a word of uncertain origin meaning "asphalt/bitumen/pitch" which some derive from α- "without" and σφάλλω "to make fall". Note that in French, the term asphalte is used for naturally-occurring bitumen-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads.
Another description has it that the term derives from the Accadian term "asphaltu" or "sphallo," meaning "to split." It was later adopted from the Homeric Greeks as a verb
meaning "to make firm or stable," "to secure". It is a significant fact that the first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. From the Greek, the word passed into late Latin, and thence into French ("asphalte") and English ("asphalt"). The expression "bitumen" originated in the Sanskrit, where we find the words "jatu," meaning "pitch," and "jatu-krit," meaning "pitch creating," "pitch producing" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally 'gwitu-men' (pertaining to pitch), and by others, "pixtumens" (exuding or bubbling pitch), which was subsequently shortened to "bitumen," thence passing via French into English. From the same root is derived the Anglo Saxon word "cwidu" (Mastix), the German word "Kitt" (cement or mastic) and the old Norse word "kvada".
Background.
Asphalt or bitumen can sometimes be confused with tar, which is a similar black thermo-plastic material produced by the destructive distillation of coal. During the early- and mid-twentieth century when town gas was produced, tar was a readily available product and extensively used as the binder for road aggregates. The addition of tar to macadam roads led to the word tarmac, which is now used in common parlance to refer to road making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt (bitumen) has completely overtaken the use of tar in these applications.
Asphalt can be separated from the other components in crude oil (such as naphtha, gasoline and diesel) by the process of fractional distillation, usually under vacuum conditions. A better separation can be achieved by further processing of the heavier fractions of the crude oil in a de-asphalting unit, which uses either propane or butane in a supercritical phase to dissolve the lighter molecules which are then separated. Further processing is possible by "blowing" the product: namely reacting it with oxygen. This makes the product harder and more viscous.
Natural deposits of asphalt include lake asphalts (primarily from the Pitch Lake in Trinidad and Tobago and Bermudez Lake in Venezuela), Gilsonite, the Dead Sea, and Tar Sands. Asphalt was mined at Ritchie Mines in Macfarlan in Ritchie County, West Virginia in the United States from 1852 to 1873.
Asphalt is typically stored and transported at temperatures around 150 degrees Celsius (300 °F). Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called bitumen feedstock, or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is sometimes used as a release agent, although it can mix with and thereby reduce the quality of the asphalt.
Ancient times.
In the ancient Middle East, natural asphalt deposits were used for mortar between bricks and stones, to cement parts of carvings such as eyes into place, for ship caulking, and for waterproofing. The Persian word for asphalt is "mumiya", which is related to the English word mummy. Asphalt was also used by ancient Egyptians to embalm mummies.
In the ancient Far East, natural asphalt was slowly boiled to get rid of the higher fractions, leaving a material of higher molecular weight which is thermoplastic and when layered on objects, became quite hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.
In North America, archaeological recovery has indicated that asphaltum was sometimes used to apply stone projectile points to a wooden shaft.
Early use in Europe.
The use of asphalt in the United Kingdom and United States was preceded by its use in Europe.
An 1838 edition of "Mechanics Magazine" cites an early use of asphalt in France. A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways - "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusin of dirt and filth", which at that time made the water unusable. "He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation". But it was generally neglected in France until the revolution of 1830. Then, in the 1830s, there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes". Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (BasRhin), the Parc (l'Ain) and the Puy-de-la-Poix (Puy-de-Dome)", although it could also be made artificially.
Early use in the United Kingdom.
William Salmon's "Polygraphice" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum.
In Britain, the first patent was 'Cassell's patent asphalte or bitumen' in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson claims that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)".
In 1838, Claridge obtained patents in Scotland on 27 March, and Ireland on 23 April, and in 1851 he sought to extend the duration of all three patents. He formed "Claridge's Patent Asphalte Company" for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France", and "laid one of the first asphalt pavements in Whitehall". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park". "The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry". "By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order". Indeed in 1838, there was a flurry of entrepreneurial activity over asphalt. On the London stockmarket, there were various claims as to the priority of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, "Claridge's was the type most used in the 1840s and 50s" Claridge's own company ceased operating in 1917.
Early use in the United States.
The first use of asphaltum in the New World was by indigenous Indian tribes. On the west coast, as early as the 1200s, the Tongva and Chumash Nations collected the naturally occurring asphaltum that seeped to the surface above underlying petroleum deposits. Both tribes used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphatum to provide decorations. It was used as a sealant on baskets to make them water tight for carrying water. Asphaltum was used also to seal the planks on ocean-going canoes.
Roads in the US have been paved with asphalt since at least 1870, when a street in front of Newark, NJ's City Hall was paved. In 1876, asphalt was used to pave Pennsylvania Avenue in Washington, DC, in time for the celebration of the national centennial. Asphalt was also used for flooring, paving and waterproofing of baths and swimming pools during the early 1900s, following similar trends in Europe.
Rolled asphalt concrete.
The largest use of asphalt is for making asphalt concrete for road surfaces and accounts for approximately 85% of the asphalt consumed in the United States. Asphalt pavement material is commonly composed of 5 percent asphalt cement and 95 percent aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt cement must be heated so that it can be mixed with the aggregates at the asphalt mixing plant. There are about 4,000 asphalt mixing plants in the U.S.
Asphalt road surface is the most widely recycled material in the US, both by gross tonnage and by percentage. According to a report issued by the Federal Highway Administration and the United States Environmental Protection Agency, 80% of the asphalt from road surfaces' that is removed each year during widening and resurfacing projects is reused as part of new roads, roadbeds, shoulders and embankments.
Roofing shingles account for most of the remaining asphalt consumption. Other uses include cattle sprays, fence post treatments, and waterproofing for fabrics.
Asphalt is widely used in airports around the world. Due to the sturdiness, it is widely used for runways dedicated to aircraft landing and taking off.
Mastic asphalt.
Mastic asphalt is a type of asphalt which differs from dense graded asphalt (asphalt concrete) in that it has a higher bitumen (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt, which has only around 5% added bitumen. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form a impervious barrier about thick. There is a proper apprenticeship and trainees go to college to learn this trade.
Asphalt emulsion.
A number of technologies allow asphalt to be mixed at much lower temperatures. These involve mixing the asphalt with petroleum solvents to form "cutbacks" with reduced melting point or mixtures with water to turn the asphalt into an emulsion. Asphalt emulsions contain up to 70% asphalt and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock or gravel. Slurry Seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth and asphalt emulsions are also blended into recycled hot-mix asphalt to create low cost pavements.
Alternatives and bioasphalt.
Certain activist groups have become increasingly concerned about the global peak oil and climate change problem in recent years due to by-products that are released into the atmosphere. Most of the emissions are derived primarily from burning fossil fuels. This has led to the introduction of petroleum bitumen alternatives that are more environmentally friendly and non-toxic.
---END.OF.DOCUMENT---

American National Standards Institute.
The American National Standards Institute or ANSI () is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide. For example, standards make sure that people who own cameras can find the film they need for that camera anywhere around the globe.
ANSI accredits standards that are developed by representatives of standards developing organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.
The organization's headquarters are in Washington, DC. ANSI's operations office is located in New York City.
History.
ANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became the United States of America Standards Institute (USASI). The present name was adopted in 1969.
Prior to 1918, these five engineering societies,
the American Institute of Electrical Engineers (AIEE, now IEEE),
American Society of Mechanical Engineers (ASME),
American Society of Civil Engineers (ASCE),
the American Institute of Mining and Metallurgical Engineers (now AIME),
and the American Society for Testing Materials (now ASTM International),
had been members of the United Engineering Society (UES).
At the behest of the AIEE, they invited the U.S. government Departments of War, Navy and Commerce to join in founding a national standards organization.
According to Paul G. Agnew, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies.
In 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.http://www.iec.ch/
Members.
ANSI's membership comprises government agencies, organizations, corporations, academic and international bodies, and individuals. In total, the Institute represents the interests of more than 125,000 companies and 3.5 million professionals.
Process.
Though ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process.
ANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.
Voluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.
International activities.
In addition to facilitating the formation of standards in the U.S., ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.
The Institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.
Examples.
Each of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.
In 2009, ANSI and the National Institute for Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC). NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.
---END.OF.DOCUMENT---

Apollo 11.
The Apollo 11 mission landed the first humans on the Moon. Launched on July 16, 1969, the third lunar mission of NASA's Apollo Program was crewed by Commander Neil Alden Armstrong, Command Module Pilot Michael Collins, and Lunar Module Pilot Edwin Eugene 'Buzz' Aldrin, Jr. On July 20, Armstrong and Aldrin became the first humans to walk on the Moon, while Collins orbited in the Command Module.
The mission fulfilled President John F. Kennedy's goal of reaching the moon by the end of the 1960s, which he had expressed during a speech given before a joint session of Congress on May 25, 1961: "I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
Crew.
Each crewmember of Apollo 11 had made a spaceflight before this mission, making it the third all-veteran crew in manned spaceflight history. Collins was originally slated to be the Command Module Pilot (CMP) on Apollo 8 but was removed when he required surgery on his back and was replaced by Jim Lovell, his backup for that flight. After Collins was medically cleared, he took what would have been Lovell's spot on Apollo 11; as a veteran of Apollo 8, Lovell was transferred to Apollo 11's backup crew, but promoted to backup commander.
Backup crew.
In early 1969 Bill Anders accepted a job with the National Space Council effective in August 1969 and announced his retirement as an astronaut. At that point Ken Mattingly was moved from the support crew into parallel training with Anders as backup Command Module Pilot in case Apollo 11 was delayed past its intended July launch (at which point Anders would be unavailable if needed) and would later join Lovell's crew and ultimately be assigned as the original Apollo 13 CMP.
Nomenclature.
The lunar module was named "Eagle" for the national bird of the United States, the bald eagle, and featured prominently on the mission insignia. The command module was named "Columbia" for the feminine personification of the United States used traditionally in song and poetry. During early mission planning, the names "Snowcone" and "Haystack" were used but changed before announcement to the press.
Launch and lunar orbit injection.
In addition to throngs of people crowding highways and beaches near the launch site, millions watched the event on television, with NASA Chief of Public Information Jack King providing commentary. President Richard Nixon viewed the proceedings from the Oval Office of the White House.
A Saturn V launched "Apollo 11" from Launch Pad 39A, part of the Launch Complex 39 site at the Kennedy Space Center on July 16, 1969 at 13:32:00 UTC (9:32:00 a.m. local time). It entered orbit 12 minutes later. After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon with the Trans Lunar Injection burn at 16:22:13 UTC. About 30 minutes later the service module pair separated from this last remaining Saturn V stage and docked with the lunar module still nestled in the Lunar Module Adaptor. After the lunar module was extracted, the combined spacecraft headed for the Moon, while the third stage booster flew on a trajectory past the moon and into solar orbit.
On July 19 at 17:21:50 UTC, "Apollo 11" passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility (Mare Tranquillitatis) about 20 kilometers (12 mi) southwest of the crater Sabine D (0.67408N, 23.47297E). The landing site was selected in part because it had been characterized as relatively flat and smooth by the automated "Ranger 8" and "Surveyor 5" landers along with the "Lunar Orbiter" mapping spacecraft and unlikely to present major landing or extra-vehicular activity (EVA) challenges.
Lunar descent.
On July 20, 1969 the lunar module (LM) "Eagle" separated from the command module "Columbia". Collins, alone aboard "Columbia", inspected "Eagle" as it pirouetted before him to ensure the craft was not damaged.
As the descent began, Armstrong and Aldrin found that they were passing landmarks on the surface 4 seconds early and reported that they were "long": they would land miles west of their target point.
Five minutes into the descent burn, and 6000 feet above the surface of the moon, the LM navigation and guidance computer distracted the crew with the first of several unexpected "1202" and "1201" program alarms. Inside Mission Control Center in Houston, Texas, computer engineer Jack Garman told guidance officer Steve Bales it was safe to continue the descent and this was relayed to the crew. The program alarms indicated "executive overflows", where the guidance computer could not complete all of its tasks in real time and had to postpone some of them. This was neither a computer error nor an astronaut error, but stemmed from a mistake in how the astronauts had been trained. Although unneeded for the landing, the rendezvous radar was intentionally turned on to make ready for a fast abort. Ground simulation setups had not foreseen that a fast stream of spurious interrupts from this radar could happen, depending upon how the hardware randomly powered up before the LM then began nearing the lunar surface: hence the computer had to deal with data from two radars, not the landing radar alone, which led to the overload.
When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a 300 meter diameter crater (later determined to be "West crater", named for its location in the western part of the originally planned landing ellipse). Armstrong took semi-automatic control and, with Aldrin calling out altitude and velocity data, landed at 20:17 UTC on July 20 with about 25 seconds of fuel left.
"Apollo 11" landed with less fuel than other missions, and the astronauts also encountered a premature low fuel warning. This was later found to have been due to greater propellant 'slosh' than expected uncovering a fuel sensor. On subsequent missions, extra baffles were added to the tanks to prevent this.
Throughout the descent Aldrin had called out navigation data to Armstrong, who was busy piloting the LM. A few moments before the landing, a light informed Aldrin that at least one of the 67-inch probes hanging from "Eagles footpads had touched the surface, and he said "Contact light!". Three seconds later, "Eagle" landed and Armstrong said "Shutdown". Aldrin immediately said "Okay, engine stop. ACA - out of detent." Armstrong acknowledged "Out of detent. Auto" and Aldrin continued "Mode control - both auto. Descent engine command override off. Engine arm - off. 413 is in."
Charles Duke, acting as CAPCOM during the landing phase, acknowledged their landing by saying "We copy you down, Eagle".
Armstrong continued with the remainder of the post landing checklist, "Engine arm is off." before responding to Duke with the famous words, "Houston, Tranquility Base here. The "Eagle" has landed." Armstrong's abrupt change of call sign from "Eagle" to "Tranquility Base" caused momentary confusion at Mission Control and Duke remained silent for a couple of seconds before replying: "Roger, Twank...Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot!" expressing the relief of Mission Control after the unexpectedly drawn-out descent.
He then took Communion privately. At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the "Apollo 8" crew reading from the Book of Genesis) which demanded that their astronauts refrain from religious activities while in space. As such, Aldrin chose to refrain from directly mentioning this. He had kept the plan quiet (not even mentioning it to his wife) and did not reveal it publicly for several years. Buzz Aldrin was an elder at Webster Presbyterian Church in Webster, TX. His communion kit was prepared by the pastor of the church, the Rev. Dean Woodruff. Aldrin described communion on the moon and the involvement of his church and pastor in the October, 1970 edition of Guideposts magazine and in his book "Return to Earth." Webster Presbyterian possesses the chalice used on the moon, and commemorates the Lunar Communion each year on the Sunday closest to July 20.
The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, since they had been awake since early morning. However, they elected to forgo the sleep period and begin the preparations for the EVA early, thinking that they would be unable to sleep.
Lunar surface operations.
The astronauts planned placement of the Early Apollo Scientific Experiment Package (EASEP) and the U.S. flag by studying their landing site through "Eagles twin triangular windows, which gave them a 60° field of view. Preparation required longer than the two hours scheduled. Armstrong initially had some difficulties squeezing through the hatch with his Portable Life Support System (PLSS). According to veteran moonwalker John Young, a redesign of the LM to incorporate a smaller hatch had not been followed by a redesign of the PLSS backpack, so some of the highest heart rates recorded from "Apollo" astronauts occurred during LM egress and ingress.
At 02:39 UTC on Monday July 21 (10:39pm EDT, Sunday July 20), 1969, Armstrong opened the hatch, and at 02:51 UTC began his descent to the Moon's surface. The Remote Control Unit controls on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the Modular Equipment Stowage Assembly (MESA) folded against "Eagles side and activate the TV camera, and at 02:56 UTC (10:56pm EDT) he set his left foot on the surface. The first landing used slow-scan television incompatible with commercial TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor, significantly reducing the quality of the picture. The signal was received at Goldstone in the USA but with better fidelity by Honeysuckle Creek Tracking Station in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Although copies of this video in broadcast format were saved and are widely available, recordings of the original slow scan source transmission from the moon were accidentally destroyed during routine magnetic tape re-use at NASA. Archived copies of the footage were eventually located in Perth, Australia, which was one of the sites that originally received the Moon broadcast.
After describing the surface dust ("fine and almost like a powder"), Armstrong stepped off "Eagles footpad and into history as the first human to set foot on another world. It was then that he uttered his famous line "That's one small step for [a] man, one giant leap for mankind" six and a half hours after landing. Aldrin joined him, describing the view as "Magnificent desolation."
Armstrong said that moving in the Moon's gravity, one-sixth of Earth's, was "even perhaps easier than the simulations... It's absolutely no trouble to walk around".
In addition to fulfilling President John F. Kennedy's mandate to land a man on the Moon before the end of the 1960s, "Apollo 11" was an engineering test of the Apollo system; therefore, Armstrong snapped photos of the LM so engineers would be able to judge its post-landing condition. He then collected a contingency soil sample using a sample bag on a stick. He folded the bag and tucked it into a pocket on his right thigh. He removed the TV camera from the MESA, made a panoramic sweep, and mounted it on a tripod 12 m (40 ft) from the LM. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA.
Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backwards, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into "Eagles shadow produced no temperature change inside the suit, though the helmet was warmer in sunlight, so he felt cooler in shadow.
After the astronauts planted a U.S. flag on the lunar surface, they spoke with President Richard Nixon through a telephone-radio transmission which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief, out of respect of the lunar landing being Kennedy's legacy.
The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust which soiled the outer part of their suits, the integrated thermal meteoroid garment.
They deployed the EASEP, which included a passive seismograph and a laser ranging retroreflector. Then Armstrong loped about 120 m (400 ft) from the LM to snap photos at the rim of East Crater while Aldrin collected two core tubes. He used the geological hammer to pound in the tubes - the only time the hammer was used on "Apollo 11". The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 min.
During this period Mission Control used a coded phrase to warn Armstrong that his metabolic rates were high and that he should slow down. He was moving rapidly from task to task as time ran out. However, as metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension.
Lunar ascent and return.
Aldrin entered "Eagle" first. With some difficulty the astronauts lifted film and two sample boxes containing more than 22 kg (48 lb) of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor. Armstrong reminded Aldrin of a bag of memorial items in his suit pocket sleeve, and Aldrin tossed the bag down; Armstrong then jumped to the ladder's third rung and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, one Hasselblad camera, and other equipment. They then repressurised the LM, and settled down to sleep.
During this time another spacecraft, Luna 15 - an unmanned Soviet spacecraft in lunar orbit, began its own descent to the lunar surface. Launched only three days before the Apollo 11 mission, this was the third Soviet attempt to return lunar soil back to Earth. The Russian craft crashed on the moon at 15:50 UT – just a few hours before the scheduled American liftoff. In a race to reach the Moon and return to Earth, the parallel missions of Luna 15 and Apollo 11 were, in many ways, the culmination of the space race that underlay the space programs of both the United States and the Soviet Union in the 1960s. The simultaneous missions became one of the first instances of Soviet/American space cooperation as the USSR released Luna 15's flight plan to ensure it would not collide with Apollo 11, though its exact mission was unknown.
While moving within the cabin, Aldrin accidentally broke the circuit breaker that would arm the main engine for lift off from the moon. There was concern this would prevent firing the engine, stranding them on the moon. Fortunately a felt-tip pen was sufficient to activate the switch. Had this not worked, the Lunar Module circuitry could have been reconfigured to allow firing the ascent engine.
After about seven hours of rest, the crew were awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54 UTC, they lifted off in "Eagles ascent stage, carrying 21.5 kilograms of lunar samples with them, to rejoin CMP Michael Collins aboard "Columbia" in lunar orbit.
After more than 2½ hours on the lunar surface, they had left behind scientific instruments which included a retroreflector array used for the Lunar Laser Ranging Experiment and a Passive Seismic Experiment used to measure moonquakes. They also left an American flag, an Apollo 1 mission patch, and a plaque (mounted on the LM Descent Stage ladder) bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Richard M. Nixon. The inscription read "Here Men From The Planet Earth First Set Foot Upon the Moon, July 1969 A.D. We Came in Peace For All Mankind."
They also left behind a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace and a silicon message disk. The disk carries the goodwill statements by Presidents Eisenhower, Kennedy, Johnson and Nixon and messages from leaders of 73 countries around the world. The disc also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and present top management. (In his 1989 book, "Men from Earth", Aldrin says that the items included Soviet medals commemorating Cosmonauts Vladimir Komarov and Yuri Gagarin.) Also, according to Deke Slayton's book 'Moonshot', Armstrong carried with him a special diamond-studded Astronaut pin from Deke.
Film taken from the LM Ascent Stage upon liftoff from the moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine. Buzz Aldrin witnessed it topple: "The ascent stage of the LM separated...I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions usually planted the American flags at least from the LM to prevent its being blown over by the ascent engine exhaust.
After rendezvous with "Columbia", "Eagle's" ascent stage was jettisoned into lunar orbit at July 21, 1969 at 23:41 UT (7:41 PM EDT). Just before the Apollo 12 flight, it was noted that "Eagle" was still likely to be orbiting the moon. Later NASA reports mentioned that "Eagle's" orbit had decayed resulting in it impacting in an "uncertain location" on the lunar surface. The location is uncertain because the "Eagle" ascent stage was not tracked after it was jettisoned and the lunar gravity field is sufficiently uncertain to make the orbit of the spacecraft unpredictable after a short time. NASA estimated that the orbit had decayed within months and would have impacted on the Moon.
On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented,
On the return to Earth, the Guam tracking station failed, which would have prevented communication on the last segment of the Earth return. Repair was not possible until a staff member had his ten-year old son, Greg Force, do repairs made possible by his small hands. Force later was thanked by Armstrong.
Splashdown and quarantine.
On July 24, the astronauts returned home aboard the command module Columbia just before dawn at, in the Pacific Ocean 2,660 km (1,440 nm) east of Wake Island, or 380 km (210 nm) south of Johnston Atoll, and 24 km (15 mi) from the recovery ship, USS "Hornet".
Initially the command module landed upside down but was righted in several minutes by flotation bags triggered by the astronauts. A diver from the Navy helicopter hovering above attached an anchor to the command module to prevent it from drifting. Additional divers attached additional flotation collars to stabilize the module and position rafts for astronaut extraction. Though the possibility of bringing back pathogen from the lunar surface was considered remote, it was not considered impossible and NASA took great precautions at the recovery site. Astronauts were provided Biological Isolation Garment (BIG suit) by divers which were worn until they reached isolation facilities onboard the Hornet. Additionally astronauts were rubbed down with a sodium-hypochlorite solution and the command module wiped with betadine to remove any lunar dust that might be present. The raft containing decontamination materials was then intentionally sunk.
A second Sea King helicopter hoisted the astronauts aboard one by one where a NASA flight surgeon gave each a brief physical check during the half mile trip back to the Hornet. After touchdown on the Hornet, all crew exited the helicopter, leaving the flight surgeon and 3 crew. The helicopter was then lowered into hangar bay #2 where the astronauts walked the 30 feet to the Mobile Quarantine Facility (MQF) where they would begin their 21 days of quarantine, a practice that would continue for the next 3 Apollo missions before the moon was proven to be barren of life and quarantine process dropped for Apollo XV through XVII.
President Richard Nixon was aboard "Hornet" to personally welcome the astronauts back to Earth. He told the astronauts: "As a result of what you've done, the world has never been closer together before." Years later, it was publicly revealed that Nixon had prepared a speech to be given if the mission resulted in death. The lunar module had not been tested to assess if it could launch from the moon surface. After Nixon departed, the "Hornet" was brought alongside the 5 ton command module where it was placed aboard by the ship's crane, placed on a dolly and moved next to the MQF. The Hornet steamed for Pearl Harbor where the command module and MQF were airlifted to the Johnson Space Center.
The astronauts were placed in quarantine after their landing on the moon for fear that the moon might contain undiscovered pathogens, and that the astronauts might have been exposed to them during their moon walks. (The decision to do so was made in accordance with the recently passed Extra-Terrestrial Exposure Law). However, after almost three weeks in confinement (first in their trailer and later in the Lunar Receiving Laboratory at the Manned Spacecraft Center), the astronauts were given a clean bill of health. On August 13, 1969, the astronauts exited quarantine to the cheers of the American public. Parades were held in their honor in New York, Chicago, and Los Angeles on the same day. A few weeks later, they were invited by Mexico for a parade honoring them in Mexico City.
That evening in Los Angeles there was an official State Dinner to celebrate "Apollo 11", attended by Members of Congress, 44 Governors, the Chief Justice, and ambassadors from 83 nations at the Century Plaza Hotel. President Richard Nixon and Vice President Spiro T. Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom. This celebration was the beginning of a 45-day "Giant Leap" tour that brought the astronauts to 25 foreign countries and included visits with prominent leaders such as Queen Elizabeth II of the United Kingdom. Many nations would honor the first manned moon landing by issuing "Apollo 11" commemorative postage stamps or coins. Also, a few POWs held in Vietnam received letters from home a few months after the landings with those stamps to covertly let the POWs know that the United States had landed men on the moon.
On September 16, 1969, the three astronauts spoke before a joint meeting of Congress on Capitol Hill. They presented two U.S. flags, one to the House of Representatives and the other to the Senate, that had been carried to the surface of the moon with them.
Spacecraft location.
The command module is displayed at the National Air and Space Museum, Washington, D.C.. It is placed in the central exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the "Spirit of St. Louis", the Bell X-1, the North American X-15, Mercury spacecraft "Friendship 7", and Gemini 4. The quarantine trailer, the flotation collar, and the righting spheres are displayed at the Smithsonian's Udvar-Hazy Center annex near Washington Dulles International Airport in Virginia.
In 2009 the Lunar Reconnaissance Orbiter imaged the various Apollo landing sites on the surface of the moon with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts.
Mission insignia.
The patch of "Apollo 11" was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States." He chose an eagle as the symbol, put an olive branch in its beak, and drew a moon background with the earth in the distance. NASA officials said the talons of the eagle looked too "warlike" and after some discussion, the olive branch was moved to the claws. The crew decided the Roman numeral XI would not be understood in some nations and went with "Apollo 11"; they decided not to put their names on the patch, so it would "be representative of "everyone" who had worked toward a lunar landing."
All colors are natural, with blue and gold borders around the patch. The LM was named "Eagle" to match the insignia. When the Eisenhower dollar coin was released a few years later, the patch design provided the eagle for its reverse side. The design was retained for the smaller Susan B. Anthony dollar which was unveiled in 1979, ten years after the Apollo 11 mission.
40th anniversary events.
On July 15, 2009, LIFE.com published a photo gallery of never-before-seen photos of Aldrin, Collins, and Armstrong in the days before their mission. LIFE Photographer Ralph Morse covered the astronauts for years—especially in the months leading up to the July 16, 1969 launch—chronicling the crew's public and private lives. In the gallery, Morse talks with LIFE about the astronauts, the moon landing, quarantine, and rare and never-before-published photographs capturing that thrilling time.
From July 16-24 2009 NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred.
In addition, it is in the process of restoring the video footage and have released a preview of key moments. More events are listed at the website.
The John F. Kennedy Library set up a Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.
It was carried out in a technically brilliant way with risks taken... that would be inconceivable in the risk-averse world of today...The Apollo programme is arguably the greatest technical achievement of mankind to date...nothing since Apollo has come close [to] the excitement that was generated by those astronauts - Armstrong, Aldrin and the 10 others who followed them.
On May 1, 2009, Congress introduced a bill granting the three astronauts on Apollo 11 a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Senator Bill Nelson and Florida Congressman Alan Grayson.
---END.OF.DOCUMENT---

Apollo 8.
Apollo 8 was the first human spaceflight mission to escape from the gravitational field of planet Earth; the first to be captured by and escape from the gravitational field of another celestial body; and the first crewed voyage to return to planet Earth from another celestial body - Earth's Moon. The three-man crew of Mission Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders became the first humans to see the far side of the Moon with their own eyes, as well as the first humans to see planet Earth from beyond low Earth orbit. The mission was accomplished with the first manned launch of a Saturn V rocket. Apollo 8 was the second manned mission of the Apollo Program.
Originally planned as a low Earth orbit Lunar Module/Command Module test, the mission profile was changed to the more ambitious lunar orbital flight in August 1968 when the Lunar Module scheduled for the flight became delayed. The new mission's profile, procedures and personnel requirements left an uncharacteristically short time frame for training and preparation, thus placing more demands than usual on the time, talent, and discipline of the crew.
After launching on December 21, 1968, the crew took three days to travel to the Moon. They orbited ten times over the course of 20 hours, during which the crew made a Christmas Eve television broadcast in which they read the first 10 verses from the Book of Genesis. At the time, the broadcast was the most watched TV program ever. Apollo 8's successful mission paved the way for Apollo 11 to fulfill U.S. President John F. Kennedy's goal of landing a man on the Moon before the end of the decade.
Backup crew.
Lovell was originally the CMP on the back-up crew, with Michael Collins as the prime crew's CMP. However, Collins was replaced in July 1968, after suffering a cervical disc herniation that required surgery to repair.
Aldrin was originally the backup LMP. When Lovell was rotated to the prime crew, no one with experience on CSM 103 (the specific spacecraft used for the mission) was available, so Aldrin was moved to CMP and Fred Haise brought in as backup LMP. Armstrong went on to command Apollo 11, where Aldrin was returned to the Lunar Module Pilot position. Michael Collins was assigned as Command Module Pilot, although Aldrin was seated in the CMP position for Apollo 11's launch due to his training advantage via Apollo 8. Fred Haise later flew on Apollo 13.
Mission control.
The Earth-based mission control teams for Apollo 8 consisted of astronauts assigned to the support crew, as well as non-astronaut flight directors and their staffs. The support crew members were not trained to fly the mission, but were able to stand in for astronauts in meetings and be involved in the minutiae of mission planning, while the prime and backup crews trained. They also served as capcoms during the mission. For Apollo 8, these crew members included astronauts John S. Bull, Vance D. Brand, Gerald P. Carr, and Ken Mattingly. The mission control teams on Earth rotated in three shifts, each led by a flight director. The directors for Apollo 8 included Cliff Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).
Mission insignia.
The triangular shape of the insignia symbolizes the shape of the Apollo command module. It shows a red figure 8 looping around the earth and moon representing the mission number as well as the circumlunar nature of the mission. On the red number 8 are the names of the three astronauts.
The initial design of the insignia was developed by Jim Lovell. Lovell reportedly sketched the initial design while riding in the backseat of a T-38 flight from California to Houston, shortly after learning of the re-designation of the flight to become a lunar orbital mission.
Planning.
Apollo 4 and Apollo 6 had been "A" missions, each launching an unmanned Block I production model of the Apollo Command and Service Modules into Earth Orbit., scheduled for October 1968, would be a manned Earth Orbit flight of the CSM, completing the objectives for Mission "C".
Further missions relied on the readiness of the Lunar Module (LM). Production of the LM was behind schedule, with the first model arriving at Cape Canaveral in June 1968. Even then, significant defects were discovered, leading Grumman, the lead contractor for the LM, to predict that the first mission-ready LM would not be ready until at least February 1969. This would mean delaying the proposed "D" mission and endangering the program's goal of a lunar landing before the end of 1969. Even more pressing was a CIA report that the Soviets were expected to attempt to send cosmonauts on a Zond circumlunar mission before the end of the year. If the Soviets were successful in being first to get humans around the Moon, then that would greatly detract from having Americans being first to land on the Moon.
George Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August. Since the Service Module (CSM) would be ready three months before the Lunar Module, a CSM-only mission could be flown in December 1968. Instead of just repeating the "C" mission flight of Apollo 7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit. The new mission would also allow NASA to test lunar landing procedures that would otherwise have to wait until Apollo 10, the scheduled "F" mission.
Almost every senior manager at NASA agreed with this new mission, citing both confidence in the hardware and personnel, and the potential for a significant morale boost provided by a circumlunar flight. The only person who needed some convincing was James E. Webb, the NASA administrator. With the rest of his agency in support of the new mission, Webb eventually approved the mission change. The mission was officially changed from a "D" mission to a "C-Prime" Lunar Orbit mission, but was still referred to in press releases as an Earth Orbit mission at Webb's direction. No public announcement was made about the change in mission until November 12, three weeks after Apollo 7's successful Earth Orbit mission and less than 40 days before launch.
With the change in mission for Apollo 8, Director of Flight Crew Operations Deke Slayton decided to swap the crews of the D and E missions. James McDivitt, the original commander of the D mission, has said he was never offered the circumlunar flight, but would probably have turned it down, as he wanted to fly the lunar module. Borman, on the other hand, jumped at the chance: his original mission would just have been a repeat of the previous flight, except in a higher orbit. This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104.
On September 9, the crew entered the simulators to begin their preparation for the flight. By the time the mission flew, the crew had spent seven hours training for every actual hour of flight. Although all crew members were trained in all aspects of the mission, it was necessary to specialize. Borman, as commander, was given training on controlling the spacecraft during the re-entry. Lovell was trained on navigating the spacecraft in case communication was lost with the Earth. Anders was placed in charge of checking that the spacecraft was in working order.
The crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch. They talked about how before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight. The total was a tenth of the amount that the Saturn V would burn every second.
The next day, the Lindberghs watched the launch of Apollo 8 from a nearby dune. Anne Morrow Lindbergh would later write a book about the Apollo program, entitled "Earth Shine", which mentions both the launch and the mission.
Saturn V.
The Saturn V rocket used by Apollo 8 was designated SA-503, or the "03rd" model of the Saturn V ("5") Rocket to be used in the Saturn-Apollo ("SA") program. When it was erected in the Vertical Assembly Building on December 20, 1967, it was thought that the rocket would be used for an unmanned Earth-orbit test flight carrying a boilerplate Command/Service Module. Apollo 6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second stage engine failures, and a third stage that failed to reignite in orbit. Without assurances that these problems had been rectified, NASA administrators could not justify risking a manned mission until additional unmanned test flights proved that the Saturn V was ready.
Teams from the Marshall Space Flight Center (MSFC) went to work on the problems. Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew. A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket. A system using helium gas to absorb some of these vibrations was installed.
Of equal importance was the failure of three engines during flight. Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two. When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it accidentally shut down engine three's liquid oxygen due to a miswired connection. As a result, engine three failed within one second of engine two's shutdown. Further investigation revealed the same problem for the third-stage engine — a faulty igniter line. The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches.
The teams tested their solutions in August 1968 at the Marshall Space Flight Center. A Saturn stage IC was equipped with shock absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions. Once NASA administrators were convinced that the problems were solved, they gave their approval for a manned mission using SA-503.
The Apollo 8 spacecraft was placed on top of the rocket on September 21 and the rocket made the slow 3-mile (5 km) journey to the launch pad on October 9. Testing continued all through December until the day before launch, including various levels of readiness testing from 5 December through 11 December. Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on 18 December, a mere three days before the scheduled launch.
Launch and trans-lunar injection.
Apollo 8 launched at 7:51:00 a.m. Eastern Standard Time on December 21, 1968, using the Saturn V's three stages, S-IC, S-II, and S-IVB, to achieve Earth orbit. The launch phase experienced only three minor problems: The engines of the first stage, S-IC, underperformed by 0.75%, causing the engines to burn for 2.45 seconds longer than planned, and toward the end of the second stage burn, S-II, the rocket underwent pogo oscillations. Frank Borman estimated the oscillations were approximately and (±2.5 m/s²). The apogee was also slightly higher than the planned circular orbit of. In its first manned mission, the Saturn V rocket placed Apollo 8 into a Earth orbit with a period of 88 minutes and 10 seconds.
All three rocket stages fired during launch; the S-IC and S-II detached during launch. The S-IC impacted the Atlantic Ocean at and the S-II second stage at. The third stage of the rocket, S-IVB, assisted in driving the craft into Earth orbit but remained attached to later perform the Trans-Lunar Injection (TLI), the burn that would put the spacecraft on a trajectory to the Moon.
Once in Earth orbit, both the Apollo 8 crew and Mission Control spent the next 2 hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI. The proper operation of third stage of the rocket, S-IVB was crucial; In the last unmanned test, the S-IVB had failed to re-ignite for TLI.
During the flight, three fellow astronauts served on the ground as capsule communicators (usually referred to as "CAPCOMs") on a rotating schedule. The CAPCOMs were the only people who regularly communicated with the crew. Michael Collins was the first CAPCOM on duty and at 2 hours, 27 minutes and 22 seconds after launch radioed, "Apollo 8. You are Go for TLI". This communication signified that Mission Control had given official permission for Apollo 8 to go to the moon. Over the next twelve minutes before the TLI burn, the Apollo 8 crew continued to monitor the spacecraft and the rocket. The S-IVB third stage rocket ignited on time and burned perfectly for 5 minutes and 17 seconds. The burn increased the velocity of Apollo 8 to and the spacecraft's altitude at the end of the burn was. At this time, the crew also set the record for the highest speed humans had ever traveled. Although the S-IVB was sufficiently powerful to accelerate the CSM to the Earth's escape velocity, the TLI burn did not achieve this; the CSM remained in an elongated elliptical Earth orbit, and would have returned to the Earth if it had not encountered the Moon's gravitational field.
After the S-IVB had performed its required tasks, it was jettisoned. The crew then rotated the spacecraft to take some photographs of the spent stage and then practiced flying in formation with it. As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it. This marked the first time humans could view the whole Earth at once. Borman became worried that the S-IVB was staying too close to the Command/Service Module and suggested to Mission Control that the crew perform a separation maneuver. Mission Control first suggested pointing the spacecraft towards Earth and using the Reaction Control System (RCS) thrusters on the Service Module to add away from the Earth, but Borman did not want to lose sight of the S-IVB. After discussion, the crew and Mission Control decided to burn in this direction, but at instead. These discussions put the crew an hour behind their flight plan.
Five hours after launch, Mission Control sent a command to the S-IVB booster to vent its remaining fuel through its engine bell to change the booster's trajectory. This S-IVB would then pass the Moon and enter into a solar orbit, posing no further hazard to Apollo 8. The S-IVB subsequently went into a solar orbit with an inclination of 23.47° and a period of 340.80 days.
The Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth. Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1 milligray (during the course of a year, the average human receives a dose of 2 to 3 mGy). To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew. By the end of the mission, the crew experienced an average radiation dose of 1.6 mGy.
Lunar trajectory.
Jim Lovell's main job as Command Module Pilot was as navigator. Although Mission Control performed all of the actual navigation calculation, it was necessary to have a crew member serving as navigator so that the crew could successfully return to Earth in case of communication loss with Mission Control. Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon. This task proved to be difficult, as a large cloud of debris around the spacecraft formed by the venting S-IVB made it hard to distinguish the stars.
By seven hours into the mission, the crew was about one hour and 40 minutes behind flight plan due to the issues of moving away from the S-IVB and Lovell's obscured star sightings. The crew now placed the spacecraft into Passive Thermal Control (PTC), also known as "barbecue" mode. PTC involved the spacecraft rotating about once per hour along its long axis to ensure even heat distribution across the surface of the spacecraft. In direct sunlight, the spacecraft could be heated to over 200 °C while the parts in shadow would be −100 °C. These temperatures could cause the heat shield to crack or propellant lines to burst. As it was impossible to get a perfect roll, the spacecraft actually swept out a cone as it rotated. The crew had to make minor adjustments every half hour as the cone pattern got larger and larger.
The first mid-course correction came 11 hours into the flight. Testing on the ground had shown that the Service Propulsion System (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was "coated" first. Burning the engine for a short period would accomplish coating. This first correction burn was only 2.4 seconds and added about prograde (in the direction of travel). This change was less than the planned due to a bubble of helium in the oxidizer lines causing lower than expected fuel pressure. The crew had to use the small Reaction Control System (RCS) thrusters to make up the shortfall. Two later planned mid-course corrections were canceled as the Apollo 8 trajectory was found to be perfect.
Eleven hours into the flight, the crew had been awake for over 16 hours. Before launch, NASA had decided that at least one crew member should be awake at all times to deal with any issues that might arise. Borman started the first sleep shift, but between the constant radio chatter and mechanical noises, he found sleep difficult.
About an hour after starting his sleep shift, Borman requested clearance to take a Seconal sleeping pill. However, the pill had little effect. Borman eventually fell asleep but then awoke feeling ill. He vomited twice and had a bout of diarrhea that left the spacecraft full of small globules of vomit and feces that the crew cleaned up to the best of their ability. Borman initially decided that he did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control. The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed. After recording a description of Borman's illness they requested that Mission Control check the recording, stating that they "would like an evaluation of the voice comments".
The Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second floor control room (there were two identical control rooms in Houston on the second and third floor, only one of which was used during a mission). The conference participants decided that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill. Researchers now believe that he was suffering from space adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness. Space adaptation syndrome had not been an issue on previous spacecraft (Mercury and Gemini), as those astronauts were unable to move freely in the comparatively smaller cabins of those spacecraft. The increased cabin space in the Apollo Command Module afforded astronauts greater freedom of movement, contributing to symptoms of spacesickness for Borman and, later, astronaut Russell Schweickart during Apollo 9.
The cruise phase was a relatively uneventful part of the flight, except for the crew checking that the spacecraft was in working order and that they were on course. During this time, NASA scheduled a television broadcast at 31 hours after launch. The Apollo 8 crew used a 2 kg camera that broadcast in black-and-white only, using a Vidicon tube. The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens.
During this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space. However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible. Additionally, the Earth image became saturated by any bright source without proper filters. In the end, all the crew could show the people watching back on Earth was a bright blob. After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday.
By this time, the crew had completely abandoned the planned sleep shifts. Lovell went to sleep 32½ hours into the flight — 3½ hours before he had planned to. A short while later, Anders also went to sleep after taking a sleeping pill.
The crew was unable to see the Moon for much of the outward cruise. Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for the PTC. It was not until the crew had gone behind the Moon that they would be able to see it for the first time.
The Apollo 8 made a second television broadcast at 55 hours into the flight. This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens. Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth. The crew spent the transmission describing the Earth and what was visible and the colors they could see. The transmission lasted 23 minutes.
Lunar sphere of influence.
At about 55 hours and 40 minutes into the flight, the crew of Apollo 8 became the first humans to enter the gravitational sphere of influence of another celestial body. In other words, the effect of the Moon's gravitational force on Apollo 8 became stronger than that of the Earth. At the time it happened, Apollo 8 was from the Moon and had a speed of relative to the Moon. This historic moment was of little interest to the crew since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center. They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit. It was only thirteen hours until they would be in lunar orbit.
The last major event before Lunar Orbit Insertion was a second mid-course correction. It was in retrograde (against direction of travel) and slowed the spacecraft down by, effectively lowering the closest distance that the spacecraft would pass the moon. At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds. They would now pass from the lunar surface.
At 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion-1 (LOI-1). This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth. After Mission Control was polled for a Go/No Go decision, the crew was told at 68 hours, they were Go and "riding the best bird we can find". At 68 hours and 58 minutes, the spacecraft went behind the Moon and out of radio contact with the Earth.
With 10 minutes before the LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in the correct place. At that time, they finally got their first glimpses of the Moon. They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface. The LOI burn was only two minutes away, so the crew had little time to appreciate the view.
Lunar orbit.
The SPS ignited at 69 hours, 8 minutes, and 16 seconds after launch and burned for 4 minutes and 13 seconds, placing the Apollo 8 spacecraft in orbit around the Moon. The crew described the burn as being the longest four minutes of their lives. If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even flung off into space. If it lasted too long they could have impacted the Moon. After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours.
On Earth, Mission Control continued to wait. If the crew had not burned the engine or the burn had not lasted the planned length of time, the crew would appear early from behind the Moon. However, this time came and went without Apollo 8 reappearing. Exactly at the calculated moment, the signal was received from the spacecraft, indicating it was in a orbit about the Moon.
Lovell continued to describe the terrain they were passing over. One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that would be the Apollo 11 landing site. The launch time of Apollo 8 had been chosen to give the best lighting conditions for examining the site. A film camera had been set up in one of the spacecraft windows to record a frame every second of the Moon below. Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest. By the end of the mission the crew had taken 700 photographs of the Moon and 150 of the Earth.
Throughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked. He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary. He also asked that they receive a Go/No Go decision before they passed behind the Moon on each orbit.
As they reappeared for their second pass in front of the Moon, the crew set up the equipment to broadcast a view of the lunar surface. Anders described the craters that they were passing over. At the end of this second orbit they performed the eleven-second LOI-2 burn of the SPS to circularize the orbit to.
Through the next two orbits, the crew continued to keep check of the spacecraft and to observe and photograph the Moon. During the third pass, Borman read a small prayer for his church. He was scheduled to participate in a service at St. Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo 8 flight was unable. A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer which could be recorded and then replayed during the service.
Earthrise.
When the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed Earthrise for the first time in human history. Borman saw the Earth emerging from behind the lunar horizon and called in excitement to the others, taking a black-and-white photo as he did so. In the ensuing scramble Anders took the more famous colour photo, later picked by "Life" magazine as one of its hundred photos of the century. Due to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the Lunar surface. Earthrise is generally only visible when orbiting the Moon, other than at selected places near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon.
Anders continued to take photographs while Lovell assumed control of the spacecraft so Borman could rest. Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status.
Borman awoke fully, however, when he started to hear his fellow crew members make mistakes. They were beginning to not understand questions and would have to ask for the answers to be repeated. Borman realized that everyone was extremely tired having not had a good night's sleep in over three days. Taking command, he ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed. At first Anders protested saying that he was fine, but Borman would not be swayed. At last Anders agreed as long as Borman would set up the camera to continue to take automatic shots of the Moon. Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching he wanted the crew to be alert. For the next two orbits Anders and Lovell slept while Borman sat at the helm. On subsequent Apollo missions, crews would avoid this situation by sleeping on the same schedule.
As they rounded the Moon for the ninth time, the second television transmission began. Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon. Borman described it as being "a vast, lonely, forbidding expanse of nothing." Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth. Each man on board read a section from the Biblical creation story (verses 1-10) from the Book of Genesis. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth. His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit. Borman said, "And from the crew of Apollo 8, we close with good night, good luck, and a Merry Christmas, God bless all of you, "all of you on the good Earth".
The only task left for the crew at this point was to perform the Trans-Earth Injection (TEI), which was scheduled for 2½ hours after the end of the television transmission. The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in Lunar orbit, with little hope of escape. As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth.
The burn occurred exactly on time. The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated. When voice contact was regained, Lovell announced, "Please be informed, there is a Santa Claus", to which Ken Mattingly, the current CAPCOM, replied, "That's affirmative, you are the best ones to know". The spacecraft began its journey back to Earth on December 25, Christmas Day.
Unplanned manual re-alignment.
Later, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard. However, he accidentally erased some of the computer's memory, which caused the inertial measuring unit (IMU) to think the module was in the same relative position it had been in before lift-off and fire the thrusters to "correct" the module's attitude.
Once the crew realized why the computer had changed the module's attitude, they realized they would have to re-enter data that would tell the computer its real position. It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another fifteen minutes to enter the corrected data into the computer.
Sixteen months later, Lovell would once again have to perform a similar manual re-alignment, under more critical conditions, during the Apollo 13 mission, after that module's IMU had to be turned off to conserve energy. In his 1994 book, ', Lovell wrote, "My training [on Apollo 8] came in handy!". In that book he dismissed the incident as a "planned experiment", requested by the ground crew. However, in subsequent interviews Lovell has acknowledged that the incident was an accident, caused by his mistake.
Cruise back to Earth and re-entry.
The cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft. As long as the trajectory specialists had calculated everything correctly, the spacecraft would re-enter 2½ days after TEI and splashdown in the Pacific.
On Christmas afternoon, the crew made their fifth television broadcast. This time they gave a tour of the spacecraft, showing how an astronaut lived in space. When they had finished broadcasting they found a small present from Deke Slayton in the food locker—real turkey with stuffing and three miniature bottles of brandy (which remained unopened). There were also small presents to the crew from their wives. The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the earth, in a short four minute broadcast.
After two uneventful days the crew prepared for re-entry. The computer would control the re-entry and all the crew had to do was put the spacecraft in the correct attitude, blunt end forward. If the computer broke down, Borman would take over.
Once the Command Module was separated from the Service Module, the astronauts were committed to re-entry. Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been predicted by the trajectory specialists. As they hit the thin outer atmosphere they noticed it was becoming hazy outside as glowing plasma formed around the spacecraft. The spacecraft started slowing down and the deceleration peaked at 6 g (59 m/s²). With the computer controlling the descent by changing the attitude of the spacecraft, Apollo 8 rose briefly like a skipping stone before descending to the ocean. At the drogue parachute stabilized the spacecraft and was followed at by the three main parachutes. The spacecraft splashdown position was estimated to be.
When it hit the water, the parachutes dragged the spacecraft over and left it upside down, in what was termed Stable 2 position. As they were buffeted by a swell, Borman was sick, waiting for the three flotation balloons to right the spacecraft. It was 43 minutes after splashdown before the first frogman from the USS "Yorktown" arrived, as the spacecraft had landed before sunrise. Forty-five minutes later, the crew was safe on the deck of the aircraft carrier.
Historical importance.
Apollo 8 came at the end of 1968, a year that had seen much upheaval around the world. Yet, "TIME" magazine chose the crew of Apollo 8 as their Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year. They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body. They had survived a mission that even the crew themselves had rated as only having a fifty-fifty chance of fully succeeding. The effect of Apollo 8 can be summed up by a telegram from a stranger, received by Borman after the mission, that simply stated, "Thank you Apollo 8. You saved 1968."
One of the most famous aspects of the flight was the Earthrise picture that was taken as they came around for their fourth orbit of the Moon. This was the first time that humans had taken such a picture whilst actually behind the camera, and it has been credited with a role in inspiring the first Earth Day in 1970. It was selected as the first of "Life" magazine's 'hundred photos that changed the world'. Apollo 8 is regarded by some as the most historically significant of all the Apollo missions.
The mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn in 1962. There were 1200 journalists covering the mission, with the BBC coverage being broadcast in 54 countries in 15 different languages. The Soviet newspaper "Pravda" featured a quote from Boris Nikolaevich Petrov, Chairman of the Soviet Intercosmos program, who described the flight as an "outstanding achievement of American space sciences and technology". It is estimated that a quarter of the people alive at the time saw — either live or delayed — the Christmas Eve transmission during the ninth orbit of the Moon. The Apollo 8 broadcasts won an Emmy, the highest honor given by the Academy of Television Arts and Sciences.
Atheist Madalyn Murray O'Hair later caused controversy by bringing a lawsuit against NASA over the reading from "Genesis". O'Hair wished the courts to ban US astronauts — who were all Government employees — from public prayer in space. Though the case was rejected by the US Supreme Court for lack of jurisdiction, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program. Buzz Aldrin, on Apollo 11, self-communicated Presbyterian Communion on the surface of the moon after landing; he refrained from mentioning this publicly for several years, and only obliquely referred to it at the time.
In 1969, the US Postal Service issued a postage stamp (1371) commemorating the Apollo 8 flight around the moon. The stamp featured a detail of the famous photograph of the Earthrise over the moon taken by Anders on Christmas Eve, and the words, "In the beginning God..."
Mission parameters.
The mission parameters for Apollo 8 differed significantly from those of previous flights, for several reasons. As the first manned spacecraft to orbit multiple celestial bodies, the mission recorded two different sets of orbital parameters. The mission was also the first to execute a translunar injection.
While in parking orbit around the Earth, Apollo 8 maintained altitude between a perigee of and an apogee of. The inclination of this orbit, or its angle in relation to the equator, was 32.51°. Each orbit had a period of 88.17 minutes.
In contrast, the spacecraft orbited the Moon at more varying altitudes. At its lowest altitude above the moon's surface, the spacecraft had a pericynthion of, while the highest altitude, or apocynthion, was. The spacecraft took 128.7 minutes to complete each of its 10 circuits around the Moon, at an inclination of 12°.
The spacecraft began its translunar injection burn on December 21, 1968, at 15:41:38 UTC. The burn represented the second of two burns on the Saturn V rocket's S-IVB third stage. The rocket burned for a total of 318 seconds, propelling the spacecraft from an Earth parking orbit velocity of to a translunar trajectory velocity of.
Spacecraft location.
The command module is now displayed at the Chicago Museum of Science and Industry, along with a collection of personal items from the flight donated by Lovell and the spacesuit worn by Frank Borman. Jim Lovell's Apollo 8 spacesuit is on public display in the Visitor Center at NASA's Glenn Research Center. Bill Anders' spacesuit is on display at the Science Museum in London, England.
In film.
Apollo 8's historic mission has been shown and referred to in several forms, both documentary and fiction. The various television transmissions and 16 mm footage shot by the crew of Apollo 8 was compiled and released by NASA in the 1969 documentary, "Debrief: Apollo 8", which was hosted by Burgess Meredith. In addition, Spacecraft Films released a three-disc DVD set covering the mission in 2003. Portions of the Apollo 8 Mission can be seen in the 1989 documentary "For All Mankind", which won the Grand Jury Prize at the Sundance Film Festival for Outstanding Documentary. The Apollo 8 mission was well covered in the British documentary: 'In the Shadow of the Moon'.
Apollo 8 was mentioned in the film "Apollo 13," though only briefly.
Portions of the Apollo 8 mission are dramatized in the miniseries "From the Earth to the Moon" episode "1968". The S-IVB stage of Apollo 8 was also portrayed as the location of an alien device in the 1970 "UFO" episode "Conflict".
---END.OF.DOCUMENT---

Astronaut.
An astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft.
While generally reserved for professional space travelers, the term is sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.
Until 2003, astronauts were sponsored and trained exclusively by governments, either by the military, or by civilian space agencies. With the sub-orbital flight of the privately-funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.
Definition.
The criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.
As of September 19, 2009, a total of 505 humans from 38 countries have reached 100 km or more in altitude, of which 502 reached Low Earth orbit or beyond.
Of these, 24 people have traveled beyond Low Earth orbit, to either lunar or trans-lunar orbit or to the surface of the moon; three of the 24 did so twice: Jim Lovell, John Young and Eugene Cernan.
Under the U. S. definition, 496 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded 50 miles in altitude, seven reached above but below 100 kilometers (about 62 miles).
Space travelers have spent over 30,400 person-days (or a cumulative total of over 83 years) in space, including over 100 astronaut-days of spacewalks.
As of 2008, the man with the longest time in space is Sergei K. Krikalev, who has spent 803 days, 9 hours and 39 minutes, or 2.2 years, in space.
Peggy A. Whitson holds the record for most time in space by a woman, 377 days.
English-speaking nations.
In the United States, Canada, United Kingdom, and many other English-speaking nations, a professional space traveler is called an "astronaut". The term derives from the Greek words "ástron" (ἄστρον), meaning "star", and "nautes" (ναύτης), meaning "sailor". The first known use of the term "astronaut" in the modern sense was by Neil R. Jones in his short story "The Death's Head Meteor" in 1930. The word itself had been known earlier. For example, in Percy Greg's 1880 book "Across the Zodiac", "astronaut" referred to a spacecraft. In "Les Navigateurs de l'Infini" (1925) of J.-H. Rosny aîné, the word "astronautique" (astronautic) was used. The word may have been inspired by "aeronaut", an older term for an air traveler first applied (in 1784) to balloonists.
NASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.
Russian.
By convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a cosmonaut in English texts. The word is an anglicisation of the Russian word "kosmonavt" (), which in turn derives from the Greek words "kosmos" (κόσμος), meaning "universe", and "nautes" (ναύτης), meaning "sailor". For the most part, "cosmonaut" and "astronaut" are synonyms in all languages, and the usage of choice is often dictated by political reasons.
Yuri Gagarin, Russian, is the first human cosmonaut. Valentina Tereshkova, Russian, is the first woman cosmonaut. On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, arguably becoming the first "American cosmonaut" in the process.
Chinese.
Official English-language texts issued by the government of the People's Republic of China use "astronaut" while texts in Russian use "космонавт" ("kosmonavt"). In China, the terms "yǔhángyuán" (, "sailing personnel in universe") or "hángtiānyuán" (, "sailing personnel in sky") have long been used for astronauts. The phrase "tàikōng rén" (, "spaceman") is often used in Taiwan and Hong Kong.
The term taikonaut is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as "a hybrid of the Chinese term "taikong" (space) and the Greek "naut" (sailor)"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the "Shenzhou 5" spacecraft. This is the term used by Xinhua in the English version of the Chinese People's Daily since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups, while Chen Lan (), almost simultaneously, announced it at his "Go Taikonauts!" GeoCities page.
Other terms.
With the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term "spaceflight participant" to distinguish those space travelers from astronauts on missions coordinated by those two agencies.
While no nation other than Russia (formerly the Soviet Union), the United States, and China has launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term spationaut (French spelling: "spationaute") is sometimes used to describe French space travelers, from the Latin word "spatium" or space, and the Malay term "angkasawan" was used to describe participants in the Angkasawan program.
Space travel milestones.
The first human in space was Russian Yuri Gagarin, who was launched into space on April 12, 1961 aboard Vostok 1 and orbited around the Earth for 108 minutes. There are allegations that Gagarin ejected from landing module after re-entering the atmosphere and parachuted back, due to safety concerns about the craft's landing systems. The first woman in space was Russian Valentina Tereshkova, launched in June 1963 aboard Vostok 6.
Alan Shepard became the first American and second person in space on May 5, 1961 on a 15-minute sub-orbital flight. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983.
The first mission to orbit the moon was "Apollo 8", which included William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968. In April 1985, Taylor Wang became the first ethnic Chinese person in space. On 15 October 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.
The Soviet Union, through its Intercosmos program, allowed people from other "socialist" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions. An example is Vladimír Remek, a Czechoslovak, who became the first non-Soviet European in space in 1978 on a Russian Soyuz-U rocket.
On July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37.
Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, Guion Bluford became the first African American to fly into space. The first person born in Africa to fly in space was Patrick Baudry, in 1985. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.
With the larger number of seats available on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft.
In 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.
In 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.
Age milestones.
The youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2. (Titov was also the first person to suffer space sickness).
The oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.
Duration and distance milestones.
The longest stay in space was 438 days, by Russian Valeri Polyakov.
As of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was 401,056 km, when Jim Lovell, John Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.
Civilian and non-government milestones.
The first civilian in space was Neil Armstrong, who had retired from the United States Navy before his first spaceflight on Gemini 8. The first person in space who had never been a member of any country's armed forces was Harrison Schmitt, a geologist who first flew in space on Apollo 17. Both Armstrong and Schmitt were directly employed by NASA.
The first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was "Research Cosmonaut". Akiyama suffered severe space-sickness during his mission, which affected his productivity.
The first self-funded space tourist was Dennis Tito onboard the Russian spacecraft Soyuz TM-3 on 28 April 2001.
Training.
The first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and Russia tended to be jet fighter pilots, and were often test pilots.
Once selected, NASA astronauts go through 20 months of training in a variety of areas, including training for extra-vehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training may also experience short periods of weightlessness in aircraft called the "vomit comet", the nickname given to a pair of modified KC-135s (retired in 2000 and 2004 respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are done out of Edwards Air Force Base.
Mission Specialist Educator.
Mission Specialist Educators, or "Educator Astronauts", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.
Barbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.
The Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.
Health risks of space travel.
Astronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, orthostatic intolerance due to volume loss, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare. For more information on the health hazards faced by astronauts, go to the article entitled Space medicine.
Insignia.
At NASA, people who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed 50 miles (80 km) in altitude.
Deaths.
Eighteen astronauts have lost their lives during spaceflight, on four missions. By nationality, they are thirteen Americans, three Russians, one Ukrainian, and one Israeli. Several others have died while training for space missions.
The Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, a civilian spaceflight participant who died in the Challenger disaster, and an international astronaut who was killed in the Columbia disaster.
---END.OF.DOCUMENT---

A Modest Proposal.
"A Modest Proposal: For Preventing the Children of Poor People in Ireland from Being a Burden to Their Parents or Country, and for Making Them Beneficial to the Publick", commonly referred to as "A Modest Proposal", is a Juvenalian satirical essay written and published anonymously by Jonathan Swift in 1729. Swift appears to suggest in his essay that the impoverished Irish might ease their economic troubles by selling children as food for rich gentlemen and ladies. By doing this he mocks the authority of the British officials.
Details.
Swift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion. He uses common methods of argument throughout his essay, such as appealing to the authority of "a very knowing American of my acquaintance in London" and "the famous Psalmanazar, a native of the island Formosa" (who had already confessed to "not" being from Formosa in 1706). Swift couches his arguments in then-current events, exploiting common prejudice against Catholics (misnomed "Papists") and pointing out their depredations of England. After enumerating the benefits of his proposal, Swift addresses possible objections including the depopulation of Ireland and a litany of other solutions which he dismisses as impractical.
This essay is widely held to be one of the greatest examples of sustained irony in the history of the English language. Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states, "A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout."
Readers unacquainted with its reputation as a satirical work often do not immediately realize that Swift was not seriously proposing cannibalism and infanticide, nor would readers unfamiliar with the satires of Horace and Juvenal recognize that Swift's essay follows the rules and structure of Latin satires.
The satirical element of the pamphlet is often only understood after the reader notes the allusions made by Swift to the attitudes of landlords, such as the following: "I grant this food may be somewhat dear, and therefore very proper for Landlords, who as they have already devoured most of the Parents, seem to have the best Title to the Children." Swift extends the metaphor to get in a few jibes at England’s mistreatment of Ireland, noting that "For this kind of commodity will not bear exportation, and flesh being of too tender a consistence, to admit a long continuance in salt, although perhaps I could name a country, which would be glad to eat up our whole nation without it."
Population solutions.
It has been argued that Swift’s main target in "A Modest Proposal" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills. Swift was especially insulted by projects that tried to fix population and labor issues with a simple cure-all solution. A memorable example of these sorts of schemes "involved the idea of running the poor through a joint-stock company". In response, Swift’s "Modest Proposal" was "a burlesque of projects concerning the poor", that were in vogue during the early 18th century.
"A Modest Proposal" also targets the calculating way people perceived the poor in designing their projects. The pamphlet targets reformers who "regard people as commodities". In the piece, Swift adopts the "technique of a political arithmetician" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics.
Critics differ about Swift’s intentions in using this faux-mathematical philosophy. Edmund Wilson argues that statistically "the logic of the 'Modest proposal' can be compared with defense of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population". Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that "springs from a spirit of bitter mockery, not from the delight in calculations for their own sake".
Rhetoric.
Charles K. Smith argues that Swift’s rhetorical style persuades the reader to detest the speaker and pity the Irish. Swift’s specific strategy is twofold, using a "trap" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, "details vividly and with rhetorical emphasis the grinding poverty" but feels emotion solely for members of his own class. Swift’s use of gripping details of poverty and his narrator’s cool approach towards them creates "two opposing points of view" which "alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way".
Swift has his proposer further degrade the Irish by using language ordinarily reserved for animals. Lewis argues that the speaker uses "the vocabulary of animal husbandry" to describe the Irish. Once the children have been commoditized, Swift’s rhetoric can easily turn "people into animals, then meat, and from meat, logically, into tonnage worth a price per pound".
Swift uses the proposer’s serious tone to highlight the absurdity of his proposal. In making his argument, the speaker uses the conventional, text book approved order of argument from Swift’s time. The contrast between the "careful control against the almost inconceivable perversion of his scheme" and "the ridiculousness of the proposal" create a situation in which the reader has "to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan".
Tertullian’s "Apology".
Some scholars have argued that "A Modest Proposal" was largely influenced and inspired by Tertullian’s "Apology". While Tertullian’s "Apology" is a satirical attack against early Roman persecution of Christianity, Swift’s "A Modest Proposal" addresses the Anglo-Irish situation in the 1720s. James William Johnson believes that Swift saw major similarities between the two situations. Johnson notes Swift’s obvious affinity for Tertullian and the bold stylistic and structural similarities between the works "A Modest Proposal" and "Apology". In structure, Johnson points out the same central theme; that of cannibalism and the eating of babies; and the same final argument; that "human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human". Stylistically, Swift and Tertullian share the same command of sarcasm and language. In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony. Baker notes the uncanny way that both authors imply an ironic "justification by ownership" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the English mistreatment of the Irish poor.
Economic themes.
Robert Phiddian's article "Have you eaten yet? The Reader in A Modest Proposal" focuses on two aspects of "A Modest Proposal": the voice of Swift and the voice of the Proposer. Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satiric voice of Jonathan Swift and the apparent economic projections of the Proposer. He reminds readers that "there is a gap between the narrator’s meaning and the text’s, and that a moral-political argument is being carried out by means of parody".
While Swift’s proposal is obviously not a serious economic proposal, George Wittkowsky, author of "Swift’s Modest Proposal: The Biography of an Early Georgian Pamphlet", argues that it in order to understand the piece fully, it is important to understand the economics of Swift’s time. Wittowsky argues that not enough critics have taken the time to focus directly on the mercantilism and theories of labor in 18th century England. "[I]f one regards the "Modest Proposal" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact". At the start of a new industrial age in the 18th century, it was believed that "people are the riches of the nation", and there was a general faith in an economy which paid its workers low wages because high wages would mean workers would work less. Furthermore, "in the mercantilist view no child was too young to go into industry". In those times, the "somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity".
People are the riches of a nation.
Louis A. Landa presents Swift’s "A Modest Proposal" as a critique of the popular and unjustified maxim of mercantilism in the eighteenth century that "people are the riches of a nation". Swift presents the dire state of Ireland and shows that mere population itself, in Ireland’s case, did not always mean greater wealth and economy. The uncontrolled maxim fails to take into account that a person that does not produce in an economic or political way makes a country poorer, not richer. Swift also recognizes the implications of such a fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens. Swift however, Landa argues, is not merely criticizing economic maxims but also addressing the fact that England was denying Irish citizens their natural rights and dehumanizing them by viewing them as a mere commodity.
Modern usage.
"A Modest Proposal" is included in many literature programs as an example of early modern western satire. It also serves as an exceptional introduction to the concept and use of argumentative language, lending itself well to secondary and post-secondary essay courses. Outside of the realm of English studies, "A Modest Proposal" is a relevant piece included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences.
It has been emulated many times as well. In his book "A Modest Proposal" (1984), evangelical author Frank Schaeffer emulated Swift's work in social conservative polemic against abortion and euthanasia in a future dystopia that advocated recycling of aborted embryos and fetuses, as well as some disabled infants with compound intellectual, physical and physiological difficulties. (Such Baby Doe Rules cases were then a major concern of the pro-life movement of the early 1980s, which viewed selective treatment of those infants as disability discrimination.)
In Hunter S. Thompson's, which contains hundreds of private letters written by Thompson over the years, contains a letter in which he uses "A Modest Proposals satire technique against the Vietnam War. Thompson writes a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find. This letter protests the burning of Vietnamese people occurring overseas.
In popular culture.
The game "Orphan Feast" on Cartoon Network's Adult Swim website is loosely based on "A Modest Proposal".
The show "Sealab 2021" references "A Modest Proposal" by the character of Jodene Sparks. It was suggested as recommended reading when Debbie wanted a child.
"A Modest Proposal" is the name of The University of Texas at Dallas', the monthly opinion paper of the University; and was the name of a regular column in of Harvard University, a satire publication which also takes its name from Johnathan Swift.
"A Modest Proposal" is mentioned in the 1996 film "The Birdcage".
Controversial American political activist and disbarred attorney Jack Thompson's A Modest Video Game Proposal draws its title from "A Modest Proposal".
One of the radio presenters in the game Saint's Row claims he has "A modest proposal" which is to apply shock collars to all immigrants in America.
---END.OF.DOCUMENT---

Alkali metal.
The alkali metals are a series of chemical elements forming Group 1 (IUPAC style) of the periodic table: lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). (Hydrogen, although nominally also a member of Group 1, very rarely exhibits behavior comparable to the alkali metals). The alkali metals provide one of the best examples of group trends in properties in the periodic table, with well characterized homologous behavior down the group.
Properties.
The alkali metals are all highly reactive and are never found in elemental form in nature. As a result, in the laboratory they are stored under mineral oil or paraffin oil. They also tarnish easily and have low melting points and densities. Potassium and rubidium possess a weak radioactive characteristic due to the presence of long duration radioactive isotopes.
The alkali metals are silver-colored (caesium has a golden tinge), soft, low-density metals, which react readily with halogens to form ionic salts, and with water to form strongly alkaline (basic) hydroxides. These elements all have one electron in their outermost shell, so the energetically preferred state of achieving a filled electron shell is to lose one electron to form a singly charged positive ion, i.e. cation.
Hydrogen, with a solitary electron, is usually placed at the top of Group 1 of the periodic table, but it is not considered an alkali metal; rather it exists naturally as a diatomic gas. Removal of its single electron requires considerably more energy than removal of the outer electron for the alkali metals. As in the halogens, only one additional electron is required to fill in the outermost shell of the hydrogen atom, so hydrogen can in some circumstances behave like a halogen, forming the negative hydride ion. Binary compounds of hydride with the alkali metals and some transition metals have been prepared. Under extremely high pressure, such as is found at the core of Jupiter, hydrogen does become metallic and behaves like an alkali metal; see metallic hydrogen.
Alkali metals have the lowest ionization potentials in their respective periods, as removing the single electron from the outermost shell gives them the stable inert gas configuration. Their second ionization potentials are very high, as removing an electron from a species having a noble gas configuration is very difficult.
Alkali metal + water → Alkali metal hydroxide + hydrogen gas
Trends.
The alkali metals show a number of trends when moving down the group - for instance: decreasing electronegativity, increasing reactivity, and decreasing melting and boiling point. Density generally increases, with the notable exception of potassium being less dense than sodium, and the possible exception of francium being less dense than caesium.
Compounds.
Alkali metals form a very wide range of amalgams.
---END.OF.DOCUMENT---

Argument form.
In logic, the argument form or "test form" of an argument results from replacing the different words, or sentences, that make up the argument with letters, along the lines of algebra; the letters represent logical "variables". This is of importance since the validity of an argument is determined solely by its form. The "sentence forms" which classify argument forms of common important arguments are studied in logic.
Example.
To demonstrate the important notion of the "form" of an argument, substitute letters for similar items throughout the sentences in the original argument.
All we have done in the "Argument form" is to put 'S' for 'human' and 'humans', 'P' for 'mortal', and 'a' for 'Socrates'; what results, is the "form" of the original argument. Moreover, each individual sentence of the "Argument form" is the "sentence form" of its respective sentence in the original argument.
Importance.
Attention is given to argument and sentence form, because "form is what makes an argument valid or cogent". Some examples of valid argument forms are modus ponens, modus tollens, disjunctive syllogism, hypothethical syllogism and dilemma. Two invalid argument forms are affirming the consequent and denying the antecedent.
---END.OF.DOCUMENT---

Alphabet.
The word "alphabet" came into Middle English from the Late Latin word Alphabetum, which in turn originated in the Ancient Greek "Αλφάβητος" Alphabetos, from "alpha" and "beta," the first two letters of the Greek alphabet. "Alpha" and "beta" in turn came from the first two letters of the Phoenician alphabet, and meant "ox" and "house" respectively. There are dozens of alphabets in use today, the most common being Latin, deriving from the first true alphabet, Greek.
Most of them are composed of lines (linear writing); notable exceptions are Braille, fingerspelling (Sign language), and Morse code.
Linguistic definition and context.
The term alphabet prototypically refers to a writing system that has characters (graphemes) which represent both consonant and vowel sounds, even though there may not be a complete one-to-one correspondence between symbol and sound.
A grapheme is an abstract entity which may be physically represented by different styles of glyphs. There are many written entities which do not form part of the alphabet, including numerals, mathematical symbols, and punctuation. Some human languages are commonly written using a combination of logograms (which represent morphemes or words) and syllabaries (which represent syllables) instead of an alphabet. Egyptian hieroglyphs and Chinese characters are two of the best-known writing systems with predominantly non-alphabetic representations.
Non-written languages may also be represented alphabetically. For example, linguists researching a non-written language (such as some of the indigenous Amerindian languages) will use the International Phonetic Alphabet to enable them to write down the sounds they hear.
Most, if not all, linguistic writing systems have some means for phonetic approximation of foreign words, usually using the native character set.
The English alphabet has 26 letters in it.
Middle Eastern Scripts.
The history of the alphabet started in ancient Egypt. By 2700 BC Egyptian writing had a set of some 24 hieroglyphs which are called uniliterals, to represent syllables that begin with a single consonant of their language, plus a vowel (or no vowel) to be supplied by the native speaker. These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names.
However, although seemingly alphabetic in nature, the original Egyptian uniliterals were not a system and were never used by themselves to encode Egyptian speech. In the Middle Bronze Age an apparently "alphabetic" system known as the Proto-Sinaitic script is thought by some to have been developed in the Sinai peninsula during the 19th century BC, by Canaanite workers in the Egyptian turquoise mines. Others suggest the alphabet was developed in central Egypt during the 15th century BC for or by Semitic workers, but only one of these early writings has been deciphered and their exact nature remains open to interpretation. Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs. This script had no characters representing vowels. An alphabetic cuneiform script with 30 signs including 3 which indicate the following vowel was invented in Ugarit before the 15th century BC. This script was not used after the destruction of Ugarit.
The Proto-Sinatic or Proto-Canaanite script eventually developed into the Proto-Canaanite alphabet, which in turn was refined into the Phoenician alphabet. The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram.This script is the parent script of all western alphabets.At the tenth century two other forms can be distinguished namely Canaanite and Aramaic.The Aramaic gave rise to Hebrew. The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez alphabet (an abugida) is descended. Note that the scripts mentioned above are not considered proper alphabets, as they all lack characters representing vowels. These vowelless alphabets are called abjads, currently exemplified in scripts including Arabic, Hebrew, and Syriac.The omission of vowels was not a satisfactory solution and some "weak" consonants were used to indicate the vowel quality of a syllable.(matres lectionis).These had dual function since they were also used as pure consonants.
The Proto-Sinatic or Proto Canaanite script and the Ugaritic script were the first scripts with limited number of signs, in contrast to the other widely used writing systems at the time, Cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenecian script was probably the first phonemic script and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn. Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically.
The script was spread by the Phoenicians, across the Mediterranean. In Greece, the script was modified to add the vowels, giving rise to the ancestor of all alphabets in the West. The indication of the vowels is the same way as the indication of the consonants, therefore it was the first true alphabet. The Greeks took letters which did not represent sounds that existed in Greek, and changed them to represent the vowels. The vowels are significant in the Greek language, and the syllabical Linear B script which was used by the Mycenean Greeks from the 16th century BC had 87 symbols including 5 vowels. In its early years, there were many variants of the Greek alphabet, a situation which caused many different alphabets to evolve from it.
European alphabets.
The Cumae form of the Greek alphabet was carried over by Greek colonists from Euboea to the Italian peninsula, where it gave rise to a variety of alphabets used to inscribe the Italic languages. One of these became the Latin alphabet, which was spread across Europe as the Romans expanded their empire. Even after the fall of the Roman state, the alphabet survived in intellectual and religious works. It eventually became used for the descendant languages of Latin (the Romance languages) and then for most of the other languages of Europe.
Another notable script is Elder Futhark, which is believed to have evolved out of one of the Old Italic alphabets. Elder Futhark gave rise to a variety of alphabets known collectively as the Runic alphabets. The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages. Its usage was mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood. These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century.
The Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic alphabet. The Cyrillic alphabet is one of the most widely used modern alphabets, and is notable for its use in Slavic languages and also for other languages within the former Soviet Union. Variants include the Serbian, Macedonian, Bulgarian, and Russian alphabets. The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was invented by the Bulgarian scholar Clement of Ohrid, who was their disciple. They feature many letters that appear to have been borrowed from or influenced by the Greek alphabet and the Hebrew alphabet.
Asian alphabets.
Beyond the logographic Chinese writing, many phonetic scripts are in existence in Asia. The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet, but because these writing systems are largely consonant-based they are often not considered true alphabets.
Most alphabetic scripts of India and Eastern Asia are descended from the Brahmi script, which is often believed to be a descendent of Aramaic.
In Korea, the Hangul alphabet was created by Sejong the Great in 1443. Understanding of the phonetic alphabet of Mongolian Phagspa script aided the creation of a phonetic script suited to the spoken Korean language. Mongolian Phagspa script was in turn derived from the Brahmi script. Hangul is a unique alphabet in a variety of ways: it is a featural alphabet, where many of the letters are designed from a sound's place of articulation (P to look like widened mouth, L sound to look like tongue pulled in, etc.); its design was planned by the government of the time; and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters, to allow for mixed script writing (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block).
Zhuyin (sometimes called "Bopomofo") is a semi-syllabary used to phonetically transcribe Mandarin Chinese in the Republic of China. After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited, but it's still widely used in Taiwan where the Republic of China still governs. Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet the phonemes of syllable initials are represented by individual symbols, but like a syllabary the phonemes of the syllable finals are not; rather, each possible final (excluding the medial glide) is represented by its own symbol. For example, "luan" is represented as ㄌㄨㄢ ("l-u-an"), where the last symbol ㄢ represents the entire final "-an". While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a romanization system that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cell phones.
European alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad (as with Urdu and Persian) and sometimes as a complete alphabet (as with Kurdish and Uyghur)
Types.
The term "alphabet" is used by linguists and paleographers in both a wide and a narrow sense. In the wider sense, an alphabet is a script that is "segmental" at the phoneme level that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish "true" alphabets from two other types of segmental script, abjads and abugidas. These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with diacritics to or a systematic graphic modification of the consonants. In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters. The earliest known alphabet in the wider sense is the Wadi el-Hol script, believed to be an abjad, which through its successor Phoenician is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet) and Hebrew (via Aramaic).
Examples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean hangul; and abugidas are used to write Tigrinya Amharic, Hindi, and Thai. The Canadian Aboriginal syllabics are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant which is modified by rotation to represent the following vowel. (In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.)
The boundaries between the three types of segmental scripts are not always clear-cut. For example, Sorani Kurdish is written in the Arabic script, which is normally an abjad. However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas. On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but all vowel marks were written after the preceding consonant rather than as diacritic marks. Although short "a" was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet. Conversely, the vowel marks of the Tigrinya abugida and the Amharic abugida (ironically, the original source of the term "abugida") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became logographic. (See below.)
Thus the primary classification of alphabets reflects how they treat vowels. For tonal languages, further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas. Such scripts are to tone what abjads are to vowels. Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas. This is the case for Vietnamese (a true alphabet) and Thai (an abugida). In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation. In the Pollard script, an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang. For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in Zhuyin not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the virama of Indic.
The number of letters in an alphabet can be quite small. The Book Pahlavi script, an abjad, had only twelve letters at one point, and may have had even fewer later on. Today the Rotokas alphabet has only twelve letters. (The Hawaiian alphabet is sometimes claimed to be as small, but it actually consists of 18 letters, including the ʻokina and five long vowels.) While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been "conflated" that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in Arabic, another script that lost many of its distinct letter shapes. For example, a comma-shaped letter represented "g, d, y, k," or "j". However, such apparent simplifications can perversely make a script more complicated. In later Pahlavi papyri, up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole that is, they had become logograms as in Egyptian Demotic.
The largest segmental script is probably an abugida, Devanagari. When written in Devanagari, Vedic Sanskrit has an alphabet of 53 letters, including the "visarga" mark for final aspiration and special letters for "kš" and "jñ," though one of the letters is theoretical and not actually used. The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the "khutma" letters (letters with a dot added) to represent sounds from Persian and English.
The largest known abjad is Sindhi, with 51 letters. The largest alphabets in the narrow sense include Kabardian and Abkhaz (for Cyrillic), with 58 and 56 letters, respectively, and Slovak (for the Latin alphabet), with 46. However, these scripts either count di- and tri-graphs as separate letters, as Spanish did with "ch" and "ll" until recently, or uses diacritics like Slovak "č". The largest true alphabet where each letter is graphically independent is probably Georgian, with 41 letters.
Syllabaries typically contain 50 to 400 glyphs (though the Múra-Pirahã language of Brazil would require only 24 if it did not denote tone, and Rotokas would require only 30), and the glyphs of logographic systems typically number from the many hundreds into the thousands. Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.
Alphabetic order.
It is not always clear what constitutes a distinct alphabet. French uses the same basic alphabet as English, but many of the letters can carry additional marks, such as é, à, and ô. In French, these combinations are not considered to be additional letters. However, in Icelandic, the accented letters such as á, í, and ö are considered to be distinct letters of the alphabet. In Spanish, ñ is considered a separate letter, but accented vowels such as á and é are not. The ll and ch were also considered single letters, distinct from a single l followed by an l and c followed by an h, respectively, but in 1994 the Real Academia Española changed them so that ll is between lk and lm in the dictionary and ch is between cg and ci.
In German, words starting with "sch-" (constituting the German phoneme /ʃ/) would be intercalated between words with initial "sca-" and "sci-" (all incidentally loanwords) instead of this graphic cluster appearing after the letter s, as though it were a single letter – a lexicographical policy which would be de rigueur in a dictionary of Albanian, i.e. "dh-", "gj-", "ll-", "rr-", "th-", "xh-" and "zh-" (all representing phonemes and considered separate single letters) would follow the letters d, g, l, n, r, t, x and z respectively. Nor is, in a dictionary of English, the lexical section with initial "th-" reserved a place after the letter t, but is inserted between "te-" and "ti-". German words with umlaut would further be alphabetized as if there were no umlaut at all – contrary to Turkish which allegedly adopted the Swedish graphemes ö and ü, and where a word like tüfek, "gun", would come after tuz, "salt", in the dictionary.
The Danish and Norwegian alphabets end with æ – ø – å, whereas the Swedish and the Finnish ones conventionally put å – ä – ö at the end.
Some adaptations of the Latin alphabet are augmented with ligatures, such as æ in Old English and Icelandic and Ȣ in Algonquian; by borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and by modifying existing letters, such as the eth ð of Old English and Icelandic, which is a modified "d". Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and Italian, which uses the letters "j, k, x, y" and "w" only in foreign words.
It is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required. However, a dozen Ugaritic tablets from the fourteenth century BC preserve the alphabet in two sequences. One, the "ABCDE" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, "HMĦLQ," was used in southern Arabia and is preserved today in Ethiopic. Both orders have therefore been stable for at least 3000 years.
The historical order was abandoned in Runic and Arabic, although Arabic retains the traditional "abjadi order" for numbering.
The Brahmic family of alphabets used in India use a unique order based on phonology: The letters are arranged according to how and where they are produced in the mouth. This organization is used in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet.
The Phoenician letter names, in which each letter is associated with a word that begins with that sound, continue to be used in Samaritan, Aramaic, Syriac, Hebrew, and Greek. However, they were abandoned in Arabic, Cyrillic and Latin.
Orthography and spelling.
Each language may establish rules that govern the association between letters and phonemes, but, depending on the language, these rules may or may not be consistently followed. In a perfectly phonological alphabet, the phonemes and letters would correspond perfectly in two directions: a writer could predict the spelling of a word given its pronunciation, and a speaker could predict the pronunciation of a word given its spelling. However, languages often evolve independently of their writing systems, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.
National languages generally elect to address the problem of dialects by simply associating the alphabet with the national standard. However, with an international language with wide variations in its dialects, such as English, it would be impossible to represent the language in all its variations with a single phonetic alphabet.
Some national languages like Finnish, Turkish and Bulgarian have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes. Strictly speaking, there is no word in the Finnish, Turkish and Bulgarian languages corresponding to the verb "to spell" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables. Similarly, the Italian verb corresponding to 'spell', "compitare", is unknown to many Italians because the act of spelling itself is almost never needed: each phoneme of Standard Italian is represented in only one way. However, pronunciation cannot always be predicted from spelling in cases of irregular syllabic stress. In standard Spanish, it is possible to tell the pronunciation of a word from its spelling, but not vice versa; this is because certain phonemes can be represented in more than one way, but a given letter is consistently pronounced. French, with its silent letters and its heavy use of nasal vowels and elision, may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation are actually consistent and predictable with a fair degree of accuracy.
At the other extreme, are languages such as English, where the spelling of many words simply has to be memorized as they do not correspond to sounds in a consistent way. For English, this is partly because the Great Vowel Shift occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate.
Sometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system itself, as when Turkey switched from the Arabic alphabet to the Roman alphabet.
The sounds of speech of all languages of the world can be written by a rather small universal phonetic alphabet. A standard for this is the International Phonetic Alphabet.
---END.OF.DOCUMENT---

Atomic number.
In chemistry and physics, the atomic number (also known as the proton number) is the number of protons found in the nucleus of an atom and therefore identical to the charge number of the nucleus. It is conventionally represented by the symbol "Z". The atomic number uniquely identifies a chemical element. In an atom of neutral charge, the atomic number is also equal to the number of electrons.
The atomic number, "Z", should not be confused with the mass number, "A", which is the total number of protons and neutrons in the nucleus of an atom. The number of neutrons, "N", is known as the neutron number of the atom; thus, "A" = "Z" + "N". Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes), the atomic mass of an atom is roughly equal to "A".
Atoms having the same atomic number Z but different neutron number "N", and hence different atomic mass, are known as isotopes. Most naturally occurring elements exist as a mixture of isotopes, and the average atomic mass of this mixture determines the element's atomic weight. The current standard for the atomic mass unit (amu), also termed the dalton (Da) is defined to be exactly of the mass of a free (unbound) neutral atom in its ground (lowest-energy) state.
History.
Loosely speaking, the existence of a periodic table creates an ordering for the elements. Such an ordering is not necessarily a numbering, but can be used to construct a numbering by fiat.
Dmitri Mendeleev claimed he arranged his tables in order of atomic weight ("Atomgewicht") However, in deference to the observed chemical properties, he violated his own rule and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, "Z", but this number was not known or suspected at the time.
A simple numbering based on periodic table position was never entirely satisfactory. Besides iodine and tellurium, several other pairs of elements (such as cobalt and nickel) were known to have nearly identical or reversed atomic weights, leaving their placement in the periodic table by chemical properties to be in violation of known physical properties. Another problem was that the gradual identification of more and more chemically similar and indistinguishable lanthanides, which were of an uncertain number, led to inconsistency and uncertainty in the numbering of all elements at least from lutetium (element 71) onwards (hafnium was not known at this time).
In 1911 Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% off the figure for the atomic number in gold (Z=79, A=197), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element Z=79 on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was "exactly" equal to its place in the periodic table (also known as element number, atomic number, and symbolized Z). This proved eventually to be the case.
The experimental situation improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fit the Bohr theory's demand that the frequency of the spectral lines be proportional to a measure of the square of Z.
To do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum (Z=13) to gold (Z= 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons (x-rays) increased from one target to the next in a linear fashion. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the proton number "Z". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members — no fewer and no more — which was far from obvious from the chemistry at that time.
The conventional symbol Z presumably comes from the German word "Atomz'"ahl" (atomic number).
Chemical properties.
Each element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is "Z". The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of "any" mixture of atoms with a given atomic number.
New elements.
The quest for new elements is usually described using atomic numbers. As of early 2007, elements with atomic numbers 1 to 116 and 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life becomes shorter as atomic number increases, though an "island of stability" may exist for undiscovered isotopes with certain numbers of protons and neutrons.
---END.OF.DOCUMENT---

Anatomy.
Anatomy (from the Greek " anatomia", from " ana: separate, apart from, and temnein", to cut up, cut open. Also from the Greek word "anatome"--ana: apart, tome: to cut-->To cut apart.) is a branch of biology and medicine that is the consideration of the structure of living things. It is a general term that includes human anatomy, animal anatomy (zootomy) and plant anatomy (phytotomy). In some of its facets anatomy is closely related to embryology, comparative anatomy and comparative embryology, through common roots in evolution.
Anatomy is subdivided into gross anatomy (or macroscopic anatomy) and microscopic anatomy. Gross anatomy (also called topographical anatomy, regional anatomy, or anthropotomy) is the study of anatomical structures that can be seen by unaided vision with the naked eye. Microscopic anatomy is the study of minute anatomical structures assisted with microscopes, which includes histology (the study of the organization of tissues), and cytology (the study of cells).
The history of anatomy has been characterized, over time, by a continually developing understanding of the functions of organs and structures in the body. Methods have also improved dramatically, advancing from examination of animals through dissection of cadavers (dead human bodies) to technologically complex techniques developed in the 20th century including X-ray, ultrasound, and MRI imaging.
Anatomy should not be confused with anatomical pathology (also called morbid anatomy or histopathology), which is the study of the gross and microscopic appearances of diseased organs.
Superficial anatomy.
Superficial anatomy or surface anatomy is important in anatomy being the study of anatomical landmarks that can be readily seen from the contours or the surface of the body. With knowledge of superficial anatomy, physicians or veterinary surgeons gauge the position and anatomy of the associated deeper structures.
Human anatomy.
Human anatomy, including gross human anatomy and histology, is primarily the scientific study of the morphology of the adult human body.
Generally, students of certain biological sciences, paramedics, physiotherapists, occupational therapy, nurses, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope; and in addition, medical students generally also learn gross anatomy with practical experience of dissection and inspection of cadavers (dead human bodies).
Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has recently been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by all medical doctors, especially surgeons, and doctors working in some diagnostic specialities, such as histopathology and radiology.
Academic human anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
---END.OF.DOCUMENT---

Affirming the consequent.
An argument of this form is invalid, i.e., the conclusion can be false even when statements 1 and 2 are true. Since "P" was never asserted as the "only" sufficient condition for "Q", other factors could account for "Q" (while "P" was false).
The name "affirming the consequent" derives from the premise "Q", which affirms the "then" clause of the conditional premise.
Owning Fort Knox is not the "only" way to be rich. There are any number of other ways to be rich.
But having the flu is not the "only" cause of a sore throat since many illnesses cause sore throat, such as the common cold or strep throat.
The following is a more subtle version of the fallacy embedded into conversation.
B attempts to falsify A's conditional statement ("if Republican then pro-life") by providing evidence he believes would contradict its implication. However, B's example of his uncle does not contradict A's statement, which says nothing about non-Republicans. What would be needed to disprove A's assertion are examples of Republicans who are not pro-life.
Tautologies.
If claims "P" and "Q" express the same proposition, then the argument would be trivially valid, as it would beg the question.
This is also the case for definitions. For example.
In everyday discourse, however, such cases are rare. The validity of such definitions is due to the fact that definitions can be expressed as an if and only if (see below).
Clearly if the definition of "bachelor" is "an unmarried male", then the propositional statement: "A is a bachelor" if and only if "A is an unmarried male", must be true.
In normal speech it is awkward to use the phrase "if and only if", so we substitute the valid but less complete "if", giving the conventional form which is similar to the form of the formal fallacy.
If and only if.
The above argument may be valid, but only if the claim "if he's outside, then he's not inside" follows from the first premise. More to the point, the validity of the argument stems not from affirming the consequent, but affirming the antecedent.
Such if and only if statements often make their way into detective mysteries.
Use of the fallacy in science.
However, such reasoning is still affirming the consequent and logically invalid (e.g., Let "P" = geocentrism and "Q" = sunrise and sunset.) The strength of such reasoning as an inductive inference depends on the likelihood of alternative hypotheses, which shows that such reasoning is based on additional premises, not merely on affirming the consequent.
In addition, testing scientific theories involves repeated rounds of affirming the consequent as new data come in. The repetitive use eliminates competing theories (those that are inconsistent with the newest data: more technically, it is the law of contraposition that plays the role in elimination), leaving behind only theories that have proved to be consistent with all tests performed to date.
---END.OF.DOCUMENT---

Andrei Tarkovsky.
Andrei Arsenyevich Tarkovsky () (April 4, 1932–December 29, 1986) was a Soviet and Russian filmmaker, writer, film editor, film theorist and opera director.
Tarkovsky's films include "Andrei Rublev", "Solaris", "The Mirror", and "Stalker". He directed the first five of his seven feature films in the Soviet Union; his last two films were produced in Italy and Sweden. They are characterized by spirituality and metaphysical themes, extremely long takes, lack of conventional dramatic structure and plot, and memorable cinematography.
Ingmar Bergman said of him: "Tarkovsky for me is the greatest [director], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream".
Childhood and early life.
Tarkovsky was born in the village of Zavrazhye in Ivanovo Oblast, the son of poet and translator Arseny Alexandrovich Tarkovsky, native of Kirovohrad, Ukraine, and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute.
Tarkovsky spent his childhood in Yuryevets. He was described by childhood friends as active and popular, having many friends and being typically in the center of action. In 1937, his father left the family, subsequently volunteering for the army in 1941. Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press. In 1939, Tarkovsky enrolled at the Moscow School № 554. During the war, the three evacuated to Yuryevets, living with his maternal grandmother. In 1943, the family returned to Moscow. Tarkovsky continued his studies at his old school, where the poet Andrey Voznesensky was one of his classmates. He learned the piano at a music school and attended classes at an art school. The family lived on Shshipok Street in the Zamoskvorechye District in Moscow. From November 1947 to spring 1948, he was in a hospital with tuberculosis. Many themes of his childhood - the evacuation, his mother and her two children, the withdrawn father, the time in the hospital - feature prominently in his film "The Mirror".
Following high school graduation, from 1951 to 1952, Tarkovsky studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the USSR. Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold. He participated in a year-long research expedition to the river Kureikye near Turukhansk in the Krasnoyarsk Province. During this time in the Taiga Tarkovsky decided to study film.
Film school student.
Upon return from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film-directing-program. He was in the same class as Irma Raush, whom he married in April 1957.
The early Khrushchev era offered unique opportunities for young film directors. Before 1953, annual film production was low and most films were directed by veteran directors. After 1953, more films were produced, many of them by young directors. The Khrushchev Thaw opened Soviet society and allowed, to some degree, Western literature, films and music. This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson and Mizoguchi. Tarkovsky absorbed the idea of the auteur as a necessary condition for creativity.
Tarkovsky’s teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors. In 1956, Tarkovsky directed his first student short film, "The Killers", from a short story of Ernest Hemingway. The short film "There Will Be No Leave Today" and the screenplay "Concentrate" followed in 1958 and 1959.
An important influence on Tarkovsky was the film director Grigori Chukhrai, who was teaching at the VGIK. Impressed by the talent of his student, Chukhrai offered Tarkovsky a position as assistant director for his film "Clear Skies". Tarkovsky initially showed interest, but then decided to concentrate on his studies and his own projects.
During his third year at the VGIK, Tarkovsky met Andrei Konchalovsky. They found much in common as they liked the same film directors and shared ideas on cinema and films. In 1959, they wrote the script "Antarctica - Distant Country", which was later published in the "Moskovskij Komsomolets". Tarkovsky submitted the script to Lenfilm, which was rejected. They were more successful with the script "The Steamroller and the Violin," which they sold to Mosfilm. This film became Tarkovsky’s diploma film, earning him his diploma in 1960 and winning first prize at the New York Student Film Festival in 1961.
Film career in the Soviet Union.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He had inherited the film from director Eduard Abalov, who had to abort the project. The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in 1962. In the same year, on September 30, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born.
In 1965, he directed the film "Andrei Rublev" about the life of Andrei Rublev, the 15th century Russian icon painter. "Andrei Rublev" was not immediately released after completion due to problems with Soviet authorities. Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths. A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize. The film was officially released in the Soviet Union in a cut version in 1971.
He divorced his wife, Irma Raush, in June 1970. In the same year, he married Larissa Kizilova (née Egorkina), who had been a production assistant for the film "Andrei Rublev" (they had been living together since 1965). Their son, Andrei Tarkovsky Jr., was born in the same year on August 7.
In 1972, he completed "Solaris", an adaptation of the novel "Solaris" by Stanisław Lem. He had worked on this together with screenwriter Fridrikh Gorenshtein, as early as 1968. The film was presented at the Cannes Film Festival and won the Grand Prix Spécial du Jury and the FIPRESCI prize and was nominated for the Palme d'Or. From 1973 to 1974, he shot the film "The Mirror", a highly autobiographical film drawing on his childhood and incorporating some of his father's poems. Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles "Confession", "White day" and "A white, white day". From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature. Russian authorities placed the film in the "third category" which meant severe limitations on its distribution, allowing it to be shown only in third class cinemas and workers' clubs. Few prints were made and the filmmakers received no returns. Third category films also placed the filmmakers in danger of being accused of wasting public funds, which could have serious effects on their future productivity. These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry.
During 1975, Tarkovsky also worked on the screenplay "Hoffmanniana", about the German writer and poet E. T. A. Hoffmann. In December 1976, he directed "Hamlet", his only stage play, at the Lenkom Theatre in Moscow. The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films. At the end of 1978, he also wrote the screenplay "Sardor" together with the writer Aleksandr Misharin.
The last film Tarkovsky completed in the Soviet Union was "Stalker", inspired by the novel "Roadside Picnic" by the brothers Arkady and Boris Strugatsky. Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986. Initially he wanted to shoot a film based on their novel "Dead Mountaineer's Hotel" and he developed a raw script. Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on "Roadside Picnic". Work on this film began in 1976. The production was mired in troubles; improper development of the negatives had ruined all the exterior shots. Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where Tarkovsky hired Alexander Knyazhinsky as a new first cinematographer. Furthermore, Tarkovsky suffered a heart attack in April 1978, resulting in further delay. The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival.
In the same year Tarkovsky also began the production of the film "The First Day" (Russian: Pervyy Dyen), based on a script by his friend and longterm collaborator Andrei Konchalovsky. The film was set in 18th century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov in the main role. To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, leaving out several scenes that were critical of the official atheism in the Soviet Union. After finishing shooting of roughly one half of the film, the project was stopped by Goskino, after it became apparent that the film differed from the script submitted to the censors. Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film.
Film career outside the Soviet Union.
During the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary "Voyage in Time", together with his longtime friend Tonino Guerra. Tarkovsky returned to Italy in 1980 for an extended trip during which he and Tonino Guerra completed the script for the film "Nostalghia". During 1981 he traveled to the United Kingdom and Sweden. During his trip to Sweden he had considered defecting from the Soviet Union, but ultimately decided to return because of his wife and his son.
Tarkovsky returned to Italy in 1982 to start shooting "Nostalghia". He did not return to his home country. As Mosfilm withdrew from the project, he had to complete the film with financial support provided by the Italian RAI. Tarkovsky completed the film in 1983. "Nostalghia" was presented at the Cannes Film Festival and won the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. Soviet authorities prevented the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again. In the same year, he also arranged the opera "Boris Godunov" at the Royal Opera House in London under the musical direction of Claudio Abbado.
He spent most of 1984 preparing the film "The Sacrifice". At a press conference in Milan on July 10, 1984, he announced that he would never return to the Soviet Union and would remain in the West. At that time, his son Andrei Jr. was still in the Soviet Union and not allowed to leave the country.
During 1985, he shot the film "The Sacrifice" in Sweden. At the end of the year he was diagnosed with terminal lung cancer. In January 1986, he began treatment in Paris, and was joined there by his wife and his son, who were finally allowed to leave the Soviet Union. "The Sacrifice" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son, Andrei Jr.
In Tarkovsky's last diary entry (December 15, 1986), he wrote: "But now I have no strength left - that is the problem". The diaries are sometimes also known as "Martyrolog" and were published posthumously in 1989 and in English in 1991.
Tarkovsky died in Paris on December 29, 1986. He was buried on January 3, 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France. The inscription on his grave stone, which was created by the Russian sculptor Ernst Neizvestny, reads: "To the man who saw the Angel".
A controversy emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes, but was assassinated by the KGB. Evidence for this hypothesis includes several testimonies by former KGB agents, who claim that Viktor Chebrikov gave the order to irradiate Tarkovsky to prevent what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky. Other evidence includes several memos that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause.
As Tarkovsky, his wife Larisa Tarkovskaya and actor Anatoli Solonitsyn all died from the very same type of lung cancer, Vladimir Sharun, sound designer in "Stalker", is convinced that they were all poisoned when shooting the film near a chemical plant.
Filmography.
Tarkovsky is mainly known as a director of films. During his career he directed only seven feature films, and three short films during his time at the film school. He also wrote several screenplays, directed the play "Hamlet" for the stage in Moscow, the opera "Boris Godunov" in London, and directed a radio production of the short story "Turnabout" by William Faulkner. He also wrote "Sculpting In Time", a book on film theory.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He then directed in the Soviet Union "Andrei Rublev" in 1966, "Solaris" in 1972, "The Mirror" in 1975 and "Stalker" in 1979. The documentary "Voyage in Time" was produced in Italy in 1982, as was "Nostalghia" in 1983. His last film "The Sacrifice" was produced in Sweden in 1986. Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a co-writer. To Tarkovsky a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.
Awards.
Numerous awards were bestowed on Tarkovsky throughout his lifetime. At the Venice Film Festival he was awarded the "Golden Lion". At the Cannes Film Festival he won several times the "FIPRESCI prize", the "Prize of the Ecumenical Jury" and the "Grand Prix Spécial du Jury". He was also nominated for the "Palme d'Or" two times. In 1987, the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to "The Sacrifice".
Under the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the fall of 1986, shortly before his death, by a retrospective of his films in Moscow. After his death, an entire issue of the film magazine "Iskusstvo Kino" was devoted to Tarkovsky. In their obituaries, the film committee of the Council of Ministers of the USSR and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile.
Posthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union. In 1989 the "Andrei Tarkovsky Memorial Prize" was established, with its first recipient being the Russian animator Yuriy Norshteyn. Since 1993, the Moscow International Film Festival awards the annual "Andrei Tarkovsky Award". In 1996 the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town. A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Georgievna Karachkina in 1982, has also been named after him.
Tarkovsky has been the subject of several documentaries. Most notable is the 1988 documentary "Moscow Elegy", by Russian film director Alexander Sokurov. Sokurov's own work has been heavily influenced by Tarkovsky. The film consists mostly of narration over stock footage from Tarkovsky's films. "Directed by Andrei Tarkovsky" is 1988 documentary film by Michal Leszczylowski, an editor of the film "The Sacrifice". Film director Chris Marker produced the television documentary "One Day in the Life of Andrei Arsenevich" as an homage to Andrei Tarkovsky in 2000.
Tarkovsky is widely considered to be one of the greatest film makers of all time. Ingmar Bergman was quoted as saying: "Tarkovsky for me is the greatest [of us all], the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream". Film historian Steven Dillon claims that much of subsequent film was deeply influenced by the films of Tarkovsky.
Influences.
Tarkovsky became a film director during the mid and late 1950s, a period during which Soviet society opened to foreign films, literature and music. This allowed Tarkovsky to see films of European, American and Japanese directors, an experience which influenced his own film making. His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director.
Tarkovsky was, according to Shavka Abdusalmov, a fellow student at the film school, fascinated by Japanese films. He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight. Tarkovsky has also expressed interest in the art of Haiku and its ability to create “images in such a way that they mean nothing beyond themselves.”
In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films. The list includes: "Diary of a Country Priest" and "Mouchette", by Robert Bresson; "Winter Light", "Wild Strawberries" and "Persona", by Ingmar Bergman; "Nazarin", by Luis Buñuel; "City Lights", by Charlie Chaplin; "Ugetsu", by Kenji Mizoguchi; "Seven Samurai", by Akira Kurosawa, and "Woman in the Dunes", by Hiroshi Teshigahara. Among his favorite directors were Luis Buñuel, Kenji Mizoguchi, Ingmar Bergman, Robert Bresson, Akira Kurosawa, Michelangelo Antonioni, Jean Vigo and Carl Theodor Dreyer.
With the exception of "City Lights", the list does not contain any films of the early silent era. The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude. The list has also no films or directors from Tarkovsky's native Russia, although he rated Soviet directors such as Boris Barnet, Sergei Paradjanov and Alexander Dovzhenko highly.
Although strongly opposed to commercial cinema, in a famous exception Tarkovsky praised the blockbuster film "The Terminator", saying its "vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art". He was critical of the "brutality and low acting skills", but nevertheless impressed by this film.
Cinematic style.
Tarkovsky's films are characterised by Christian and metaphysical themes, extremely long takes, and memorable images of exceptional beauty. Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera.
Tarkovsky included levitation scenes into several of his films, most notably "Solaris". To him these scenes possess great power and are used for their photogenic value and magical inexplicability.
Water, clouds, and reflections were used by him for its surreal beauty and photogenic value, as well as its symbolism, such as waves or the form of brooks or running water.
Bells and candles are also frequent symbols. These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self reflection.
Tarkovsky developed a theory of cinema that he called "sculpting in time". By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it. Unedited movie footage transcribes time in real time. By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another.
Up to, and including, his film "The Mirror", Tarkovsky focused his cinematic works on exploring this theory. After "The Mirror", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day.
Several of Tarkovsky's films have color or black and white sequences, including for example "Andrei Rublev" which features an epilogue in color of religious icon paintings, as well as "Solaris", "The Mirror", and "Stalker", which feature monochrome and sepia sequences while otherwise being in color. In 1966, in an interview conducted shortly after finishing "Andrei Rublev", Tarkovsky dismissed color film as a "commercial gimmick" and cast doubt on the idea that contemporary films meaningfully use color. He claimed that in everyday life one does not consciously notice colors most of the time. Hence in film color should be used mainly to emphasize certain moments, but not all the time as this distracts the viewer. To him, films in color are like moving paintings or photographs, which are too beautiful to be a realistic depiction of life.
The natural elements play a large role in Tarkovsky's films. The soundtracks often contain the sounds of water dripping while the earth seems to be perpetually damp. Fire and water are usually represented together, the burning barn from "The Mirror" and candle in "Nostalghia" being two examples. "The Mirror", "Stalker", and "Nostalghia" all contain scenes in which one or several characters lay on the earth in contemplation. Wind is also used often in "The Mirror". This emphasis of moments in nature, as well as the theory of "sculpting in time" has been cited by the remodernist film movement as a major influence on their own ideas on filmmaking.
---END.OF.DOCUMENT---

Ambiguity.
Ambiguity is the property of being ambiguous, where a word, term, notation, sign, symbol, phrase, sentence, or any other form used for communication, is called ambiguous if it can be interpreted in more than one way. Ambiguity is different from vagueness, which arises when the boundaries of meaning are indistinct. Ambiguity is context-dependent: the same linguistic item (be it a word, phrase, or sentence) may be ambiguous in one context and unambiguous in another context. For a word, ambiguity typically refers to an unclear choice between different definitions as may be found in a dictionary. A sentence may be ambiguous due to different ways of parsing the same sequence of words.
Linguistic forms.
The lexical ambiguity of a word or phrase consists in its having more than one meaning in the language to which the word belongs. "Meaning" hereby refers to whatever should be captured by a good dictionary. For instance, the word “bank” has several distinct lexical definitions, including “financial institution” and “edge of a river”. Another example is as in apothecary. You could say "I bought herbs from the apothecary." This could mean you actually spoke to the apothecary (pharmacist) or went to the apothecary (drug store).
The context in which an ambiguous word is used often makes it evident which of the meanings is intended. If, for instance, someone says “I deposited $100 in the bank,” most people would not think you used a shovel to dig in the mud.
However, some linguistic contexts do not provide sufficient information to disambiguate a used word. For example, "Biweekly" can mean "fortnightly" (once every two weeks - 26 times a year), OR "twice a week" (104 times a year). If "biweekly" is used in a conversation about a meeting schedule, it may be difficult to infer which meaning was intended.
Many people believe that such lexically ambiguous, miscommunication-prone words should be avoided wherever possible, since the user generally has to waste time, effort, and attention span to define what is meant when they are used.
The use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used). The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed. An exception to this could include a politician whose "wiggle words" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from their candidate of choice. Ambiguity is a powerful tool of political science.
More problematic are words whose senses express closely related concepts. “Good,” for example, can mean “useful” or “functional” ("That’s a good hammer"), “exemplary” ("She’s a good student"), “pleasing” ("This is good soup"), “moral” ("a good person" versus "the lesson to be learned from a story"), "righteous", etc. “I have a good daughter” is not clear about which sense is intended. The various ways to apply prefixes and suffixes can also create ambiguity (“unlockable” can mean “capable of being unlocked” or “impossible to lock”).
Syntactic ambiguity arises when a complex phrase or a sentence can be parsed in more than one way. “He ate the cookies on the couch,” for example, could mean that he ate those cookies which were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies.
Spoken language can contain many more types of ambiguities, where there is more than one way to compose a set of sounds into words, for example “ice cream” and “I scream.” Such ambiguity is generally resolved according to the context. A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen.
Semantic ambiguity arises when a word or concept has an inherently diffuse meaning based on widespread or informal usage. This is often the case, for example, with idiomatic expressions whose definitions are rarely or never well-defined, and are presented in the context of a larger argument that invites a conclusion.
For example, “You could do with a new automobile. How about a test drive?” The clause “You could do with” presents a statement with such wide possible interpretation as to be essentially meaningless. Lexical ambiguity is contrasted with semantic ambiguity. The former represents a choice between a finite number of known and meaningful context-dependent interpretations. The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning. This form of ambiguity is closely related to vagueness.
Linguistic ambiguity can be a problem in law (see Ambiguity (law)), because the interpretation of written documents and oral agreements is often of paramount importance.
Intentional application.
Philosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments, because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments. For example, a politician might say “I oppose taxes that hinder economic growth.” Some will think he opposes taxes in general, because they hinder economic growth. Others may think he opposes only those taxes that he believes will hinder economic growth. In writing, the correct insertion or omission of a comma after “taxes” and the use of "which" can help reduce ambiguity here (for the first meaning, “, which” is properly used in place of “that”), or the sentence can be restructured to completely eliminate possible misinterpretation. The devious politician hopes that each constituent (politics) will interpret the above statement in the most desirable way, and think the politician supports everyone's opinion. However, the opposite can also be true - An opponent can turn a positive statement into a bad one, if the speaker uses ambiguity (intentionally or not). The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases.
In literature and rhetoric, on the other hand, ambiguity can be a useful tool. Groucho Marx’s classic joke depends on a grammatical ambiguity for its humor, for example: “Last night I shot an elephant in my pajamas. What he was doing in my pajamas I’ll never know.” Ambiguity can also be used as a comic device through a genuine intention to confuse, as does Magic: The Gathering's Unhinged © Ambiguity, which makes puns with homophones, mispunctuation, and run-ons: “Whenever a player plays a spell that counters a spell that has been played[,] or a player plays a spell that comes into play with counters, that player may counter the next spell played[,] or put an additional counter on a permanent that has already been played, but not countered.” Songs and poetry often rely on ambiguous words for artistic effect, as in the song title “Don’t It Make My Brown Eyes Blue” (where “blue” can refer to the color, or to sadness).
In narrative, ambiguity can be introduced in several ways: motive, plot, character. F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel "The Great Gatsby".
All religions debate the orthodoxy or heterodoxy of ambiguity. Christianity and Judaism employ the concept of paradox synonymously with 'ambiguity'. Ambiguity within Christianity (and other religions) is resisted by the conservatives and fundamentalists, who regard the concept as equating with 'contradiction'. Non-fundamentalist Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery which fascinates humans.
Metonymy involves the use of the name of a subcomponent part as an abbreviation, or jargon, for the name of the whole object (for example "wheels" to refer to a car, or "flowers" to refer to beautiful offspring, an entire plant, or a collection of blooming plants). In modern vocabulary critical semiotics, metonymy encompasses any potentially ambiguous word substitution that is based on contextual contiguity (located close together), or a function or process that an object performs, such as "sweet ride" to refer to a nice car. Metonym miscommunication is considered a primary mechanism of linguistic humour.
Psychology and management.
In sociology and social psychology, the term "ambiguity" is used to indicate situations that involve uncertainty. An increasing amount of research is concentrating on how people react and respond to ambiguous situations. Much of this focuses on ambiguity tolerance. A number of correlations have been found between an individual’s reaction and tolerance to ambiguity and a range of factors.
Apter and Desselles (2001) for example, found a strong correlation with such attributes and factors like a greater preference for safe as opposed to risk-based sports, a preference for endurance-type activities as opposed to explosive activities, a more organized and less casual lifestyle, greater care and precision in descriptions, a lower sensitivity to emotional and unpleasant words, a less acute sense of humor, engaging a smaller variety of sexual practices than their more risk-comfortable colleagues, a lower likelihood of the use of drugs, pornography and drink, a greater likelihood of displaying obsessional behavior.
In the field of leadership David Wilkinson (2006) found strong correlations between an individual leader's reaction to ambiguous situations and the Modes of Leadership they use, the type of creativity (Kirton (2003) and how they relate to others.
Music.
In music, pieces or sections which confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p. 79) any aspect of music. The music of Africa is often purposely ambiguous. To quote Sir Donald Francis Tovey (1935, p. 195), “Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value.”
Visual art.
In visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways. Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception.
The opposite of such ambiguous images are impossible objects.
Pictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?
Constructed language.
Some languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity. Lojban and Loglan are two related languages which have been created with this in mind. The languages can be both spoken and written. These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized. Languages composed from many diverse sources contain much ambiguity and inconsistency. The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.
Mathematical notation.
Mathematical notation, widely used in physics and other sciences, avoids many ambiguities compared to expression in natural language. However, for various reasons, several lexical, syntactic and semantic ambiguities remain.
Expressions.
Ambiguous expressions often appear in physical and mathematical texts.
It is common practice to omit multiplication signs in mathematical expressions. Also, it is common, to give the same name to a variable and a function, for example, formula_1. Then, if one sees formula_2, there is no way to distinguish, does it mean formula_1 multiplied by formula_4, or function formula_5 evaluated at argument equal to formula_4. In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning.
Creators of algorithmic languages try to avoid ambiguities. Many algorithmic languages (C++, MATLAB, Fortran) require the character * as symbol of multiplication. The language Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions. Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression f=f(x) is qualified as an error.
The order of operations may depend on the context. In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right. Until the last century, many editorials assumed that multiplication is performed first, for example, formula_7 is interpreted as formula_8; in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language. In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity.
Sometimes, one uses "italics" letters to denote elementary functions.
In the scientific journal style, the expression
formula_9
formula_10,
formula_11,
formula_12 and
formula_13, although in a slideshow, it may mean formula_14.
Comma in subscripts and superscripts sometimes is omitted; it is also ambiguous notation.
If it is written formula_15, the reader should guess from the context, does it mean a single-index object, evaluated while the subscript is equal to product of variables
formula_16, formula_12 and formula_18, or it is indication to a three-valent tensor.
The writing of formula_15 instead of formula_20 may mean that the writer either is stretched in space (for example, to reduce the publication fees, or aims to increase number of publications without considering readers. The same may apply to any other use of ambiguous notations.
Subscripts are also used to denote the argument to a function, as in formula_21.
Examples of potentially confusing ambiguous mathematical expressions.
formula_22, which could be understood to mean either formula_23 or formula_24. In addition, formula_25 may mean formula_26, as formula_27 means formula_28 (see tetration).
formula_29, which by convention means formula_30, though it might be thought to mean formula_31 since formula_32 means formula_33.
formula_34, which arguably should mean formula_35 but would commonly be understood to mean formula_36
Notations in quantum optics and quantum mechanics.
It is common to define the coherent states in quantum optics with formula_37 and states with fixed number of photons with formula_38. Then, there is an "unwritten rule": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_12photon state if the Latin characters dominate. The ambiguity becomes even worse, if formula_40 is used for the states with certain value of the coordinate, and formula_41 means the state with certain value of the momentum, which may be used in books on quantum mechanics. Such ambiguities easy lead to confusions, especially if some normalized adimensional, dimensionless variables are used. Expression formula_42 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on. The reader is supposed to guess from the context.
Ambiguous terms in physics and mathematics.
Some physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients) depends on the system of notations. Many terms are ambiguous. Each use of an ambiguous term should be preceded by the definition, suitable for a specific case.
A highly confusing term is "gain". For example, the sentence "the gain of a system should be doubled", without context, means close to nothing.
It may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled.
It may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled.
It may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state).
The term "intensity" is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.
Also, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail which still can be resolved at the background of statistical noise. See also Accuracy and precision and its talk.
The Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as "definable" or "nameable". Terms of this kind give rise to vicious circle fallacies. Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.
Mathematical interpretation of ambiguity.
In mathematics and logic, ambiguity can be considered to be an "underdetermined system" (of equations or logic) – for example, formula_43 leaves open what the value of "X" is – while its opposite is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, in an overdetermined system – such as formula_44, which has no solution – see also underdetermination.
Logical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher.
Pedagogic use of ambiguous expressions.
Ambiguity can be used as a pedagogical trick, to force students to reproduce the deduction by themselves. Some textbooks
Rigorously speaking, such an expression requires that formula_46;
even if function formula_47 is a self-Fourier function, the expression should be written as
formula_48; however, it is assumed that
the shape of the function (and even its norm
formula_49) depend on the character used to denote its argument.
If the Greek letter is used, it is assumed to be a Fourier transform of another function,
The first function is assumed, if the expression in the argument contains more characters formula_50 or formula_51, than characters formula_52, and the second function is assumed in the opposite case. Expressions like formula_53 or formula_54 contain symbols formula_50 and formula_52 in equal amounts; they are ambiguous and should be avoided in serious deduction.
---END.OF.DOCUMENT---

Animal (disambiguation).
An animal is a taxonomic member of the Kingdom Animalia.
---END.OF.DOCUMENT---

Aardvark.
The Aardvark ("Orycteropus afer") (afer: from Africa) is a medium-sized, burrowing, nocturnal mammal native to Africa. It is the only living species of all Tubulidentata, but there are known other prehistoric species and genera of Tubulidentata.
It is sometimes called "antbear", "anteater", "Cape anteater" (after the Cape of Good Hope), "earth hog" or "earth pig". The word "aardvark" is famous for being one of the first entries to appear in many encyclopaedias and even abridged dictionaries. The name comes from the Afrikaans/Dutch for "earth pig" or "ground pig" ("aarde" earth/ground, "varken" pig), because early settlers from Europe thought it resembled a domesticated pig. However, the aardvark is not closely related to the pig; rather, it is the sole recent representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form a single variable species of the genus "Orycteropus", coextensive with the family Orycteropodidae. The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance. The closest living relatives of the aardvark are the elephant shrews, along with the sirenians, hyraxes, tenrecs, and elephants. Together, these animals form the superorder Afrotheria.
Description.
Genetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa.
The aardvark is vaguely pig-like in appearance. Its body is stout with an arched back and is sparsely covered with coarse hairs. The limbs are of moderate length. The front feet have lost the pollex (or 'thumb') — resulting in four toes — but the rear feet have all five toes. Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof. The ears are disproportionately long, and the tail is very thick at the base and gradually tapers. The greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. The mouth is small and tubular, typical of species that feed on termites. The aardvark has a long, thin, snakelike, protruding tongue and elaborate structures supporting a keen sense of smell.
An aardvark's weight is typically between 40 and 65 kg. An aardvark's length is usually between 1 and 1.3 metres, and can reach lengths of 2.2 metres when its tail (which can be up to 70 centimetres) is taken into account. The aardvark is pale yellowish gray in color and often stained reddish-brown by soil. The aardvark's coat is thin and the animal's primary protection is its tough skin. The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.
Behavior.
The aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (formicivore); the only fruit eaten by aardvarks is the aardvark cucumber. An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing 10 to 30 kilometers, swinging its long nose from side to side to pick up the scent of food. When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded. It is an exceptionally fast digger, but otherwise moves fairly slowly. Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly, avoiding the dust by sealing the nostrils. When successful, the aardvark's long (as long as 30 centimeters) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin. Its keen hearing warns it of predators: lions, leopards, hyenas, and pythons.
Aside from digging out ants and termites, the aardvark also excavates burrows in which to live: temporary sites are scattered around the home range as refuges, and a main burrow is used for breeding. Main burrows can be deep and extensive, have several entrances and can be as long as 13 meters. The aardvark changes the layout of its home burrow regularly, and from time to time moves on and makes a new one; the old burrows are then inhabited by smaller animals like the African Wild Dog. Only mothers and young share burrows. If attacked in the tunnel, it will seal the tunnel off behind itself or turn around and attack with its claws.
Aardvarks only pair during the breeding season; after a gestation period of 7 months, a single cub weighing around 2 kg is born, and is able to leave the burrow to accompany its mother after only two weeks, and is eating termites at 14 weeks and is weaned by 16 weeks. At six months of age it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually capable by the season after that.
Aardvarks can live to be over 24 years old in captivity.
The aardvark's main predators are lions, leopards, hunting dogs and pythons. Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs to lash with all fours. Their thick skin also protects them to some extent.
Habitat.
Aardvarks live in subsaharan Africa, where there is suitable habitat for them to live, such as savannas, grasslands, woodlands and bushland, and available food (i.e., ants and termites).
Mythology and popular culture.
In African folklore the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants. Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree. Wrapped in a piece of skin and worn on the chest the charm is said to give the owner the ability to pass through walls or roofs at night. The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission.
The main character of Arthur, a popular animated television series for children produced by WGBH-TV and shown in more than 100 countries, is an aardvark.
One of the main characters of The Ant and the Aardvark, is a blue aardvark, voiced by John Byner, doing an impersonation of Jackie Mason. It depicts the Aardvark attempting, and failing, to catch and eat his antagonist, the Ant.
Cerebus the Aardvark was the title character of a comic-book series by Dave Sim and Gerhard that ran from 1977 to 2004, and is still sold in collected volumes of reprints.
During character development for what would eventually be Spiderman, Stan Lee and Steve Ditko originally wanted a mammalian themed superhero to add to the Marvel Universe. Preliminary ideas led to the creation of "Aardvarkman," a superhero with the ability to control hordes of soldier ants. However, in the words of Ditko "the idea of a long gross tongue seemed downright evil in nature; a tongue that long with a mouth that small seems ridiculous." The soldier ant theme eventually led to the creation of Spider-Man, and the supervillain Toad was eventually created from the failed artwork for Aardvarkman
---END.OF.DOCUMENT---

Aardwolf.
The aardwolf ("Proteles cristata") is a small, insectivorous hyena-like mammal, native to Eastern and Southern Africa. The name means "earth wolf" in Afrikaans/Dutch. It is also called "maanhaar-jackal" and "protelid". Unlike other hyenas, the diet of the aardwolf almost completely consists of termites, other insect larvae and carrion.
The aardwolf is the only surviving species of the subfamily Protelinae. Two subspecies are recognized: "Proteles cristatus cristatus" of Southern Africa, and "Proteles cristatus septentrionalis" of eastern and northeastern Africa. It is usually placed in the Hyaenidae, though formerly separated into a monotypic family, Protelidae. The aardwolf lives in the scrublands of eastern and southern Africa. These are the areas of land covered with stunted trees or shrubs. The aardwolf hides in a burrow during the day and comes out at night to search for food. It is related to hyenas, but unlike its relatives, it does not hunt large prey. This unusual animal is a mass killer-of insects. It feeds mainly on termites and can eat more than 200,000 in a single night, using its long, sticky tongue to collect them.
Physical characteristics.
The aardwolf looks most like the Striped Hyena, but is significantly smaller with a more slender muzzle, sharper ears utilized in the hunt for harvester termites, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the middle line of the neck and back, which is raised during a confrontation to make the aardwolf's size appear bigger. It is 55–80 cm long, excluding its bushy 20–30 cm tail, stands about 40–50 cm at the shoulder, and weighs between 9 and 14 kg. Its front feet have 5 toes, unlike other hyenas which have four toes. Its teeth and skull are similar to that of the hyena, although the cheek teeth are specialised for eating insects, and its tongue for licking them up. As the aardwolf ages, it will normally lose some of its teeth, though this has little impact on their feeding habits due to the soft nature of the insects they consume. It has two glands at the rear that secrete a musky fluid for marking territory and communicating with other aardwolves.
Distribution and habitat.
The aardwolf lives on open, dry plains and bushland, while avoiding mountainous areas. Due to its specific food requirements, the animal is only found in regions where termites of the family Hodotermitidae occur. Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland. For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens which are occupied for six weeks at a time.
There are two distinct populations: one in Southern Africa, and another in East and Northeast Africa. The species does not occur in the intermediary miombo forests.
Behavior.
Aardwolves are shy and nocturnal, sleeping in underground burrows by day. They usually use existing burrows of aardvarks, Old World porcupines or springhares, despite being capable of creating their own. By night, an aardwolf can consume up to 200,000 harvester termites using its sticky, long tongue. They take special care not to destroy the termite mound or consume the entire colony, which ensures that the termites can rebuild and provide a continuous supply of food. They will often memorise and return to nests to save the trouble of finding a new one. They are also known to feed on other insects, larvae, and eggs, and occasionally small mammals and birds. Unlike other hyenas, aardwolves do not scavenge or kill larger animals.
The adult aardwolf is primarily solitary while foraging for food, necessary because of the scarcity and homogeneous distribution of their insect prey. They have often been mistaken for solitary animals. In fact, they live as monogamous pairs, with their young, defending the same territory. Young aardwolves generally achieve sexual maturity after two years, and the breeding season varies depending on their location, but normally takes place during the autumn or spring. During the breeding season, unpaired male aardwolves will search their own territory as well as others' for a female to mate with. Dominant males will also mate opportunistically with the females of less dominant neighboring aardwolves. This can often result in conflict between two male aardwolves when one has wandered into another's territory. Gestation lasts between 90 and 110 days, producing one to five cubs (most often two or three) during the rainy season, when termites are active. The first six to eight weeks are spent in the den with the mother. After three months, they begin supervised foraging and by four months are normally independent. However, they will often use the same den as their mother until the next breeding season. They can achieve a lifespan of up to 15 years when in captivity.
Interaction with humans.
Agriculture may create a negative impact on their population due to use of poisons by farmers. They are often considered useful, non-dangerous animals by farmers. However, in some areas the aardwolf is hunted for its fur. Encounters with dogs are another threat.
---END.OF.DOCUMENT---

Adobe.
Adobe is a natural building material made from sand, clay, horse manure and water, with some kind of fibrous or organic material (sticks and/or straw,), which is shaped into bricks using frames and dried in the sun. It is similar to cob and mudbrick. Adobe structures are extremely durable and account for some of the oldest extant buildings on the planet. In hot climates, compared to wooden buildings, adobe buildings offer significant advantages due to their greater thermal mass, but they are known to be particularly susceptible to seismic damage in an event such as an earthquake.
During the Black Friday bushfires in Victoria, Australia in February 2009 there was no observed difference in building survivial rates between adobe construction and traditional methods.
Buildings made of sun-dried earth are common in the West Asia, North Africa, South America, southwestern North America, and in Spain (usually in the Mudéjar style). Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andean region of South America for several thousand years, although often substantial amounts of stone are used in the walls of Pueblo buildings. (Also, the Pueblo people built their adobe structures with handfuls or basketfuls of adobe, until the Spanish introduced them to the making of bricks.) Adobe brickmaking was used in Spain already in the Late Bronze Age and Iron Age, from the eighth century B.C. on. Its wide use can be attributed to its simplicity of design and make, and the cheapness thereby in creating it.
A distinction is sometimes made between the smaller "adobes", which are about the size of ordinary baked bricks, and the larger "adobines", some of which may be one to two yards (2 m) long.
Etymology.
The word "adobe" () has come to us over some 4000 years with little change in either pronunciation or meaning: the word can be traced from the Middle Egyptian (c. 2000 BC) word "dj-b-t" "mud ["i.e.", sun-dried] brick." As Middle Egyptian evolved into Late Egyptian, Demotic, and finally Coptic (c. 600 BC), "dj-b-t" became "tobe" "[mud] brick." This evolved into Arabic "al-tub" (الطّوب "al" "the" + "tub" "brick") "[mud] brick," which was assimilated into Old Spanish as "adobe", still with the meaning "mud brick." English borrowed the word from Spanish in the early 18th century.
In more modern English usage, the term "adobe" has come to include a style of architecture that is popular in the desert climates of North America, especially in New Mexico. (Compare with stucco).
Composition.
An adobe brick is a composite material made of clay mixed with water and an organic material such as straw or dung. The soil composition typically contains clay and sand. Straw is useful in binding the brick together and allowing the brick to dry evenly. Dung offers the same advantage and is also added to repel insects. The mixture is roughly half sand (50%), one-third clay (35%), and one-sixth straw (15%).
Adobe bricks.
Bricks are made in an open frame, by being a reasonable size, but any convenient size is acceptable. The mixture is molded by the frame, and then the frame is removed quickly. After drying a few hours, the bricks are turned on edge to finish drying. Slow drying in shade reduces cracking.
The same mixture to make bricks, without the straw, is used for mortar and often for plaster on interior and exterior walls. Some ancient cultures used lime-based cement for the plaster to protect against rain damage.
The brick’s thickness is preferred partially due to its thermal capabilities, and partially due to the stability of a thicker brick versus a more standard size brick. Depending on the form that the mixture is pressed into, adobe can encompass nearly any shape or size, provided drying time is even and the mixture includes reinforcement for larger bricks. Reinforcement can include manure, straw, cement, rebar or wooden posts. Experience has shown that straw, cement, or manure added to a standard adobe mixture can all produce a strong brick. A general testing is done on the soil content first. To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid. After the jar is sealed the container is shaken vigorously for at least one minute. It is then allowed to sit on a flat surface for a day or so until the soil has settled into layers or remains in suspension. Heavier particles settle out first so gravel will be on the bottom, sand above, silt above that and very fine clay and organic matter will stay in suspension for days. After the water has cleared percentages of the various particles can be determined. Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks. The New Mexico US Extension Service recommends a mix of not more than 1/3 clay, not less than 1/2 sand, and never more than 1/3 silt.
The largest structure ever made from adobe (bricks) was the Bam Citadel, which suffered serious damage (up to 80%) by an earthquake on December 26, 2003. Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks, the ciudellas of Chan Chan and Tambo Colorado, both in Peru.
Thermal properties.
An adobe wall can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction. In desert and other climates typified by hot days and cool nights, the high thermal mass of adobe levels out the heat transfer through the wall to the living space. The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior and begin to transfer heat to the living space. After the sun sets and the temperature drops, the warm wall will then continue to transfer heat to the interior for several hours due to the time lag effect. Thus a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material. In addition, the exterior of an adobe wall can be covered with glass to increase heat collection. In a passive solar home, this is called a Trombe wall.
Adobe wall construction.
When building an adobe structure, the ground should be compressed because the weight of adobe bricks is significantly greater than a frame house and may cause cracking in the wall. The footing is dug and compressed once again. Footing depth depends on the region and its ground frost level. The footing and stem wall are commonly 24" and 14", much larger than a frame house because of the weight of the walls. Adobe bricks are laid by course. Each course is laid the whole length of the wall, overlapping at the corners on a layer of adobe mortar. Adobe walls usually never rise above 2 stories because they're load bearing and have low structural strength. When placing window and door openings, a lintel is placed on top of the opening to support the bricks above. Within the last courses of brick, bond beams are laid across the top of the bricks to provide a horizontal bearing plate for the roof to distribute the weight more evenly along the wall. To protect the interior and exterior adobe wall, finishes can be applied, such as mud plaster, whitewash or stucco. These finishes protect the adobe wall from water damage, but need to be reapplied periodically, or the walls can be finished with other nontraditional plasters providing longer protection.
Adobe roof.
The traditional adobe roof has been generally constructed using a mixture of soil/clay, water, sand, and other available organic materials. The mixture was then formed and pressed into wood forms producing rows of dried, earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe. For a deeper understanding of adobe, one might examine a cob building. Cob, a close cousin to adobe, contains proportioned amounts of soil, clay, water, manure, and straw. This is blended, but not formed like adobe. Cob is spread and piled around a frame and allowed to air dry for several months before habitation. Adobe, then, can be described as dried bricks of cob, stacked and mortared together with more adobe mixture to create a thick wall and/or roof.
Roof materials.
Depending on the materials available, a roof can be assembled using lengths of wood or metal to create a frame work to begin layering adobe bricks. Depending on the thickness of the adobe bricks, the frame work has been performed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly. This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent major cracking.
Traditional adobe roof.
More traditional adobe roofs were often flatter than the familiar steeped roof as the native climate yielded more sun and heat than mass amounts of snow or rain that would find use in precipitous roofs. Cement may be introduced to prevent moisture from penetrating the composite of mud and organic matter. Vigas are beams across the roof that support the roof.
Raising a traditional adobe roof.
To raise a flattened adobe roof, beams of wood or metal should be assembled and span the extent of the building. The ends of the beams should then be fixed to the tops of the walls using the builder’s preferred choice of attachments. Taking into account the material the beams and walls are made from, choosing the attachments may prove difficult. In combination to the bricks and adobe mortar that are laid across the beams creates an even load-bearing pressure that can last for many years depending on attrition.
Once the beams are laid across the building, it is then time to begin the placing of adobe bricks to create the roof. An adobe roof is often laid with bricks slightly larger in width to ensure a larger expanse is covered when placing the bricks onto the beams. This wider shape also provides the future homeowner with thermal protection enough to stabilize an even temperature through out the year. Following each individual brick should be a layer of adobe mortar, recommended to be at least an inch thick to make certain there is ample strength between the brick’s edges and also to provide a relative moisture barrier during the seasons where the arid climate does produce rain.
Attributes.
Adobe roofs can be inherently fire-proof, an attribute well received when the fireplace is kept lit during the cold nights, depending on the materials used. This feature leads the homeowner and builders to begin thinking about the installation of a chimney, a feat regarded as a necessity in any adobe building. The construction of the chimney can also greatly influence the construction of the roof supports, creating an extra need for care in choosing the right materials. An adobe chimney can be made from simple adobe bricks and stacked in similar fashion as the surrounding walls. Basically outline the location and perimeter of the hearth, minding the safety elements common to a fireplace, and begin to stack and mortar the walls with pre-made adobe bricks, cut to size.
---END.OF.DOCUMENT---

Adventure.
An adventure is an activity that is perceived to involve risky, dangerous or exciting experiences.
The term is often used to refer to activities with some potential for physical danger, such as skydiving, mountain climbing, and extreme sports. However, the term also broadly refers to any enterprise that is potentially fraught with physical, financial or psychological risk, such as a business venture, a love affair, or other major life undertakings.
Adventurous experiences create psychological and physiological arousal, which can be interpreted as negative (e.g. fear) or positive (e.g. flow), and which can be detrimental as stated by the Yerkes-Dodson law. For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his "La Condition Humaine" (1933), "If a man is not ready to risk his life, where is his dignity?". Similarly, Helen Keller famously stated that "Life is either a daring adventure or nothing."
About.
Outdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism. Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneers. Adventure education intentionally uses challenging experiences for learning.
Adventure in mythology.
The oldest and most widespread stories in the world are adventure stories.
Joseph Campbell discussed his notion of the monomyth in his book, "The Hero with a Thousand Faces". Campbell proposed that the heroic mythological stories from culture to culture comprised of a similar underlying pattern, starting with the "call to adventure", followed by a hazardous journey and eventual triumph. The adventure novel exhibits these "protagonist on adventurous journey" characteristics as do many popular feature films, such as Star Wars.
Adventurer.
In fiction, the adventurer figure or Picaro may be regarded as a descendant of the knight-errant of Medieval romance. Like the knight, the adventurer roams through episodic encounters, usually involving wealth, romance, or fighting. Unlike the knight, the adventurer was a realistic figure, often lower class or otherwise impoverished, who is forced to make his way to fortune, often by deceit. Also, an adventurer is a roguish hero of low social class who lives by his or her wits in a corrupt society. The picaresque novel originated in Spain in the middle of the fifteenth century. Novels such as Lazarillo de Tormes were influential across Europe. Throughout the eighteenth century, a great number of novels featured bold, amoral, adventuring protagonists, who made their way into wealth and happiness, sometimes with and sometimes without the moral conversion that generally accompanies the Spanish model.
Under Victorian morality the term, used without qualifiers, came to imply a person of low moral character, often someone trying to marry for money.
In comic book handbooks such as "Official Handbook of the Marvel Universe" and ', the term "adventurer" is used as a synonym for "super-hero" when listing a character's occupation.
In role-playing games, the player characters are often professional adventurers, who earn wealth and fame by adventure, such as undertaking hazardous missions, exploring ruins, and slaying monsters. This stereotype is strong enough that "the adventurers" can often be used as a synonym for "the player characters". However non-player character groups of adventurers can also exist, and can be an interesting encounter for the players.
---END.OF.DOCUMENT---

Asia.
Asia is the world's largest and most populous continent, located primarily in the eastern and northern hemispheres. It covers 8.6% of the Earth's total surface area (or 29.9% of its land area) and with approximately 4 billion people, it hosts 60% of the world's current human population.
Asia is traditionally defined as part of the landmass of Eurasia — with the western portion of the latter occupied by Europe — located to the east of the Suez Canal, east of the Ural Mountains and south of the Caucasus Mountains (or the Kuma-Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Given its size and diversity, Asia — a toponym dating back to classical antiquity — is more a cultural concept incorporating a number of regions and peoples than a homogeneous physical entity (see "Subregions of Asia", "Asian people").
The wealth of Asia differs very widely among and within its regions, due to its vast size and huge range of different cultures, environments, historical ties and government systems. In terms of nominal GDP, Japan has the largest economy on the continent and the second largest in the world. In purchasing power parity terms, however, China has the largest economy in Asia and the second largest in the world.
Etymology.
The term "Asia" is originally a concept exclusively of Western civilization. The peoples of ancient "Asia" (Chinese, Japanese, Indians, Persians, Arabs etc.) never conceived the idea of "Asia", simply because they did not see themselves collectively. In their perspective, they were vastly varied civilizations, contrary to ancient European belief.
The word "Asia" originated from the Greek word "Ἀσία", first attributed to Herodotus (about 440 BC) in reference to Anatolia or — in describing the Persian Wars — to the Persian Empire, in contrast to Greece and Egypt. Herodotus comments that he is puzzled as to why three women's names are used to describe one enormous and substantial land mass (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asias, son of Cotys, who passed the name on to a tribe in Sardis. Even before Herodotus, Homer knew of two figures in the Trojan War named Asios; and elsewhere he describes a marsh as ασιος (Iliad 2, 461).
Usage of the term soon became common in ancient Greece, and subsequently by the ancient Romans. Ancient and medieval European maps depict the Asian continent as a "huge amorphous blob" extending eastward. It was presumed in antiquity to end with India — the Macedonian king Alexander the Great believing he would reach the "end of the world" upon his arrival in the East.
Other alternatives.
Alternatively, the etymology of the term may be from the Akkadian word ', which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word "asa" meaning east. This may be contrasted to a similar etymology proposed for "Europe", as being from Akkadian "erēbu(m)" 'to enter' or 'set' (of the sun).
T.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from "asu", meaning 'east' in Assyrian ("ereb" for "Europe" meaning 'west'). The ideas of "Occidental" (form Latin "Occidens" 'setting') and "Oriental" (from Latin "Oriens" for 'rising') are also European invention, synonymous with "Western" and "Eastern". Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two Japanese outspoken figures over the subject.
However, this etymology is considered doubtful, because it does not explain how the term "Asia" first came to be associated with Anatolia, which is "west" of the Semitic-speaking areas, unless they refer to the viewpoint of a Phoenician sailor sailing through the straits between the Mediterranean Sea and the Black Sea.
Physical geography.
Medieval Europeans considered Asia as a continent a distinct landmass. The European concept of the three continents in the Old World goes back to Classical Antiquity, but during the Middle Ages was notably due to 7th century Spanish scholar Isidore of Sevilla (see T and O map). The demarcation between Asia and Africa (to the southwest) is the Isthmus of Suez and the Red Sea. The boundary between Asia and Europe is conventionally considered to run through the Dardanelles, the Sea of Marmara, the Bosporus, the Black Sea, the Caucasus Mountains, the Caspian Sea, the Ural River to its source and the Ural Mountains to the Kara Sea near Kara, Russia. While this interpretation of tripartite continents (i.e., of Asia, Europe and Africa) remains common in modernity, discovery of the extent of Africa and Asia have made this definition somewhat anachronistic. This is especially true in the case of Asia, which has several regions that would be considered distinct landmasses if these criteria were used (for example, Southern Asia and Eastern Asia).
In the far northeast of Asia, Siberia is separated from North America by the Bering Strait. Asia is bounded on the south by the Indian Ocean (specifically, from west to east, the Gulf of Aden, Arabian Sea and Bay of Bengal), on the east by the waters of the Pacific Ocean (including, counterclockwise, the South China Sea, East China Sea, Yellow Sea, Sea of Japan, Sea of Okhotsk and Bering Sea) and on the north by the Arctic Ocean. Australia (or Oceania) is to the southeast.
Some geographers do not consider Asia and Europe to be separate continents, as there is no logical physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia." Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass – or of Afro-Eurasia: geologically, Asia, Europe and Africa comprise a single continuous landmass (save the Suez Canal) and share a common continental shelf. Almost all of Europe and most of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Cherskiy Range) on the North American Plate.
In geography, there are two schools of thought. One school follows historical convention and treats Europe and Asia as different continents, categorizing subregions within them for more detailed analysis. The other school equates the word "continent" with a geographical region when referring to Europe, and use the term "region" to describe Asia in terms of physiography. Since, in linguistic terms, "continent" implies a distinct landmass, it is becoming increasingly common to substitute the term "region" for "continent" to avoid the problem of disambiguation altogether.
Given the scope and diversity of the landmass, it is sometimes not even clear exactly what "Asia" consists of. Some definitions exclude Turkey, the Middle East, Central Asia and Russia while only considering the Far East, Southeast Asia and the Indian subcontinent to compose Asia, especially in the United States after World War II. The term is sometimes used more strictly in reference to the Asia-Pacific region, which does not include the Middle East or Russia, but does include islands in the Pacific Ocean—a number of which may also be considered part of Australasia or Oceania, although Pacific Islanders are not considered Asian.
Country name changes.
Various Asian countries have undergone name changes during the previous century as the result of consolidations, secessions, territories gaining sovereignty and regime changes.
Economy.
Asia has the third largest nominal GDP of all continents, after North America and Europe, but the largest when measured in PPP. As of 2007, the largest national economy within Asia, in terms of gross domestic product (GDP), is that of China followed by that of Japan, India, South Korea and Indonesia. However, in nominal (exchange value) terms, they rank as follows: Japan, China, India, South Korea, Saudi Arabia, Taiwan, Indonesia. Since the 1960s, South Korea had maintained the highest economic growth rate in Asia, nicknamed as an Asian tiger, becoming a newly industrialized country in the 1980s and a developed country by the 21st century.
In the late 1990s and early 2000s, the economies of the PRC and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very high growth nations in Asia include Malaysia, the Philippines, Pakistan, Vietnam, Mongolia, Uzbekistan, Cyprus, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.
China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid 19th century. Japan has had for only several dacades after WW2 the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC).
In the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equalled that of the USA to tie as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/dollar. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.
It is forecasted that the People's Republic of China will surpass Japan to have the largest nominal and PPP-adjusted GDP in Asia within a decade. India is also forecast to overtake Japan in terms of Nominal GDP by 2020. In terms of GDP per capita, both nominal and PPP-adjusted, South Korea will become the second wealthiest country in Asia by 2025, overtaking Germany, the United Kingdom and France. By 2050, according to a 2006 report by Price Waterhouse Cooper, China will have the largest economy in the world (43% greater than the United States when PPP adjusted, although perhaps smaller than the United States in nominal terms).
Natural resources.
Asia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver.
Manufacturing.
Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in mainland China, Taiwan, South Korea, Japan, India, Philippines and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly mainland China, and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
Financial and other services.
Asia has four main financial centres: Tokyo, Hong Kong, Singapore and Shanghai. Call centres and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly-skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centres. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
Early history.
The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.
The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Huanghe shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.
The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.
The center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
Languages and literature.
Asia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.
Nobel prizes.
The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.
Tagore is said to have named another Bengali Indian Nobel prize winner, the 1998 laureate in Economics, Amartya Sen. Sen's work has centered around global issues including famine, welfare, and third-world development. Amartya Sen was Master of Trinity College, Cambridge University, UK, from 1998–2004, becoming the first Asian to head an 'Oxbridge' College.
Other Asian writers who won Nobel Prizes include Yasunari Kawabata (Japan, 1966), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (People's Republic of China, 2000) and Orhan Pamuk (Turkey, 2006).
Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma(Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991.
Sir C.V.Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him".
Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Shmuel Yosef Agnon, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Yaser Arafat, Jose Ramos Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and thirteen Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Salam (Pakistan), Arafat (Palestinian Territories) and Kim (South Korea).
In 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his Ph.D. in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitutes with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low.
The Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.
Mythology.
Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Christians in the Old Testament, is first found in Mesopotamian mythology, in the "Epic of Gilgamesh". Hindu mythology tells about an avatar of the God Vishnu in the form of a fish who warned Manu of a terrible flood. In ancient Chinese mythology, Shan Hai Jing, the Chinese ruler Da Yu, had to spend 10 years to control a deluge which swept out most of ancient China and was aided by the goddess Nüwa who literally fixed the broken sky through which huge rains were pouring.
Religions.
Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of material world. Christianity is also present in most Asian countries.
Abrahamic.
The Abrahamic religions of Judaism, Christianity and Islam originated in West Asia. Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel (which has the world's largest Jewish population), though small communities exist in other countries, such as the Bene Israel in India. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Russia, Eastern Orthodoxy is the predominant religion. Various Christian denominations have adherents in portions of the Middle East, as well as China and India. The world's largest Muslim community (within the bounds of one nation) is in Indonesia. South Asia (mainly Pakistan, India and Bangladesh) holds 30% of Muslims. There are also significant Muslim populations in China, Iran, Malaysia, southern Philippines (Mindanao), Russia and most of West Asia and Central Asia.
Dharmic and Taoist.
The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.
---END.OF.DOCUMENT---

Aruba.
Aruba () is a -long island of the Lesser Antilles in the southern Caribbean Sea, located a mere north of the coast of Venezuela. Together with Bonaire and Curaçao, it forms a group referred to as the ABC islands of the Leeward Antilles, the southern island chain of the Lesser Antilles.
Aruba, which has no administrative subdivisions, is one of the three countries that form the Kingdom of the Netherlands, together with the Netherlands and the Netherlands Antilles. Aruban citizens hold Dutch passports. Unlike much of the Caribbean region, Aruba has a dry climate and an arid, cactus-strewn landscape. This climate has helped tourism as visitors to the island can reliably expect warm, sunny weather. It has a land area of and lies outside the hurricane belt.
History.
Aruba's first inhabitants are thought to have been Caquetíos Amerinds from the Arawak tribe, who migrated there from Venezuela to escape attacks by the Caribs. Fragments of the earliest known Indian settlements date back from 1,000 AD. Sea currents made canoe travel to other Caribbean islands difficult, thus Caquetio culture remained closer to that of mainland South America.
Europeans first learned of Aruba when Amerigo Vespucci and Alonso de Ojeda happened upon it in August 1499. Vespucci, in one of his four letters to Lorenzo di Pierfrancesco de' Medici, described his voyage to the islands along the coast of Venezuela. He wrote about an island where most trees are of brazil wood and, from this island, he went to one ten leagues away, where they had houses built as in Venice. In another letter he described a small island inhabited by very large people, which the expedition thought was not inhabited.
Aruba was colonized by Spain for over a century. The Cacique or Indian Chief in Aruba, Simas, welcomed the first priests in Aruba and received from them a wooden cross as a gift. In 1508, Alonso de Ojeda was appointed as Spain's first Governor of Aruba, as part of "Nueva Andalucía."
Another governor appointed by Spain was Juan Martinez de Ampíes. A "cédula real" decreed in November 1525 gave Ampíes, factor of Española, the right to repopulate the depopulated islands of Aruba, Curaçao and Bonaire.
In 1528, Ampíes was replaced by a representative of the "House of Welser". Aruba has been under Dutch administration since 1636, initially under Peter Stuyvesant. Stuyvesant was on a special mission in Aruba in November and December 1642.
Under the Dutch W.I.C. administration, as "New Netherland and Curaçao" from 1648 to 1664 and the Dutch government regulations of 1629, also applied in Aruba. The Dutch administration appointed an Irishman as "Commandeur" in Aruba in 1667.
The United Kingdom occupied Aruba from the years 1799 to 1802 and from 1805 to 1816.
In August 1806, General Francisco de Miranda and a group of 200 freedom fighters on their voyage to liberate Venezuela from Spain stayed in Aruba for several weeks.
In 1933 Aruba sent its first petition for Aruba's separate status and autonomy to the Queen.
During World War II, together with Curaçao the then world-class exporting oil refineries were the main suppliers of refined products to the Allies. Aruba became a British protectorate from 1940 to 1942 and a US protectorate from 1942 to 1945. On February 16, 1942, its oil processing refinery was attacked by a German submarine ("U-156") under the command of Werner Hartenstein, but the mission failed. "U-156" was later destroyed by a US plane as the crew was sunbathing; only one survived. In March 1944, Eleanor Roosevelt briefly visited American troops stationed in Aruba. In attendance were: His Excellency, Dr. P. Kasteel, the Governor of Curaçao, and his aide, Lieutenant Ivan Lansberg; Rear Admiral T. E. Chandler and his Aide, Lieutenant W. L. Edgington; Captain Jhr. W. Boreel and his aide, Lieutenant E. O. Holmberg; and the Netherlands aide to Mrs. Roosevelt, Lieutenant Commander v.d. Schatte Olivier.
The island's economy has been dominated by five main industries: gold mining, phosphate mining (The Aruba Phosphaat Maatschappij), aloe export, petroleum refineries (The Lago Oil & Transport Company and the Arend Petroleum Maatschappij Shell Co.), and tourism.
Politics.
As a Constituent Country of the Kingdom of the Netherlands, Aruba's politics take place within a framework of a 21-member Parliament and an eight-member Cabinet. The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are elected by the Staten (or "Parlamento") for four-year terms. The Staten is made up of 21 members elected by direct, popular vote to serve a four-year term.
Together with the Netherlands, the country of the Netherlands Antilles, and the country of Aruba form the Kingdom of the Netherlands. As they share the same Dutch citizenship, these three countries still also share the Dutch passport as the Kingdom of the Netherlands passport. As Aruba and the Antilles have small populations, the two countries had to limit immigration. To protect their population, they have the right to control the admission of people from the Netherlands. There is the supervision of the admission and expulsion of people from the Netherlands and the setting of general conditions for the admission and expulsion of aliens. Aruba is officially not a part of the European Union.
The move toward independence.
In August 1947, Aruba presented its first "Staatsreglement" (constitution), for Aruba's "status aparte" as the status of an autonomous state within the Kingdom of the Netherlands. This would come to pass in 1986.
In November 1955, J. Irausquin of Aruba's PPA political party spoke in front of the United Nations Trust Committee. He ended his speech saying that in the future there will be changes to come.
In 1972, at a conference in Suriname, Betico Croes (MEP) proposed a "sui-generis" Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname and the Netherlands Antilles, each with its own nationality. Mr. C. Yarzagaray, a parliamentary member representing the AVP political party, proposed a referendum for the people of Aruba to determine Aruba's separate status or "Status Aparte" as a full autonomous state under the crown. He proclaimed: "Aruba shall never accept a federation and a second class nationality."
Betico Croes worked in Aruba to inform and prepare the people of Aruba for independence. In 1976, a committee appointed by Croes introduced the national flag and anthem as the symbols of Aruba's sovereignty and independence, and he also set 1981 as a target for Aruba's independence. In March 1977, the first Referendum for Self Determination was held with the support of the United Nations and 82% of the participants voted for independence.
The Island Government of Aruba assigned the Institute of Social Studies in The Hague to prepare a study of Aruba's independence, which was published in 1978, titled "Aruba en Onafhankelijkheid, achtergronden, modaliteiten en mogelijkheden; een rapport in eerste aanleg".
At the conference in the Hague in 1981, Aruba's independence was then set for the year 1991.
In March 1983, based on the Referendum, Aruba finally reached an official agreement within the Kingdom, for Aruba's Independence, first becoming an autonomous country within the Kingdom of the Netherlands, with its own constitution, unanimously approved and proclaimed in August 1985, and after an election held for Aruba's first parliament, Aruba seceded from the Netherlands Antilles and officially became a country of the Kingdom of the Netherlands on January 1, 1986, with full independence set for 1996. This achievement is largely due to Betico Croes and the political support of other nations like the USA, Panama, Venezuela and various European countries. Croes was later proclaimed "Libertador di Aruba" after his death in 1986.
In 1990, movement toward independence was postponed upon the request of Aruba's Prime Minister, Nelson O. Oduber. The article scheduling Aruba's complete independence was rescinded in 1995, although the process can begin again after a referendum.
Since January 1, 1986, the Kingdom has consisted of three countries: Netherlands, Netherlands Antilles, and Aruba.
Although the "equality" of the countries is explicitly laid down in the preamble to the Charter, which states "..considering that they have expressed freely their will to establish a new constitutional order in the Kingdom of the Netherlands, in which they will conduct their internal interests autonomously and their common interests on a basis of equality, and in which they will accord each other reciprocal assistance, have resolved by mutual consent", in practice, the Netherlands has considerably more power than either the Netherlands Antilles or Aruba.
Law.
The Aruban legal system is based on the Dutch model. Instead of juries or grand juries, in Aruba, legal jurisdiction lies with a "Gerecht in Eerste Aanleg" (Court of First Instance) on Aruba, a "Gemeenschappelijk Hof van Justitie voor de Nederlandse Antillen en Aruba" (Common Court of Justice of the Netherlands Antilles and Aruba) and the "Hoge Raad der Nederlanden" (Supreme Court of Justice of the Netherlands).
Education.
Aruba's educational system, patterned after the Dutch system, provides for education at all levels. The Government finances the national education system, except for private schools, such as the International School of Aruba (ISA), which finance their own activities. The percentage of money earmarked for education is higher than the average for the Caribbean/Latin American region.
Arubans benefit from a strong primary school education. A segmented secondary school program includes vocational training (VMBO), basic education (MAVO), college prep (HAVO) and advanced placement (VWO).
Higher education goals can be pursued through the Professional Education program (EPI), the teachers college (IPA) as well as through the University of Aruba (UA) which offers bachelors and masters programs in law, finance and economics and hospitality and tourism management. Since the choice for higher education on the island itself is limited, many students choose to study abroad in countries in North America, South America as well as Europe.
There are 68 schools for primary education, 12 schools for secondary education, and 5 universities. In 2007, there were 22,930 fulltime students registered.
There are two private medical schools in Aruba: All Saints University of Medicine, Aruba and Xavier University School of Medicine, Aruba. All courses are presented in English. School's curriculum is based on the United States medical school model and will lead to a Doctor of Medicine degree that is recognized in North America.
Geography.
Aruba is a generally flat, riverless island in the Leeward Antilles island arc of the Lesser Antilles. Aruba is renowned for its white, sandy beaches on the western and southern coasts of the island, relatively sheltered from fierce ocean currents, and this is where most tourist development has taken place. The northern and eastern coasts, lacking this protection, are considerably more battered by the sea and have been left largely untouched by humans. The hinterland of the island features some rolling hills, the best known of which are called Hooiberg at 165 meters (541 ft) and Mount Jamanota, the highest on the island at 188 metres (617 ft) above sea level. Oranjestad, the capital, is located at.
To the east of Aruba are Bonaire and Curaçao, two island territories which form the southwest part of the Netherlands Antilles; Aruba and these two Netherlands Antilles islands are sometimes called the ABC islands.
The isothermal temperature of Aruba's pleasantly tropical marine climate attracts tourists to the island all year round. Temperature varies little from 28 °C (82 °F), moderated by constant trade winds from the Atlantic Ocean. Yearly precipitation barely reaches 500 mm (19.7 in), most of it falling in late autumn.
Economy.
Aruba enjoys one of the highest standards of living in the Caribbean region; the low unemployment rate is also positive for Aruba. About three quarters of the Aruban gross national product is earned through tourism or related activities. Most of the tourists are from Venezuela and the United States (predominately from eastern and southern states), Aruba's largest trading partner. Before the "Status Aparte" (a separate completely autonomous country/state within the Kingdom), oil processing was the dominant industry in Aruba despite expansion of the tourism sector. Today, the influence of the oil processing business is minimal. The size of the agriculture and manufacturing sectors also remains minimal.
The GDP per capita for Aruba is calculated to be $23,831 in 2007; among the highest in the Caribbean and the Americas. Its main trading partners are Venezuela, the United States and Netherlands.
Deficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well. Recent efforts at tightening monetary policy are correcting this and will have its first balanced budget in 2009. Aruba received some development aid from the Dutch government each year, up until 2009 as part of a deal (signed as "Aruba's Financial Independence") in which the Netherlands gradually reduced its financial help to the island each successive year. The Aruban florin is pegged to the United States dollar, with a fixed exchange rate where 1.77 Florin equals 1 U.S. dollar. In most stores near Oranjestad, the exchange rate is 1.75 florin equals U.S 1 dollar
In 2006 the Aruban government has also changed several tax laws to further reduce the deficit. Direct taxes have been converted to indirect taxes as proposed by the IMF. A 3% tax has been introduced on sales and services, while income taxes have been lowered and revenue taxes for business reduced with 20%. The government compensated workers with 3.1% for the effect that the B.B.O. would have on the inflation for 2007. The inflation on Aruba in 2007 was 8,7%.
Demographics.
Aruba is situated in the deep southern part of the Caribbean. Because it has almost no rainfall, Aruba was saved from the plantation system and the economics of the slave trade.
Aruba's population is estimated to be 80% mestizo and 20% other ethnicities. Arawaks spoke the "broken Spanish" which their ancestors had learned on Hispaniola. The Dutch took control 135 years after the Spanish, left the Arawaks to farm and graze livestock, and used the island as a source of meat for other Dutch possessions in the Caribbean. The Arawak heritage is stronger on Aruba than on most Caribbean islands. Although no full-blooded Aboriginals remain, the features of the islanders clearly indicate their genetic Arawak heritage. Most of the population is descended mostly from Arawak, and to a lesser extent Spanish, Italian, Dutch, and a few French, Portuguese, British, and African ancestors.
Recently there has been substantial immigration to the island from neighboring American and Caribbean nations, possibly attracted by the higher paid jobs. In 2007, new immigration laws were introduced to help control the growth of the population by restricting foreign workers to a maximum of 3 years residency on the island.
The demographics of Aruba far more than neighboring Curaçao and Bonaire has been impacted by its proximity to Venezuela. Much of Aruba's families are present by way of Venezuela and there is a seasonal increase of Venezuelans living in second homes.
Towns.
The island, with a population of just over 100,000 inhabitants, does not have major cities.
Culture.
On March 18 Aruba celebrates its National Day. In 1976, Aruba presented its National Anthem (Aruba Dushi Tera) and Flag.
The origins of the population and location of the island give Aruba a mixed culture. Dutch influence can still be seen, as in the celebration of "Sinterklaas" on December 5 and 6 and other national holidays like April 30, when in Aruba and the rest of the Kingdom of the Netherlands the Queen's birthday or "Dia di La Reina" (Koninginnedag) is celebrated.
Christmas and New Year are celebrated with the typical music and songs of gaitas for Christmas and the Dande for New Year, and the "ayaca", the "ponchi crema" and "ham", and other typical foods and drinks. Millions of dollars worth of fireworks are burnt at midnight on New Year's.
On January 25, Betico Croes' birthday is celebrated.
The holiday of Carnival is also an important one in Aruba, as it is in many Caribbean and Latin American countries, and, like Mardi Gras, that goes on for weeks. Its celebration in Aruba started, around the 1950s, influenced by the inhabitants from the nearby islands (Venezuela, St Vincent, Trinidad, Barbados, St. Maarten and Anguilla) who came to work for the Oil refinery. Over the years the Carnival Celebration has changed and now starts from the beginning of January till the Tuesday before Ash Wednesday with a large parade on the last Sunday of the festivities (Sunday before Ash Wednesday).
In June there is the celebration of the "Dia di San Juan", with the song of "Dera Gai".
Tourism from the United States has recently also increased the visibility of American culture on the island, with such celebrations as Halloween and Thanksgiving Day in November.
Religion also has its influences; the days of Ascension and Good Friday are also two holidays on the island.
According to the "Bureau Burgelijke Stand en Bevolkingsregister" (BBSB), as of 2005 there are ninety-two different nationalities living on the island.
Language.
Language can be seen as an important part of island culture in Aruba. The cultural mixture has given rise to a linguistic mixture known as Papiamento, the predominant language on Aruba. The official language is Dutch. The local language used by its inhabitants is Papiamento and is a language that has been evolving through the centuries and absorbed many words from other languages like Dutch, English, French, diverse African dialects, and most importantly, from Portuguese and Spanish. However, like many islands in the region, Spanish is also often spoken. English has historical connections (with the British Empire) and is known by many; English usage has also grown due to tourism. Other common languages spoken based on the size of their community are Portuguese, Chinese, German and French. The latter is offered in high school and college, since a high percentage of Aruban students continue their studies in Europe.
In recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of its native language. Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento. The orthography differs per island and even per group of people. Some are more oriented towards the Portuguese roots and use the equivalent spelling (e.g. "y" instead of "j"), where others are more oriented towards the Dutch roots.
In a book "The Buccaneers of America", first published in 1678, it is stated by eyewitness account that the Indians on Aruba spoke "Spanish". The oldest government official statement written in Papiamento dates from 1803.
Aruba has four newspapers published in Papiamento: "Diario", "Bon Dia", "Solo di Pueblo" and "Awe Mainta" and two in English: "Aruba Today" and "The News". Amigoe is the newspaper published in Dutch.
Aruba also has 18 radio stations (2 AM and 16 FM) and three local television stations (Tele-Aruba, Aruba Broadcast Company and Channel 22).
Infrastructure.
Aruba's Queen Beatrix International Airport is located near Oranjestad. This airport has daily flights to various cities across the United States, to San Juan, Puerto Rico; Miami, Florida; Chicago, Illinois; Philadelphia and Pittsburgh Pennsylvania; Houston, Texas; Atlanta, Georgia; Charlotte, North Carolina; Washington DC; New York City; and Boston, Massachusetts. It also connects Aruba with Toronto, Ontario, and South America, with daily flights to the international airports of Venezuela, Colombia, Peru, Brazil, Germany, France, Spain, U.K and most of Europe through the Schiphol Airport in the Netherlands. Direct flights from Italy started in November 2008.
According to the Aruba Airport Authority, almost 1.7 million travelers used the airport in 2005, of which 61% were Americans.
In cooperation with the United States government, and for the facilitation for the passengers that arrive into the United States, the United States Department of Homeland Security (DHS), U.S. Customs and Border Protection (CBP) full pre-clearance facility in Aruba has been in effect since February 1, 2001 with the expansion in the Queen Beatrix Airport, United States and Aruba have the agreement since 1986 that begins as a USDA and Customs post, and since 2008, the only island to have this service for private flights. In 1999, the U.S. Department of Defense established a Forward Operating Location (FOL) at the airport.
Aruba has two ports Barcadera and Playa (this one) which is located in Oranjestad, The Port of Playa welcomes all the cruise-ship lines, including Royal Caribbean, Carnival Cruise Lines, NCL, Holland America Line, Disney Cruiseships and many more; an estimated almost one million tourists enter in this port per year, Aruba Ports Authority, owned and operated by the Aruban government is the authority in these seaports.
Aruba's public buses transportation services is in charge of Arubus, a government based company which operates from 3:30am until 12:30am 365 days a year. Small private vans also provide the transportation services in certain areas such Hotel Area, San Nicolaas, Santa Cruz and Noord.
Aruba also counts two telecommunications providers, Setar the government based company and Digicel Irish ownership company based in Kingston, Jamaica. Setar is the provider of services such as Internet, video conference, GSM wireless tech and land lines and offer the latest in telecom services, Digicel is the Setar competitor in wireless technology using the GSM platform.
Utilities on the island.
WEB also produces potable industrial water, at the world's third largest desalination plant. Average daily consumption in 2005 was about 37,043 metric tons.
---END.OF.DOCUMENT---

Articles of Confederation.
The Articles of Confederation and Perpetual Union, customarily referred to as the Articles of Confederation, was the first constitution of the United States of America and legally established the union of the states. The Second Continental Congress appointed a committee to draft the Articles in June 1776 and sent the draft to the states for ratification in November 1777. The ratification process was completed in March 1781, legally federating the sovereign and independent states, already cooperating through the Continental Congress, into a new federation styled the "United States of America". Under the Articles the states retained sovereignty over all governmental functions not specifically relinquished to the central government.
On June 12, 1776, a day after appointing a committee to prepare a draft declaration of independence, the Second Continental Congress resolved to appoint a committee of thirteen to prepare a draft of a constitution for a confederate type of union. The last draft of the Articles was written in the summer of 1777 and the Second Continental Congress approved them for ratification by the States on November 15, 1777, in York, Pennsylvania after a year of debate. In practice the final draft of the Articles served as the de facto system of government used by the Congress ("the United States in Congress assembled") until it became de jure by final ratification on March 1, 1781; at which point Congress became the Congress of the Confederation. The "Articles" set the rules for operations of the "United States" confederation. The confederation was capable of making war, negotiating diplomatic agreements, and resolving issues regarding the western territories. An important element of the Articles was that Article XIII stipulated that "their provisions shall be inviolably observed by every state" and "the Union shall be perpetual".
The Articles were created by the chosen representatives of the states in the Second Continental Congress out of a perceived need to have "a plan of confederacy for securing the freedom, sovereignty, and independence of the United States." Although serving a crucial role in the victory in the American Revolutionary War, a group of reformers, known as "federalists", felt that the Articles lacked the necessary provisions for a sufficiently effective government. Fundamentally, a federation was sought to replace the confederation. The key criticism by those who favored a more powerful central state (i.e. the federalists) was that the government (i.e. the Congress of the Confederation) lacked taxing authority; it had to request funds from the states. Also various federalist factions wanted a government that could impose uniform tariffs, give land grants, and assume responsibility for unpaid state war debts ("assumption".) Those opposed to the Constitution, known as "anti-federalists," considered these limits on government power to be necessary and good. Another criticism of the Articles was that they did not strike the right balance between large and small states in the legislative decision making process. Due to its "one-state, one-vote" plank, the larger states were expected to contribute more but had only one vote.
The Articles were replaced by the U.S. Constitution on June 21, 1788.
Background.
The political push for the colonies to increase cooperation began in the French and Indian War in the mid 1750s. The American Revolution in response to lack of elected representation in the British government, followed by the beginning of the American Revolutionary War in 1775 and a proclamation by the monarchy that Congress were traitors in rebellion, induced the various states to cooperate in declaring their independence from the British Empire. Starting 1775, the Second Continental Congress acted as the provisional national government that ran the war. Congress presented the Articles for enactment by the states in 1777, while prosecuting the American Revolutionary War.
Ratification.
"Permit us, then, earnestly to recommend these articles to the immediate and dispassionate attention of the legislatures of the respective states. Let them be candidly reviewed under a sense of the difficulty of combining in one general system the various sentiments and interests of a continent divided into so many sovereign and independent communities, under a conviction of the absolute necessity of uniting all our councils and all our strength, to maintain and defend our common liberties..."
The document could not become officially effective until it was ratified by all thirteen colonies. The first state to ratify was Virginia on December 16, 1777. The process dragged on for several years, stalled by the refusal of some states to rescind their claims to land in the West. Maryland was the last holdout; it refused to go along until Virginia and New York agreed to cede their claims in the Ohio River Valley. A little over three years passed before Maryland's ratification on March 1, 1781.
Article summaries.
Even though the Articles of Confederation and the Constitution were established by many of the same people, the two documents are very different. The original five-paged Articles contained thirteen articles, a conclusion, and a signatory section. The following list contains short summaries of each of the thirteen articles.
Still at war with Great Britain, the Founding Fathers were divided between those seeking a powerful, centralized national government, and those seeking a loosely-structured one. Jealously guarding their new independence, members of the Continental Congress arrived at a compromise solution dividing sovereignty between the states and the federal government, with a unicameral legislature that protected the liberty of the individual states. While calling on Congress to regulate military and monetary affairs, for example, the Articles of Confederation provided no mechanism to force the states to comply with requests for troops or revenue. At times, this left the military in a precarious position, as George Washington wrote in a 1781 letter to the governor of Massachusetts, John Hancock.
Military.
As Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, “As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution.”
Once the war was won, the Continental Army was largely disbanded. A very small national force was maintained to man frontier forts and protect against Native American attacks. Meanwhile, each of the states had an army (or militia), and 11 of them had navies. The wartime promises of bounties and land grants to be paid for service were not being met. In 1783, Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced the Congress to leave Philadelphia temporarily.
Foreign policy.
Even after peace was achieved, the weakness of the government frustrated the ability of the government to conduct foreign policy. In 1786 Thomas Jefferson, concerned over the failure to fund a naval expedition against the Barbary pirates, wrote to James Monroe, "It will be said there is no money in the treasury. There never will be money in the treasury till the confederacy shows its teeth. The states must see the rod.”
Also, the Jay-Gardoqui Treaty with Spain in 1786 also showed weakness in foreign policy. In the treaty (which was never ratified due to its immense unpopularity) the US had to give up rights to the Mississippi River for 20 years which would have economically strangled the settlers west of the Appalachian Mountains. Finally, due to the Confederation's military weakness, they could not force the British out of the frontier forts (which the British promised they would leave in 1783). This violation of the Treaty of Paris was amended with Jay's Treaty in 1795 under the new constitution.
Taxation and commerce.
Under the articles, Congress could make decisions, but had no power to enforce them. There was a requirement for unanimous approval before any modifications could be made to the Articles. Because the majority of lawmaking rested with the states, the central government was also kept limited.
Congress was denied the power of taxation: it could only request money from the states. The states did not generally comply with the requests in full, leaving the Confederation Congress and the Continental Army chronically short of funds. As more money was printed, continental dollars depreciated. Washington in 1779 wrote to John Jay, serving as President of the Continental Congress, "that a wagon load of money will scarcely purchase a wagon load of provisions." Jay and the Congress responded in May by requesting $45 million from the states. In an appeal to the states to comply Jay wrote that the taxes were "the price of liberty, the peace and the safety of yourselves and posterity." He argued that Americans should avoid having it said "that America had no sooner become independent than she became insolvent" or that "her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith." The states did not respond with the money requested.
Congress was also denied the power to regulate commerce, and as a result, the states maintained control over their own trade policy as well. The states and the national congress had both incurred debts during the war, and how to pay the debts became a major issue after the war. Some states paid off their debts; however, the centralizers favored federal assumption of states' debts.
Accomplishments.
Nevertheless, the Congress of the Confederation did take two actions with lasting impact. The Land Ordinance of 1785 established the general land survey and ownership provisions used throughout later American expansion. The Northwest Ordinance of 1787 noted the agreement of the original states to give up western land claims and cleared the way for the entry of new states.
History during the Articles.
The Treaty of Paris left the United States independent and at peace but with an unsettled governmental structure. The Second Continental Congress had drawn up Articles of Confederation in November 15, 1777, to regularize its own status. These described a permanent confederation, but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced. The Articles of Confederation were weak and did not give a strong political or economic base for the newly formed nation. However, the articles did serve as the lead up to the much stronger and more agreed upon Constitution.
Although historians generally agree that the articles were a spectacular failure in terms of workable governance, they do give much credit to the Land Ordinance of 1785 and Northwest Ordinance that set up protocol for the admission of new states, the division of land into homesteads and states, as well as setting aside land in each township for public use. This system represented a sharp break from imperial colonization, as in Europe, and provided the basis for the rest of American continental expansion through the 19th Century.
During the latter years of the war, most people were living in comparative comfort. Farmers found a ready market for their produce within the lines of the British and French armies. Blockade runners and the prizes from privateers added rich cargoes and merchandise to northern shops. Speculators went in debt in preparation for the economic boom which was sure to follow the war.
These dreams vanished in the economic depression that followed the war. Orders in council closed the ports of the British West Indies to all staple products which were not carried in British ships. France and Spain established similar policies. Simultaneously, new manufacturers were stifled by British products which were suddenly filling American ports. Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution. The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation.
The Continental Congress had issued bills of credit, but by the end of the war its paper money had so far depreciated that it ceased to pass as currency, spawning the expression "not worth a continental". Congress could not levy taxes and could only make requisitions upon the States. Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone.
When John Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce. Demands were made for favors and there was no assurance that individual states would agree to a treaty. Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain. Congress had already requested and failed to get power over navigation laws. Meanwhile, each State acted individually against Great Britain to little effect. When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports.
Debtor's problems came to a head in Shays' Rebellion in Massachusetts. Congress was unable to protect manufacturing and shipping. State legislatures were unable or unwilling to resist attacks upon private contracts and public credit. Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population. The idea of a convention to revise the Articles of Confederation grew in favor. Alexander Hamilton, a Revolutionary War veteran who determined while serving as Washington's aide-de-camp that a strong central government was necessary to avoid the frustrations endured by the Army due to an ineffectual Congress, called for what would be referred to as the Annapolis Convention of 1786 to revise the Articles. Only five states sent delegates, but plans were made for another meeting in Philadelphia the next year.
Signatures.
The Second Continental Congress approved the Articles for distribution to the states on November 15, 1777. A copy was made for each state and one was kept by the Congress. The copies sent to the states for ratification were unsigned, and a cover letter had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress.
The "Articles", however, were unsigned, and the date was blank. Congress began the signing process by examining their copy of the "Articles" on June 27, 1778. They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification.
On July 9, 1778, the prepared copy was ready. They dated it, and began to sign. They also requested each of the remaining states to notify its delegation when ratification was completed. On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified. New Jersey, Delaware and Maryland could not, since their states had not ratified. North Carolina and Georgia also didn't sign that day, since their delegations were absent.
After the first signing, some delegates signed at the next meeting they attended. For example, John Wentworth of New Hampshire added his name on August 8. John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the "Articles" on July 21, 1778.
The other states had to wait until they ratified the "Articles" and notified their Congressional delegation. Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779. Maryland refused to ratify the "Articles" until every state had ceded its western land claims.
On February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis. As the last piece of business during the afternoon Session, "among engrossed Bills" was "signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation" and perpetual union among the states. The Senate then adjourned "to the first Monday in August next." The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12. The formal signing of the "Articles" by the Maryland delegates took place in Philadelphia at noon time on March 1, 1781 and was celebrated in the afternoon. With these events, the Articles entered into force and the United States came into being as a united, sovereign and national state.
Congress had debated the "Articles" for over a year and a half, and the ratification process had taken nearly three and a half years. Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived. The "Articles of Confederation and Perpetual Union" were signed by a group of men who were never present in the Congress at the same time.
Roger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
Robert Morris (Pennsylvania) was the only person besides Sherman to sign three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
John Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).
Presidents of the Congress.
The following list is of those who led the Congress of the Confederation under the "Articles of Confederation" as the Presidents of the United States in Congress Assembled. Under the Articles, the president was the presiding officer of Congress, chaired the Cabinet (the Committee of the States) when Congress was in recess, and performed other administrative functions. He was not, however, a "chief" executive in the way the successor President of the United States is a chief executive, but all of the functions he executed were under the auspices and in service of the Congress.
"For a full list of Presidents of the Congress Assembled and Presidents under the two Continental Congresses before the Articles, see President of the Continental Congress."
Revision and replacement.
In May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation. Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries. Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus. The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts.
When approached after leaving the close of the Federal Convention, Benjamin Franklin was asked a question. This is the conversation as has been recorded,
The lady asked "Well, Doctor, what have we got—a Republic or a Monarchy?"
“A Republic, if you can keep it.” was the response of Benjamin Franklin.
According to their own terms for modification (Article XIII), the Articles would still have been in effect until 1790, the year in which the last of the 13 states ratified the new Constitution. The Congress under the Articles continued to sit until November 1788, overseeing the adoption of the new Constitution by the states, and setting elections. By that date, 11 of the 13 states had ratified the new Constitution.
Historians have given many reasons for the perceived need to replace the articles in 1787. Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines.
Rakove (1988) identifies several factors that explain the collapse of the Confederation. The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power. It could not collect customs after the war because tariffs were vetoed by Rhode Island. Rakove concludes that their failure to implement national measures "stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace." The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy. Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power.
When the war ended in 1783, certain special interests had incentives to create a new "merchant state," much like the British state people had rebelled against. In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims. Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government.
Political scientist David C. Hendrickson writes that two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that "the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges."
However, what if one or more states do violate the compact? One view, not only about the Articles but also the later Constitution, was that the state or states injured by such a breach could rightfully secede. This position was held by, among others, Thomas Jefferson and John Calhoun.
This view, among others, was presented against declarations of secession from the Union by southern slave states as the American Civil War began.
---END.OF.DOCUMENT---

Atlantic Ocean.
The Atlantic Ocean is the second-largest of the world's oceanic divisions. With a total area of about 106.4 million square kilometres (41.1 million square miles), it covers approximately twenty percent of the Earth's surface and about twenty-six percent of its water surface area. The first part of its name refers to the Atlas of Greek mythology, making the Atlantic the "Sea of Atlas".
The oldest known mention of this name is contained in "The Histories" of Herodotus around 450 BC (I 202); see also: "Atlas Mountains". Another name historically used was the ancient term Ethiopic Ocean, derived from Ethiopia, whose name was sometimes used as a synonym for all of Africa and thus for the ocean. Before Europeans discovered other oceans, the term "ocean" itself was to them synonymous with the waters beyond Western Europe that we now know as the Atlantic and which the Greeks had believed to be a gigantic river encircling the world; see Oceanus.
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between the Americas to the west, and Eurasia and Africa to the east. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean (which is sometimes considered a sea of the Atlantic), to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south. (Other definitions describe the Atlantic as extending southward to Antarctica.) The equator subdivides it into the North Atlantic Ocean and South Atlantic Ocean.
Geography.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe, the Strait of Gibraltar (where it connects with the Mediterranean Sea, one of its marginal seas and, in turn, the Black Sea) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. Some authorities show it extending south to Antarctica, while others show it bounded at the 60° parallel by the Southern Ocean.
In the southwest, the Drake Passage connects it to the Pacific Ocean. The man-made Panama Canal links the Atlantic and Pacific. Besides those mentioned, other large bodies of water adjacent to the Atlantic are the Caribbean Sea, the Gulf of Mexico, Hudson Bay, the Arctic Ocean, the Mediterranean Sea, the North Sea, the Baltic Sea, and the Celtic Sea.
Covering approximately 22% of Earth's surface, the Atlantic is second in size to the Pacific. With its adjacent seas it occupies an area of about; without them, it has an area of. The land that drains into the Atlantic covers four times that of either the Pacific or Indian oceans. The volume of the Atlantic with its adjacent seas is 354,700,000 cubic kilometers (85,100,000 cu mi) and without them 323,600,000 cubic kilometres (77,640,000 cu mi).
The average depth of the Atlantic, with its adjacent seas, is; without them it is. The greatest depth, is in the Puerto Rico Trench. The Atlantic's width varies from between Brazil and Sierra Leone to over in the south.
Extent.
"On the West." The Eastern limits of the Caribbean Sea, the Southeastern limits of the Gulf of Mexico from the North coast of Cuba to Key West, the Southwestern limit of the Bay of Fundy and the Southeastern and Northeastern limits of the Gulf of St. Lawrence.
"On the North." The Southern limit of Davis Strait from the coast of Labrador to Greenland and the Southwestern limit of the Greenland Sea and Norwegian Sea from Greenland to the Shetland Islands.
"On the East." The Northwestern limit of the North Sea, the Northern and Western limits of the Scottish Seas, the Southern limit of the Irish Sea, the Western limits of the Bristol and English Channels, of the Bay of Biscay and of the Mediterranean Sea.
"On the South." The equator, from the coast of Brazil to the Southwestern limit of the Gulf of Guinea.
"On the Southwest." The meridian of Cape Horn, Chile (67°16'W) from Tierra del Fuego to the Antarctic Continent; a line from Cape Virgins () to Cape Espiritu Santo, Tierra del Fuego, the Eastern entrance to Magellan Strait, Chile
"On the West." The limit of the Rio de La Plata.
"On the North." The Southern limit of the North Atlantic Ocean.
"On the Northeast." The limit of the Gulf of Guinea.
"On the Southeast." From Cape Agulhas along the meridian of 20° East to the Antarctic continent.
"On the South." The Antarctic Continent.
Note that these definitions exclude any marginal waterbodies that are separately defined by the IHO (such as the Bay of Biscay and Gulf of Guinea), though these are usually considered to be part of the Atlantic Ocean.
In 2000 the IHO redefined the Atlantic Ocean, moving its southern limit to 60°S, with the waters south of that line identified as the Southern Ocean. This new definition has not yet been ratified (a reservation has been lodged by Australia) though it is in use by the IHO and others. If and when adopted, the 2000 definition will be published in the 4th edition of "Limits of Oceans and Seas", restoring the Southern Ocean as originally outlined in the 2nd edition and subsequently omitted from the 3rd edition.
Cultural significance.
Transatlantic travel played a major role in the expansion of Western civilization into the Americas. Today, it can be referred to in a humorously diminutive way as the Pond in idioms, in reference to the geographical and cultural divide between North America and Europe. Some British people refer to the USA as "across the pond".
Ocean bottom.
The principal feature of the bathymetry (bottom topography) is a submarine mountain range called the Mid-Atlantic Ridge. It extends from Iceland in the north to approximately 58° South latitude, reaching a maximum width of about. A great rift valley also extends along the ridge over most of its length. The depth of water at the apex of the ridge is less than in most places, the bottom of the ridge is three times as deep and of course several peaks rise above the water and form islands. The South Atlantic Ocean has an additional submarine ridge, the Walvis Ridge.
The Mid-Atlantic Ridge separates the Atlantic Ocean into two large troughs with depths from. Transverse ridges running between the continents and the Mid-Atlantic Ridge divide the ocean floor into numerous basins. Some of the larger basins are the Blake, Guiana, North American, Cape Verde, and Canaries basins in the North Atlantic. The largest South Atlantic basins are the Angola, Cape, Argentina, and Brazil basins.
The deep ocean floor is thought to be fairly flat with occasional deeps, trenches, seamounts and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
Water characteristics.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 - 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below. Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by 7-8 °C (12-15 °F).
The Atlantic Ocean consists of four major water masses. The North and South Atlantic central waters make up the surface. The sub-Antarctic intermediate water extends to depths of. The North Atlantic Deep Water reaches depths of as much as. The Antarctic Bottom Water occupies ocean basins at depths greater than 4,000 meters.
Within the North Atlantic, ocean currents isolate the Sargasso Sea, a large elongated body of water, with above average salinity. The Sargasso Sea contains large amounts of seaweed and is also the spawning ground for both the European eel and the American eel.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation occurs.
Climate.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great heat retention capacity, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift, for example, warms the atmosphere of the British Isles and north-western Europe, and the cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas. Hurricanes develop in the southern part of the North Atlantic Ocean.
History.
The Atlantic Ocean appears to be the second youngest of the five oceans. Apparently it did not exist prior to 130 million years ago, when the continents that formed from the breakup of the ancestral super continent, Pangaea, were drifting apart from seafloor spreading. The Atlantic has been extensively explored since the earliest settlements along its shores.
The Vikings, the Portuguese, and Christopher Columbus were the most famous among early explorers. After Columbus, European exploration rapidly accelerated, and many new trade routes were established.
As a result, the Atlantic became and remains the major artery between Europe and the Americas (known as transatlantic trade). Scientific explorations include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
Ethiopic Ocean.
The Ethiopic Ocean or Ethiopian Ocean (Okeanos Aithiopos) is an old name for what is now called the South Atlantic Ocean, which is separated from the North Atlantic Ocean by a narrow region between Natal, Brazil and Monrovia, Liberia. Use of this term illustrates a past trend towards referring to the whole continent of Africa by the name "Aethiopia". The modern nation of Ethiopia, in northeast Africa, is nowhere near the Ethiopic Ocean, which would be said to lie off the west coast of Africa. The term "Ethiopian Ocean" sometimes appeared until the mid-19th century.
Economy.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves. The Atlantic hosts the world's richest fishing resources, especially in the waters covering the shelves. The major fish are cod, haddock, hake, herring, and mackerel.
The most productive areas include Newfoundland's Grand Banks, the Nova Scotia shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Dogger Bank of the North Sea, and the Falkland Banks. Eel, lobster, and whales appear in great quantities. Because environmental threats from oil spills, marine debris, and the incineration of toxic wastes at sea, various international treaties attempt to reduce pollution.
Terrain.
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea. A clockwise warm-water gyre occupies the northern Atlantic, and a counter-clockwise warm-water gyre appears in the southern Atlantic. The Mid-Atlantic Ridge, a rugged north-south centerline for the entire Atlantic basin, first discovered by the Challenger Expedition dominates the ocean floor. This was formed by the vulcanism that also formed the ocean floor and the islands rising from it.
The Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Norwegian Sea, Baltic Sea, North Sea, Labrador Sea, Black Sea, Gulf of Saint Lawrence, Bay of Fundy, Gulf of Maine, Mediterranean Sea, Gulf of Mexico, and Caribbean Sea.
Islands include Greenland, Iceland, Faroe Islands, Great Britain (including numerous surrounding islands), Ireland, Rockall, Newfoundland, Sable Island, Azores, Madeira, Bermuda, Canary Islands, Caribbean, Cape Verde, São Tomé and Príncipe, Annobón Province, St. Peter Island, Fernando de Noronha, Rocas Atoll, Ascension Island, Saint Helena, The Islands of Trindad, Tristan da Cunha, Gough Island (Also known as Diego Alvarez), Falkland Islands, Tierra del Fuego, South Georgia Island, South Sandwich Islands, and Bouvet Island.
Natural resources.
The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones.
Natural hazards.
Icebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December).
The United States' southeast coast has a long history of shipwrecks due to its many shoals and reefs. The Virginia and North Carolina coasts were particularly dangerous.
The Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.
Current environmental issues.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
In 2005, there was some concern that warm northern European currents were slowing down, but no scientific consensus formed from that evidence.
On June 7, 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
References.
Much of this article comes from the public domain site http://oceanographer.navy.mil/atlantic.html (dead link). It is now accessible from the Internet Archive at http://web.archive.org/web/20020221215514/http%3a//oceanographer.navy.mil/atlantic.html.
---END.OF.DOCUMENT---

Arthur Schopenhauer.
Arthur Schopenhauer (22 February 1788 – 21 September 1860) was a German philosopher known for his atheistic pessimism and philosophical clarity. At age 25, he published his doctoral dissertation, "On the Fourfold Root of the Principle of Sufficient Reason", which examined the fundamental question of whether reason alone can unlock answers about the world.
Schopenhauer's most influential work, "The World as Will and Representation", emphasized the role of man's basic motivation, which Schopenhauer called will. His analysis of will led him to the conclusion that emotional, physical, and sexual desires can never be fulfilled. Consequently, he favored a lifestyle of negating human desires, similar to the teachings of ancient Greek Stoic philosophers, Buddhism, and Vedanta.
Schopenhauer's metaphysical analysis of will, his views on human motivation and desire, and his aphoristic writing style influenced many well-known thinkers including Friedrich Nietzsche, Richard Wagner, Ludwig Wittgenstein, Erwin Schrödinger, Albert Einstein, Sigmund Freud, Otto Rank, Carl Gustav Jung, and Jorge Luis Borges.
Life.
Arthur Schopenhauer was born in the city of Danzig (Gdańsk) as the son of Heinrich Floris Schopenhauer and Johanna Schopenhauer, both descendants of wealthy German Patrician families. When the Kingdom of Prussia acquired the Polish-Lithuanian Commonwealth city of Danzig in 1793, Schopenhauer's family moved to Hamburg. In 1805, Schopenhauer's father committed suicide. Schopenhauer's mother Johanna shortly after moved to Weimar, then the centre of German literature, to pursue her writing career. After one year, Schopenhauer left the family business in Hamburg to join her.
Schopenhauer became a student at the University of Göttingen in 1809. There he studied metaphysics and psychology under Gottlob Ernst Schulze, the author of "Aenesidemus", who advised him to concentrate on Plato and Kant. In Berlin, from 1811 to 1812, he had attended lectures by the prominent post-Kantian philosopher J. G. Fichte and the theologian Schleiermacher.
In 1814, Schopenhauer began his seminal work "The World as Will and Representation" ("Die Welt als Wille und Vorstellung"). He would finish it in 1818 and publish it the following year. In Dresden in 1819, Schopenhauer fathered an illegitimate child who was born and died the same year. In 1820, Schopenhauer became a lecturer at the University of Berlin. He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a "clumsy charlatan". However, only five students turned up to Schopenhauer's lectures, and he dropped out of academia. A late essay, "On University Philosophy", expressed his resentment towards university philosophy.
While in Berlin, Schopenhauer was named as a defendant in an action at law initiated by a woman named Caroline Marquet.
She asked for damages, alleging that Schopenhauer had pushed her. According to Schopenhauer's court testimony, she deliberately annoyed him by raising her voice while standing right outside his door. Marquet alleged that the philosopher had assaulted and battered her after she refused to leave his doorway. Her companion testified that she saw Marquet prostrate outside his apartment. Because Marquet won the lawsuit, he made payments to her for the next twenty years. When she died, he wrote on a copy of her death certificate, "Obit anus, abit onus" ("The old woman dies, the burden flies").
In 1821, he fell in love with nineteen-year old opera singer, Caroline Richter (called Medon), and had a relationship with her for several years. He discarded marriage plans, however, writing, "Marrying means to halve one's rights and double one's duties", and "Marrying means, to grasp blindfolded into a sack hoping to find out an eel out of an assembly of snakes." When he was forty-three years old, seventeen-year old Flora Weiss recorded rejecting him in her diary.
Schopenhauer had a notably strained relationship with his mother Johanna Schopenhauer. After his father's death, Arthur Schopenhauer endured two long years of drudgery as a merchant, in honor of his dead father. Afterwards, his mother retired to Weimar, and Arthur dedicated himself wholly to studies in the gymnasium of Gotha. After he left it in disgust after seeing one of the masters lampooned, he went to live with his mother. But by that time she had already opened her infamous salon, and Arthur was not compatible with the vain, ceremonious ways of the salon. He was also disgusted by the ease with which Johanna had forgotten his father's memory. Therefore, he gave university life a shot. There, he wrote his first book, "On the Fourfold Root of the Principle of Sufficient Reason". She informed him that the book was incomprehensible and it was unlikely that anyone would ever buy a copy. In a fit of temper Arthur told her that his work would be read long after the rubbish she wrote would have been totally forgotten.
In 1831, a cholera epidemic broke out in Berlin and Schopenhauer left the city. Schopenhauer settled permanently in Frankfurt in 1833, where he remained for the next twenty-seven years, living alone except for a succession of pet poodles named Atma and Butz. Schopenhauer had a robust constitution, but in 1860 his health began to deteriorate. He died of heart failure on 21 September 1860, while sitting in his armchair at home. He was 72.
Philosophy of the "will".
A key focus of Schopenhauer was his investigation of individual motivation. Before Schopenhauer, Hegel had popularized the concept of "Zeitgeist", the idea that society consisted of a collective consciousness which moved in a distinct direction, dictating the actions of its members. Schopenhauer, a reader of both Kant and Hegel, criticized their logical optimism and the belief that individual morality could be determined by society and reason. Schopenhauer believed that humans were motivated only by their own basic desires, or "Wille zum Leben" (Will to Live), which directed all of mankind. For Schopenhauer, human desire was futile, illogical, directionless, and, by extension, so was all human action in the world. To Schopenhauer, the Will is a metaphysical existence which controls not only the actions of individual, intelligent agents, but ultimately all observable phenomena. Will, for Schopenhauer, is what Kant called the "thing-in-itself".
Art and aesthetics.
For Schopenhauer, human desiring, "willing," and craving cause suffering or pain. A temporary way to escape this pain is through aesthetic contemplation (a method comparable to Zapffe's "Sublimation"). This is the next best way, short of not willing at all, which is the best way. Total absorption in the world as representation prevents a person from suffering the world as will. Art diverts the spectator's attention from the grave everyday world and lifts him or her into a world that consists of mere play of images. With music, the auditor becomes engrossed with a playful form of the will, which is normally deadly serious. Music was also given a special status in Schopenhauer's aesthetics as it did not rely upon the medium of phenomenal representation. Music artistically presents the will itself, not the way that the will appears to an individual observer. According to Daniel Albright, "Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself."
Ethics.
Schopenhauer's moral theory proposed that of three primary moral incentives, compassion, malice and egoism, compassion is the major motivator to moral expression. Malice and egoism are corrupt alternatives.
Psychology.
Schopenhauer was perhaps even more influential in his treatment of man's psychology than he was in the realm of philosophy.
...one ought rather to be surprised that a thing [sex] which plays throughout so important a part in human life has hitherto practically been disregarded by philosophers altogether, and lies before us as raw and untreated material.
He gave a name to a force within man which he felt had invariably precedence over reason: the Will to Live or Will to Life ("Wille zum Leben"), defined as an inherent drive within human beings, and indeed all creatures, to stay alive and to reproduce.
The ultimate aim of all love affairs... is more important than all other aims in man's life; and therefore it is quite worthy of the profound seriousness with which everyone pursues it.
What is decided by it is nothing less than the composition of the next generation...
These ideas foreshadowed Darwin's discovery of evolution and Freud's concepts of the libido and the unconscious mind.
Politics.
Schopenhauer's politics were, for the most part, an echo of his system of ethics (the latter being expressed in "Die beiden Grundprobleme der Ethik", available in English as two separate books, "On the Basis of Morality" and "On the Freedom of the Will"). Ethics also occupies about one quarter of his central work, "The World as Will and Representation".
In occasional political comments in his "Parerga and Paralipomena" and "Manuscript Remains", Schopenhauer described himself as a proponent of limited government. What was essential, he thought, was that the state should "leave each man free to work out his own salvation", and so long as government was thus limited, he would "prefer to be ruled by a lion than one of [his] fellow rats" — i.e., by a monarch, rather than a democrat. Schopenhauer did, however, share the view of Thomas Hobbes on the necessity of the state, and of state violence, to check the destructive tendencies innate to our species.
Schopenhauer, by his own admission, did not give much thought to politics, and several times he writes proudly of how little attention he had paid "to political affairs of [his] day". In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he did indeed maintain his aloof position of "minding not the times but the eternities". He wrote many disparaging remarks about Germany and the Germans. A typical example is, "For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect."
The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate. This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization.
Despite this, he was adamantly against differing treatment of races, was fervently anti-slavery, and supported the abolitionist movement in the United States. He describes the treatment of "[our] innocent black brothers whom force and injustice have delivered into [the slave-master's] devilish clutches" as "belonging to the blackest pages of mankind's criminal record".
While all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations.
Je me croyais loin de la religion pourtant. Je ne songeais pas que, de Schopenhauer que j'admirais plus que de raison, à l'"Ecclésiaste", et au "Livre de Job", il n'y avait qu'un pas. Les prémisses sur le Pessimisme sont les mêmes, seulement lorsqu'il s'agit de conclure, le philosophe se dérobe. [...] L'Eglise, elle, explique les origines et les causes, signale les fins, présente les remèdes; elle ne se contente pas de vous donner une consultation d'âme, elle vous traite et elle vous guérit alors que le médicastre allemand, après vous avoir bien démontré que l'affection dont vous souffrez est incurable, vous tourne, en ricanant, le dos.
Views on women.
In Schopenhauer's 1851 essay "Of Women" ("Über die Weiber", full text), he expressed his opposition to what he called "Teutonico-Christian stupidity" on female affairs. He claimed that "woman is by nature meant to obey", and opposed Schiller's poem in honor of women, "Würde der Frauen" ("Dignity of Women"). The essay does give two compliments, however: that "women are decidedly more sober in their judgment than [men] are" and are more sympathetic to the suffering of others. However, the latter was discounted as weakness rather than humanitarian virtue.
Schopenhauer's controversial writings have influenced many, from Friedrich Nietzsche to nineteenth-century feminists. Schopenhauer's biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists in the twentieth century.
After the elderly Schopenhauer sat for a sculpture portrait by Elisabet Ney, he told Richard Wagner's friend Malwida von Meysenbug, "I have not yet spoken my last word about women. I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man."
Heredity and eugenics.
With our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation. Plato had something of the kind in mind when, in the fifth book of his "Republic", he explained his plan for increasing and improving his warrior caste. If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles.
In another context, Schopenhauer reiterated his antidemocratic-eugenic thesis: "If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women. This proposal constitutes my Utopia and my Platonic Republic". Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's advocacy of anti-egalitarianism and eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor.
Views on homosexuality & pederasty.
Schopenhauer was also one of the first philosophers since the days of Greek philosophy to address the subject of male homosexuality. In the third, expanded edition of "The World as Will and Representation" (1856), Schopenhauer added an appendix to his chapter on the "Metaphysics of Sexual Love". He also wrote that homosexuality did have the benefit of preventing ill-begotten children. Concerning this, he stated, "... the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her, it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils." Shrewdly anticipating the interpretive distortion on the part of the popular mind of his attempted scientific "explanation" of pederasty as a personal "advocacy" of a phenomenon Schopenhauer otherwise describes, in terms of spiritual ethics, as an "objectionable aberration", Schopenhauer sarcastically concludes the appendix with the statement that "by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour, for they are very disconcerted by the ever-increasing publicization of my philosophy which they so carefully concealed. I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty."
Influences.
Schopenhauer said he was influenced by the Upanishads, Immanuel Kant, and Plato. References to Eastern philosophy and religion appear frequently in Schopenhauer's writing. As noted above, he appreciated the teachings of the Buddha and even called himself a "Buddhist". He said that his philosophy could not have been conceived before these teachings were available.
If the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him. It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there.
He summarised the influence of the Upanishads thus: “It has been the solace of my life, it will be the solace of my death!”
Other influences were: Shakespeare, Jean Jacques Rousseau, John Locke, Baruch Spinoza, Matthias Claudius, George Berkeley, David Hume, René Descartes.
Criticism of Kant.
Schopenhauer accepted Kant's double-aspect of the universe—the phenomenal (world of experience) and the noumenal (world independent of experience). Some commentators suggest that Schopenhauer claimed that the noumenon, or thing-in-itself, was the basis for Schopenhauer's concept of the will. Other commentators suggest that Schopenhauer considered will to be only a subset of the "thing-in-itself" class, namely that which we can most directly experience.
Schopenhauer's identification of the Kantian "noumenon" (i.e., the actually existing entity) with what he termed "will" deserves some explanation. The noumenon was what Kant called the "Ding an Sich", the "Thing in Itself", the reality that is the foundation of our sensory and mental representations of an external world. In Kantian terms, those sensory and mental representations are mere phenomena. Schopenhauer departed from Kant in his description of the relationship between the phenomenon and the noumenon. According to Kant, things-in-themselves ground the phenomenal representations in our minds; Schopenhauer, on the other hand, believed phenomena and noumena to be two different sides of the same coin. Noumena do not "cause" phenomena, but rather phenomena are simply the way by which our minds perceive the noumena, according to the Principle of Sufficient Reason. This is explained more fully in Schopenhauer's doctoral thesis, "On the Fourfold Root of the Principle of Sufficient Reason".
Schopenhauer's second major departure from Kant's epistemology concerns the body. Kant's philosophy was formulated as a response to the radical philosophical skepticism of David Hume, who claimed that causality could not be observed empirically. Schopenhauer begins by arguing that Kant's demarcation between external objects, knowable only as phenomena, and the Thing in Itself of noumenon, contains a significant omission. There is, in fact, one physical object we know more intimately than we know any object of sense perception: our own body.
We know our human bodies have boundaries and occupy space, the same way other objects known only through our named senses do. Though we seldom think of our body as a physical object, we know even before reflection that it shares some of an object's properties. We understand that a watermelon cannot successfully occupy the same space as an oncoming truck; we know that if we tried to repeat the experiment with our own body, we would obtain similar results – we know this even if we do not understand the physics involved.
We know that our consciousness inhabits a physical body, similar to other physical objects only known as phenomena. Yet our consciousness is not commensurate with our body. Most of us possess the power of voluntary motion. We usually are not aware of the breathing of our lungs or the beating of our heart unless somehow our attention is called to them. Our ability to control either is limited. Our kidneys command our attention on their schedule rather than one we choose. Few of us have any idea what our liver is doing right now, though this organ is as needful as lungs, heart, or kidneys. The conscious mind is the servant, not the master, of these and other organs; these organs have an agenda which the conscious mind did not choose, and over which it has limited power.
When Schopenhauer identifies the "noumenon" with the desires, needs, and impulses in us that we name "will," what he is saying is that we participate in the reality of an otherwise unachievable world outside the mind through will. We cannot "prove" that our mental picture of an outside world corresponds with a reality by reasoning; through will, we know – without thinking – that the world can stimulate us. We suffer fear, or desire: these states arise involuntarily; they arise prior to reflection; they arise even when the conscious mind would prefer to hold them at bay. The rational mind is, for Schopenhauer, a leaf borne along in a stream of pre-reflective and largely unconscious emotion. That stream is will, and through will, if not through logic, we can participate in the underlying reality beyond mere phenomena. It is for this reason that Schopenhauer identifies the "noumenon" with what we call our will.
In his criticism of Kant, Schopenhauer claimed that sensation and understanding are separate and distinct abilities. Yet, for Kant, an object is known through each of them. Kant wrote: "… [T]here are two stems of human knowledge... namely, sensibility and understanding, objects being given by the former [sensibility] and thought by the latter [understanding]." Schopenhauer disagreed. He asserted that mere sense impressions, not objects, are given by sensibility. According to Schopenhauer, objects are intuitively perceived by understanding and are discursively thought by reason (Kant had claimed that (1) the understanding thinks objects through concepts and that (2) reason seeks the unconditioned or ultimate answer to "why?"). Schopenhauer said that Kant's mistake regarding perception resulted in all of the obscurity and difficult confusion that is exhibited in the Transcendental Analytic section of his critique.
Influence.
Schopenhauer has had a massive influence upon later thinkers, though more so in the arts (especially literature and music) and psychology than in philosophy. His popularity peaked in the early twentieth century, especially during the Modernist era, and waned somewhat thereafter. Nevertheless, a number of recent publications have reinterpreted and modernised the study of Schopenhauer. His theory is also being explored by some modern philosophers as a precursor to evolutionary theory and modern evolutionary psychology.
Schopenhauer versus Hegel.
If I were to say that the so-called philosophy of this fellow Hegel is a colossal piece of mystification which will yet provide posterity with an inexhaustible theme for laughter at our times, that it is a pseudo-philosophy paralyzing all mental powers, stifling all real thinking, and, by the most outrageous misuse of language, putting in its place the hollowest, most senseless, thoughtless, and, as is confirmed by its success, most stupefying verbiage, I should be quite right.
Further, if I were to say that this summus philosophus [...] scribbled nonsense quite unlike any mortal before him, so that whoever could read his most eulogized work, the so-called "Phenomenology of the Mind", without feeling as if he were in a madhouse, would qualify as an inmate for Bedlam, I should be no less right.
In his Foreword to the first edition of his work "Die beiden Grundprobleme der Ethik", Schopenhauer suggested that he had shown Hegel to have fallen prey to the "Post hoc ergo propter hoc" fallacy.
Schopenhauer thought that Hegel used deliberately impressive but ultimately vacuous verbiage. He suggested his works were filled with "castles of abstraction" that sounded impressive but ultimately had no content. He also thought that his glorification of church and state were designed for personal advantage and had little to do with the search for philosophical truth. For instance, the Right Hegelians interpreted Hegel as viewing the Prussian state of his day as perfect and the goal of all history up until then.
Indology.
It is the most satisfying and elevating reading (with the exception of the original text) which is possible in the world; it has been the solace of my life and will be the solace of my death.
It is well known that the book "Oupnekhat" (Upanishad) always lay open on his table, and he invariably studied it before sleeping at night. He called the opening up of Sanskrit literature "the greatest gift of our century", and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West.
Animal rights.
As a consequence of his philosophy, Schopenhauer was very concerned about the rights of animals. For him, all animals, including humans, are phenomenal manifestations of Will. The word "will" designated, for him, force, power, impulse, energy, and desire; it is the closest word we have that can signify both the real essence of all external things and also our own direct, inner experience. Since everything is basically Will, then humans and animals are fundamentally the same and can recognize themselves in each other. For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers.
Compassion for animals is intimately associated with goodness of character, and it may be confidently asserted that he, who is cruel to living creatures, cannot be a good man.
Nothing leads more definitely to a recognition of the identity of the essential nature in animal and human phenomena than a study of zoology and anatomy.
“The assumption that animals are without rights and the illusion that our treatment of them has no moral significance is a positively outrageous example of Western crudity and barbarity. Universal compassion is the only guarantee of morality."
In 1841, he praised the establishment, in London, of the Society for the Prevention of Cruelty to Animals, and also the Animals' Friends Society in Philadelphia. Schopenhauer even went so far as to protest against the use of the pronoun "it" in reference to animals because it led to the treatment of them as though they were inanimate things. To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter. He was very attached to his succession of pet poodles. Schopenhauer criticized Spinoza's belief that animals are to be used as a mere means for the satisfaction of humans.
Schopenhauer and Buddhism.
Many Europeans, in the 1830s and 1840s, including Schopenhauer himself, found a correspondence between Schopenhauerian thought and the Four Noble Truths of Buddhism. Similarities centered on the principles that life involves suffering, that suffering is caused by desire, and that the extinction of desire leads to salvation. Thus three of the four "truths of the Buddha" correspond to Schopenhauer's doctrine of the will. In Buddhism, however, while greed and lust are always unskillful, desire is ethically variable - it can be skillful, unskillful, or neutral. In the Buddhist perspective, the enemy to be defeated is craving rather than desire in general.
For Schopenhauer, Will had ontological primacy over the intellect; in other words, desire is understood to be prior to thought. Schopenhauer felt this was similar to notions of purushartha or goals of life in Vedanta Hinduism.
If I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others. In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other. And this agreement must be yet the more pleasing to me, inasmuch as "in my philosophizing I have certainly not been under its influence" [emphasis added]. For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism.
Buddhist philosopher Nishitani Keiji, however, sought to distance Buddhism from Schopenhauer.
Philosophy... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions.
This actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.
---END.OF.DOCUMENT---

Angola.
Angola, officially the Republic of Angola (,;), is a country in south-central Africa bordered by Namibia on the south, Democratic Republic of the Congo on the north, and Zambia on the east; its west coast is on the Atlantic Ocean. The exclave province of Cabinda has a border with the Republic of the Congo and the Democratic Republic of the Congo.
Angola was a Portuguese overseas territory from the 16th century to 1975. After independence, Angola was the scene of an intense civil war from 1975 to 2002. The country is the second-largest petroleum and diamond producer in sub-Saharan Africa; however, its life expectancy and infant mortality rates are both among the worst ranked in the world. In August 2006, a peace treaty was signed with a faction of the FLEC, a separatist guerrilla group from the Cabinda exclave in the North, which is still active. About 65% of Angola's oil comes from that region.
Early migrations.
Khoisan hunter-gatherers are some of the earliest known modern human inhabitants of the area. They were largely replaced by Bantu tribes during the Bantu migrations, though small numbers of Khoisans remain in parts of southern Angola to the present day. The Bantu came from the north, probably from somewhere near the present-day Republic of Cameroon. When they reached what is now Angola, they encountered the Khoisans, Bushmen and other groups considerably less technologically advanced than themselves, whom they easily dominated with their superior knowledge of metal-working, ceramics and agriculture. The establishment of the Bantus took many centuries and gave rise to various groups who took on different ethnic characteristics.
The BaKongo kingdoms of Angola established trade routes with other trading cities and civilizations up and down the coast of southwestern and West Africa but engaged in little or no transoceanic trade. This contrasts with the Great Zimbabwe Mutapa civilization which traded with India, the Persian Gulf civilizations and China. The BaKongo engaged in limited trading with Great Zimbabwe, exchanging copper and iron for salt, food and raffia textiles across the Kongo River.
Portuguese rule.
The geographical areas now designated as Angola, first became subject to incursions by the Portuguese in the late 15th century. In 1483, when Portugal established relations with the Kongo State, Ndongo and Lunda existed. The Kongo State stretched from modern Gabon in the north to the Kwanza River in the south. Angola became a link in European trade with India and Southeast Asia. The Portuguese explorer Paulo Dias de Novais founded Luanda in 1575 as "São Paulo de Loanda", with a hundred families of settlers and four hundred soldiers.
Benguela, a Portuguese fort from 1587 which became a town in 1617, was another important early settlement they founded and ruled. The Portuguese would establish several settlements, forts and trading posts along the coastal strip of current-day Angola, which relied on slave trade, commerce in raw materials, and exchange of goods for survival. The African slave trade provided a large number of black slaves to Europeans and their African agents. For example, in what is now Angola, the Imbangala economy was heavily focused on the slave trade.
European traders would export manufactured goods to the coast of Africa where they would be exchanged for slaves. Within the Portuguese Empire, most black African slaves were traded to Portuguese merchants who bought them to sell as cheap labour for use on Brazilian agricultural plantations. This trade would last until the first half of the 1800s.
The Portuguese gradually took control of the coastal strip during the sixteenth century by a series of treaties and wars forming the Portuguese colony of Angola. Taking advantage of the Portuguese Restoration War, the Dutch occupied Luanda from 1641 to 1648, where they allied with local peoples, consolidating their colonial rule against the remaining Portuguese resistance.
In 1648, a fleet under the command of Salvador de Sá retook Luanda for Portugal and initiated a conquest of the lost territories, which restored Portugal to its former possessions by 1650. Treaties regulated relations with Congo in 1649 and Njinga's Kingdom of Matamba and Ndongo in 1656. The conquest of Pungo Andongo in 1671 was the last great Portuguese expansion, as attempts to invade Congo in 1670 and Matamba in 1681 failed. Portugal expanded its territory behind the colony of Benguela in the eighteenth century, and began the attempt to occupy other regions in the mid-nineteenth century.
The process resulted in few gains until the 1880s. Development of the hinterland began after the Berlin Conference in 1885 fixed the colony's borders, and British and Portuguese investment fostered mining, railways, and agriculture. Full Portuguese administrative control of the hinterland did not occur until the beginning of the twentieth century. In 1951, the colony was designated as an overseas province, called Overseas Province of Angola. Portugal had a presence in Angola for nearly five hundred years, and the population's initial reaction to calls for independence was mixed. More overtly political organisations first appeared in the 1950s, and began to make organised demands for their rights, especially in international forums such as the Non-Aligned Movement.
The Portuguese regime, meanwhile, refused to accede to the nationalists' demands of separatism, provoking an armed conflict that started in 1961 when black guerrillas attacked both white and black civilians in cross-border operations in northeastern Angola. The war came to be known as the Colonial War. In this struggle, the principal protagonists were the MPLA (Popular Movement for the Liberation of Angola), founded in 1956, the FNLA (National Front for the Liberation of Angola), which appeared in 1961, and UNITA (National Union for the Total Independence of Angola), founded in 1966. After many years of conflict, Angola gained its independence on 11 November 1975, after the 1974 coup d'état in the metropole's capital city of Lisbon which overthrew the Portuguese regime headed by Marcelo Caetano.
Portugal's new revolutionary leaders began a process of democratic change at home and acceptance of its former colonies' independence abroad. These events prompted a mass exodus of Portuguese citizens from Portugal's African territories (mostly from Portuguese Angola and Mozambique), creating over a million destitute Portuguese refugees — the "retornados".
Independence and civil war.
After independence in November 1975, Angola faced a devastating civil war which lasted several decades and claimed millions of lives and refugees. Following negotiations held in Portugal, itself under severe social and political turmoil and uncertainty due to the April 1974 revolution, Angola's three main guerrilla groups agreed to establish a transitional government in January 1975.
Within two months, however, the FNLA, MPLA and UNITA were fighting each other and the country was well on its way to being divided into zones controlled by rival armed political groups. The superpowers were quickly drawn into the conflict, which became a flash point for the Cold War. The United States, Portugal, Brazil and South Africa supported the FNLA and UNITA. The Soviet Union and Cuba supported the MPLA.
Ceasefire with UNITA.
On February 22, 2002, Jonas Savimbi, the leader of UNITA, was killed in combat with government troops, and a cease-fire was reached by the two factions. UNITA gave up its armed wing and assumed the role of major opposition party. Although the political situation of the country began to stabilize, President Dos Santos has so far refused to institute regular democratic processes. Among Angola's major problems are a serious humanitarian crisis (a result of the prolonged war), the abundance of minefields, and the actions of guerrilla movements fighting for the independence of the northern exclave of Cabinda (Frente para a Libertação do Enclave de Cabinda). While most of the internally displaced have now returned home, the general situation for most Angolans remains desperate, and the development facing the government challenging as a consequence.
Politics.
Angola's motto is "Virtus Unita Fortior", a Latin phrase meaning "Virtue is stronger when united." The executive branch of the government is composed of the President, the Prime Minister (currently Paulo Kassoma) and the Council of Ministers. For decades, political power has been concentrated in the Presidency. The Council of Ministers, composed of all government ministers and vice ministers, meets regularly to discuss policy issues.
Governors of the 18 provinces are appointed by and serve at the pleasure of the president. The Constitutional Law of 1992 establishes the broad outlines of government structure and delineates the rights and duties of citizens. The legal system is based on Portuguese and customary law but is weak and fragmented, and courts operate in only twelve of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court with powers of judicial review has never been constituted despite statutory authorization.
Parliamentary elections held on 5 September 2008, announced MPLA as the winning party with 81% of votes. The closest opposition party was UNITA with 10%. These elections were the first since 1992 and were described as only partly free but certainly not as fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
Angola scored poorly on the 2008 Ibrahim Index of African Governance. It was ranked 44 from 48 sub-Saharan African countries, scoring particularly badly in the areas of Participation and Human Rights, Sustainable Economic Opportunity and Human Development. The Ibrahim Index uses a number of different variables to compile its list which reflects the state of governance in Africa.
Exclave of Cabinda.
With an area of approximately, the Northern Angolan province of Cabinda is unique in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo (DRC) along the lower Congo river. Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south. The town of Cabinda is the chief population center.
According to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom live in neighboring countries. Population estimates are, however, highly unreliable. Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil. The product for which it is best known, however, is its oil, which has given it the nickname, "the Kuwait of Africa". Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output. Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards.
Since Portugal handed over sovereignty of its former overseas province of Angola to the local independentist groups (MPLA, UNITA, and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its military forces, the FAA – Forças Armadas Angolanas) and Cabindan separatists. The Cabindan separatists, FLEC-FAC, announced a virtual Federal Republic of Cabinda under the Presidency of N'Zita Henriques Tiago. One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions, in a process which although not totally fomented by the Angolan government, is undoubtedly encouraged and duly exploited by it.
Military.
The Angolan Armed Forces (AAF) is headed by a Chief of Staff who reports to the Minister of Defense. There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA), and National Air Force (Força Aérea Nacional, FAN). Total manpower is about 110,000. The army is by far the largest of the services with about 100,000 men and women. The Navy numbers about 3,000 and operates several small patrol craft and barges.
Air force personnel total about 7,000; its equipment includes Russian-manufactured fighters, bombers, and transport planes. There are also Brazilian-made EMB-312 Tucano for Training role, Czech-made L-39 for training and bombing role, Czech Zlin for training role and a variety of western made aircraft such as C-212\Aviocar, Sud Aviation Alouette III, etc. A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
Police.
The National Police departments are: Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police. The National Police are in the process of standing up an air wing, which will provide helicopter support for police operations. The National Police are also developing their criminal investigation and forensic capabilities. The National Police has an estimated 6,000 patrol officers, 2,500 Taxation and Frontier Supervision officers, 182 criminal investigators and 100 financial crimes detectives and around 90 Economic Activity Inspectors.
The National Police have implemented a modernization and development plan to increase the capabilities and efficiency of the total force. In addition to administrative reorganization; modernization projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programs and the replacement of AKM rifles with 9 mm UZIs for police officers in urban areas.
Geography.
At, Angola is the world's twenty-third largest country (after Niger). It is comparable in size to Mali and is nearly twice the size of the US state of Texas, or five times the area of the United Kingdom.
Angola is bordered by Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east, and the South Atlantic Ocean to the west. The exclave of Cabinda also borders the Republic of the Congo to the north. Angola's capital, Luanda, lies on the Atlantic coast in the north-west of the country. Angola's average temperature on the coast is in the winter and in the summer.
Economy.
Angola's economy has undergone a period of transformation in recent years, moving from the disarray caused by a quarter century of civil war to being the fastest growing economy in Africa and one of the fastest in the world. In 2004, China's Eximbank approved a $2 billion line of credit to Angola. The loan is being used to rebuild Angola's infrastructure, and has also limited the influence of the International Monetary Fund in the country.
Growth is almost entirely driven by rising oil production which surpassed in late-2005 and was expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. The economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007 and it's expected to stay above 10% for the rest of the decade. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
The country's economy has grown since achieving political stability in 2002. However, it faces huge social and economic problems as a result of the almost continual state of conflict from 1961 onwards, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. The oil sector, with its fast-rising earnings has been the main driving force behind improvements in overall economic activity – nevertheless, poverty remains widespread. Anti-corruption watchdog Transparency International rated Angola one of the 10 most corrupt countries in the world in 2005. The capital city is the most developed and the only large economic centre worth mentioning in the country, however, slums called "musseques", stretch for miles beyond Luanda's former city limits.
According to the Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil.
Before independence in 1975, Angola was a breadbasket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war (1975–2002) destroyed the fertile countryside, leaving it littered with landmines and driving millions into the cities. The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90 percent of farming is done at family and subsistence level. Thousands of Angolan small-scale farmers are trapped in poverty.
Demographics.
Angola is composed of Ovimbundu 37%, Mbundu 25%, Bakongo 13%, "mestiços" (mixed European and native African) 2%, European 1%, and 22% 'other' ethnic groups. The two Mbundu and Ovimbundu nations combined form a majority of the population, at 62%.
It is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007. 11,400 of those refugees were originally from the Democratic Republic of Congo (Congo-Kinshasa) who arrived in the 1970s. As of 2008 there were an estimated 400,000 DRC migrant workers, at least 30,000 Portuguese, and 100,000+ Chinese living in Angola. Prior to independence in 1975, Angola had a community of approximately 500,000 Portuguese.
Languages.
Portuguese is spoken as a first language by 80% of the population, and as a second language by another 20%. The dominance of Portuguese over the native Kimbundu and other African languages is due to a strong influence from Portugal, as opposed to in Mozambique, which being more remote from the Lusosphere, retained a majority of Bantu language speakers.
Religion.
Christianity is the major religion in Angola. The World Christian Database states that the Angolan population is 93.5% Christian, 4.7% ethnoreligionist (indigenous), 0.6% Muslim, 0.9% Agnostic and 0.2% non-religious. However, other sources put the percent of Christians at 53% with the remaining population adhering to indigenous beliefs. According to these sources, of Christians in Angola, 72% are Roman Catholic, and 28% are Baptist, Presbyterian, Reformed Evangelical, Pentecostal, Methodists and a few small Christian sects.
In a study assessing nations' levels of religious regulation and persecution with scores ranging from 0–10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution.
The largest Protestant denominations include the Methodists, Baptists, Congregationalists (United Church of Christ), and Assemblies of God. The largest syncretic religious group is the Kimbanguist Church, whose followers believe that a mid-20th century Congolese pastor named Joseph Kimbangu was a prophet. A small portion of the country's rural population practices animism or traditional indigenous religions. There is a small Islamic community based around migrants from West Africa.
In colonial times, the country's coastal populations primarily were Catholic while the Protestant mission groups were active inland. With the massive social displacement caused by 26 years of civil war, this rough division is no longer valid.
Foreign missionaries were very active prior to independence in 1975, although the Portuguese colonial authorities expelled many Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments. Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them from restoring many of their former inland mission stations.
The Roman Catholic denomination mostly keeps to itself in contrast to the major Protestant denominations which are much more active in trying to win new members. The major Protestant denominations provide help for the poor in the form of crop seeds, farm animals, medical care and education in the English language, math, history and religion.
Health.
A 2007 survey concluded that low and deficient niacin status was common in Angola. Epidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country. Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates. Dengue, filariasis, leishmaniasis, and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region. Angola has one of the highest infant mortality rates in the world and the world's 2nd lowest life expectancies.
Education.
Although by law, education in Angola is compulsory and free for 8 years, the government reports that a certain percentage of students are not attending school due to a lack of school buildings and teachers. Students are often responsible for paying additional school-related expenses, including fees for books and supplies.
In 1999, the gross primary enrollment rate was 74 percent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 percent. Gross and net enrollment ratios are based on the number of students formally registered in primary school and therefore do not necessarily reflect actual school attendance. There continue to be significant disparities in enrollment between rural and urban areas. In 1995, 71.2 percent of children ages 7 to 14 years were attending school. It is reported that higher percentages of boys attend school than girls. During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding.
The Ministry of Education hired 20,000 new teachers in 2005, and continued to implement teacher trainings. Teachers tend to be underpaid, inadequately trained, and overworked (sometimes teaching two or three shifts a day). Teachers also reportedly demand payment or bribes directly from their students. Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health also prevent children from regularly attending school. Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded.
Literacy is quite low, with 67.4% of the population over the age of 15 able to read and write in Portuguese. 82.9% of males and 54.2% of women are literate as of 2001. Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes, and universities Portuguese, Brazilian and Cuban through bilateral agreements; in general these students belong to the Angolan elites.
Culture.
Portugal ruled over Angola for 400 years and both countries share cultural aspects: language (Portuguese) and main religion (Roman Catholic Christianity). The Angolan culture is mostly native Bantu which was mixed with Portuguese culture.
---END.OF.DOCUMENT---

Demographics of Angola.
This article is about the demographic features of the population of Angola, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
The demographics of Angola consist of three main ethnic groups, each speaking a Bantu language: Ovimbundu 37%, Mbundu 25%, and Bakongo 13%. Other groups include Chokwe (or Lunda), Ganguela, Nhaneca-Humbe, Ambo, Herero, and Xindunga. In addition, mixed race (European and African) people amount to about 2%, with a small (1%) population of whites, mainly ethnically Portuguese. As a former overseas territory of Portugal (until 1975), the Portuguese make up the largest non-African population, with about 100,000 (though many other native-born Angolans can claim Portuguese nationality under Portuguese law). In 1975, 250,000 Cuban soldiers arrived in Angola to help the MPLA forces during the civil war.
The largest denomination is Roman Catholicism, but there are also followers of Protestantism, increasingly Pentecostal communities, and African Initiated Churches. As of 2006, one out of 221 people were Jehovah's Witnesses. Blacks from Mali, Nigeria, and Senegal are mostly Muslims. By now few Angolans retain African Traditional Religion following different ethnic faiths.
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population.
There are 12,799,293 Angolan citizens as of July 2009, which ranks 70th in the world.
However the results of the voters' registration of 2007-2008, 8.256.000 adults (from 18 years onwards) combined with the data from 2000/2001 that show that only about 45% of the population is in adult age (Multi-Indicator Cluster Survey, INE/UNICEF), give a total population of at least 18,3 million in 2008.The population of the capital Luanda is about 5,3 million.
Population distribution.
The population is predominately quite young, with 43.7% between the ages of less than one and 14 years old, 2,678,185 males and 2,625,933 females. 53.5% of the population is between the ages of 15 and 64 years old, with 3,291,954 males and 3,195,688 females. 2.8% are 65 years and over, 148,944 males and 186,367 females as of 2006. The median age for males and females is 18 years old.
Population growth.
The population is growing by 2.184% annually. There are 44.51 births and 24.81 deaths per 1,000 citizens. The net migration rate is 2.14 migrants per 1,000 citizens. The fertility rate of Angola is 6.27 children born per woman as of 2006. The infant mortality rate is 184.44 deaths for every 1,000 live births with 196.55 deaths for males and 171.72 deaths for females for every 1,000 live births. Life expectancy at birth is 37.63 years; 36.73 years for males and 38.57 years for females.
Sex ratio.
There are 1.05 males for every female with 1.02 males to female for those under the age of 15, 1.03 males for the 15 to 64 age bracket, 0.79 males for the 65 years and over bracket and 1.02 males to female for the total population as of a 2009 estimate.
Health.
The adult prevalence rate of HIV/AIDS infection is 3.9% as of 2003. There are 240,000 citizens living with AIDS and 21,000 die annually. The risk of contracting disease is very high. There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.
Ethnic groups.
37% of Angolans are Ovimbundu, 25% are Kimbundu, 13% are Bakongo, 2% are mestiço, 1% are Portuguese, and other ethnicities make up 22% of Angola's population.
Religions.
Angola is a majority Christian country, with 53% of citizens professing the religion. Most Angolan Christians are Roman Catholic, 38%, or Protestant, 15%. 46.8% of Angolans practice indigenous beliefs.
Education.
Portuguese is the official language of Angola, but Bantu and other African languages are also widely spoken. Literacy is quite low, with 67.4% of the population over the age of 15 able to read and write in Portuguese. 82.9% of males and 54.2% of women are literate as of 2001.
---END.OF.DOCUMENT---

Politics of Angola.
Politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. Angola changed from a one-party Marxist-Leninist system ruled by the MPLA to a formal multiparty democracy following the 1992 elections. President dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%. A runoff never has taken place. The subsequent renewal of civil war and collapse of the Lusaka Protocol have left much of this process stillborn, but democratic forms exist, notably the National Assembly.
Currently, political power is concentrated in the Presidency. The executive branch of the government is composed of the President, the Prime Minister (currently Paulo Kassoma) and Council of Ministers. The Council of Ministers, composed of all government ministers and vice ministers, meets regularly to discuss policy issues. Governors of the 18 provinces are appointed by and serve at the pleasure of the president. The Constitutional Law of 1992 establishes the broad outlines of government structure and the rights and duties of citizens. The legal system is based on Portuguese and customary law but is weak and fragmented. Courts operate in only 12 of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court with powers of judicial review has never been constituted despite statutory authorization.
The 26-year long civil war has ravaged the country's political and social institutions. The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million. Daily conditions of life throughout the country and specifically Luanda (population approximately 4 million) mirror the collapse of administrative infrastructure as well as many social institutions. The ongoing grave economic situation largely prevents any government support for social institutions. Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work.
Currently, a constitutional commission is putting together a new constitutions. According to the commission, which is composed of 45 permanent members from the main political parties, a new constitution will be ready by the end of the first quarter of 2010.
Legislative branch.
The National Assembly ("Assembleia Nacional") has 223 members, elected for a four year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad. The next general elections, due for 1997, have been rescheduled for 5 September 2008. The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats). The elections however have been described as only partly free but certainly not fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
Political parties and elections.
Parliamentary elections were held in September 2008. These elections were the first since 1992. Presidential elections are planned for 2009.
Judicial branch.
Supreme Court or Tribunal da Relacao, judges of the Supreme Court are appointed by the president
Administrative divisions.
Angola has eighteen provinces (provincias, singular - provincia); Bengo, Benguela, Bie, Cabinda, Cuando Cubango, Cuanza Norte, Cuanza Sul, Cunene, Huambo, Huila, Luanda, Lunda Norte, Lunda Sul, Malanje, Moxico, Namibe, Uige, Zaire
Political pressure groups and leaders.
Front for the Liberation of the Enclave of Cabinda or FLEC [N'zita Henriques TIAGO; Antonio Bento BEMBE]
International organization participation.
ACP, AfDB, CEEAC, ECA, FAO, G-77, IAEA, IBRD, ICAO, ICCt (signatory), ICFTU, ICRM, IDA, IFAD, IFC, IFRCS, ILO, IMF, IMO, Interpol, IOC, IOM, ISO (correspondent), ITU, Non-Aligned dfrfgbfghgCouncil (temporary), UNCTAD, UNESCO, UNIDO, UPU, WCO, WFTU, WHO, WIPO, WMO, WToO, WTrO
---END.OF.DOCUMENT---

Economy of Angola.
The Economy of Angola is one of the fastest-growing economies in the world, but is still recovering from the Angolan Civil War that plagued Angola from independence in 1975 until 2002. Despite extensive oil and gas resources, diamonds, hydroelectric potential, and rich agricultural land, Angola remains poor, and a third of the population relies on subsistence agriculture. Since 2002, when the 27-year civil war ended, the country has worked to repair and improve ravaged infrastructure and weakened political and social institutions. High international oil prices and rising oil production have led to a very strong economic growth in recent years, but corruption and public-sector mismanagement remain, particularly in the oil sector, which accounts for over 50 percent of GDP, over 90 percent of export revenue, and over 80 percent of government revenue.
History.
Kingdom of Portugal's explorers and settlers, founded trading posts and forts along the coast of Africa since the 15th century, and reached the Angolan coast in the 16th century. Portuguese explorer Paulo Dias de Novais founded Luanda in 1575 as "São Paulo de Loanda", and the region developed as a slave trade market with the help of local Imbangala and Mbundu peoples who were notable slave hunters. Trade was mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the ports of Luanda and Benguela. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong Brazilian influence was also exercised by the Jesuits in religion and education. War gradually gave way to the philosophy of trade. The great trade routes and the agreements that made them possible were the driving force for activities between the different areas; warlike states become states ready to produce and to sell. In the Planalto (the high plains), the most important states were those of Bié and Bailundo, the latter being noted for its production of foodstuffs and rubber. The colonial power, Portugal, becoming ever richer and more powerful, would not tolerate the growth of these neighbouring states and subjugated them one by one, so that by the beginning of this century the Portuguese had complete control over the entire area. During the period of the Iberian Union (1580–1640), Portugal lost influence and power and made new enemies. The Dutch, a major enemy of Castile, invaded many Portuguese overseas possessions, including Luanda. The Dutch ruled Luanda from 1640 to 1648 as Fort Aardenburgh. They were seeking black slaves for use in sugarcane plantations of Northeastern Brazil (Pernambuco, Olinda, Recife) which they had also seized from Portugal. John Maurice, Prince of Nassau-Siegen, conquered the Portuguese possessions of Saint George del Mina, Saint Thomas, and Luanda, Angola, on the west coast of Africa. After the dissolution of the Iberian Union in 1640, Portugal would reestablish its authority over the lost territories of the Portuguese Empire.
The Portuguese started to develop townships, trading posts, logging camps and small processing factories. From 1764 onwards, there was a gradual change from a slave-based society to one based on production for domestic consumption and export. Meanwhile, with the independence of Brazil in 1822, the slave trade was abolished in 1836, and in 1844 Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Mainland Portugal, full of trading companies, exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products. Maize, tobacco, dried meat and cassava flour also began to be produced locally. The Angolan bourgeoisie was born. From the 1920s to the 1960s, strong economic growth, abundant natural resources and development of infrastruture, led to the arrival of even more Portuguese settlers from the metropole.
The Portuguese discovered petroleum in Angola in 1955. Production began in the Cuanza basin in the 1950s, in the Congo basin in the 1960s, and in the exclave of Cabinda in 1968. The Portuguese government granted operating rights for Block Zero to the Cabinda Gulf Oil Company, a subsidiary of ChevronTexaco, in 1955. Oil production surpassed the exportation of coffee as Angola's largest export in 1973.
A leftist military-led coup d'état, started on April 25, 1974, in Lisbon, overthrew the Marcelo Caetano government in Portugal, and promised to hand over power to an independent Angolan government. Mobutu Sese Seko, the President of Zaire, met with António de Spínola, the transitional President of Portugal, on September 15, 1974 on Sal island in Cape Verde, crafting a plan to empower Holden Roberto of the National Liberation Front of Angola, Jonas Savimbi of UNITA, and Daniel Chipenda of the MPLA's eastern faction at the expense of MPLA leader Agostinho Neto while retaining the facade of national unity. Mobutu and Spínola wanted to present Chipenda as the MPLA head, Mobutu particularly preferring Chipenda over Neto because Chipenda supported autonomy for Cabinda. The Angolan exclave has immense petroleum reserves estimated at around 300 million tons (~300 kg) which Zaire, and thus the Mobutu government, depended on for economic survival. After independence thousands of white Portuguese left, most of them to Portugal and many travelling overland to South Africa. There was an immediate crisis because the indigenous African population lacked the skills and knowledge needed to run the country and maintain its well-developed infrastructure.
The Angolan government created Sonangol, a state-run oil company, in 1976. Two years later Sonangol received the rights to oil exploration and production in all of Angola. After independence from Portugal in 1975, Angola was ravaged by a horrific civil war between 1975 and 2002.
1990s.
United Nations Angola Verification Mission III and MONUA spent USD1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war. The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile. UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso. In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts. While the U.S. government gave USD250 million to UNITA between 1986 to 1991, UNITA made USD1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe. At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, the People's Republic of China, and South Africa. While no arms shipment to the government violated the protocol, no country informed the U.N. Register on Conventional Weapons as required.
Despite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999. The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.
2000s.
An economic reform effort was launched in 1998. The Angolan economy ranked 160 out of 174 nations in the United Nations Human Development Index of 2000. In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP). The program formally lapsed in June 2001, but the IMF remains engaged. In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates. The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized. A privatization effort, prepared with World Bank assistance, has begun with the BCI bank. Nevertheless, a legacy of fiscal mismanagement and corruption persists. The civil war internally displaced 3.8 million people, 32% of the population, by 2001. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Angola produced over 3 million carats of diamonds per year in 2003, with its production expected to grow to 10 million carats per year by 2007. In 2004 China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure. The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade.
ChevronTexaco started pumping from Block 14 in January 2000, but production has decreased to in 2007 due to the poor quality of the oil. Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007.
Cabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.
Overview.
Despite its abundant natural resources, output per capita is among the world's lowest. Subsistence agriculture provides the main livelihood for 85% of the population. Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. With revenues booming from oil exports, the government has started to implement ambitious development programs in building roads and other basic infrastructure for the nation.
In the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food. Because of severe wartime conditions, including extensive planting of landmines throughout the countryside, agricultural activities have been brought to a near standstill. Some efforts to recover have gone forward, however, notably in fisheries. Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports. In sharp contrast to a bleak picture of devastation and bare subsistence is expanding oil production, now almost half of GDP and 90% of exports, at. Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade. Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits.
This is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.
Foreign trade.
Exports in 2004 reached US$10,530,764,911. The vast majority of Angola's exports, 92% in 2004, are petroleum products. US$785 million worth of diamonds, 7.5% of exports, were sold abroad that year. Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to the People's Republic of China, in 2006. In the first quarter of 2008, Angola became the main exporter of oil to China. The rest of its petroleum exports go to Europe and Latin America. U.S. companies account for more than half the investment in Angola, with Chevron-Texaco leading the way. The U.S. exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum. Trade between Angola and South Africa exceeded USD 300 million in 2007.
Petroleum.
Angola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria in the 2000s. In January 2007 Angola became a member of OPEC. By 2010 production is expected to double the 2006 output level with development of deep-water offshore oil fields. Oil sales generated USD 1.71 billion in tax revenue in 2004 and now makes up 80% of the government's budget, a 5% increase from 2003, and 45% of GDP.
Chevron Corporation produces and receives, 27% of Angolan oil. Elf Oil, Texaco, ExxonMobil, Agip, Petrobras, and British Petroleum also operate in the country.
Block Zero provides the majority of Angola's crude oil production with produced annually. The largest fields in Block Zero are Takula (Area A), Numbi (Area A), and Kokongo (Area B). ChevronTexaco operates in Block Zero with a 39.2% share. SONANGOL, the state oil company, Total, and ENI-Agip own the rest of the block. ChevronTexaco also operates Angola's first producing deepwater section, Block 14, with.
The United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output.
Angola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports. The U.S. imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991. The U.S. Government has invested USD $4 billion in Angola's petroleum sector.
Diamonds.
Angola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling. Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually. The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge.
The Angolan government loses $375 million annually from diamond smuggling. In 2003 the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006. Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 "Angola's Deadly Diamonds" report as plagued by "murders, beatings, arbitrary detentions and other human rights violations." Marques called on foreign countries to boycott Angola's "conflict diamonds."
Iron.
Under Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971. In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan. After independence in 1975, the Angolan Civil War (1975 - 2002) destroyed most of the territory's mining infrastructure. The redevelopment of the Angolan mining industry started in the late 2000s.
---END.OF.DOCUMENT---

Transport in Angola.
Railways.
There are three separate lines which do not link up. The major railway is the Benguela railway. A fourth system once linked Gunza and Gabala.
Railways in Angola suffered a lot of damage in the civil war, particularly the Benguela railway. A $4b project is proposed to restore the lines, and even to extend the system. It was reported in January 2008 that the repair of the Northern Line (a.k.a. Luanda Railway),
started in October 2003 will be completed by August 2008. The work was carried out by the Chinese firm MEC-TEC.
A link to Namibia is partly under construction.
Highways.
Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. Whilst a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road.
The Angolan government has contracted the restoration of many of the country's roads, though. Many companies are coming into the country from China and surrounding nations to help improve road surfaces. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Progress to complete the road infrastructure is likely to take some decades, but substantial efforts are already being made in the right directions.
History.
Angola had an estimated total of 43 airports as of 2004, of which 31 had paved runways as of 2005. There is an international airport at Luanda. International and domestic services are maintained by TAAG, Air France, Air Namibe, Sabena, South African Airways, TAP (Portugal) and several regional carriers. In 2003, domestic and international carriers carried 198,000 passengers. There are airstrips for domestic transport at Benguela, Cabinda, Huambo, Namibe, and Catumbela.
References.
"This article comes from the CIA World Factbook 2003."
---END.OF.DOCUMENT---

Angolan Armed Forces.
The Angolan Armed Forces (FAA) is headed by a Chief of Staff who reports to the Minister of Defense.
There are three components, the Army, Navy (Marinha de Guerra, MdG), and National Air Force of Angola (FAPA). Total manpower is about 65,000. The army is by far the largest of the services with about 55,000+ men and women. The Navy numbers about 1,000 and operates several small patrol craft and barges. Air force personnel total about 3,500; its equipment includes Russian-manufactured fighters and transport planes.
A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
In 2005, FAPLA had a total of 90,000 active personnel of which the Army had 100,000 members, whose major equipment included over 300 main battle tanks, 600 reconnaissance vehicles, over 250 armored infantry fighting vehicles, 170 armored personnel carriers and more than 1,396 artillery pieces.
In 1990-91, the Army had ten military regions and an estimated 73+ 'brigades', each with a mean strength of 1,000 and comprising inf, tank, APC, arty, and AA units as required (IISS Military Balance 1990 or 1991). The Navy had an estimated 2,400 personnel whose major naval units consisted of nine patrol/coastal vessels. The Air Force /Air Defense Forces had 6,000 personnel and 90 combat capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters.
The defense budget in 2005 totalled $1.16 billion.
Fleet - Navy (Marinha de Guerra).
Most of the craft detailed are from the 1980s or earlier, but the navy acquired new boats from Spain and France in the 1990s.
Air Force.
See National Air Force of Angola
---END.OF.DOCUMENT---

Foreign relations of Angola.
The foreign relations of Angola are based on Angola's strong support of U.S. foreign policy as the Angolan economy is dependent on U.S. foreign aid.
From 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba. Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention. In 1993, it established formal diplomatic relations with the United States. It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south. Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government. It also has intervened in the Republic of the Congo (Brazzaville) to support the existing government in that country.
Since 1998, Angola has successfully worked with the UN Security Council to impose and carry out sanctions on UNITA. More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA. At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular.
Cape Verde.
Cape Verde signed a friendship accord with Angola in December 1975, shortly after Angola gained its independence. Cape Verde and Guinea-Bissau served as stop-over points for Cuban troops on their way to Angola to fight UNITA rebels and South African troops. Prime Minister Pedro Pires sent FARP soldiers to Angola where they served as the personal bodyguards of Angolan President José Eduardo dos Santos.
Democratic Republic of the Congo.
Many thousands of Angolans fled the country after the civil war. More than 20,000 people were forced to leave the Democratic Republic of the Congo in 2009, an action the DR Congo said was in retaliation for regular expulsion of Congolese diamond miners who were in Angola illegally. Angola sent a delegation to DR Congo's capital Kinshasa and succeeded in stopping government-forced expulsions which had become a "tit-for-tat" immigration dispute. "Congo and Angola have agreed to suspend expulsions from both sides of the border," said Lambert Mende, DR Congo information minister, in October 2009. "We never challenged the expulsions themselves; we challenged the way they were being conducted — all the beating of people and looting their goods, even sometimes their clothes," Mende said.
Namibia.
Namibia borders Angola to the south. In 1999 Namibia signed a mutual defense pact with its northern neighbor Angola.
This affected the Angolan Civil War that had been ongoing since Angola's independence in 1975. Namibia's ruling party SWAPO sought to support the ruling party MPLA in Angola against the rebel movement UNITA, whose stronghold is in southern Angola, bordering to Namibia. The defence pact allowed Angolan troops to use Namibian territory when attacking Jonas Savimbi's UNITA.
Nigeria.
Angolan-Nigerian relations are primarily based on their roles as oil exporting nations. Both are members of the Organization of the Petroleum Exporting Countries, the African Union and other multilateral organizations.
South Africa.
Angola-South Africa relations are quite strong as the ruling parties in both nations, the African National Congress in South Africa and the MPLA in Angola, fought together during the Angolan Civil War and South African Border War. They fought against UNITA rebels, based in Angola, and the apartheid-era government in South Africa who supported them. Nelson Mandela mediated between the MPLA and UNITA factions during the last years of Angola's civil war.
Zimbabwe.
Angola-Zimbabwe relations have remained cordial since the birth of both states, Angola in 1975 and Zimbabwe in 1979, during the Cold War. While Angola's foreign policy shifted to a pro-U.S. stance based on substantial economic ties, under the rule of President Robert Mugabe Zimbabwe's ties with the West soured in the late 1990s.
France.
Relations between the two countries have not always been cordial due to the former French government's policy of supporting militant separatists in Angola's Cabinda province and the international Angolagate scandal embarrassed both governments by exposing corruption and illicit arms deals. Following French President Nicolas Sarkozy's visit in 2008, relations have improved.
Portugal.
Angola-Portugal relations have significantly improved since the Angolan government abandoned communism and nominally embraced democracy in 1991, embracing a pro-U.S. and to a lesser degree pro-Europe foreign policy. Portugal ruled Angola for 400 years, colonizing the territory from 1483 until independence in 1975. Angola's war for independence did not end in a military victory for either side, but was suspended as a result of a coup in Portugal that replaced the Caetano regime.
Russia.
Russia has an embassy in Luanda. Angola has an embassy in Moscow and an honorary consulate in Saint Petersburg. Angola and the precursor to Russia, the Soviet Union, established relations upon Angola's independence.
Brazil.
Commercial and economic ties dominate the relations of each country. Parts of both countries were part of the Portuguese Empire from the early 16th century until Brazil's independence in 1822. As of November 2007, "trade between the two countries is booming as never before"
Cuba.
During Angola's civil war Cuban forces fought to install a Marxist-Leninist MPLA-PT government, against Western-backed UNITA and FLNA guerrillas and the South-African army.
United States.
From the mid-1980s through at least 1992, the United States was the primary source of military and other support for the UNITA rebel movement, which was led from its creation through 2002 by Jonas Savimbi. The U.S. refused to recognize Angola diplomatically during this period.
Relations between the United States of America and the Republic of Angola (formerly the People's Republic of Angola) have warmed since Angola's ideological renunciation of Marxism before the 1992 elections.
Israel.
Angola-Israel relations, primarily based on trade and pro-United States foreign policies, are excellent. In March 2006, the trade volume between the two countries amounted to $400 million. The Israeli ambassador to Angola is Avraham Benjamin.[1] In 2005, President José Eduardo dos Santos visited Israel.
Japan.
As of 2007, economic relations played "a fundamental role in the bilateral relations between the two governments". Japan has donated towards demining following the civil war.
People's Republic of China.
Chinese Prime Minister Wen Jiabao visited Angola in June 2006, offering a US$9 billion loan for infrastructure improvements in return for petroleum. The PRC has invested heavily in Angola since the end of the civil war in 2002. João Manuel Bernardo, the current ambassador of Angola to China, visited the PRC in November 2007.
In February 2006, Angola surpassed Saudi Arabia to become the number one supplier of oil to China.
Vietnam.
Angola-Vietnam relations were established in August 1971, four years before Angola gained its independence, when future President of Angola Agostinho Neto visited Vietnam. Angola and Vietnam have steadfast partners as both transitioned from Cold War-era foreign policies of international communism to pro-Western pragmatism following the fall of the Soviet Union.
---END.OF.DOCUMENT---

Albert Sidney Johnston.
Albert Sidney Johnston (February 2, 1803 – April 6, 1862) was a career United States Army officer, a Texas Army general, and a Confederate States general. He saw extensive combat during his military career, fighting actions in the Texas War of Independence, the Mexican-American War, the Utah War, as well as the American Civil War.
Considered by Confederate President Jefferson Davis to be the finest general officer in the Confederacy before the emergence of Robert E. Lee, he was killed early in the Civil War at the Battle of Shiloh and was the highest ranking officer, Union or Confederate, killed during the entire war. Davis believed the loss of Johnston "was the turning point of our fate"
Early life.
Johnston was born in Washington, Kentucky, the youngest son of Dr. John and Abigail Harris Johnston. His father was a native of Salisbury, Connecticut. Although Albert Johnston was born in Kentucky, he lived much of his life in Texas, which he considered his home. He was first educated at Transylvania University in Lexington, where he met fellow student Jefferson Davis. Both were appointed to the United States Military Academy, Davis two years behind Johnston. In 1826 Johnston graduated eighth of 41 cadets in his class from West Point with a commission as a brevet second lieutenant in the 2nd U.S. Infantry.
Johnston was assigned to posts in New York and Missouri and served in the Black Hawk War in 1832 as chief of staff to Bvt. Brig. Gen. Henry Atkinson. In 1829 he married Henrietta Preston, sister of Kentucky politician and future civil war general William Preston. He resigned his commission in 1834 to return to Kentucky to care for his dying wife, who succumbed two years later to tuberculosis. They had one son, Col. William Preston Johnston, who would also serve in the Confederate Army.
Texas Army.
In April 1834, Johnston took up farming in Texas, but enlisted as a private in the Texas Army during the Texas War of Independence against the Republic of Mexico in 1836. One month later, Johnston was promoted to major and the position of aide-de-camp to General Sam Houston. He was named Adjutant General as a colonel in the Republic of Texas Army on August 5, 1836. On January 31, 1837, he became senior brigadier general in command of the Texas Army.
On February 7, 1837, he fought in a duel with Texas Brig. Gen. Felix Huston, challenging each other for the command of the Texas Army; Johnston refused to fire on Huston and lost the position after he was wounded in the pelvis. The second president of the Republic of Texas, Mirabeau B. Lamar, appointed him Secretary of War on December 22, 1838. Johnston was to provide the defense of the Texas border against Mexican invasion, and in 1839 conducted a campaign against Indians in northern Texas. In February 1840, he resigned and returned to Kentucky, where he married Eliza Griffin in 1843. They settled on a large plantation he named China Grove in Brazoria County, Texas.
U.S. Army.
Johnston returned to the Texas Army during the Mexican-American War under General Zachary Taylor as a colonel of the 1st Texas Rifle Volunteers. The enlistments of his volunteers ran out just before the Battle of Monterrey. Johnston managed to convince a few volunteers to stay and fight as he himself served as the inspector general of volunteers and fought at the battles of Monterrey and Buena Vista. Johnston remained on his plantation after the war until he was appointed by President Taylor to the U.S. Army as a major and was made a paymaster in December 1849. He served in that role for more than five years, making six tours, and traveling more than 4,000 miles annually on the Indian frontier of Texas. He served on the Texas frontier and elsewhere in the West. In 1855 President Franklin Pierce appointed him colonel of the new 2nd U.S. Cavalry (the unit that preceded the modern 5th U.S.), a new regiment, which he organized. As a key figure in the Utah War, he led U.S. troops who established a non-Mormon government in the formerly Mormon territory. He received a brevet promotion to brigadier general in 1857 for his service in Utah. He spent 1860 in Kentucky until December 21, when he sailed for California to take command of the Department of the Pacific.
Civil War.
At the outbreak of the Civil War, Johnston was the commander of the U.S. Army Department of the Pacific in California. He was approached by some Californians who urged him to take his forces east to join the Union against the Confederacy. He resigned his commission on April 9, 1861, as soon as he heard of the secession of Texas. He moved to Los Angeles where he had family and remained there until May when, suspected by local Union authorities, he evaded arrest and joined the Los Angeles Mounted Rifles as a private, leaving Warner's Ranch May 27. He participated in their trek across the southwestern deserts to Texas, crossing the Colorado River into the Confederate Territory of Arizona on July 4, 1861. He reached Richmond, Virginia, on or about September 1, 1861. There Johnston was appointed a full general by his friend, Jefferson Davis. On May 30, 1861, Johnston became the second highest ranking Confederate general (after the little-known Samuel Cooper) as commander of the Western Department. He raised the Army of Mississippi to defend Confederate lines from the Mississippi River to Kentucky and the Allegheny Mountains.
Although the Confederate States Army won a morale-boosting victory at First Battle of Bull Run in the East in 1861, matters in the West turned ugly by early 1862. Johnston's subordinate generals lost Fort Henry on February 6, 1862, and Fort Donelson on February 16, 1862, to Union Brig. Gen. Ulysses S. Grant. Johnston has been faulted for poor judgment in selecting Brig. Gens. Lloyd Tilghman and John B. Floyd for those crucial positions and for not supervising adequate construction of the forts. Union Maj. Gen. Don Carlos Buell subsequently captured the vital city of Nashville, Tennessee. Gen. P.G.T. Beauregard was sent west to join Johnston and they organized their forces at Corinth, Mississippi, planning to ambush Grant's forces at Pittsburg Landing, Tennessee.
Shiloh.
Johnston concentrated many of his forces from around the theater and launched a massive surprise attack against Grant at the Battle of Shiloh on April 6, 1862. As the Confederate forces overran the Union camps, Johnston seemed to be everywhere, personally leading and rallying troops up and down the line. At about 2:30 p.m., while leading one of those charges, he was wounded, taking a bullet behind his right knee. He did not think the wound serious at the time, and sent his personal physician to attend to some wounded Union soldiers instead. The bullet had in fact clipped his popliteal artery and his boot was filling up with blood. Within a few minutes Johnston was observed by his staff to be nearly fainting off his horse, and asked him if he was wounded, to which he replied "Yes, and I fear seriously." It is possible that Johnston's duel in 1837 had caused nerve damage or numbness to that leg and that he did not feel the wound to his leg as a result. Johnston was taken to a small ravine, where he bled to death in minutes.
It is probable that a Confederate soldier fired the fatal round. No Union soldiers were observed to have ever gotten behind Johnston during the fatal charge, while it is known that many Confederates were firing at the Union lines while Johnston charged well in advance of his soldiers. He was the highest-ranking casualty of the war on either side, and his death was a strong blow to the morale of the Confederacy. Jefferson Davis considered him the best general in the country; this was two months before the emergence of Robert E. Lee as the pre-eminent general of the Confederacy.
Epitaph.
Johnston was buried in New Orleans, Louisiana. In 1866, a joint resolution of the Texas Legislature was passed to have his body reinterred to the Texas State Cemetery in Austin The re-interment occurred in 1867. Forty years later, the state appointed Elisabet Ney to design a monument and sculpture of him to be erected at his gravesite.
The Texas Historical Commission has erected a historical marker near the entrance of what was once his plantation. An adjacent marker was erected by the San Jacinto Chapter of the Daughters of The Republic of Texas and the Lee, Roberts, and Davis Chapter of the United Daughters of the Confederate States of America.
The University of Texas at Austin has also recognized Johnston with a statue on the South Mall.
---END.OF.DOCUMENT---

Android.
An android is a robot or synthetic organism designed to look and act like a human. Until recently, androids have largely remained within the domain of science fiction, frequently seen in film and television. However, Honda and several other corporations and private enterprises have developed impressive androids although there is a long way to go to make them fully human like.
Etymology.
The word derives from ανδρός, the genitive of the Greek ανήρ "anēr", meaning "man", and the suffix "-eides", used to mean "of the species; alike" (from "eidos," "species"). Though the word derives from a gender-specific root, its usage in English is usually gender neutral. The term was first mentioned by St. Albertus Magnus in 1270 and was popularized by the French writer Villiers in his 1886 novel "L'Ève future", although the term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons.
The term "droid", invented by George Lucas in "Star Wars" (1977) but now used widely within science fiction, originated as an abbreviation of "android", but has been used by Lucas and others to mean any robot, including distinctly non-humaniform machines like R2-D2. Another abbreviation, "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?", has seen some limited further currency, e.g., in the TV series "Total Recall 2070".
Projects.
Androids have been mainly an element of science fiction, yet it is increasingly becoming a reality in Japan and South Korea. The two countries are in a heated competition to make them a commercial success in the global market and have developed a handful of successful androids so far.
Japan.
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and Kokoro Co., Ltd. have demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan. In 2006, Kokoro Co. developed a new "DER 2" android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The "air servosystem" which Kokoro Co. developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. The smoothness of her movement has also been improved, making it now even more likely for the uninitiated to confuse her with an actual human being. Once programmed, she is able to choreograph her motions and gestures with her voice.
The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is "working" at the Science University of Tokyo as a guide.
The Waseda University (Japan) and NTT Docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2". It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses his/her face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D mask.
Korea.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is tall and weighs, matching the average figure of Korean women in their twenties. EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot". EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2009 at a cost of 500 billion won, of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
United States.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history (see video at). Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works. In 2005, the PKD android won a first place artificial intelligence award from AAAI.
Usage and distinctions.
Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence).
The term "android" was popularized by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886), featuring an artificial humanlike robot named Hadaly. As said by the officer in the story, "In this age of Realien advancement, who knows what goes on in the mind of those responsible for these mechanical dolls."
Although Karel Čapek's robots in "R.U.R. (Rossum's Universal Robots)" (1921) - the play that introduced the word "robot" to the world - were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings. The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts.
The word "android" is a combination of Ancient Greek "andros" and the suffix "-oid", which literally means "in the form of a man (male human being)". This could be contrasted with the more general term "anthropoid", which means humanlike. According to this fashion, a female human-like robot would be a "gynoid".
Fiction.
Androids are a staple of science fiction. Authors have used the term "android" in more diverse ways than "robot" or "cyborg". In some fictional works, the difference between a robot and android is only their appearance, with androids being made to look like humans on the outside but with robot-like internal mechanics. In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation. Other fictional depictions of androids fall somewhere in between.
One thing common to most fictional androids, though, is that the real-life technological challenges associated with creating thoroughly human-like robots – such as the creation of strong artificial intelligence – are assumed to have been solved. Fictional androids are generally depicted as mentally and physically equal or superior to humans – moving, thinking and speaking as fluidly as them.
The tension between the nonhuman substance and the human appearance – or even human ambitions – of androids is the dramatic impetus behind most of their fictional depictions. Some android heroes seek, like Pinocchio, to become human, as in the films "Bicentennial Man" and "A.I. Artificial Intelligence", or Data in the science-fiction show '. Others, as in the film "Westworld", rebel against abuse by careless humans. Android hunter Deckard in American writer Philip K. Dick's "Do Androids Dream of Electric Sheep?" discovers that his targets are, in some ways, more human than he is. Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human.
One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Do Androids Dream of Electric Sheep?" and its film adaptation "Blade Runner". Perhaps the clearest example of such an exploration is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human. More recently, the androids Lance Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other".
Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman". Examples include the Greek myth of "Pygmalion", the fembots in the Austin Powers series, and the female robot Maria in Fritz Lang's "Metropolis". Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires". Fiction about gynoids or female cyborgs has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of female androids is a way of exploring racism and misogyny in society.
---END.OF.DOCUMENT---

Alberta.
Alberta is the most populous and fastest growing of Canada's three prairie provinces. It is approximately the same size as France or Texas and had a population of 3.7 million in 2009. It became a province on September 1, 1905, on the same day as Saskatchewan. It is economically important primarily because of its vast oil reserves, and its large tertiary and quaternary economic sector.
Alberta is located in western Canada, bounded by the provinces of British Columbia to the west, Saskatchewan to the east, the Northwest Territories to the north, and the U.S. state of Montana to the south. Alberta is one of three Canadian provinces and territories to border only a single U.S. state (the others being New Brunswick and Yukon). It is also one of only two Canadian provinces that are landlocked (the other being Saskatchewan).
The capital city of Alberta is Edmonton, located just south of the centre of the province. Roughly
south of the capital is Calgary, Alberta's largest city and a major distribution and transportation hub as well as one of Canada's major commerce centres. Edmonton is the primary supply and service hub for Canada's oil sands and other northern resource industries. According to recent population estimates, these two metropolitan areas have now both exceeded 1 million people. Other municipalities in the province include Red Deer, Lethbridge, Medicine Hat, Fort McMurray, Grande Prairie, Camrose, Lloydminster, Brooks, Wetaskiwin, Banff, Cold Lake, and Jasper.
Alberta is named after Princess Louise Caroline Alberta (1848–1939), the fourth daughter of Queen Victoria and her husband, Prince Albert. Princess Louise was the wife of the Marquess of Lorne, Governor General of Canada from 1878 to 1883. Lake Louise, the village of Caroline, and Mount Alberta were also named in honour of Princess Louise. Since December 14, 2006, the Premier of the province has been Ed Stelmach, a Progressive Conservative.
Geography.
Alberta covers an area of, an area about 5% smaller than Texas or 20% larger than France. This makes it the fourth largest province after Quebec, Ontario, and British Columbia. To the south, the province borders on the 49th parallel north, separating it from the U.S. state of Montana, while on the north the 60th parallel north divides it from the Northwest Territories. To the east the 110th meridian west separates it from the province of Saskatchewan, while on the west its boundary with British Columbia follows the 120th meridian west south from the Northwest Territories at 60°N until it reaches the Continental Divide at the Rocky Mountains, and from that point follows the line of peaks marking the Continental Divide in a generally southeasterly direction until it reaches the Montana border at 49°N.
The province extends north to south and east to west at its maximum width. Its highest point is at the summit of Mount Columbia in the Rocky Mountains along the southwest border, while its lowest point is on the Slave River in Wood Buffalo National Park in the northeast.
With the exception of the semi-arid steppe of the southeastern section, the province has adequate water resources. Alberta contains numerous rivers and lakes used for swimming, water skiing, fishing and a full range of other water sports. There are three large lakes and a multitude of smaller lakes less than each. Part of Lake Athabasca () lies in the province of Saskatchewan. Lake Claire () lies just west of Lake Athabasca in Wood Buffalo National Park. Lesser Slave Lake () is northwest of Edmonton. The longest river in Alberta is the Athabasca River which travels from the Columbia Icefield in the Rocky Mountains to Lake Athabasca.
Alberta's capital city, Edmonton, is located approximately in the geographic centre of the province, with most of western Canada's oil refinery capacity located nearby, in proximity to most of Canada's largest oil fields. Edmonton is the most northerly major city in Canada, and serves as a gateway and hub for resource development in northern Canada. Alberta's other major city, Calgary, is located approximately south of Edmonton and north of Montana, surrounded by extensive ranching country. Almost 75% of the province's population lives in the Calgary-Edmonton Corridor, in and between the two major cities.
Most of the northern half of the province is boreal forest, while the Rocky Mountains along the southwestern boundary are largely forested. The southern quarter of the province is prairie, ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it. The central aspen parkland region extending in a broad arc between the prairies and the forests, from Calgary, north to Edmonton, and then east to Lloydminster, contains the most fertile soil in the province and most of the population. Much of the unforested part of Alberta is given over either to grain or to dairy farming, with mixed farming more common in the north and centre, while ranching and irrigated agriculture predominate in the south.
The Alberta badlands are located in southeastern Alberta, where the Red Deer River crosses the flat prairie and farmland, and features deep gorges and striking landforms. Dinosaur Provincial Park, near Brooks, Alberta, showcases the badlands terrain, desert flora, and remnants from Alberta's past when dinosaurs roamed the then lush landscape.
Climate.
Alberta has a dry continental climate with warm summers and cold winters. The province is open to cold arctic weather systems from the north, which often produce extremely cold conditions in winter. As the fronts between the air masses shift north and south across Alberta, temperature can change rapidly. Arctic air masses in the winter produce extreme minimum temperatures varying from in northern Alberta to in southern Alberta. In the summer, continental air masses produce maximum temperatures from in the mountains to in southern Alberta.
Because Alberta extends for over from north to south, its climate varies considerably. Average temperatures in January range from in the south to in the north, and in July from in the south to in the north. The climate is also influenced by the presence of the Rocky Mountains to the southwest, which disrupt the flow of the prevailing westerly winds and cause them to drop most of their moisture on the western slopes of the mountain ranges before reaching the province, casting a rain shadow over much of Alberta. The northerly location and isolation from the weather systems of the Pacific Ocean cause Alberta to have a dry climate with little moderation from the ocean. Annual precipitation ranges from in the southeast to in the north, except in the foothills of the Rocky Mountains where rainfall can reach annually.
In the summer, the average daytime temperatures range from around in the Rocky Mountain valleys and far north to near in the dry prairie of the southeast. The northern and western parts of the province experience higher rainfall and lower evaporation rates caused by cooler summer temperatures. The south and east-central portions are prone to drought-like conditions sometimes persisting for several years, although even these areas can receive heavy precipitation.
Alberta is a sunny province. Annual bright sunshine totals range between 1900 and 2500 hours per year. Northern Alberta receives about 18 hours of daylight in the summer.
The long summer days make summer the sunniest season of the year in Alberta.
In southwestern Alberta, the winter cold is frequently interrupted by warm, dry chinook winds blowing from the mountains, which can propel temperatures upward from frigid conditions to well above the freezing point in a very short period. During one chinook recorded at Pincher Creek, temperatures soared from to in one hour. The region around Lethbridge has the most chinooks, averaging 30 to 35 chinook days per year, while Calgary has a white Christmas only 59% of the time as a result of these winds.
Northern Alberta is mostly covered by boreal forest and has fewer frost-free days than southern Alberta due to its subarctic climate. The agricultural area of southern Alberta has a semi-arid steppe climate because the annual precipitation is less than the water that evaporates or is used by plants. The southeastern corner of Alberta, part of the Palliser Triangle, experiences greater summer heat and lower rainfall than the rest of the province, and as a result suffers frequent crop yield problems and occasional severe droughts. Western Alberta is protected by the mountains and enjoys the mild temperatures brought by winter chinook winds. Central and parts of northwestern Alberta in the Peace River region are largely aspen parkland, a biome transitional between prairie to the south and boreal forest to the north. After southern Ontario, Central Alberta is the most likely region in Canada to experience tornadoes. Thunderstorms, some of them severe, are frequent in the summer, especially in central and southern Alberta. The region surrounding the Calgary-Edmonton Corridor is notable for having the highest frequency of hail in Canada, which is caused by orographic lifting from the nearby Rocky Mountains, enhancing the updraft/downdraft cycle necessary for the formation of hail.
History.
The province of Alberta, as far north as about 53° north latitude, was a part of Rupert's Land from the time of the incorporation of the Hudson's Bay Company (1670). After the arrival in the North-West of the French around 1731 they settled the prairies of the west, establishing communities such as Lac La Biche and Bonnyville. Fort La Jonquière was established near what is now Calgary in 1752. The North West Company of Montreal occupied the northern part of Alberta territory before the Hudson's Bay Company arrived from Hudson Bay to take possession of it. The first explorer of the Athabasca region was Peter Pond, who, on behalf of the North West Company of Montreal, built Fort Athabasca on Lac La Biche in 1778. Roderick Mackenzie built Fort Chipewyan on Lake Athabasca ten years later in 1788. His cousin, Sir Alexander Mackenzie, followed the North Saskatchewan River to its northernmost point near Edmonton, then setting northward on foot, trekked to the Athabasca River, which he followed to Lake Athabasca. It was there he discovered the mighty outflow river which bears his name—the Mackenzie River—which he followed to its outlet in the Arctic Ocean. Returning to Lake Athabasca, he followed the Peace River upstream, eventually reaching the Pacific Ocean, and so he became the first white man to cross the North American continent north of Mexico.
Most of Alberta's territory was included in Rupert's Land, transferred to Canada in 1870. The southernmost portion of Alberta was part of the French (and Spanish) territory of Louisiana, sold to the United States in 1803; in 1818, the portion of Louisiana north of the Forty-Ninth Parallel was ceded to Great Britain. Northern Alberta was included in the North-Western Territory until 1870, when it and Rupert's land became Canada's Northwest Territories.
The district of Alberta was created as part of the North-West Territories in 1882. As settlement increased, local representatives to the North-West Legislative Assembly were added. After a long campaign for autonomy, in 1905 the district of Alberta was enlarged and given provincial status, with the election of Alexander Cameron Rutherford as the first premier.
Demographics.
Alberta has enjoyed a relatively high rate of growth in recent years, mainly because of its burgeoning economy. Between 2003 and 2004, the province had high birthrates (on par with some larger provinces such as British Columbia), relatively high immigration, and a high rate of interprovincial migration when compared to other provinces. Approximately 81% of the population live in urban areas and only about 19% live in rural areas. The Calgary-Edmonton Corridor is the most urbanized area in the province and is one of the most densely populated areas of Canada. Many of Alberta's cities and towns have also experienced very high rates of growth in recent history. Over the past century, Alberta's population rose from 73,022 in 1901 to 2,974,807 in 2001 and 3,290,350 according to the 2006 census.
Languages.
The 2006 census found that English, with 2,576,670 native speakers, was the mother tongue of 79.99% of Albertans. The next most common mother tongues were Chinese languages with 97,275 native-speakers (3.02%); followed by German with 84,505 native-speakers (2.62%); and French with 61,225 (1.90%); then Punjabi 36,320 (1.13%); Tagalog 29,740 (0.92%); Ukrainian 29,455 (0.91%); Spanish 29,125 (0.90%); and Polish 21,990 (0.68%); Arabic 20,495 (0.64%); Dutch 19,980 (0.62%); and Vietnamese 19,350 (0.60%). The most common aboriginal language is Cree 17,215 (0.53%). Other common mother tongues include Italian with 13,095 speakers (0.41%); Urdu with 11,275 (0.35%); and Korean with 10,845 (0.33%); then Hindi 8,985 (0.28%); Persian 7,700 (0.24%); Portuguese 7,205 (0.22%); and Hungarian 6,770 (0.21%)."(Figures shown are for the number of single language responses and the percentage of total single-language responses.)"
Ethnicity.
Alberta has considerable ethnic diversity. In line with the rest of Canada, many immigrants originated from Scotland, Ireland and Wales, but large numbers also came from other parts of Europe, notably Germans, French, Ukrainians and Scandinavians. According to Statistics Canada, Alberta is home to the second highest proportion (two percent) of Francophones in western Canada (after Manitoba). Many of Alberta's French-speaking residents live in the central and northwestern regions of the province. As reported in the 2001 census, the Chinese represented nearly four percent of Alberta's population, and East Indians represented more than two percent. Both Edmonton and Calgary have historic Chinatowns, and Calgary has Canada's third largest Chinese community. The Chinese presence began with workers employed in the building of the Canadian Pacific Railway in the 1880s. Aboriginal Albertans make up approximately three percent of the population.
In the 2006 Canadian census, the most commonly reported ethnic origins among Albertans were: 885,825 English (27.2%); 679,705 German (20.9%); 667,405 Canadian (20.5%); 661,265 Scottish (20.3%); 539,160 Irish (16.6%); 388,210 French (11.9%); 332,180 Ukrainian (10.2%); 172,910 Dutch (5.3%); 170,935 Polish (5.2%); 169,355 North American Indian (5.2%); 144,585 Norwegian (4.4%); and 137,600 Chinese (4.2%). (Each person could choose more than one ethnicity.)"
Amongst those of British origins, the Scots have had a particularly strong influence on place-names, with the names of many cities and towns including Calgary, Airdrie, Canmore, and Banff having Scottish origins.
Religion.
As of the Canada 2001 Census the largest religious group was Roman Catholic, representing 25.7% of the population. Alberta had the second highest percentage of non-religious residents in Canada (after British Columbia) at 23.1% of the population. Of the remainder, 13.5% of the population identified themselves as belonging to the United Church of Canada, while 5.9% were Anglican. Lutherans made up 4.8% of the population while Baptists comprised 2.5%. The remainder had a wide variety of different religious affiliations, although no individual group constituted more than 2% of the population.
The Mormons of Alberta reside primarily in the extreme south of the province and made up 1.7% of the population. Alberta has a population of Hutterites, a communal Anabaptist sect similar to the Mennonites (Hutterites represented 0.4% of the population while Mennonites were 0.8%), and has a significant population of Seventh-day Adventists at 0.3%.Alberta is home to several Byzantine Rite Churches as part of the legacy of Eastern European immigration, including the Ukrainian Catholic Eparchy of Edmonton, and the Ukrainian Orthodox Church of Canada's Western Diocese which is based in Edmonton.
Muslims, Sikhs, and Hindus live in Alberta. Muslims constituted 1.7% of the population, Sikhs 0.8% and Hindus 0.5%. Many of these are recent immigrants, but others have roots that go back to the first settlers of the prairies. Canada's oldest mosque the Al-Rashid Mosque is located in Edmonton.
Jews constituted 0.4% of Alberta's population. Most of Alberta's 13,000 Jews live in Calgary (7,500) and Edmonton (5,000).
Visible Minorities and Aboriginal Peoples.
Alberta is the third most diverse province in terms of visible minorities after British Columbia and Ontario with 13.9% of the population consisting of visible minorities. Calgary and Edmonton are very diverse cities in Canada with almost one quarter of their population belonging a visible minorities group. Alberta has been attracting immigrants who are for the most part are visible minorities with the opportunities available in a booming economy.
Aboriginal Identity Peoples make up 5.8% of the population with half that consisting North American Indians and the other half consisting of Metis. There are also small number of Inuit people in Alberta. The number of Aboriginal Identity Peoples have been increasing at a rate greater than the population of Alberta.
Economy.
Alberta's economy is one of the strongest in Canada, supported by the burgeoning petroleum industry and to a lesser extent, agriculture and technology. The per capita GDP in 2007 was by far the highest of any province in Canada at C$74,825. This was 61% higher than the national average of C$46,441 and more than twice that of some of the Atlantic provinces. In 2006 the deviation from the national average was the largest for any province in Canadian history. According to the 2006 census, the median annual family income after taxes was $70,986 in Alberta (compared to $60,270 in Canada as a whole).
The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada. The region covers a distance of roughly 400 kilometres north to south. In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population). It is also one of the fastest growing regions in the country. A 2003 study by TD Bank Financial Group found the corridor to be the only Canadian urban centre to amass a U.S. level of wealth while maintaining a Canadian style quality of life, offering universal health care benefits. The study found that GDP per capita in the corridor was 10% above average U.S. metropolitan areas and 40% above other Canadian cities at that time.
According to the Fraser Institute, Alberta also has very high levels of economic freedom. It is by far the most free economy in Canada, and is rated as the 2nd most free economy of U.S. states and Canadian provinces.
Industry.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in the country. Alberta is the world’s 2nd largest exporter of natural gas and the 4th largest producer. Two of the largest producers of petrochemicals in North America are located in central and north central Alberta. In both Red Deer and Edmonton, world class polyethylene and vinyl manufacturers produce products shipped all over the world, and Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton.
The Athabasca Oil Sands (sometimes known as the Athabasca Tar Sands) have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be 1.6 trillion barrels (254 km³). With the development of new extraction methods such as steam assisted gravity drainage, which was developed in Alberta, bitumen and synthetic crude oil can be produced at costs close to those of conventional crude. Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands. With current technology and at current prices, about 315 billion barrels (50 km³) of bitumen are recoverable. Fort McMurray, one of Canada's fastest growing cities, has grown enormously in recent years because of the large corporations which have taken on the task of oil production. As of late 2006 there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta.
Another factor determining the viability of oil extraction from the Tar Sands is the price of oil. The oil price increases since 2003 have made it more than profitable to extract this oil, which in the past would give little profit or even a loss.
With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid crystal display systems. With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Agriculture and forestry.
Agriculture has a significant position in the province's economy. The province has over three million head of cattle, and Alberta beef has a healthy worldwide market. Nearly one half of all Canadian beef is produced in Alberta. Alberta is one of the prime producers of plains buffalo (bison) for the consumer market. Sheep for wool and mutton are also raised.
Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production; other grains are also prominent. Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation. Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion. Across the province, the once common grain elevator is slowly being lost as rail lines are decreasing; farmers typically truck the grain to central points.
Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed. Hybrid canola also requires bee pollination, and some beekeepers service this need.
The vast northern forest reserves of softwood allow Alberta to produce large quantities of lumber, oriented strand board (OSB) and plywood, and several plants in northern Alberta supply North America and the Pacific Rim nations with bleached wood pulp and newsprint.
Tourism.
Alberta has been a tourist destination from the early days of the twentieth century, with attractions including outdoor locales for skiing, hiking and camping, shopping locales such as West Edmonton Mall, Calgary Stampede, outdoor festivals, professional athletic events, international sporting competitions such as the Commonwealth Games and Olympic Games, as well as more eclectic attractions. There are also natural attractions like Elk Island National Park, Wood Buffalo National Park, and the Columbia Icefield.
According to Alberta Economic Development, Calgary and Edmonton both host over four million visitors annually. Banff, Jasper and the Rocky Mountains are visited by about three million people per year. Alberta tourism relies heavily on Southern Ontario tourists, as well as tourists from other parts of Canada, the United States, and many international countries.
Alberta's Rocky Mountains include well known tourist destinations Banff National Park and Jasper National Park. The two mountain parks are connected by the scenic Icefields Parkway. Banff is located west of Calgary on Highway 1, and Jasper is located west of Edmonton on Yellowhead Highway. Five of Canada's fourteen UNESCO World heritage sites are located within the province: Canadian Rocky Mountain Parks, Waterton-Glacier International Peace Park, Wood Buffalo National Park, Dinosaur Provincial Park and Head-Smashed-In Buffalo Jump.
About 1.2 million people visit the of Calgary Stampede, a celebration of Canada's own Wild West and the cattle ranching industry. About 800,000 people enjoy Edmonton's Capital Ex (formerly Klondike Days). Edmonton was the gateway to the only all-Canadian route to the Yukon gold fields, and the only route which did not require gold-seekers to travel the exhausting and dangerous Chilkoot Pass.
Another tourist destination that draws more than 650,000 visitors each year is the Drumheller Valley, located northeast of Calgary. Drumheller, "Dinosaur Capital of The World", offers the Royal Tyrrell Museum of Palaeontology. Drumheller also had a rich mining history being one of Western Canada's largest coal producers during the war years. The Canadian Badlands has much to offer in the way of attractions, cultural events, celebrations, accommodations and service.
Located in east-central Alberta is Alberta Prairie Railway Excursions, a popular tourist attraction operated out of Stettler. It boasts one of the few operable steam trains in the world, offering trips through the rolling prairie scenery. Alberta Prairie Railway Excursions caters to tens of thousands of visitors every year.
Alberta is an important destination for tourists who love to ski and hike; Alberta boasts several world-class ski resorts such as Sunshine Village, Lake Louise, Marmot Basin, Norquay and Nakiska. Hunters and fishermen from around the world are able to take home impressive trophies and tall tales from their experiences in Alberta's wilderness.
Taxation.
The province's revenue comes mainly from royalties on non-renewable natural resources (30.4%), personal income taxes (22.3%), corporate and other taxes (19.6%), and grants from the federal government primarily for infrastructure projects (9.8%). Albertans are the lowest-taxed people in Canada, and Alberta is the only province in Canada without a provincial sales tax (though residents are still subject to the federal sales tax, the Goods and Services Tax of 5%.) It is also the only Canadian province to have a single rate of taxation for personal income taxes which is 10% of taxable income. The Alberta tax system maintains a progressive flavour by allowing residents to earn $16,161 before becoming subject to provincial taxation in addition to a variety of tax deductions for persons with disabilities, students, and the aged. Alberta's municipalities and school jurisdictions have their own governments which (usually) work in co-operation with the provincial government.
Transportation.
Alberta has over of highways and roads, of which nearly are paved. The main north-south corridor is Highway 2, which begins south of Cardston at the Carway border crossing and is part of the CANAMEX Corridor. Highway 4, which effectively extends Interstate 15 into Alberta and is the busiest U.S. gateway to the province, begins at the Coutts border crossing and ends at Lethbridge. Highway 3 joins Lethbridge to Fort Macleod and links Highway 4 to Highway 2. Highway 2 travels northward through Fort Macleod, Calgary, Red Deer, and Edmonton. North of Edmonton the highway continues to Athabasca, then northwesterly along the south shore of Lesser Slave Lake into High Prairie, north to Peace River, west to Fairview and finally south to Grande Prairie. The section of Highway 2 between Calgary and Edmonton has been named the Queen Elizabeth II Highway to commemorate the visit of the monarch in 2005. Highway 2 is supplemented by two more highways that run parallel to it: Highway 22, west of highway 2, known as "the Cowboy Trail," and Highway 21, east of highway 2. Highway 43 travels northwest into Grande Prairie and the Peace River Country; Highway 63 travels northeast to Fort McMurray, the location of the Athabasca Oil Sands.
Alberta has two main east-west corridors. The southern corridor, part of the Trans-Canada Highway system, enters the province near Medicine Hat, runs westward through Calgary, and leaves Alberta through Banff National Park. The northern corridor, also part of the Trans-Canada network and known as the Yellowhead Highway (Highway 16), runs west from Lloydminster in eastern Alberta, through Edmonton and Jasper National Park into British Columbia. One of the most scenic drives is along the Icefields Parkway, which runs for between Jasper and Lake Louise, with mountain ranges and glaciers on either side of its entire length.
Another major corridor through central Alberta is Highway 11 (also known as the David Thompson Highway), which runs east from the Saskatchewan River Crossing in Banff National Park through Rocky Mountain House and Red Deer, connecting with Highway 12 west of Stettler. The highway connects many of the smaller towns in central Alberta with Calgary and Edmonton, as it crosses Highway 2 just west of Red Deer.
Urban stretches of Alberta's major highways and freeways are often called "trails". For example, Highway 2, the main north-south highway in the province, is called Deerfoot Trail as it passes through Calgary but becomes Calgary Trail as it enters Edmonton and then turns into Saint Albert Trail as it leaves Edmonton for the city of St. Albert. Calgary, in particular, has a tradition of calling its largest urban expressways "trails" and naming many of them after prominent First Nations individuals and tribes, such as Crowchild Trail, Deerfoot Trail, and Stoney Trail.
Calgary, Edmonton, Red Deer, Medicine Hat, and Lethbridge have substantial public transit systems. In addition to buses, Calgary and Edmonton operate light rail transit (LRT) systems. Edmonton LRT, which is underground in the downtown core and on the surface outside of it, was the first of the modern generation of light rail systems to be built in North America, while the Calgary C-Train, although operating mostly on the surface, has almost 4 times more track than the Edmonton LRT and the highest ridership of any LRT system in North America.
Alberta is well-connected by air, with international airports in both Calgary and Edmonton. Calgary International Airport and Edmonton International Airport are the fourth and fifth busiest in Canada respectively. Calgary's airport is a hub for WestJet Airlines and a regional hub for Air Canada. Calgary's airport primarily serves the Canadian prairie provinces (Alberta, Saskatchewan and Manitoba) for connecting flights to British Columbia, eastern Canada, 15 major US centres, nine European airports, and four destinations in Mexico and the Caribbean. Edmonton's airport acts as a hub for the Canadian north and has connections to all major Canadian airports as well as 10 major US airports, 3 European airports and 6 Mexican and Caribbean airports.
There are over of operating mainline railway, and many tourists see Alberta aboard Via Rail or Rocky Mountaineer. The Canadian Pacific Railway and Canadian National Railway companies operate railway freight across the province.
Government.
The government of Alberta is organized as a parliamentary democracy with a unicameral legislature. Its unicameral legislature—the Legislative Assembly—consists of eighty-three members.
Locally municipal governments and school boards are elected and operate separately. Their boundaries do not necessarily coincide. Municipalities where the same body act as both local government and school board are formally referred to as "counties" in Alberta.
As Canada's head of state, Queen Elizabeth II is the head of state for the Government of Alberta. Her duties in Alberta are carried out by Lieutenant Governor Norman Kwong. Although the lieutenant governor is technically the most powerful person in Alberta, he is in reality a figurehead whose actions are restricted by custom and constitutional convention. The government is therefore headed by the premier. The current premier is Ed Stelmach who was elected as leader of the governing Progressive Conservatives on December 2, 2006. Stelmach was sworn in as the 13th Premier of Alberta on December 15, 2006.
The Premier is a Member of the Legislative Assembly, and he draws all the members of his Cabinet from among the members of the Legislative Assembly.
The City of Edmonton is the seat of the provincial government—the capital of Alberta.
Alberta's elections tend to yield results which are much more conservative than those of other Canadian provinces. Alberta has traditionally had three political parties, the Progressive Conservatives ("Conservatives" or "Tories"), the Liberals, and the social democratic New Democrats. A fourth party, the strongly conservative Social Credit Party, was a power in Alberta for many decades, but fell from the political map after the Progressive Conservatives came to power in 1971. Since that time, no other political party has governed Alberta. In fact, only four parties have governed Alberta: the Liberals, from 1905 to 1921; the United Farmers of Alberta, from 1921 to 1935; the Social Credit Party, from 1935 to 1971, and the currently governing Progressive Conservative Party, from 1971 to the present.
Alberta has had occasional surges in separatist sentiment. Even during the 1980s, when these feelings were at their strongest, there has never been enough interest in secession to initiate any major movements or referendums. There are several groups wishing to promote the independence of Alberta in some form currently active in the province.
In the 2008 provincial election, held on March 3, 2008, the Progressive Conservative Party was re-elected as a majority government with 72 of 83 seats, the Alberta Liberal Party was elected as the Official Opposition with nine members, and the Alberta New Democratic Party elected two members.
Health care.
As with all Canadian provinces, Alberta provides for all citizens and residents through a publicly funded health care system. Alberta became Canada's second province (after Saskatchewan) to adopt a Tommy Douglas-style program in 1950, a precursor to the modern medicare system.
Alberta's health care budget is currently $13.2 billion during the 2008-2009 fiscal year (approximately 36% of all government spending), making it the best funded health care system per-capita in Canada. Every hour more than $1.5 million is spent on health care in the province.
A highly educated population and burgeoning economy have made Alberta a national leader in health education, research, and resources. Many notable facilities include the Foothills Medical Centre, the Peter Lougheed Centre, Rockyview General Hospital, Alberta Children's Hospital, Grace Women's Health Centre, The University of Calgary Medical Centre (UCMC), Tom Baker Cancer Centre and Libin Cardiovascular Institute of Alberta, in Calgary; In Edmonton, the University of Alberta Hospital, the Royal Alexandra Hospital, the Mazankowski Alberta Heart Institute, the Lois Hole Hospital for Women, the Stollery Children's Hospital, the Alberta Diabetes Institute, the Cross Cancer Institute, and the Rexall Centre for Pharmacy and Health Research in Edmonton. Currently under construction in Edmonton is the new $909 million Edmonton Clinic, which will provide a similar research, education, and care environment as the Mayo Clinic in the United States.
Health Care in Alberta is administered by the unified Alberta Health Services Board. Prior to July 1, 2008 Alberta was divided into nine health regions: Aspen Regional Health Authority: Calgary Health Region, Capital Health (Edmonton), Chinook Health, David Thompson Regional Health Authority, East Central Health, Northern Lights Health Region, Palliser Health Region and Peace Country Health Region.
The Shock Trauma Air Rescue Society, a nonprofit organization, provides an air ambulance service to all but the most remote areas of Alberta, and some adjoining areas of British Columbia.
Education.
As with any Canadian province, the Alberta Legislature has (almost) exclusive authority to make laws respecting education. Since 1905 the Legislature has used this capacity to continue the model of locally elected public and separate school boards which originated prior to 1905, as well as to create and/or regulate universities, colleges, technical institutions and other educational forms and institutions (public charter schools, private schools, home schooling).
Elementary schools.
There are forty-two public school jurisdictions in Alberta, and seventeen operating separate school jurisdictions. Sixteen of the operating separate school jurisdictions have a Catholic electorate, and one (St. Albert) has a Protestant electorate. In addition, one Protestant separate school district, Glen Avon, survives as a ward of the St. Paul Education Region. The City of Lloydminster straddles the Alberta/Saskatchewan border, and both the public and separate school systems in that city are counted in the above numbers: both of them operate according to Saskatchewan law.
For many years the provincial government has funded the greater part of the cost of providing K–12 education. Prior to 1994 public and separate school boards in Alberta had the legislative authority to levy a local tax on property, as supplementary support for local education. In 1994 the government of the province eliminated this right for public school boards, but not for separate school boards. Since 1994 there has continued to be a tax on property in support of K–12 education; the difference is that the mill rate is now set by the provincial government, the money is collected by the local municipal authority and remitted to the provincial government. The relevant legislation requires that all the money raised by this property tax must go to the support of K–12 education provided by school boards. The provincial government pools the property tax funds from across the province and distributes them, according to a formula, to public and separate school jurisdictions and Francophone authorities.
Public and separate school boards, charter schools, and private schools all follow the Program of Studies and the curriculum approved by the provincial department of education (Alberta Education). Home schoolers may choose to follow the Program of Studies or develop their own Program of Studies. Public and separate schools, charter schools, and approved private schools all employ teachers who are certificated by Alberta Education, they administer Provincial Achievement Tests and Diploma Examinations set by Alberta Education, and they may grant high school graduation certificates endorsed by Alberta Education.
Universities.
Alberta's oldest and largest university is Edmonton's University of Alberta established in 1908. The University of Calgary, once affiliated with the University of Alberta, gained its autonomy in 1966 and is now the second largest university in Alberta. There is also Athabasca University, which focuses on distance learning, and the University of Lethbridge, both of which are located in their title cities. In early September, 2009, Mount Royal University became Calgary's second public university, and in late September, 2009, a similar move made Grant MacEwan University Edmonton's second public university. There are 15 colleges that receive direct public funding, along with two technical institutes, Northern Alberta Institute of Technology and Southern Alberta Institute of Technology. There is also a large and active private sector of post-secondary institutions, mostly Christian Universities, bringing the total number of universities to twelve, plus a DeVry University location in Calgary. Students may also receive government loans and grants while attending selected private institutions. There has been some controversy in recent years over the rising cost of post-secondary education for students (as opposed to taxpayers). In 2005, Premier Ralph Klein made a promise that he would freeze tuition and look into ways of reducing schooling costs. So far, no plan has been released by the government of Alberta.
Culture.
Summer brings many festivals to the province of Alberta, especially in Edmonton. The Edmonton Fringe Festival is the world's second largest after Edinburgh's. The Folk music festivals in both Calgary and Edmonton are two of Canada's largest and both cities host a number of annual multicultural events. With a large number of summer and winter events, Edmonton prides itself as being the "Festival City". The city's "heritage days" festival sees the participation of over 70 ethnic groups. Edmonton's Churchill Square is home to a large number of the festivals, including the large Taste of Edmonton & The Works Art & Design Festival throughout the summer months.
Calgary is also home to Carifest, the second largest Caribbean festival in the nation (after Caribana in Toronto). Edmonton has Cariwest, a smaller Caribbean Parade in the downtown streets. Both Edmonton and Calgary are also known for decent Film festivals. The city of Calgary is also famous for its Calgary Stampede, dubbed "The Greatest Outdoor Show on Earth." The Stampede is Canada's biggest rodeo festival and features various races and competitions, such as calf roping and bull riding. In line with the western tradition of rodeo are the cultural artisans that reside and create unique Alberta western heritage crafts. The Banff Centre also hosts a range of festivals and other events including the internationally known Mountain Film Festival. These cultural events in Alberta highlight the province's cultural diversity and love of entertainment. Most of the major cities have several performing theatre companies who entertain in venues as diverse as Edmonton's Arts Barns and the Francis Winspear Centre for Music. Both Calgary and Edmonton are home to Canadian Football League and National Hockey League teams. Soccer, rugby union and lacrosse are also played professionally in Alberta.
---END.OF.DOCUMENT---

Arctic Circle.
The Arctic Circle is one of the five major circles of latitude that mark maps of the Earth. For Epoch 2010, it is the parallel of latitude that runs 66º 33′ 44″ (or 66.56222°) north of the Equator.
The region north of this circle is known as the Arctic, and the zone just to the south is called the Northern Temperate Zone. The equivalent polar circle in the Southern Hemisphere is called the Antarctic Circle.
The Arctic Circle marks the southern extremity of the polar day (24-hour sunlit day, often referred to as the "midnight sun") and polar night (24-hour sunless night). North of the Arctic Circle, the sun is above the horizon for 24 continuous hours at least once per year and below the horizon for 24 continuous hours at least once per year. On the Arctic Circle those events occur, in principle, exactly once per year, at the June and December solstices, respectively.
In fact, because of atmospheric refraction and because the sun appears as a disk and not a point, part of the midnight sun may be seen on the night of the summer solstice up to about 50′ () south of the Arctic Circle; similarly, on the day of the winter solstice, part of the sun may be seen up to about 50′ north of the Arctic Circle. That is true at sea level; those limits increase with elevation above sea level although in mountainous regions, there is often no direct view of the horizon.
The position of the Arctic Circle is not fixed, but directly depends on the Earth's axial tilt, which fluctuates within a margin of 2° over a 40,000 year period, notably due to tidal forces resulting from the orbit of the Moon. The Arctic Circle is currently drifting northwards at a speed of about per year, see "Circle of latitude" for more information.
__TOC__ Murmansk Oblast Karelia again Murmansk again
---END.OF.DOCUMENT---

List of anthropologists.
Please make no further additions to the list.
For scientists and scholars of anthropology, refer to the category '.
__NOTOC__
---END.OF.DOCUMENT---

Actinopterygii.
The Actinopterygii constitute the class or sub-class of the ray-finned fishes.
The ray-finned fishes are so called because they possess lepidotrichia or "fin rays", their fins being webs of skin supported by bony or horny spines ("rays"), as opposed to the fleshy, lobed fins that characterize the class Sarcopterygii which also, however, possess lepidotrichia. These actinopterygian fin rays attach directly to the proximal or basal skeletal elements, the radials, which represent the link or connection between these fins and the internal skeleton (e.g., pelvic and pectoral girdles).
In terms of numbers, actinopterygians are the dominant class of vertebrates, comprising nearly 95% of the 25,000 species of fish. They are ubiquitous throughout fresh water and marine environments from the deep sea to the highest mountain streams. Extant species can range in size from "Paedocypris", at, to the massive Ocean Sunfish, at, and the long-bodied Oarfish, to at least.
Fossil record.
The earliest known fossil Actinopterygiian is "Andreolepis hedei", dating back 420 million years (Late Silurian). This microvertebrate has been uncovered in Russia, Sweden, and Estonia.
Classification.
Traditionally three grades of actinopterygians have been recognised: the Chondrostei, Holostei, and Teleostei. Some morphological evidence suggests that the second is paraphyletic and should be abandoned; however, recent work based on more complete sampling of fossil taxa, and also an analysis of DNA sequence data from the complete mitochondrial genome, supports its recognition. Nearly all living bony fishes are teleosts.
A listing of the different groups is given below, down to the level of orders, arranged in what has been suggested to represent the evolutionary sequence down to the level of order based primarily on the long history of morphological studies. This classification, like any other taxonomy based on phylogenetic research is in a state of flux. Many of these ordinal and higher-level groupings have not been supported in both the recent morphological and molecular literature. Examples of demonstrably paraphyletic or unnatural groups include the Paracanthopterygii, Scorpaeniformes, and Perciformes. The listing follows FishBase with notes when this differs from Nelson and ITIS.
---END.OF.DOCUMENT---

Albert Einstein.
Albert Einstein (;; 14 March 1879–18 April 1955) was a German-born Swiss-American theoretical physicist, philosopher and author who is widely regarded as one of the most influential and best known scientists and intellectuals of all time. He is often regarded as the father of modern physics. He received the 1921 Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect."
His many contributions to physics include the special and general theories of relativity, the founding of relativistic cosmology, the first post-Newtonian expansion, explaining the perihelion advance of Mercury, prediction of the deflection of light by gravity and gravitational lensing, the first fluctuation dissipation theorem which explained the Brownian movement of molecules, the photon theory and wave-particle duality, the quantum theory of atomic motion in solids, the zero-point energy concept, the semiclassical version of the Schrödinger equation, and the quantum theory of a monatomic gas which predicted Bose–Einstein condensation.
Einstein published more than 300 scientific and over 150 non-scientific works. Einstein additionally wrote and commentated prolifically on numerous philosophical and political issues.
Early life and education.
Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire on 14 March 1879. His father was Hermann Einstein, a salesman and engineer. His mother was Pauline Einstein (née Koch). In 1880, the family moved to Munich, where his father and his uncle founded " Elektrotechnische Fabrik J. Einstein & Cie," a company that manufactured electrical equipment based on direct current.
The Einsteins were non-observant Jews. Their son attended a Catholic elementary school from the age of five until ten. Although Einstein had early speech difficulties, he was a top student in elementary school. As he grew, Einstein built models and mechanical devices for fun and began to show a talent for mathematics. In 1889 Max Talmud (later changed to Max Talmey) introduced the ten-year old Einstein to key texts in science, mathematics and philosophy, including Kant’s "Critique of Pure Reason" and Euclid’s "Elements" (which Einstein called the "holy little geometry book"). Talmud was a poor Jewish medical student from Poland. The Jewish community arranged for Talmud to take meals with the Einsteins each week on Thursdays for six years. During this time Talmud wholeheartedly guided Einstein through many secular educational interests.
In 1894, his father’s company failed: Direct current (DC) lost the War of Currents to alternating current (AC). In search of business, the Einstein family moved to Italy, first to Milan and then, a few months later, to Pavia. When the family moved to Pavia, Einstein stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school’s regimen and teaching method. He later wrote that the spirit of learning and creative thought were lost in strict rote learning. In the spring of 1895, he withdrew to join his family in Pavia, convincing the school to let him go by using a doctor’s note. During this time, Einstein wrote his first scientific work, "The Investigation of the State of Aether in Magnetic Fields".
Einstein applied directly to the Eidgenössische Polytechnische Schule (ETH) in Zürich, Switzerland. Lacking the requisite Matura certificate, he took an entrance examination, which he failed, although he got exceptional marks in mathematics and physics.
The Einsteins sent Albert to Aarau, in northern Switzerland to finish secondary school. While lodging with the family of Professor Jost Winteler, he fell in love with the family’s daughter, Marie. (His sister Maja later married the Winteler son, Paul.) In Aarau, Einstein studied Maxwell’s electromagnetic theory. At age 17, he graduated, and, with his father’s approval, renounced his citizenship in the German Kingdom of Württemberg to avoid military service, and enrolled in 1896 in the mathematics and physics program at the Polytechnic in Zurich. Marie Winteler moved to Olsberg, Switzerland for a teaching post.
In the same year, Einstein’s future wife, Mileva Marić, also entered the Polytechnic to study mathematics and physics, the only woman in the academic cohort. Over the next few years, Einstein and Marić’s friendship developed into romance. In a letter to her, Einstein called Marić “a creature who is my equal and who is as strong and independent as I am.” Einstein graduated in 1900 from the Polytechnic with a diploma in mathematics and physics; Although historians have debated whether Marić influenced Einstein’s work, the majority of academic historians of science agree that she did not.
Marriages and children.
In early 1902, Einstein and Mileva Marić had a daughter they named Lieserl in their correspondence, who was born in Novi Sad where Marić's parents lived. Her full name is not known, and her fate is uncertain after 1903.
Einstein and Marić married in January 1903. In May 1904, the couple’s first son, Hans Albert Einstein, was born in Bern, Switzerland. Their second son, Eduard, was born in Zurich in July 1910. In 1914, Einstein moved to Berlin, while his wife remained in Zurich with their sons. Marić and Einstein divorced on 14 February 1919, having lived apart for five years.
Einstein married Elsa Löwenthal (née Einstein) on 2 June 1919, after having had a relationship with her since 1912. She was his first cousin maternally and his second cousin paternally. In 1933, they emigrated permanently to the United States. In 1935, Elsa Einstein was diagnosed with heart and kidney problems and died in December 1936.
Patent office.
After graduating, Einstein spent almost two frustrating years searching for a teaching post, but a former classmate’s father helped him secure a job in Bern, at the Federal Office for Intellectual Property, the patent office, as an assistant examiner. He evaluated patent applications for electromagnetic devices. In 1903, Einstein’s position at the Swiss Patent Office became permanent, although he was passed over for promotion until he "fully mastered machine technology".
Much of his work at the patent office related to questions about transmission of electric signals and electrical-mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time.
With friends he met in Bern, Einstein formed a weekly discussion club on science and philosophy, which he jokingly named "The Olympia Academy." Their readings included the works of Henri Poincaré, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.
Academic career.
In 1901, Einstein had a paper on the capillary forces of a straw published in the prestigious "Annalen der Physik". In 1905, he received his doctorate from the University of Zurich. His thesis was titled "On a new determination of molecular dimensions". That same year, which has been called Einstein's "annus mirabilis" or "miracle year", he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of matter and energy, which were to bring him to the notice of the academic world.
By 1908, he was recognized as a leading scientist, and he was appointed lecturer at the University of Berne. The following year, he quit the patent office and the lectureship to take the position of physics professor at the University of Zurich. He became a full professor at Karl-Ferdinand University in Prague in 1911. In 1914, he returned to Germany after being appointed director of the Kaiser Wilhelm Institute for Physics and professor at the University of Berlin.
In 1911, he had calculated that, based on his new theory of general relativity, light from another star would be bent by the Sun's gravity. That prediction was claimed confirmed by observations made by a British expedition led by Sir Arthur Eddington during the solar eclipse of May 29, 1919. International media reports of this made Einstein world famous. (Much later, questions were raised whether the measurements were accurate enough to support such a claim.)
In 1921, Einstein was awarded the Nobel Prize in Physics. Because relativity was still considered somewhat controversial, it was officially bestowed for his explanation of the photoelectric effect. He also received the Copley Medal from the Royal Society in 1925.
Emigration to the United States.
In 1933, Einstein emigrated because of the rise to power of the Nazis and took up a position at the Institute for Advanced Study at Princeton, New Jersey, an affiliation that lasted until his death in 1955. There, he tried unsuccessfully to develop a unified field theory and to refute the accepted interpretation of quantum physics.
He and Kurt Gödel, another Institute member, became close friends. They would take long walks together discussing their work.
Just prior to the beginning of World War II in Europe, Einstein was persuaded to lend his enormous prestige to a letter sent to President Franklin D. Roosevelt on August 2, 1939, alerting him to the possibility that Nazi Germany might be developing an atomic bomb.
In 1940, he became an American citizen.
In 1952, he was offered the position of President of Israel, but declined.
Death.
On 17 April 1955, Albert Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Dr. Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel’s seventh anniversary with him to the hospital, but he did not live long enough to complete it. Einstein refused surgery, saying: "I want to go when I want. It is tasteless to prolong life artificially. I have done my share, it is time to go. I will do it elegantly." He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end. Einstein’s remains were cremated and his ashes were scattered around the grounds of the Institute for Advanced Study, Princeton, New Jersey.
During the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey removed Einstein’s brain for preservation, without the permission of his family, in hope that the neuroscience of the future would be able to discover what made Einstein so intelligent.
Scientific career.
Throughout his life, Einstein published hundreds of books and articles. Most were about physics, but a few expressed leftist political opinions about pacifism, socialism, and zionism. In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.
Physics in 1900.
Einstein’s early papers all come from attempts to demonstrate that atoms exist and have a finite nonzero size. At the time of his first paper in 1902, it was not yet completely accepted by physicists that atoms were real, even though chemists had good evidence ever since Antoine Lavoisier’s work a century earlier. The reason physicists were skeptical was because no 19th century theory could fully explain the properties of matter from the properties of atoms.
Ludwig Boltzmann was a leading 19th century atomist physicist, who had struggled for years to gain acceptance for atoms. Boltzmann had given an interpretation of the laws of thermodynamics, suggesting that the law of entropy increase is statistical. In Boltzmann’s way of thinking, the entropy is the logarithm of the number of ways a system could be configured inside. The reason the entropy goes up is only because it is more likely for a system to go from a special state with only a few possible internal configurations to a more generic state with many. While Boltzmann’s statistical interpretation of entropy is universally accepted today, and Einstein believed it, at the turn of the 20th century it was a minority position.
The statistical idea was most successful in explaining the properties of gases. James Clerk Maxwell, another leading atomist, had found the distribution of velocities of atoms in a gas, and derived the surprising result that the viscosity of a gas should be independent of density. Intuitively, the friction in a gas would seem to go to zero as the density goes to zero, but this is not so, because the mean free path of atoms becomes large at low densities. A subsequent experiment by Maxwell and his wife confirmed this surprising prediction. Other experiments on gases and vacuum, using a rotating slitted drum, showed that atoms in a gas had velocities distributed according to Maxwell’s distribution law.
In addition to these successes, there were also inconsistencies. Maxwell noted that at cold temperatures, atomic theory predicted specific heats that are too large. In classical statistical mechanics, every spring-like motion has thermal energy "k"B"T" on average at temperature "T", so that the specific heat of every spring is Boltzmann’s constant "k"B. A monatomic solid with "N" atoms can be thought of as "N" little balls representing "N" atoms attached to each other in a box grid with 3"N" springs, so the specific heat of every solid is 3"Nk"B, a result which became known as the Dulong–Petit law. This law is true at room temperature, but not for colder temperatures. At temperatures near zero, the specific heat goes to zero.
Similarly, a gas made up of a molecule with two atoms can be thought of as two balls on a spring. This spring has energy "k"B"T" at high temperatures, and should contribute an extra "k"B to the specific heat. It does at temperatures of about 1000 degrees, but at lower temperature, this contribution disappears. At zero temperature, all other contributions to the specific heat from rotations and vibrations also disappear. This behavior was inconsistent with classical physics.
The most glaring inconsistency was in the theory of light waves. Continuous waves in a box can be thought of as infinitely many spring-like motions, one for each possible standing wave. Each standing wave has a specific heat of "k"B, so the total specific heat of a continuous wave like light should be infinite in classical mechanics. This is obviously wrong, because it would mean that all energy in the universe would be instantly sucked up into light waves, and everything would slow down and stop.
These inconsistencies led some people to say that atoms were not physical, but mathematical. Notable among the skeptics was Ernst Mach, whose positivist philosophy led him to demand that if atoms are real, it should be possible to see them directly. Mach believed that atoms were a useful fiction, that in reality they could be assumed to be infinitesimally small, that Avogadro’s number was infinite, or so large that it might as well be infinite, and "k"B was infinitesimally small. Certain experiments could then be explained by atomic theory, but other experiments could not, and this is the way it will always be.
Einstein opposed this position. Throughout his career, he was a realist. He believed that a single consistent theory should explain all observations, and that this theory would be a description of what was really going on, underneath it all. So he set out to show that the atomic point of view was correct. This led him first to thermodynamics, then to statistical physics, and to the theory of specific heats of solids.
In 1905, while he was working in the patent office, the leading German language physics journal "Annalen der Physik" published four of Einstein’s papers. The four papers eventually were recognized as revolutionary, and 1905 became known as Einstein’s "Miracle Year", and the papers as the "Annus Mirabilis Papers".
Thermodynamic fluctuations and statistical physics.
Einstein’s earliest papers were concerned with thermodynamics. He wrote a paper establishing a thermodynamic identity in 1902, and a few other papers which attempted to interpret phenomena from a statistical atomic point of view.
His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena. As in Maxwell’s work, the finite nonzero size of atoms leads to effects which can be observed. This research, and the thermodynamic identity, were well within the mainstream of physics in his time. They would eventually form the content of his PhD thesis.
His first major result in this field was the theory of thermodynamic fluctuations. When in equilibrium, a system has a maximum entropy and, according to the statistical interpretation, it can fluctuate a little bit. Einstein pointed out that the statistical fluctuations of a macroscopic object, like a mirror suspended on spring, would be completely determined by the second derivative of the entropy with respect to the position of the mirror.
Searching for ways to test this relation, his great breakthrough came in 1905. The theory of fluctuations, he realized, would have a visible effect for an object which could move around freely. Such an object would have a velocity which is random, and would move around randomly, just like an individual atom. The average kinetic energy of the object would be formula_1, and the time decay of the fluctuations would be entirely determined by the law of friction.
The law of friction for a small ball in a viscous fluid like water was discovered by George Stokes. He showed that for small velocities, the friction force would be proportional to the velocity, and to the radius of the particle (see Stokes’ law). This relation could be used to calculate how far a small ball in water would travel due to its random thermal motion, and Einstein noted that such a ball, of size about a micron, would travel about a few microns per second. This motion could be easily detected with a microscope and indeed, as Brownian motion, had actually been observed by the botanist Robert Brown. Einstein was able to identify this motion with that predicted by his theory. Since the fluctuations which give rise to Brownian motion are just the same as the fluctuations of the velocities of atoms, measuring the precise amount of Brownian motion using Einstein’s theory would show that Boltzmann’s constant is non-zero and would measure Avogadro’s number.
These experiments were carried out a few years later, and gave a rough estimate of Avogadro’s number consistent with the more accurate estimates due to Max Planck’s theory of blackbody light, and Robert Millikan’s measurement of the charge of the electron. Unlike the other methods, Einstein’s required very few theoretical assumptions or new physics, since it was directly measuring atomic motion on visible grains.
Einstein’s theory of Brownian motion was the first paper in the field of statistical physics. It established that thermodynamic fluctuations were related to dissipation. This was shown by Einstein to be true for time-independent fluctuations, but in the Brownian motion paper he showed that dynamical relaxation rates calculated from classical mechanics could be used as statistical relaxation rates to derive dynamical diffusion laws. These relations are known as Einstein relations.
The theory of Brownian motion was the least revolutionary of Einstein’s Annus mirabilis papers, but it had an important role in securing the acceptance of the atomic theory by physicists.
Thought experiments and a-priori physical principles.
Einstein’s thinking underwent a transformation in 1905. He had come to understand that quantum properties of light mean that Maxwell’s equations were only an approximation. He knew that new laws would have to replace these, but he did not know how to go about finding those laws. He felt that guessing formal relations would not go anywhere.
So he decided to focus on a-priori principles instead, which are statements about physical laws which can be understood to hold in a very broad sense even in domains where they have not yet been shown to apply. A well accepted example of an a-priori principle is rotational invariance. If a new force is discovered in physics, it is assumed to be rotationally invariant almost automatically, without thought. Einstein sought new principles of this sort, to guide the production of physical ideas. Once enough principles are found, then the new physics will be the simplest theory consistent with the principles and with previously known laws.
The first general a-priori principle he found was the principle of relativity, that uniform motion is indistinguishable from rest. This was understood by Hermann Minkowski to be a generalization of rotational invariance from space to space-time. Other principles postulated by Einstein and later vindicated are the principle of equivalence and the principle of adiabatic invariance of the quantum number. Another of Einstein’s general principles, Mach’s principle, is fiercely debated, and whether it holds in our world or not is still not definitively established.
The use of a-priori principles is a distinctive unique signature of Einstein’s early work, and has become a standard tool in modern theoretical physics.
Special relativity.
His 1905 paper on the electrodynamics of moving bodies introduced his theory of special relativity, which showed that the observed independence of the speed of light on the observer’s state of motion required fundamental changes to the notion of simultaneity. Consequences of this include the time-space frame of a moving body slowing down and contracting (in the direction of motion) relative to the frame of the observer. This paper also argued that the idea of a luminiferous aether – one of the leading theoretical entities in physics at the time – was superfluous.
In his paper on "mass–energy equivalence", which had previously been considered to be distinct concepts, Einstein deduced from his equations of special relativity what has been called the twentieth century’s best-known equation: "E" = "mc"2. This equation suggests that tiny amounts of mass could be converted into huge amounts of energy and presaged the development of nuclear power.
Einstein’s 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
Photons.
In a 1905 paper, Einstein postulated that light itself consists of localized particles ("quanta"). Einstein’s light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan’s detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein’s paper on the light particles was almost entirely motivated by thermodynamic considerations. He was not at all motivated by the detailed experiments on the photoelectric effect, which did not confirm his theory until fifteen years later.
Einstein considers the entropy of light at temperature "T", and decomposes it into a low-frequency part and a high-frequency part. The high-frequency part, where the light is described by Wien’s law, has an entropy which looks exactly the same as the entropy of a gas of classical particles.
Since the entropy is the logarithm of the number of possible states, Einstein concludes that the number of states of short wavelength light waves in a box with volume "V" is equal to the number of states of a group of localizable particles in the same box. Since (unlike others) he was comfortable with the statistical interpretation, he confidently postulates that the light itself is made up of localized particles, as this is the only reasonable interpretation of the entropy.
This leads him to conclude that each wave of frequency "f" is associated with a collection of photons with energy "hf" each, where "h" is Planck’s constant. He does not say much more, because he is not sure how the particles are related to the wave. But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.
Quantized atomic vibrations.
Einstein continued his work on quantum mechanics in 1906, by explaining the specific heat anomaly in solids. This was the first application of quantum theory to a mechanical system.
Since Planck’s distribution for light oscillators had no problem with infinite specific heats, the same idea could be applied to solids to fix the specific heat problem there. Einstein showed in a simple model that the hypothesis that solid motion is quantized explains why the specific heat of a solid goes to zero at zero temperature.
Einstein’s model treats each atom as connected to a single spring. Instead of connecting all the atoms to each other, which leads to standing waves with all sorts of different frequencies, Einstein imagined that each atom was attached to a fixed point in space by a spring. This is not physically correct, but it still predicts that the specific heat is 3"Nk"B, since the number of independent oscillations stays the same.
Einstein then assumes that the motion in this model is quantized, according to the Planck law, so that each independent spring motion has energy which is an integer multiple of hf, where f is the frequency of oscillation. With this assumption, he applied Boltzmann’s statistical method to calculate the average energy of the spring. The result was the same as the one that Planck had derived for light: for temperatures where "k"B"T" is much smaller than "hf", the motion is frozen, and the specific heat goes to zero.
So Einstein concluded that quantum mechanics would solve the main problem of classical physics, the specific heat anomaly. The particles of sound implied by this formulation are now called phonons. Because all of Einstein’s springs have the same stiffness, they all freeze out at the same temperature, and this leads to a prediction that the specific heat should go to zero exponentially fast when the temperature is low. The solution to this problem is to solve for the independent normal modes individually, and to quantize those. Then each normal mode has a different frequency, and long wavelength vibration modes freeze out at colder temperatures than short wavelength ones. This was done by Debye, and after this modification Einstein’s quantization method reproduced quantitatively the behavior of the specific heats of solids at low temperatures.
This work was the foundation of condensed matter physics.
Adiabatic principle and action-angle variables.
Throughout the 1910s, quantum mechanics expanded in scope to cover many different systems. After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements.
Einstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics. The law that the action variable is quantized was the basic principle of the quantum theory as it was known between 1900 and 1925.
Wave-particle duality.
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on "academia." In 1908, he became a "privatdozent" at the University of Bern.
In "über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung" ("The Development of Our Views on the Composition and Essence of Radiation"), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck’s energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the "photon" concept (although the name "photon" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave-particle duality in quantum mechanics.
Theory of Critical Opalescence.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Raleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue.
Zero-point energy.
Einstein’s physical intuition led him to note that Planck’s oscillator energies had an incorrect zero point. He modified Planck’s hypothesis by stating that the lowest energy state of an oscillator is equal to "hf", to half the energy spacing between levels. This argument, which was made in 1913 in collaboration with Otto Stern, was based on the thermodynamics of a diatomic molecule which can split apart into two free atoms.
Principle of equivalence.
In 1907, while still working at the patent office, Einstein had what he would call his "happiest thought". He realized that the principle of relativity could be extended to gravitational fields.
He thought about the case of a uniformly accelerated box not in a gravitational field, and noted that it would be indistinguishable from a box sitting still in an unchanging gravitational field. He used special relativity to see that the rate of clocks at the top of a box accelerating upward would be faster than the rate of clocks at the bottom. He concludes that the rates of clocks depend on their position in a gravitational field, and that the difference in rate is proportional to the gravitational potential to first approximation.
Although this approximation is crude, it allowed him to calculate the deflection of light by gravity, and show that it is nonzero. This gave him confidence that the scalar theory of gravity proposed by Gunnar Nordström was incorrect. But the actual value for the deflection that he calculated was too small by a factor of two, because the approximation he used doesn’t work well for things moving at near the speed of light. When Einstein finished the full theory of general relativity, he would rectify this error and predict the correct amount of light deflection by the sun.
From Prague, Einstein published a paper about the effects of gravity on light, specifically the gravitational redshift and the gravitational deflection of light. The paper challenged astronomers to detect the deflection during a solar eclipse. German astronomer Erwin Finlay-Freundlich publicized Einstein’s challenge to scientists around the world.
Einstein thought about the nature of the gravitational field in the years 1909–1912, studying its properties by means of simple thought experiments. A notable one is the rotating disk. Einstein imagined an observer making experiments on a rotating turntable. He noted that such an observer would find a different value for the mathematical constant pi than the one predicted by Euclidean geometry. The reason is that the radius of a circle would be measured with an uncontracted ruler, but, according to special relativity, the circumference would seem to be longer because the ruler would be contracted.
Since Einstein believed that the laws of physics were local, described by local fields, he concluded from this that spacetime could be locally curved. This led him to study Riemannian geometry, and to formulate general relativity in this language.
Hole argument and Entwurf theory.
While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.
The Entwurf ("draft") theory was the result of these investigations. As its name suggests, it was a sketch of a theory, with the equations of motion supplemented by additional gauge fixing conditions. Simultaneously less elegant and more difficult than general relativity, Einstein abandoned the theory after realizing that the hole argument was mistaken.
General relativity.
In 1912, Einstein returned to Switzerland to accept a professorship at his "alma mater," the ETH. Once back in Zurich, he immediately visited his old ETH classmate Marcel Grossmann, now a professor of mathematics, who introduced him to Riemannian geometry and, more generally, to differential geometry. On the recommendation of Italian mathematician Tullio Levi-Civita, Einstein began exploring the usefulness of general covariance (essentially the use of tensors) for his gravitational theory. For a while Einstein thought that there were problems with the approach, but he later returned to it and, by late 1915, had published his general theory of relativity in the form in which it is used today. This theory explains gravitation as distortion of the structure of spacetime by matter, affecting the inertial motion of other matter.
During World War I, the work of Central Powers scientists was available only to Central Powers academics, for national security reasons. Some of Einstein’s work did reach the United Kingdom and the United States through the efforts of the Austrian Paul Ehrenfest and physicists in the Netherlands, especially 1902 Nobel Prize-winner Hendrik Lorentz and Willem de Sitter of Leiden University. After the war ended, Einstein maintained his relationship with Leiden University, accepting a contract as an "Extraordinary Professor"; for ten years, from 1920 to 1930, he travelled to Holland regularly to lecture.
In 1917, several astronomers accepted Einstein ’s 1911 challenge from Prague. The Mount Wilson Observatory in California, U.S., published a solar spectroscopic analysis that showed no gravitational redshift. In 1918, the Lick Observatory, also in California, announced that it too had disproved Einstein’s prediction, although its findings were not published.
However, in May 1919, a team led by the British astronomer Arthur Stanley Eddington claimed to have confirmed Einstein’s prediction of gravitational deflection of starlight by the Sun while photographing a solar eclipse with dual expeditions in Sobral, northern Brazil, and Príncipe, a west African island. Nobel laureate Max Born praised general relativity as the "greatest feat of human thinking about nature"; fellow laureate Paul Dirac was quoted saying it was "probably the greatest scientific discovery ever made".
The international media guaranteed Einstein’s global renown.
There have been claims that scrutiny of the specific photographs taken on the Eddington expedition showed the experimental uncertainty to be comparable to the same magnitude as the effect Eddington claimed to have demonstrated, and that a 1962 British expedition concluded that the method was inherently unreliable. The deflection of light during a solar eclipse was confirmed by later, more accurate observations. Some resented the newcomer’s fame, notably among some German physicists, who later started the "Deutsche Physik" (German Physics) movement.
Cosmology.
In 1917, Einstein applied the General theory of relativity to model the structure of the universe as a whole. He wanted the universe to be eternal and unchanging, but this type of universe is not consistent with relativity. To fix this, Einstein modified the general theory by introducing a new notion, the cosmological constant. With a positive cosmological constant, the universe could be an eternal static sphere
Einstein believed a spherical static universe is philosophically preferred, because it would obey Mach’s principle. He had shown that general relativity incorporates Mach’s principle to a certain extent in frame dragging by gravitomagnetic fields, but he knew that Mach’s idea would not work if space goes on forever. In a closed universe, he believed that Mach’s principle would hold.
Mach’s principle has generated much controversy over the years.
Modern quantum theory.
In 1917, at the height of his work on relativity, Einstein published an article in "Physikalische Zeitschrift" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.
This article showed that the statistics of absorption and emission of light would only be consistent with Planck’s distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.
Einstein discovered Louis de Broglie’s work, and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the Hamilton–Jacobi equation of mechanics. This paper would inspire Schrödinger’s work of 1926.
Bose–Einstein statistics.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose’s statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose’s paper to the "Zeitschrift für Physik". Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein’s sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.
Energy momentum pseudotensor.
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether’s theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether’s presecriptions do not make a real tensor for this reason.
Einstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field. This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard.
The use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrödinger and others.
Unified field theory.
Following his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation, which would allow the explanation of electromagnetism. In 1950, he described his "unified field theory" in a "Scientific American" article entitled "On the Generalized Theory of Gravitation." Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful.
In his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death. Mainstream physics, in turn, largely ignored Einstein’s approaches to unification. Einstein’s dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.
Wormholes.
Einstein collaborated with others to produce a model of a wormhole. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches.
If one end of a wormhole was positively charged, the other end would be negatively charged. These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.
Einstein–Cartan theory.
In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.
Einstein–Podolsky–Rosen paradox.
In 1935, Einstein returned to the question of quantum mechanics. He considered how a measurement on one of two entangled particles would affect the other. He noted, along with his collaborators, that by performing different measurements on the distant particle, either of position or momentum, different properties of the entangled partner could be discovered without disturbing it in any way.
He then used a hypothesis of local realism to conclude that the other particle had these properties already determined. The principle he proposed is that if it is possible to determine what the answer to a position or momentum measurement would be, without in any way disturbing the particle, then the particle actually has values of position or momentum.
This principle distilled the essence of Einstein’s objection to quantum mechanics. As a physical principle, it has since been shown to be incompatible with experiments.
Equations of motion.
The theory of general relativity has two fundamental laws – the Einstein equations which describe how space curves, and the geodesic equation which describes how particles move.
Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law. So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself.
This was established by Einstein, Infeld and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.
Einstein’s mistakes.
Einstein himself considered the use of the "fudge factor" lambda in his 1917 paper founding cosmology as a "blunder". The theory of general relativity predicted an expanding or contracting universe, but Einstein wanted a universe which is an unchanging three dimensional sphere, like the surface of a three dimensional ball in four dimensions. He wanted this for philosophical reasons, so as to incorporate Mach’s principle in a reasonable way. He stabilized his solution by introducing a cosmological constant, and when the universe was shown to be expanding, he retracted the constant as a blunder. This is not really much of a blunder – the cosmological constant is necessary within general relativity as it is currently understood, and it is widely believed to have a nonzero value today.
Einstein took the wrong side in a few scientific debates.
In addition to these well known mistakes, it is sometimes claimed that the general line of Einstein’s reasoning in the 1905 relativity paper is flawed, or the photon paper, or one or another of the most famous papers. None of these claims are widely accepted.
Collaboration with other scientists.
In addition to long time collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.
Einstein-de Haas experiment.
Einstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin. In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum. They confirmed that this leads the bar to rotate, because the electron’s angular momentum changes as the magnetization changes. This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.
Schrödinger gas model.
Einstein suggested to Erwin Schrödinger that he might be able to reproduce the statistics of a Bose–Einstein gas by considering a box. Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator. Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it.
This formulation is a form of second quantization, but it predates modern quantum mechanics. Erwin Schrödinger applied this to derive the thermodynamic properties of a semiclassical ideal gas. Schrödinger urged Einstein to add his name as co-author, although Einstein declined the invitation.
Einstein refrigerator.
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This Absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, was awarded to Albert Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, as the most promising of their patents were quickly bought up by the Swedish company Electrolux to protect its refrigeration technology from competition.
Bohr versus Einstein.
In the 1920s, quantum mechanics developed into a more complete theory. Einstein was unhappy with the Copenhagen interpretation of quantum theory developed by Niels Bohr and Werner Heisenberg. In this interpretation, quantum phenomena are inherently probabilistic, with definite states resulting only upon interaction with classical systems. A public debate between Einstein and Bohr followed, lasting on and off for many years (including during the Solvay Conferences). Einstein formulated thought experiments against the Copenhagen interpretation, which were all rebutted by Bohr. In a 1926 letter to Max Born, Einstein wrote: "I, at any rate, am convinced that He [God] does not throw dice."
Einstein was never satisfied by what he perceived to be quantum theory’s intrinsically incomplete description of nature, and in 1935 he further explored the issue in collaboration with Boris Podolsky and Nathan Rosen, noting that the theory seems to require non-local interactions; this is known as the EPR paradox. The EPR experiment has since been performed, with results confirming quantum theory’s predictions. Repercussions of the Einstein–Bohr debate have found their way into philosophical discourse.
Religious views.
The question of scientific determinism gave rise to questions about Einstein’s position on theological determinism, and whether or not he believed in God, or in a god. In 1929, Einstein told Rabbi Herbert S. Goldstein "I believe in Spinoza’s God, who reveals Himself in the lawful harmony of the world, not in a God Who concerns Himself with the fate and the doings of mankind." In a 1954 letter, he wrote, "I do not believe in a personal God and I have never denied this but have expressed it clearly.” In a letter to philosopher Erik Gutkind, Einstein remarked, "The word God is for me nothing more than the expression and product of human weakness, the Bible a collection of honorable, but still purely primitive, legends which are nevertheless pretty childish."
Political views.
Throughout the November Revolution in Germany Einstein signed an appeal for the foundation of a nationwide liberal and democratic party, which was published in the Berliner Tageblatt on 16 November 1918, and became a member of the German Democratic Party.
Einstein flouted the ascendant Nazi movement, tried to be a voice of moderation in the tumultuous formation of the State of Israel and braved anti-communist politics and resistance to the civil rights movement in the United States. He participated in the 1927 congress of the League against Imperialism in Brussels. He was a socialist Zionist who supported the creation of a Jewish national homeland in the British mandate of Palestine.
After World War II, as enmity between the former allies became a serious issue, Einstein wrote, “I do not know how the third World War will be fought, but I can tell you what they will use in the Fourth – rocks!” In a 1949 "Monthly Review" article entitled “Why Socialism?” Albert Einstein described a chaotic capitalist society, a source of evil to be overcome, as the “predatory phase of human development”. With Albert Schweitzer and Bertrand Russell, Einstein lobbied to stop nuclear testing and future bombs. Days before his death, Einstein signed the Russell–Einstein Manifesto, which led to the Pugwash Conferences on Science and World Affairs.
Einstein was a member of several civil rights groups, including the Princeton chapter of the NAACP. When the aged W. E. B. Du Bois was accused of being a Communist spy, Einstein volunteered as a character witness, and the case was dismissed shortly afterward. Einstein’s friendship with activist Paul Robeson, with whom he served as co-chair of the American Crusade to End Lynching, lasted twenty years.
Non-scientific legacy.
While travelling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to The Hebrew University. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Barbara Wolff, of The Hebrew University’s Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.
Einstein bequeathed the royalties from use of his image to The Hebrew University of Jerusalem. Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
In popular culture.
In the period before World War II, Albert Einstein was so well-known in America that he would be stopped on the street by people wanting him to explain "that theory." He finally figured out a way to handle the incessant inquiries. He told his inquirers "Pardon me, sorry! Always I am mistaken for Professor Einstein."
Albert Einstein has been the subject of or inspiration for many novels, films, and plays. Einstein is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. "Time" magazine’s Frederic Golden wrote that Einstein was "a cartoonist’s dream come true."
Einstein’s association with great intelligence and originality has made the name "Einstein" synonymous with genius.
Awards.
In 1922, Einstein was awarded the 1921 Nobel Prize in Physics, "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect". This refers to his 1905 paper on the photoelectric effect, "On a Heuristic Viewpoint Concerning the Production and Transformation of Light", which was well supported by the experimental evidence by that time. The presentation speech began by mentioning "his theory of relativity [which had] been the subject of lively debate in philosophical circles [and] also has astrophysical implications which are being rigorously examined at the present time."
It was long reported that Einstein gave the Nobel prize money directly to his first wife, Mileva Marić, in compliance with their 1919 divorce settlement. However, personal correspondence made public in 2006 shows that he invested much of it in the United States, and saw much of it wiped out in the Great Depression.
Einstein traveled to New York City in the United States for the first time on 2 April 1921. When asked where he got his scientific ideas, Einstein explained that he believed scientific work best proceeds from an examination of physical reality and a search for underlying axioms, with consistent explanations that apply in all instances and avoid contradicting each other. He also recommended theories with visualizable results.
In 1999, Albert Einstein was named Person of the Century by "Time" magazine.
---END.OF.DOCUMENT---

Afghanistan.
The Islamic Republic of Afghanistan is a landlocked country in South-Central Asia. It is variously described as being located within Central Asia, South Asia, Western Asia, or the Middle East. It is bordered by Iran in the west, Pakistan in the south and east, Turkmenistan, Uzbekistan and Tajikistan in the north, and China in the far northeast.
Afghanistan has a long history, and has been an ancient focal point of the Silk Road and migration. It is an important geostrategic location, connecting East and West Asia or the Middle East. The land has been a target of various invaders, as well as a source from which local powers invaded neighboring regions to form their own empires. Ahmad Shah Durrani created the Durrani Empire in 1747, which is considered the beginning of modern Afghanistan. Its capital was shifted in 1776 from Kandahar to Kabul and most of its territories ceded to neighboring empires. In the late 19th century, Afghanistan became a buffer state in "The Great Game" played between the British Empire and Russian Empire. On August 19, 1919, following the third Anglo-Afghan war, the country regained independence from the United Kingdom over its foreign affairs.
Since the late 1970s Afghanistan has experienced a continuous state of civil war punctuated by foreign occupations in the forms of the 1979 Soviet invasion and the October 2001 US-led invasion that overthrew the Taliban government. In December 2001, the United Nations Security Council authorized the creation of an International Security Assistance Force (ISAF) to help maintain security and assist the Karzai administration. The country is being rebuilt slowly with support from the international community and dealing with a strong Taliban insurgency.
Etymology.
The name "Afghānistān," Persian: افغانستان, means "Land of Afghans"," from the word "Afghan".
Origin of the name.
The first part of the name, "Afghan", is an alternative name for the Pashtuns who are the founders and the largest ethnic group of the country. They probably began using the term "Afghan" as a name for themselves since at least the Islamic period and onwards. According to W. K. Frazier Tyler, M. C. Gillet and several other scholars "the word Afghan first appears in history in the Ḥudūd al-ʿĀlam in 982 AD." Al-Biruni referred to Afghans as various tribes living on the western frontier mountains of the Indus River, which would be the Sulaiman Mountains.
The last part of the name, "-stān" is an ancient Iranian languages suffix for "place", prominent in many languages of the region.
The term "Afghanistan", meaning the "Land of Afghans", was mentioned by the 16th century Mughal Emperor Babur in his memoirs, referring to the territories south of Kabul that were inhabited by Pashtuns (called "Afghans" by Babur).
Until the 19th century the name was only used for the traditional lands of the Pashtuns, while the kingdom as a whole was known as the "Kingdom of Kabul", as mentioned by the British statesman and historian Mountstuart Elphinstone. Other parts of the country were at certain periods recognized as independent kingdoms, such as the "Kingdom of Balkh" in the late 18th and early 19th centuries.
With the expansion and centralization of the country, Afghan authorities adopted and extended the name "Afghanistan" to the entire kingdom, after its English translation had already appeared in various treaties between the British Raj and Qajarid Persia, referring to the lands subject to the Pashtun Barakzai Dynasty of Kabul. "Afghanistan" as the name for the entire kingdom was mentioned in 1857 by Friedrich Engels. It became the official name when the country was recognized by the world community in 1919, after regaining full independence over its foreign affairs from the British, and was confirmed as such in the nation's 1923 constitution.
Geography.
Afghanistan is landlocked and mountainous, with plains in the north and southwest. The highest point is Nowshak, at 7,485 m (24,557 ft) above sea level. The climate varies by region and tends to change quite rapidly. Large parts of the country are dry, and fresh water supplies are limited. The endorheic Sistan Basin is one of the driest regions in the world.
Afghanistan has a continental climate with very harsh winters in the central highlands, the glacierized northeast (around Nuristan) and the Wakhan Corridor, where the average temperature in January is below −15°C, and hot summers in the low-lying areas of Sistan Basin of the southwest, the Jalalabad basin of the east, and the Turkistan plains along the Amu River of the north, where temperature averages over 35°C in July. The country is frequently subject to minor earthquakes, mainly in the northeast of Hindu Kush mountain areas. Some 125 villages were damaged and 4000 people killed by the May 31, 1998 earthquake.
At 249,984 sq mi (647,500 km²), Afghanistan is the world's 41st-largest country (after Burma).
Tajikistan, Turkmenistan and Uzbekistan border Afghanistan to the north, Iran to the west, Pakistan to the south and the People's Republic of China to the east.
The country's natural resources include gold, silver, copper, zinc, and iron ore in the Southeast; precious and semi-precious stones (such as lapis, emerald, and azure) in the Northeast; and potentially significant petroleum and natural gas reserves in the North. The country also has uranium, coal, chromite, talc, barites, sulfur, lead, and salt. However, these significant mineral and energy resources remain largely untapped, due to the effects of the Soviet invasion and the subsequent civil war. Plans are under way to begin extracting them in the near future.
History.
Though the modern nation state of Afghanistan was founded or created in 1747 by Ahmad Shah Durrani, the land has an ancient history and various timelines of different civilizations. Excavation of prehistoric sites by Louis Dupree, the University of Pennsylvania, the Smithsonian Institution and others suggest that humans were living in what is now Afghanistan at least 50,000 years ago, and that farming communities of the area were among the earliest in the world.
Afghanistan is a country at a unique nexus point where numerous Indo-European civilizations have interacted and often fought, and was an important site of early historical activity. The region has been home to various people through the ages, among them the Aryan tribes, such as the Pactyans, Arians, Scythians, Bactrians, and etc. It also has been conquered by a host of people, including the Medes, Achaemenids, Alexander the Great, Seleucids, Indo-Greeks, Samanids, Arabs, Turks, Mongols, and others. On the other hand, native entities such as Kushans, Saffarids, Ghaznavids, Ghurids, Timurids, Mughals, Hotakis, Durranis and others have risin to power in what is now Afghanistan and invaded the surrounding regions to form empires of their own.
Pre-Islamic period.
After 2000 BC, waves of Indo-European-speaking Aryans from Central Asia moved south into the area of Afghanistan. These Indo-Iranians later migrated further south to India, west to what is now Iran, and towards Europe via north of the Caspian. They set up a nation which became known as Airyānem Vāejah. During the rule of the Parthian, Sasanian and later, it was called Erānshahr (– "Īrānšahr") meaning "Dominion of the Aryans".
The ancient Zoroastrianism religion is believed to have originated in what is now Afghanistan between 1800 to 800 BC, as Zoroaster lived and died in Balkh. Ancient Eastern Iranian languages, such as Avestan, may have been spoken in the region around the time of the rise of Zoroastrianism. By the middle of the sixth century BC, the Achaemenid Persian Empire overthrew the Medes and incorporated Afghanistan (known as Arachosia Aria, and Bactria to the Greeks) within its boundaries.
Alexander the Great entered and conquered Afghanistan in 330 BCE. Following Alexander's brief occupation, the successor state of the Seleucid Empire controlled the area until 305 BCE, when they gave most of the area to the Hindu Maurya Empire as part of an alliance treaty. During the Mauryan rule, Hinduism and Buddhism was widely practiced in the area. The Mauryans were overthrown in about 185 BCE, leading to the Hellenistic reconquest of Afghanistan by the Greco-Bactrians by 180 BCE. Much of Afghanistan soon broke away from the Greco-Bactrians and became part of the Indo-Greek Kingdom. The Indo-Greeks were defeated by the Indo-Scythians and expelled from most of Afghanistan by the end of the 2nd century BCE.
During the first century, the Parthian Empire subjugated Afghanistan, but lost it to their Indo-Parthian vassals. In the mid to late 1st century AD the vast Kushan Empire, centered in modern Afghanistan, became great patrons of Buddhist culture. The Kushans were defeated by the Sassanids in the third century. Although various rulers calling themselves Kushanshas (generally known as Indo-Sassanids) continued to rule at least parts of the region, they were probably more or less subject to the Sassanids. The late Kushans were followed by the Kidarite Huns who, in turn, were replaced by the short-lived but powerful Hephthalites, as rulers of the region in the first half of the fifth century.
The Hephthalites were defeated by the Sasanian king Khosrau I in AD 557, who re-established Sassanid power in Persia. However, the successors of Kushans and Hepthalites established a small dynasty in Kabulistan called Kushano-Hephthalites or Kabul-Shahan, who were defeated by the Muslim Arab armies in the 7th century and conquered later by the Ghaznavids.
Islamic conquests and Mongol invasion.
In the Middle Ages, upto the 19th century, part of Afghanistan was known as Khorasan. Several important centers of Khorāsān are thus located in modern Afghanistan, such as Balkh, Herat, Ghazni and Kabul. It was during the 7th century to the 9th century that Islam was introduced and spread in the area. Prior to the arrival of Islam, the area was inhabited by people of multi-religions, which included Zoroastrians, Hindus, Buddhists, possibly Jews, shamanists and others.
The region of Afghanistan became the center of various important empires, including that of the Samanids (875–999), Ghaznavids (977–1187), Seljukids (1037–1194), Ghurids (1149–1212), Ilkhanate (1225–1335), and Timurids (1370–1506). Among them, the periods of the Ghaznavids and Timurids are considered as some of the most brilliant eras of the region's history.
Afghanistan was overrun in 1219 by Genghis Khan and his Mongols army, who devastated much of the land. For example, his troops are said to have exterminated or annihilated all living creatures in the ancient Khorāsānian cities of Herat and Balkh. The destruction caused by the Mongols depopulated major cities and caused much of the population to revert to an agrarian rural society. Their rule continued with the Ilkhanate, and was extended further following the invasion of Timur (Tamerlane).
In 1504, Babur, a descendant of both Timur and Genghis Khan, established the Mughal Empire with its capital at Kabul. By the early 1700s, Afghanistan was controlled by several ruling groups: Uzbeks to the north, Safavid Persians to the west and the remaining larger area by the Mughals or self-ruled by local Afghan tribes. Some Urdu-speaking Muhajir and Indian Muslims claim descent from Pashtun soldiers who settled in India and married local Muslim women during the Muslim conquest in the Indian subcontinent. Notably, the Rohilla Pashtuns are known to have settled in parts of northern India.
Hotaki dynasty.
In 1709, Mir Wais Hotak, a local Afghan (Pashtun) from the Ghilzai clan, overthrew and killed Gurgin Khan, the Safavid governor of Kandahar. Mir Wais successfully defeated a Safavid army sent for retaliation and held the region of Kandahar until his death in 1715. He was succeeded by his son Mir Mahmud Hotaki. In 1722, Mir Mahmud led an Afghan army to Isfahan (Iran), sacked the city and proclaimed himself King of Persia. However, the great majority still rejected the Afghan regime as usurping, and after the massacre of thousands of civilians in Isfahan by the Afghans – including more than three thousand religious scholars, nobles, and members of the Safavid family – the Hotaki dynasty was eventually removed from power by a new ruler, Nadir Shah of Persia.
Durrani Empire: beginning of the Afghan state.
In 1738, Nadir Shah and his army, which included Ahmad Khan and four thousand of his Pashtun soldiers of the Abdali tribe, conquered the region of Kandahar from the Hotak Ghilzais; in the same year he occupied Ghazni, Kabul and Lahore. On June 19, 1747, Nadir Shah was assassinated by the Persians and Ahmad Shah Abdali called for a loya jirga ("grand assembly") to select a leader among his people. The Afghans gathered near Kandahar in October 1747 and chose him as their new head of state. Ahmad Shah Durrani is often regarded as the founder of modern Afghanistan. After the inauguration, Ahmad Shah adopted the title "padshah durr-i dawran" ('King, "pearl of the age") and the Abdali tribe became known as the Durrani tribe there after.
By 1751, Ahmad Shah Durrani and his Afghan army conquered the entire present-day Afghanistan, Pakistan, Khorasan and Kohistan provinces of Iran, along with Delhi in India. He defeated the Sikhs of the Maratha Empire in the Punjab region nine times, one of the biggest battles was the 1761 Battle of Panipat. In October 1772, Ahmad Shah retired to his home in Kandahar where he died peacefully and was buried there at a site that is now adjacent to the Mosque of the Cloak of the Prophet Mohammed. He was succeeded by his son, Timur Shah Durrani, who transferred the capital of their Afghan Empire from Kandahar to Kabul. Timur died in 1793 and was finally succeeded by his son Zaman Shah Durrani.
Zaman Shah and his brothers had a weak hold on the legacy left to them by their famous ancestor. They sorted out their differences through a "round robin of expulsions, blindings and executions", which resulted in the deterioration of the Afghan hold over far-flung territories, such as Attock and Kashmir. Durrani's other grandson, Shuja Shah Durrani, fled the wrath of his brother and sought refuge with the Sikhs. Not only had Durrani and his Afghans invaded the Punjab region many times, but have destroyed the holiest shrine of the Sikhs – the Golden Temple in Amritsar, defiling its "sarowar" with the blood of cows and then killing Baba Deep Singh in 1757.
The Sikhs, under Ranjit Singh, rebelled in 1809 and eventually wrest a large part of the Kingdom of Kabul (present day Pakistan, but not including Sindh) from the Afghans. Hari Singh Nalwa, the Commander-in-Chief of the Sikh Empire along its Afghan frontier, invaded the Afghan territory as far as the city of Jalalabad. In 1837, the Afghan Army descended through the Khyber Pass on Sikh forces at Jamrud. Hari Singh Nalwa's forces held off the Afghan offensive for over a week – the time it took reinforcements to reach Jamrud from Lahore.
European influence.
During the nineteenth century, following the Anglo-Afghan wars (fought 1839–42, 1878–80, and lastly in 1919) and the ascension of the Barakzai dynasty, Afghanistan saw much of its territory and autonomy ceded to the United Kingdom. The UK exercised a great deal of influence, and it was not until King Amanullah Khan acceded to the throne in 1919 that Afghanistan re-gained complete independence over its foreign affairs (see "The Great Game").
During the period of British intervention in Afghanistan, ethnic Pashtun territories were divided by the Durand Line. This would lead to strained relations between Afghanistan and British India – and later the new state of Pakistan – over what came to be known as the Pashtunistan debate.
Kingdom of Afghanistan.
King Amanullah Khan moved to end his country's traditional isolation in the years following the Third Anglo-Afghan War. He established diplomatic relations with most major countries and, following a 1927 tour of Europe and Turkey (during which he noted the modernization and secularization advanced by Atatürk), introduced several reforms intended to modernize Afghanistan.
A key force behind these reforms was Mahmud Tarzi, Amanullah's Foreign Minister and father-in-law – and an ardent supporter of the education of women. He fought for Article 68 of Afghanistan's first constitution (declared through a Loya Jirga), which made elementary education compulsory. Some of the reforms that were actually put in place, such as the abolition of the traditional Muslim veil for women and the opening of a number of co-educational schools, quickly alienated many tribal and religious leaders. Faced with overwhelming armed opposition, Amanullah was forced to abdicate in January 1929 after Kabul fell to forces led by Habibullah Kalakani.
Prince Mohammed Nadir Shah, a cousin of Amanullah's, in turn defeated and killed Habibullah Kalakani in October of the same year, and with considerable Pashtun tribal support he was declared King Nadir Shah. He began consolidating power and regenerating the country. He abandoned the reforms of Amanullah Khan in favour of a more gradual approach to modernisation. In 1933, however, he was assassinated in a revenge killing by a Kabul student.
Mohammed Zahir Shah, Nadir Shah's 19-year-old son, succeeded to the throne and reigned from 1933 to 1973. The longest period of stability in Afghanistan was when the country was under the rule of King Zahir Shah. Until 1946 Zahir Shah ruled with the assistance of his uncle, who held the post of Prime Minister and continued the policies of Nadir Shah. In 1946, another of Zahir Shah's uncles, Shah Mahmud Khan, became Prime Minister and began an experiment allowing greater political freedom, but reversed the policy when it went further than he expected. In 1953, he was replaced as Prime Minister by Mohammed Daoud Khan, the king's cousin and brother-in-law. Daoud sought a closer relationship with the Soviet Union and a more distant one towards Pakistan.
During this period Afghanistan remained neutral. It was not a participant in World War II, nor aligned with either power bloc in the Cold War. However, it was a beneficiary of the latter rivalry as both the Soviet Union and the U.S. vied for influence by building such works as hotels and sewer systems. A good two lane road was constructed from Iran. Running through Herat, Kandahar, and Kabul, it ended at the Pakistani border. By the late 1960s large numbers of travelers were using it as part of the hippie trail.
Republic of Afghanistan.
In 1973, Zahir Shah's brother-in-law, Mohammed Daoud Khan, launched a bloodless coup and became the first President of Afghanistan while Zahir Shah was on an official overseas visit. Mohammed Daoud Khan jammed Afghan radio with anti-Pakistani broadcasts and looked to the Soviet Union and the United States for aid for development.
In 1978 a prominent member of the People's Democratic Party of Afghanistan (PDPA), Mir Akbar Khyber (or "Kaibar"), was killed by the government. The leaders of PDPA apparently feared that Daoud was planning to exterminate them all, especially since most of them were arrested by the government shortly after. Hafizullah Amin and a number of military wing officers of the PDPA managed to remain at large and organised an uprising.
The PDPA, led by Nur Mohammad Taraki, Babrak Karmal and Amin overthrew the regime of Mohammad Daoud, who was killed along with his family. The uprising was known as the Khalq, or Great Saur Revolution ('Saur' means 'April' in Pashto). On May 1, 1978, Taraki became President, Prime Minister and General Secretary of the PDPA. The country was then renamed the Democratic Republic of Afghanistan (DRA), and the PDPA regime lasted, in some form or another, until April 1992.
The 1978 Khalq uprising against the government of Daoud Khan was essentially a resurgence by the Ghilzai tribe of the Pashtun against the Durrani (the tribe of Daoud Khan and the previous monarchy).
Once in power, the PDPA moved to permit freedom of religion and carried out an ambitious land reform, waiving farmers' debts countrywide. They also made a number of statements on women's rights and introduced women to political life. A prominent example was Anahita Ratebzad, who was a major Marxist leader and a member of the Revolutionary Council. Ratebzad wrote the famous May 28, 1978 "New Kabul Times" editorial which declared: "Privileges which women, by right, must have are equal education, job security, health services, and free time to rear a healthy generation for building the future of the country... Educating and enlightening women is now the subject of close government attention."
Many people in the cities including Kabul either welcomed or were ambivalent to these policies. However, the secular nature of the government made it unpopular with religiously conservative Afghans in the villages and the countryside, who favoured traditionalist 'Islamic' law.
The U.S. saw the situation as a prime opportunity to weaken the Soviet Union. As part of a Cold War strategy, in 1979 the United States government (under President Jimmy Carter) began to covertly fund forces ranged against the pro-Soviet government, although warned that this might prompt a Soviet intervention, according to President Carter's National Security Advisor, Zbigniew Brzezinski. Brzezinski described the U.S. activities as the successful setting of a trap that drew the Soviet Union into "its Vietnam War" and brought about the breakup of the Soviet empire. Regarding U.S. support for Islamic fundamentalism, Brzezinski said, "What is most important to the history of the world? The Taliban or the collapse of the Soviet empire? Some stirred-up Moslems or the liberation of Central Europe and the end of the cold war?" The Mujahideen belonged to various different factions, but all shared, to varying degrees, a similarly conservative 'Islamic' ideology.
In March 1979 Hafizullah Amin took over as prime minister, retaining the position of field marshal and becoming vice-president of the Supreme Defence Council. Taraki remained President and in control of the Army. On September 14, Amin overthrew Taraki, who died or was killed. Amin's tenure as prime minister lasted only a few months.
Soviet invasion and civil war.
In order to bolster the Parcham faction, the Soviet Union – citing the 1978 Treaty of Friendship, Cooperation and Good Neighborliness that had been signed between the two countries – intervened on December 24, 1979. Over 100,000 Soviet troops took part in the invasion backed by another one hundred thousand and by members of the Parcham faction. Amin was killed and replaced by Babrak Karmal.
In response to the Soviet occupation of Afghanistan and part of its overall Cold War strategy, the United States responded by arming and otherwise supporting the Afghan mujahideen, which had taken up arms against the Soviet occupiers. U.S. support began during the Carter administration, but increased substantially during the Reagan administration, in which it became a centerpiece of the so-called Reagan Doctrine under which the U.S. provided support to anti-communist resistance movements in Afghanistan and also in Angola, Nicaragua, and other nations. The New York Times reported that the Reagan administration delivered several hundred FIM-92 Stinger surface-to-air missiles to Afghan resistance groups, including the Taliban. In addition to U.S. support, the mujahideen received support from Pakistan, Saudi Arabia and other nations.
The Soviet occupation resulted in the killings of between 600,000 and two million Afghan civilians. Over 5million fled as Afghan refugees, mostly to Pakistan and Iran. Over 38,000 made it to the United States and many more to the European Union. Faced with mounting international pressure and great number of casualties on both sides, the Soviets withdrew in 1989.
The Soviet withdrawal from the DRA was seen as an ideological victory in the U.S., which had backed the Mujahideen through three U.S. presidential administrations in order to counter Soviet influence in the vicinity of the oil-rich Persian Gulf.
Following the removal of the Soviet forces, the U.S. and its allies lost interest in Afghanistan and did little to help rebuild the war-ravaged country or influence events there. The USSR continued to support President Mohammad Najibullah (former head of the Afghan secret service, "KHAD") until 1992 when the new Russian government refused to sell oil products to the Najibullah regime.
Because of the fighting, a number of elites and intellectuals fled to take refuge abroad. This led to a leadership imbalance in Afghanistan. Fighting continued among the victorious Mujahideen factions, which gave rise to a state of warlordism. The most serious fighting during this period occurred in 1994, when over 10,000 people were killed in Kabul alone. It was at this time that the Taliban developed as a politico-religious force, eventually seizing Kabul in 1996 and establishing the Islamic Emirate of Afghanistan. By the end of 2000 the Taliban had captured 95% of the country.
During the Taliban's seven-year rule, much of the population experienced restrictions on their freedom and violations of their human rights. Women were banned from jobs, girls forbidden to attend schools or universities. Communists were systematically eradicated and thieves were punished by amputating one of their hands or feet. Opium production was nearly wiped out by the Taliban by 2001.
War in Afghanistan 2001–present.
Following the September 11, 2001 attacks in the United States, the U.S. and British air forces began bombing Afghanistan during Operation Enduring Freedom. On the ground, American and British special forces along with CIA Special Activities Division teams worked with the Tajik-dominated Northern Alliance to begin a military offensive to overthrow the Taliban. These attacks led to the fall of Mazar-i-Sharif and then Kabul in November 2001, as the Taliban retreated from most of northern Afghanistan. The International Security Assistance Force (ISAF) was established by the UN Security Council in December 2001 to secure Kabul and the surrounding areas. In the same month the Karzai administration was also established to run the country.
As more coalition troops entered the war and the Northern Alliance forces fought their way southwards, the Taliban and al-Qaida retreated toward the mountainous Durand Line border region between Afghanistan and Pakistan. From 2002 onward, the Taliban focused on survival and on rebuilding its forces. Meanwhile NATO assumed control of ISAF in 2003. From 2003 onwards, the Taliban increased its attacks using insurgency tactics. Firmly entrenched in the borders between Pakistan and Afghanistan the Taliban enjoyed a resurgence, showing it could launch large, coordinated and effective attacks on coalition and Afghan forces. Over the course of the years, NATO-lead troops lead several offensives against the entrenched Taliban, but proved unable to completely dislodge their presence. By 2009, a Taliban lead shadow government began to form complete with their own verson of mediation court.
On December 1, 2009, U.S. President Barack Obama announced that he would escalate U.S. military involvement by deploying an additional 30,000 soldiers over a period of six months. He also proposed to begin troop withdrawals 18 months from that date. On January 26, 2010, at the International Conference on Afghanistan in London which brought together some 70 countries and organizations, Afghan President Hamid Karzai told world leaders that he intends to reach out to the top echelons of the Taliban within a few weeks with a peace initiative. Karzai set the framework for dialogue with Taliban leaders when he called on the group's leadership to take part in a "loya jirga" -- or large assembly of elders—to initiate peace talks.
Government and politics.
Politics in Afghanistan has historically consisted of power struggles, bloody coups and unstable transfers of power. With the exception of a military junta, the country has been governed by nearly every system of government over the past century, including a monarchy, republic, theocracy and communist state. The constitution ratified by the 2003 Loya jirga restructured the government as an Islamic republic consisting of three branches, executive, legislative and judicial.
The nation is currently led by the Karzai administration with Hamid Karzai as the President and leader since December 20, 2001. The current parliament was elected in 2005. Among the elected officials were former mujahadeen, Taliban members, communists, reformists, and Islamic fundamentalists. 28% of the delegates elected were women, three points more than the 25% minimum guaranteed under the constitution. This made Afghanistan, long known under the Taliban for its oppression of women, 30th amongst nations in terms of female representation. Construction for a new parliament building began on August 29, 2005.
The Supreme Court of Afghanistan is currently led by Chief Justice Abdul Salam Azimi, a former university professor who had been legal advisor to the president. The previous court, appointed during the time of the interim government, had been dominated by fundamentalist religious figures, including Chief Justice Faisal Ahmad Shinwari. The court issued several rulings, such as banning cable television, seeking to ban a candidate in the 2004 presidential election and limiting the rights of women, as well as overstepping its constitutional authority by issuing rulings on subjects not yet brought before the court. The current court is seen as more moderate and led by more technocrats than the previous court.
The 2004 Afghan presidential election went relatively smooth in which Hamid Karzai won in the first round with 55.4% of the votes. However, the 2009 presidential election was characterized by lack of security, low voter turnout and widespread electoral fraud. The vote, along with elections for 420 provincial council seats, took place in August 2009, but remained unresolved during a lengthy period of vote counting and fraud investigation. Two months later, under U.S. and ally pressure, a second round run-off vote between Karzai and remaining challenger Abdullah was announced for November 7, 2009, but on the 1st of November Abdullah announced that he would no longer be participating in the run-off because his demands for changes in the electoral commission had not been met, and claiming a transparent election would not be possible. A day later, officials of the election commission cancelled the run-off and declared Hamid Karzai as President of Afghanistan for another 5 year term.
Corruption is many Afghans’ chief grievance against their leaders, pervading nearly all aspects of daily life. A number of government ministries are believed to be rife with corruption, including Interior, Education and Health. They either tolerate widespread malfeasance or have been powerless to stop it. A January 2010 report published by the United Nations Office on Drugs and Crime revealed that bribery consumes an amount equal to 23 percent of the Gross Domestic Product (GDP) of Afghanistan. Afghans are forced by corrupt government culture to pay more than a third of their income in bribes.
Women in public life in many parts of the country are subject to routine threats and intimidation, according to a December, 2009 report by Human Rights Watch. Several high profile women have been assassinated, but their killers have not been brought to justice. When Sitara Achakzai, an outspoken and courageous human rights defender and politician, was murdered by the Taliban in April 2009, her death was seen as another warning to all women who are active in public life.
In the aftermath of the election, Peter Galbraith – a senior UN official in Kabul who was fired after pushing for the UN to reveal the extent of the preparation for fraud before the first vote – wrote that before the election, Karzai was seen as ineffectual and corrupt, and that now he was ineffectual, corrupt and illegitimate. Later that month, the U.S. ambassador in Kabul sent two classified cables to Washington expressing deep concerns about sending more U.S. troops to Afghanistan until President Hamid Karzai's government demonstrates that it is willing to tackle the corruption and mismanagement that has fueled the Taliban's rise.
In November 2009, Afghanistan slipped three places in Transparency International's annual index of corruption perceptions, becoming the world's second most-corrupt country ahead of Somalia.
In January 2010, President Karzai reinstated Abdul Rashid Dostum to a high ranking army post despite Western demands for sweeping reform. Dostum is among Afghanistan's most notorious warlords, accused of widespread abuses including the massacre of thousands of Taliban prisoners, something he denies.
Police.
Afghanistan currently has more than 90,000 national police officers, with plans to recruit more so that the total number can reach 160,000. They are being trained by and through the Afghanistan Police Program. In many areas, crimes have gone uninvestigated because of insufficient police or lack of equipment. Afghan National Army soldiers have been sent to quell fighting in some regions lacking police protection. Many of the police officers are illiterate due to the 30 years of civil unrest in the country. Approximately 17 percent of them test positive for illegal drugs. They are widely accused of demanding bribes, which is not surprising to see in most developing countries. Every year many Afghan police officers are killed by militants, and in some cases by NATO forces due to friendly fire incidents. See List of Afghan security forces fatality reports in Afghanistan
Attempts to build a credible Afghan police force are faltering badly, according to NATO officials, even as they acknowledge that the force will be a crucial piece of the effort to have Afghans manage their own security so American forces can begin leaving. Taliban infiltration is a constant worry; incompetence an even bigger one. A quarter of the officers quit every year, making the Afghan government's goals of substantially building up the police force even harder to achieve.
Helmand is the most dangerous place in Afghanistan due to its distance from Kabul as well as the drug trade that flourishes there. Other turbulent provinces in Afghanistan include Kandahar and Oruzgan, although security in the latter has improved recently due to Dutch and Afghan counter offensives. The Afghan Border Police are responsible for protecing the nation's borders, especially the Durand Line border which is often used by criminals and terrorists.
Women and girls in Afghanistan suffer high levels of violence and discrimination and have poor access to justice and education, Human Rights Watch concluded in a December, 2009 report. One recent nationwide survey of levels of violence against Afghan women found that 52 percent of respondents experienced physical violence and 17 percent reported sexual violence. Yet because of social and legal obstacles to accessing justice, few women and girls report violence to the authorities. These barriers are particularly formidable in rape cases.
The Afghan government rates 121 out of 160 countries in terms of corruption. In 2009, President Hamid Karzai created two anti-corruption units within the Afghan Interior Ministry at the insistence of the United States, Europe and Iran. Afghan Interior Minister Hanif Atmar told reporters in Kabul on November 16, 2009 that security officials from the U.S. (FBI), Britain (Scotland Yard) and the European Union (ELOPE) will train prosecutors in the unit.
Military.
The Afghan National Army currently has about 100,000 soldiers, with plans to increase this number to 260,000 in the coming years. It is plagued by inefficiency and endemic corruption. U.S. training efforts have been drastically slowed by the corruption, widespread illiteracy, vanishing supplies, and lack of discipline. U.S. trainers report missing vehicles, weapons and other military equipment, and outright theft of fuel provided by the U.S. Death threats have been leveled against U.S. officers who try to stop Afghan soldiers from stealing. Afghan soldiers often find improvised explosive devices and snip the command wires instead of marking them and waiting for U.S. forces to come to detonate them. The Americans say this just allows the insurgents to return and reconnect them. U.S. trainers frequently must remove the cell phones of Afghan soldiers hours before a mission for fear that the operation will be compromised. American trainers often spend large amounts of time verifying that Afghan rosters are accurate – that they are not padded with "ghosts" being "paid" by Afghan commanders who quietly collect the bogus wages.
The Afghan Army has severely limited fighting capacity. Even the best Afghan units lack training, discipline and adequate reinforcements. In one new unit in Baghlan Province, soldiers have been found cowering in ditches rather than fighting. Some are suspected of collaborating with the Taliban against the Americans. "They don’t have the basics, so they lay down," said Capt. Michael Bell, who is one of a team of U.S. and Hungarian mentors tasked with training Afghan soldiers. "I ran around for an hour trying to get them to shoot, getting fired on. I couldn’t get them to shoot their weapons." In addition, 9 out of 10 soldiers in the Afghan National Army cannot read. In multiple firefights during the February, 2010 NATO offensive in Helmand Province, many Afghan soldiers did not aim — they pointed their American-issued M-16 rifles in the rough direction of the incoming small-arms fire and pulled their triggers without putting rifle sights to their eyes. Their rifle muzzles were often elevated several degrees high.
Desertion is a significant problem in the Afghan Army. One in every four combat soldiers quit the Afghan Army during the 12-month period ending in September, 2009, according to data from the U.S. Defense Department and the Inspector General for Reconstruction in Afghanistan.
Provinces.
Afghanistan is administratively divided into thirty-four (34) provinces ("welayats"), and for each province there is a capital. Each province is then divided into many provincial districts, and each district normally covers a city or several townships.
The Governor of the province is appointed by the Ministry of Interior, and the Prefects for the districts of the province will be appointed by the provincial Governor. The Governor is the representative of the central government of Afghanistan, and is responsible for all administrative and formal issues. The provincial Chief of Police is appointed by the Ministry of Interior, who works together with the Governor on law enforcement for all the cities or districts of that province.
There is an exception in the capital city (Kabul) where the Mayor is selected by the President of Afghanistan, and is completely independent from the prefecture of Kabul Province.
Foreign relations.
Since the overthrow of the Taliban regime, Afghanistan's new government has maintained strong relations with the United States and other members of NATO. More than 22 NATO nations deploy thousands of troops in Afghanistan as a part of the International Security Assistance Force (ISAF). Apart from close military links, Afghanistan also enjoys strong economic relations with NATO members and other allies. The United States is the largest donor to Afghanistan, followed by Japan, United Kingdom, Germany and India.
Relations between Afghanistan and neighboring Pakistan often fluctuate. During the Taliban regime, Pakistan had strong influence in Afghanistan due to close links with most Taliban leaders. However, Pakistan's influence has gradually waned since the overthrow of the Taliban. Though Pakistan maintains strong security and economic links with Afghanistan, dispute between the two countries remain due to Pakistani concerns over growing influence of rival India in Afghanistan and the continuing border dispute over the Durand Line. Since 2007, Afghan and Pakistani forces have been involved in a number of border skirmishes. Relations between the two strained further after Afghan officials alleged that Pakistani intelligence agencies were involved in some terrorist attacks on Afghanistan.
Afghanistan has strong historical and cultural links with neighboring Iran as both the countries were a part of Greater Persia. Relations between the two, which had previously soured after the rise of radical Sunni Islamist Taliban regime in Afghanistan, rebounded after the establishment of Hamid Karzai government. Iran has also actively participated in Afghan reconstruction efforts. Afghanistan also enjoys good relations with Russia and neighboring Central Asian nations, especially Uzbekistan, Tajikistan and Turkmenistan.
India is often regarded as one of Afghanistan's most influential allies. India is the largest regional donor to Afghanistan and has extensively participated in several Afghan reconstruction efforts, including power, agricultural and educational projects. Since 2002, India has extended more than US$1.2 billion in aid to Afghanistan. Strong military ties also exist – Afghan security forces regularly get counter-insurgency training in India and India is also considering the deployment of troops in Afghanistan.
Population.
A 2009 UN estimate shows that the Afghan population is 28,150,000, with about 2.7 million Afghan refugees currently staying in neighoboring Pakistan and Iran. A partial census conducted in 1979 showed around 13,051,358 people living in the country. By 2050, the population is estimated to increase to 82 million.
Largest cities.
The only city in Afghanistan with over one million residents is its capital, Kabul. The other major cities in the country are, in order of population size, Herat, Kandahar, Mazar-e Sharif, Jalalabad, Ghazni and Kunduz. Urban areas are experiencing rapid population growth following the establishment of the Islamic Republic in 2002.
Languages.
Other minor languages include Nuristani (Ashkunu, Kamkata-viri, Vasi-vari, Tregami and Kalasha-ala), Pamiri (Shughni, Munji, Ishkashimi and Wakhi), Brahui, Hindko, Kyrgyz, etc.
According to older numbers in the Encyclopædia Iranica, the Persian language is the most widely used language of the country, spoken by most of the population (although ca. 25% native), while Pashto is spoken and understood by around 60% of the population (50–55% native). According to "A survey of the Afghan people – Afghanistan in 2006", Persian is the first language of 49% of the population, while additional 37% speak the language as a second language (combined 86%). Pashto is the first language of 40% of the population, while additional 27% know the language (combined 67%). Uzbek is spoken or understood by 6% of the population, Turkmen by 3%. In the survey "Afghanistan: Where Things Stand" (average numbers from 2005 to 2009), 69% of the interviewed people preferred Persian, while 31% preferred Pashto. Additionally, 45% of the polled people said that they can read Persian, while 36% said that they can read Pashto.
Culture.
Afghans display pride in their religion, country, ancestry, and above all, their independence. Like other highlanders, Afghans are regarded with mingled apprehension and condescension, for their high regard for personal honor, for their clan loyalty and for their readiness to carry and use arms to settle disputes. As clan warfare and internecine feuding has been one of their chief occupations since time immemorial, this individualistic trait has made it difficult for foreign invaders to hold the region.
Afghanistan has a complex history that has survived either in its current cultures or in the form of various languages and monuments. However, many of the country's historic monuments have been damaged in recent wars. The two famous statues of Buddha in Bamyan Province were destroyed by the Taliban, who regarded them as idolatrous. Other famous sites include the cities of Kandahar, Herat, Ghazni and Balkh. The Minaret of Jam, in the Hari River valley, is a UNESCO World Heritage site. A cloak reputedly worn by Muhammad is stored inside the famous Mosque of the Cloak of the Prophet Mohammed in Kandahar City.
Buzkashi is a national sport in Afghanistan. It is similar to polo and played by horsemen in two teams, each trying to grab and hold a goat carcass. Afghan hounds (a type of running dog) also originated in Afghanistan.
Although literacy levels are very low, classic Persian poetry plays a very important role in the Afghan culture. Poetry has always been one of the major educational pillars in Iran and Afghanistan, to the level that it has integrated itself into culture. Persian culture has, and continues to, exert a great influence over Afghan culture. Private poetry competition events known as "musha’era" are quite common even among ordinary people. Almost every homeowner owns one or more poetry collections of some sort, even if they are not read often.
The eastern dialects of the Persian language are popularly known as "Dari". The name itself derives from "Pārsī-e Darbārī", meaning "Persian of the royal courts". The ancient term "Darī" – one of the original names of the Persian language – was revived in the Afghan constitution of 1964, and was intended "to signify that Afghans consider their country the cradle of the language. Hence, the name "Fārsī", the language of Fārs, is strictly avoided."
Many of the famous Persian poets of the tenth to fifteenth centuries stem from Khorasan where is now known as Afghanistan. They were mostly also scholars in many disciplines like languages, natural sciences, medicine, religion and astronomy.
Most of these individuals were of Persian (Tājīk) ethnicity who still form the second-largest ethnic group in Afghanistan. Also, some of the contemporary Persian language poets and writers, who are relatively well-known in Persian-speaking world, include Khalilullah Khalili, Sufi Ghulam Nabi Ashqari, Sarwar Joya, Parwin Pazwak and others. In 2003, Khaled Hosseini published The Kite Runner which though fiction, captured much of the history, politics and culture experienced in Afghanistan from the 1930s to present day.
In addition to poets and authors, numerous Persian scientists were born or worked in the region of present-day Afghanistan. Most notable was Avicenna (Abu Alī Hussein ibn Sīnā) whose father hailed from Balkh. Ibn Sīnā, who travelled to Isfahan later in life to establish a medical school there, is known by some scholars as "the father of modern medicine". George Sarton called ibn Sīnā "the most famous scientist of Islam and one of the most famous of all races, places, and times." His most famous works are "The Book of Healing" and "The Canon of Medicine", also known as the Qanun. Ibn Sīnā's story even found way to the contemporary English literature through Noah Gordon's "The Physician", now published in many languages. Moreover, according to Ibn al-Nadim, Al-Farabi, a well-known philosopher and scientist, was from Faryab Province in Afghanistan.
Before the Taliban gained power, the city of Kabul was home to many musicians who were masters of both traditional and modern Afghan music, especially during the Nauroz-celebration. Kabul in the middle part of the twentieth century has been likened to Vienna during the eighteenth and nineteenth centuries.
There are an estimated 60 major Pashtun tribes. The tribal system, which orders the life of most people outside metropolitan areas, is potent in political terms. Men feel a fierce loyalty to their own tribe, such that, if called upon, they would assemble in arms under the tribal chiefs and local clan leaders. In theory, under Islamic law, every believer has an obligation to bear arms at the ruler's call.
Heathcote considers the tribal system to be the best way of organizing large groups of people in a country that is geographically
difficult, and in a society that, from a materialistic point of view, has an uncomplicated lifestyle.
The population of nomads in Afghanistan is estimated at about 2-3 million. Nomads contribute importantly to the national economy in terms of meat, skins and wool.
Religions.
Religiously, Afghans are over 99% Muslims: approximately 80% Sunni, 19% Shi'a, and 1% other. Until the 1890s, the region around Nuristan was known as Kafiristan (land of the kafirs) because of its inhabitants: the Nuristani, an ethnically distinctive people who practiced animism, polytheism and shamanism.
Up until the mid-1980s, there were possibly about 50,000 Hindus and Sikhs living in different cities, mostly in Kabul, Kandahar, Jalalabad, and Ghazni.
There was also a small Jewish community in Afghanistan who immigrated to Israel and the United States by the end of the last century, and only one individual, Zablon Simintov, remains today.
Economy.
Afghanistan is a member of the South Asian Association for Regional Cooperation (SAARC), Economic Cooperation Organization (ECO) and the Organization of the Islamic Conference (OIC). It is an impoverished country, one of the world's poorest and least developed. In 2010, 40% of Afghans live below the poverty ine. Two-thirds of the population lives on fewer than 2 US dollars a day. Its economy has suffered greatly from the 1979 Soviet invasion and subsequent conflicts, while severe drought added to the nation's difficulties in 1998–2001. According to the World Bank, "economic growth has been strong and has generated better livelihoods" since 2001.
The economically active population in 2002 was about 11 million (out of a total of an estimated 29 million). As of 2005, the official unemployment rate is at 40%. The number of non-skilled young people is estimated at 3 million, which is likely to increase by some 300,000 per annum.
The nation's economy began to improve since 2002 due to the infusion of multi-billion US dollars in international assistance and investments, as well as remittances from expats. It is also due to dramatic improvements in agricultural production and the end of a four-year drought in most of the country.
The real value of "non-drug" GDP increased by 29% in 2002, 16% in 2003, 8% in 2004 and 14% in 2005. As much as one-third of Afghanistan's GDP comes from growing poppy and illicit drugs including opium and its two derivatives, morphine and heroin, as well as hashish production. Opium production in Afghanistan has soared to a new record in 2007, with an increase on last year of more than a third, the United Nations has said. Some 3.3 million Afghans are now involved in producing opium. In a recent article in the Washington Quarterly, Peter van Ham and Jorrit Kamminga argue that the international community should establish a pilot project and investigate a licensing scheme to start the production of medicines such as morphine and codeine from poppy crops to help it escape the economic dependence on opium.
According to a 2004 report by the Asian Development Bank, the present reconstruction effort is two-pronged: first it focuses on rebuilding critical physical infrastructure, and second, on building modern public sector institutions from the remnants of Soviet style planning to ones that promote market-led development. In 2006, two U.S. companies, Black & Veatch and the Louis Berger Group, have won a US 1.4 billion dollar contract to rebuild roads, power lines and water supply systems of Afghanistan.
One of the main drivers for the current economic recovery is the return of over 5 million Afghan refugees from neighbouring countries, who brought with them fresh energy, entrepreneurship and wealth-creating skills as well as much needed funds to start up businesses. What is also helping is the estimated US 2–3 billion dollars in international assistance every year, the partial recovery of the agricultural sector, and the reestablishment of market institutions. Private developments are also beginning to get underway. In 2006, a Dubai-based Afghan family opened a $25 million Coca Cola bottling plant in Afghanistan.
While the country's current account deficit is largely financed with the donor money, only a small portion – about 15% – is provided directly to the government budget. The rest is provided to non-budgetary expenditure and donor-designated projects through the United Nations system and non-governmental organizations. The government had a central budget of only $350 million in 2003 and an estimated $550 million in 2004. The country's foreign exchange reserves totals about $500 million. Revenue is mostly generated through customs, as income and corporate tax bases are negligible.
Inflation had been a major problem until 2002. However, the depreciation of the Afghani in 2002 after the introduction of the new notes (which replaced 1,000 old Afghani by one new Afghani) coupled with the relative stability compared to previous periods has helped prices to stabilize and even decrease between December 2002 and February 2003, reflecting the turnaround appreciation of the new Afghani currency. Since then, the index has indicated stability, with a moderate increase toward late 2003.
The Afghan government and international donors seem to remain committed to improving access to basic necessities, infrastructure development, education, housing and economic reform. The central government is also focusing on improved revenue collection and public sector expenditure discipline. The rebuilding of the financial sector seems to have been so far successful. Money can now be transferred in and out of the country via official banking channels. Since 2003, over sixteen new banks have opened in the country, including Afghanistan International Bank, Kabul Bank, Azizi Bank, Standard Chartered Bank, First Micro Finance Bank, and others. A new law on private investment provides three to seven-year tax holidays to eligible companies and a four-year exemption from exports tariffs and duties.
Some private investment projects, backed with national support, are also beginning to pick up steam in Afghanistan. An initial concept design called the City of Light Development, envisioned by Dr. Hisham N. Ashkouri, Principal of ARCADD, Inc. for the development and the implementation of a privately based investment enterprise has been proposed for multi-function commercial, historic and cultural development within the limits of the Old City of Kabul along the Southern side of the Kabul River and along Jade Meywand Avenue, revitalizing some of the most commercial and historic districts in the City of Kabul, which contains numerous historic mosques and shrines as well as viable commercial activities among war damaged buildings. Also incorporated in the design is a new complex for the Afghan National Museum.
According to the U.S. Geological Survey and the Afghan Ministry of Mines and Industry, Afghanistan may be possessing up to of natural gas, of petroleum and up to of natural gas liquids. This could mark the turning point in Afghanistan's reconstruction efforts. Energy exports could generate the revenue that Afghan officials need to modernize the country's infrastructure and expand economic opportunities for the beleaguered and fractious population. Other reports show that the country has huge amounts of gold, copper, coal, iron ore and other minerals. The government of Afghanistan is in the process of extracting and exporting its copper reserves, which will be earning $1.2 billion US dollars in royalties and taxes every year for the next 30 years. It will also provide permanent labor to 3,000 of its citizens.
Transport.
Ariana Afghan Airlines is the national airlines carrier, with domestic flights between Kabul, Kandahar, Herat and Mazar-e Sharif. International flights include to Dubai, Frankfurt, Istanbul and a number of other destinations. There are also limited domestic and international flight services available from Kam Air, Pamir Airways and Safi Airways.
The country has limited rail service with Turkmenistan. There are two railway projects currently in progress, one is between Herat and the Iranian city Mashad while another is between Kandahar and Quetta in Pakistan. Most people who travel from one city to another use bus services. Automobiles have recently become more widely available, with Toyota, Nissan and Hyundai dealerships in Kabul. A large number of second-hand vehicles are also arriving from the UAE. Nearly all highways and roads are being rebuilt in the country.
Media and communications.
The media was tightly controlled under the Taliban and other periods in its history, and was relatively free in others. Under the Taliban, television was shut down in 1996, and print media were forbidden to publish commentary, photos or readers letters. The only radio station broadcast religious programmes and propaganda, and aired no music.
After the overthrow of the Taliban in 2001, press restrictions were gradually relaxed and private media diversified. Freedom of expression and the press is promoted in the 2004 constitution and censorship is banned, though defaming individuals or producing material contrary to the principles of Islam is prohibited. In 2008, Reporters Without Borders listed the media environment as 156 out of 173, with 1st being most free. 400 publications are now registered and 60 radio stations, a major source of information, currently exist. Foreign radio stations, such as the BBC World Service, also broadcast into the country.
Television.
Telecommunication services in the country are provided by Afghan Wireless, Etisalat, Roshan, Areeba and Afghan Telecom. In 2006, the Afghan Ministry of Communications signed a US$64.5 million agreement with ZTE Corporation for the establishment of a countrywide fibre optic cable network. This will improve telephone, internet, television and radio broadcast services throughout the country. Around 500,000 (1.5% of the population) had internet access by the end of 2008.
Television and radio broadcastings are available in most parts of the country, with local and international channels or stations.
The nation's post service is also operating. Package delivery services such as FedEx, DHL and others are also available.
Education.
As of 2006 more than four million male and female students were enrolled in schools throughout the country. However, there are still significant obstacles to education in Afghanistan, stemming from lack of funding, unsafe school buildings and cultural norms. A lack of women teachers is an issue that concerns some Afghan parents, especially in more conservative areas. Some parents will not allow their daughters to be taught by men.
UNICEF estimates that more than 80 percent of females and around 50 percent of males lack access to education centers. According to the United Nations, 700 schools have been closed in the country because of poor security.
Literacy of the entire population is estimated at 34%. Female literacy is 10%.
Another aspect of education that is rapidly changing in Afghanistan is the face of higher education. Following the fall of the Taliban, Kabul University was reopened to both male and female students. In 2006, the American University of Afghanistan also opened its doors, with the aim of providing a world-class, English-language, co-educational learning environment in Afghanistan. The university accepts students from Afghanistan and the neighboring countries. Construction work will soon start at the new site selected for University of Balkh in Mazari Sharif. The new building for the university, including the building for the Engineering Department, would be constructed at 600 acres (2.4 km²) of land at the cost of 250 million US dollars.
Since the 1930s there have been two French lycées (secondary schools) (AEFE contracted school) in Kabul, the "Lycée Esteqlal" and "Lycée Malalaï".
A new military school has been set up to properly train and educate Afghan soldiers.
Health.
Every half hour, an average of one Afghan woman dies from pregnancy-related complications, another dies of tuberculosis and 14 children die, largely from preventable causes. Eight years after the fall of the Taliban, the humanitarian and development needs in Afghanistan remain acute.
According to a November, 2009 UNICEF report, Afghanistan is now the most dangerous place in the world for a child to be born. Afghanistan has the highest infant mortality rate in the world – 257 deaths per 1,000 live births – and 70 percent of the population lacks access to clean water.
The Afghan government has ambitious plans to cut the infant mortality rate to 400 from 1,600 for every 100,000 live births by 2020.
Before the start of the Afghan wars in 1978, Afghanistan had an improving health care system and a semi-modernized health care system in cities like Kabul. Ibn Sina Hospital in Kabul and Ali Abad Hospital in Kabul were two of the leading health institutions in Central Asia at the time. Following the Soviet invasion and the civil war that followed, the health care system was limited only to urban areas and was eventually destroyed. The Taliban made some improvements, but health care was not available for women during their six year rule. Following the establishment of the Islamic Republic in 2002, the health system began to improve dramatically in Afghanistan due to international aid and all institutions accepted women for the first time since 1996. Non-governmental charities such as Mahboba's promise assist orphans in association with governmental structures. According to Reuters, "Afghanistan's healthcare system is widely believed to be one of the country's success stories since reconstruction began."
An estimated 80,000 Afghans have lost limbs, mainly as a result of landmines. After years of war in Afghanistan, there are an estimated one million handicapped people. This is one of the highest percentages anywhere in the world.
According to the Human Development Index Afghanistan is the second least developed country in the world.
---END.OF.DOCUMENT---

Albania.
Albania (, Gheg Albanian: "Shqipnia" or "Shqypnia"), officially the Republic of Albania (,), is a country in South Eastern Europe. It is bordered by Montenegro to the northwest, Kosovo to the northeast, the Republic of Macedonia to the east and Greece to the south and southeast. It has a coast on the Adriatic Sea to the west, and on the Ionian Sea to the southwest. It is less than from Italy, across the Strait of Otranto which links the Adriatic Sea to the Ionian Sea.
Albania is a member of the United Nations, NATO, the Organization for Security and Co-operation in Europe, Council of Europe, World Trade Organisation, Organisation of the Islamic Conference and one of the founding members of the Union for the Mediterranean. Albania has been a potential candidate for accession to the European Union since January 2003, and it formally applied for EU membership on 28 April 2009.
Albania is a parliamentary democracy and a transition economy. The Albanian capital, Tirana, is home to approximately 727,000 of the country's 3.6 million people, and it is also the financial capital of the country. Free-market reforms have opened the country to foreign investment, especially in the development of energy and transportation infrastructure.
Etymology.
"Albania" is the Medieval Latin name of the country which is called "Shqipëri" by its inhabitants. In Medieval Greek, the country's name is "Albania" () besides variants "Albanitia", "Arbanitia".
The name may be derived from the Illyrian tribe of the Albani recorded by Ptolemy, the geographer and astronomer from Alexandria who drafted a map in 150 AD that shows the city of Albanopolis (located northeast of Durrës).
The name may have a continuation in the name of a medieval settlement called Albanon and Arbanon, although it is not certain this was the same place. In his "History" written in 1079–1080, Byzantine historian Michael Attaliates was the first to refer to "Albanoi" as having taken part in a revolt against Constantinople in 1043 and to the "Arbanitai" as subjects of the Duke of Dyrrachium. During the Middle Ages, the Albanians called their country "Arbër" or "Arbën" and referred to themselves as Arbëresh or "Arbnesh".
As early as the 16th century, a new name for their home evolved among Albanian people: "Shqipëria", popularly interpreted as "Land of the Eagles" or "Land of the Mountain Eagle" hence the two-headed bird on the national flag, though most likely the origin lies in Skanderbeg's use of the Byzantine double-headed eagle on his seals.
Under the Ottoman Empire Albania was referred to officially as "Arnavutluk" and its inhabitants as arnaut.
Antiquity.
The first recorded inhabitants in the territory of Albania were the Illyrians, an Indo-European people that inhabited the area corresponding to northern and central Albania. The Illyrian tribes that resided in the region of modern Albania were the Taulantii the Parthini, the Abri, the Caviii, the Enchelei, and several others. In the westernmost parts of the territory of Albania there lived the Bryges, a Phrygian people, and in the south were the Greek Chaonians.
Beginning in the 8th century BC, Greek colonies were established on the Illyrian coast. The most important were Apollonia, Avlona (modern-day Vlorë), Epidamnos (modern-day Durrës), and Lissus (modern-day Lezhë). The rediscovered Greek city of Buthrotum (modern-day Butrint), a UNESCO World Heritage Site, is probably more significant today than it was when Julius Caesar used it as a provisions depot for his troops during his campaigns in the 1st century BC. At that time, it was considered an unimportant outpost, overshadowed by Apollonia and Epidamnos.
In the 4th century BC, the Illyrian king Bardyllis united several Illyrian tribes and engaged in conflict with Macedon to the southeast, but was defeated. Bardyllis was succeeded by Grabos, then by Bardyllis II, and then by Cleitus the Illyrian, who was defeated by Alexander the Great. Later on, in 229 BC, Queen Teuta of the Ardiaei clashed with the Romans and initiated the Illyrian Wars, which resulted in defeat and in the end of Illyrian independence by 168 B.C., when King Gentius was defeated by a Roman army.
The lands comprising modern-day Albania were incorporated into the Roman empire as part of the province of Illyricum above the river Drin, and Roman Macedonia (specifically as Epirus Nova) below it. The western part of the Via Egnatia ran inside modern Albania, ending at Dyrrachium. Illyricum was later divided into the provinces of Dalmatia and Pannonia.
When the Roman Empire was divided into East and West in 395, the territories of modern Albania became part of the Byzantine Empire. Beginning in the first decades of Byzantine rule (until 461), the region suffered devastating raids by Visigoths, Huns, and Ostrogoths. In the 6th and 7th centuries, the region was overrun by the Slavs.
The territory of Albania would remain under Byzantine and Bulgarian rule until the 14th century, when the Ottoman Turks began to make incursions into the Empire. The Ottomans captured Constantinople in 1453, and by 1460 most former Byzantine territories were in the hands of the Turks.
Byzantine era.
The new administrative system of the themes, or military provinces created by the Byzantine Empire, contributed to the eventual rise of feudalism in Albania, as peasant soldiers who served military lords became serfs on their landed estates. Among the leading families of the Albanian feudal nobility were the Thopia, Shpata, Muzaka, Dukagjini and Kastrioti. The first three of these rose to become rulers of principalities were vassals of Byzantium, and Albania mostly neglected by their Greek masters at Constantinople. Many Albanians converted to the Roman Catholic Church at this time. During the Byzantine Era the powerful Serbs had occupied almost all of Northern Albania and Kosovo, and the Venetians had gained control and colonized the coastal regions of Albania.
Ottoman era.
In the Middle Ages, the name "Arberia" (see "Origin and history of the name Albania") began to be increasingly applied to the region now comprising the nation of Albania.
Beginning in the late-14th century, the Ottoman Turks expanded their empire from Anatolia to the Balkans (Rumelia). By the 15th century, the Ottomans ruled all of the Balkan Peninsula. Many Albanians had been recruited into the Janissary, including the feudal heir Gjergj Kastrioti who was renamed Skanderbeg (Iskandar Bey) by his Turkish trainers at Edrine. After some Ottoman defeats at the hands of the Serbs, Skanderbeg deserted and began a rebellion against the Ottoman Empire.
After deserting, Gjergj Kastrioti Skanderbeg re-converted to Roman Catholicism and declared a holy war against the Ottoman Empire, which he led from 1443 to 1468. Under a red flag bearing Skanderbeg's heraldic emblem, an Albanian force of about 30,000 men at Krujë held off Ottoman campaigns against their lands for twenty-four years. Thrice the Albanians overcame sieges of Krujë (see Siege of Krujë) led by many Ottoman commanders, including the influential Iljaz Hoxha and his Albanian Janissary led by Hamza Kastrioti. However, Skanderbeg was unable to receive any of the help which had been promised him by the popes. He later abandoned Christianity and died in 1468, leaving no worthy successor. After his death the rebellion continued, but without its former success. The loyalties and alliances created and nurtured by Skanderbeg faltered and fell apart, and the Ottomans reconquered the territory of Albania in 1478. Shortly after the fall of Kruje's castle, some Albanians fled to neighboring Italy, giving rise to the modern Arbëreshë communities.
After the defeat of Skanderbeg, Albania completely transformed under Ottoman rule, and its culture and society closely resembled that of neighboring Bosnia. The Ottomans had urbanized the landscape creating new cities, Bazaars, garrisons and Mosques throughout the Albanian regions. The majority of the remaining Albanian population converted to Islam, with many joining the Sufi Order of the Bektashi. Converting from Christianity to Islam brought considerable advantages, including access to Ottoman trade networks, bureaucratic positions and the army. As a result many Albanians came to serve in the elite Janissary and the administrative Devşirme system. Among these were important historical figures, including Iljaz Hoxha, Hamza Kastrioti, Köprülü Mehmed Pasha (head of the Köprülü family of Grand Viziers), the Bushati family, Sulejman Pasha, Ethem Pasha, Nezim Frakulla, Ali Pasha of Tepelena, Hasan Zyko Kamberi, Ali-paša Šabanagić, and Mehmet Ali ruler of Egypt. and Emin Pasha.
Many Albanians gained prominent positions in the Ottoman government, Albanians highly active during the Ottoman Era and leaders such as Ali Pasha of Tepelena is known to have aided the Bosnian Hero Husein Gradaščević on various occasions, no fewer than 42 Grand Viziers of the Empire were Albanian in origin, including Mehmet Akif Ersoy (1873–1936) an Albanian from Peć who composed the Turkish National Anthem in 1921, "İstiklâl Marşı" (The Independence March). As Hupchik states, "Albanians had little cause of unrest" and "if anything, grew important in Ottoman internal affairs", and sometimes persecuted Christians harshly on behalf of their Turkish allies.
Albania became pivotal for the Ottomans in the Balkans, although Albanians never rested, always having small rebellions wchich were put down by the Ottomans. As a cosequene of the continuous rebellions, the Albanians got the nickname "Arnauts" by the Ottomans, which meant "stubborn". Anyway, this period saw the rising of semi-autonomous Albanian ruled Pashaliks, and Albanians were also an important part of the Ottoman army and Ottoman administration like the case of Köprülü family. Albania would remain a part of the Ottoman Empire as the provinces of Shkodra, Manastir and Yanya until 1912.
1913 to 1928.
After five hundred years of Ottoman domination, an independent Albania was proclaimed on November 28, 1912.
The initial sparks of the first Balkan War in 1912 were ignited by the Albanian uprising between 1908 and 1910 which were directed at opposing the Young Turk policies of consolidation of the Ottoman Empire. Following the eventual weakening of the Ottoman Empire in the Balkans, Serbia, Greece and Bulgaria declared war and sought to aggrandize their respective boundaries on the remaining territories of the Empire. Albania was thus invaded by Serbia in the north and Greece in the south, restricting the country to only a patch of land around the southern coastal city of Vlora. In 1912 Albania, still under foreign occupation declared its independence and with the aid of Austria-Hungary, the Great Powers drew its present borders leaving more than half of the Albanian population outside the new country.
The border between Albania and its neighbours was delineated in 1913 following the dissolution of most of the Ottoman Empire's territories in the Balkans. The delineation of the new state's borders left a significant number of Albanian communities outside Albania. This population was largely divided between Montenegro and Serbia (which then included what is now the Republic of Macedonia). A substantial number of Albanians thus found themselves under Serbian rule. At the same time, an uprising in the country's south by local Greeks, led to the formation of an autonomous region inside its borders (1914). After a period of political instability caused during World War I, the country adopted a republican form of government in 1920.
1928 to 1946.
Starting in 1928, but especially during the Great Depression, the government of King Zog, which brought law and order to the country, began to cede Albania's sovereignty to Italy.
Despite some strong resistance, especially at Durrës, Italy invaded Albania on 7 April 1939 and took control of the country, with the Italian Fascist dictator Benito Mussolini proclaiming Italy's figurehead King Victor Emmanuel III of Italy as King of Albania. The nation thus became one of the first to be occupied by the Axis Powers in World War II. As Hitler began his aggressions, Mussolini decided to occupy Albania as a means to compete with Hitler's territorial gains. Mussolini and the Italian Fascists saw Albania as a historical part of the Roman Empire, and the occupation was intended to fulfill Mussolini's dream of creating an Italian Empire. During the Italian occupation, Albania's population was subject to a policy of forced Italianisation by the kingdom's Italian governors, in which the use of the Albanian language was discouraged in schools while the Italian language was promoted. At the same time, the colonization of Albania by Italians was encouraged.
Mussolini, in October 1940, used his Albanian base to launch an attack on Greece, which led to the defeat of the Italian forces and the Greek occupation of Southern Albania in what was seen by the Greeks as the liberation of Northern Epirus. While preparing for the Invasion of Russia, Hitler decided to attack Greece in December 1940 to prevent a British attack on his southern flank.
During World War II, the Party of Labour was created on 8 November 1941. With the intention of organizing a partisan resistance, they called a general conference in Pezë on 16 September 1942 where the Albanian National Liberation Front was set up. The Front included nationalist groups, but it was dominated by communist partisans.
In December 1942, more Albanian nationalist groups were organized under Visar Kola. Albanians fought against the Italians while, during Nazi German occupation, Balli Kombëtar allied itself with the Germans and clashed with Albanian communists, which continued their fight against Germans and Balli Kombëtar at the same time.
With the collapse of the Mussolini government in line with the Allied invasion of Italy, Germany occupied Albania in September 1943, dropping paratroopers into Tirana before the Albanian guerrillas could take the capital. The German Army soon drove the guerrillas into the hills and to the south. The Nazi German government subsequently announced it would recognize the independence of a neutral Albania and set about organizing a new government, police and armed forces. Many Balli Kombëtar units cooperated with the Germans against the communists and several Balli Kombëtar leaders held positions in the German-sponsored regime.
The partisans entirely liberated Albania from German occupation on 28 November 1944. The Albanian partisans also liberated Kosovo, part of Montenegro, and southern Bosnia and Herzegovina. By November 1944, they had thrown out the Germans, one of the few East European nations to do so without any assistance from Soviet troops. Enver Hoxha became the leader of the country by virtue of his position as Secretary General of the Albanian Communist Party.
Albania was one of the European countries occupied by the Axis powers that ended World War II with a larger Jewish population than before the war. Some 1,200 Jewish residents and refugees from other Balkan countries were hidden by Albanian families during World War II, according to official records.
Communist state.
Albania became an ally of the Soviet Union, but this came to an end in 1960 over the advent of de-Stalinization. A strong political alliance with China followed, leading to several billion dollars in aid, which was curtailed after 1974. China cut off aid in 1978 when Albania attacked its policies after the death of the Chinese ruler Mao Zedong. Large-scale purges of officials occurred during the 1970s.
Enver Hoxha, a dictator who ruled Albania for four decades with an iron fist, died on 11 April 1985. Eventually the new regime introduced some liberalization, and granting the freedom to travel abroad in 1990. The new government made efforts to improve ties with the outside world. The elections of March 1991 left the former Communists in power, but a general strike and urban opposition led to the formation of a coalition cabinet that included non-Communists.
Recent history – 1992 to present.
Albania's former Communists were routed in elections March 1992, causing economic collapse and social unrest. The blood feud has returned in rural areas after more than 40 years of being abolished by Albanian communists, with nearly 10,000 Albanians being killed due to blood feuds since 1991. Sali Berisha was elected as the first non-Communist president since World War II. The next crisis occurred in 1997, during his presidency, as riots ravaged the country. The state institutions collapsed and an EU military mission led by Italy was sent to stabilize the country. In summer 1997, Berisha was defeated in elections, winning just 25 seats out of a total of 156.
His return to power in the elections of 3 July 2005 ended eight years of Socialist Party rule. In 2009, Albania – along with Croatia – joined NATO.
Government, politics and armed forces.
The Albanian republic is a parliamentary democracy established under a constitution renewed in 1998. Elections are now held every four years to a unicameral 140-seat chamber, the People's Assembly. In June 2002, a compromise candidate, Alfred Moisiu, former Army General, was elected to succeed President Rexhep Meidani. Parliamentary elections in July 2005 brought Sali Berisha, as leader of the Democratic Party, back to power. The Euro-Atlantic integration of Albania has been the ultimate goal of the post-communist governments. Albania's EU membership bid has been set as a priority by the European Commission.
Albania, along with Croatia, joined NATO on 1 April 2009 becoming the 27th and 28th members of the alliance.
The workforce of Albania has continued to migrate to Greece, Italy, Germany, other parts of Europe, and North America. However, the migration flux is slowly decreasing, as more and more opportunities are emerging in Albania itself as its economy steadily develops.
Executive branch.
The head of state in Albania is the President of the Republic. The President is elected to a 5-year term by the Assembly of the Republic of Albania by secret ballot, requiring a 50%+1 majority of the votes of all deputies. The next election will run in 2012. The current President of the Republic is Bamir Topi.
The President has the power to guarantee observation of the constitution and all laws, act as commander in chief of the armed forces, exercise the duties of the Assembly of the Republic of Albania when the Assembly is not in session, and appoint the Chairman of the Council of Ministers (prime minister).
Executive power rests with the Council of Ministers (cabinet). The Chairman of the Council (prime minister) is appointed by the president; ministers are nominated by the president on the basis of the prime minister's recommendation. The People's Assembly must give final approval of the composition of the Council. The Council is responsible for carrying out both foreign and domestic policies. It directs and controls the activities of the ministries and other state organs.
Legislative branch.
The Assembly of the Republic of Albania ("Kuvendi i Republikës së Shqipërisë") is the lawmaking body in Albania. There are 140 deputies in the Assembly, which are elected through a party-list proportional representation system. The President of the Assembly (or Speaker) has two deputies and chairs the Assembly. There are 15 permanent commissions, or committees. Parliamentary elections are held at least every four years.
The Assembly has the power to decide the direction of domestic and foreign policy; approve or amend the constitution; declare war on another state; ratify or annul international treaties; elect the President of the Republic, the Supreme Court and the Attorney General and his or her deputies; and control the activity of state radio and television, state news agency and other official information media.
Armed forces.
The Albanian Armed Forces ("Forcat e Armatosura të Shqipërisë") first formed after independence in 1912. Albania reduced the number of active troops from a 1988 number of 65,000 to a 2009 number of 14,500 with a small fleet of aircraft and sea vessels. In the 1990s, the country scrapped enormous amount of obsolete hardware, such as tanks and SAM systems from China.
Today, it consists of the General Staff Headquarters, the Albanian Joint Forces Command, the Albanian Support Command and the Albanian Training and Doctrine Command. Increasing the military budget was one of the most important conditions for NATO integration. Military spending accounted for about 2.7% of GDP in 2008. Since February 2008, Albania participates officially in NATO's Operation Active Endeavor in the Mediterranean Sea. and received a NATO membership invitation on 3 April 2008. Albania became a full member of NATO on 1 April, 2009.
Geography.
Albania has a total area of 28,748 square kilometers. Its coastline is 362 kilometers long and extends along the Adriatic and Ionian Seas. The lowlands of the west face the Adriatic Sea. The 70% of the country that is mountainous is rugged and often inaccessible from the outside. The highest mountain is Korab situated in the district of Dibër, reaching up to. The climate on the coast is typically Mediterranean with mild, wet winters and warm, sunny, and rather dry summers. Inland conditions vary depending on altitude but the higher areas above 1,500 m/5,000 ft are rather cold and frequently snowy in winter; here cold conditions with lying snow may linger into spring. Besides the capital city of Tirana, which has 800,000 inhabitants, the principal cities are Durrës, Korçë, Elbasan, Shkodër, Gjirokastër, Vlorë and Kukës. In Albanian grammar, a word can have indefinite and definite forms, and this also applies to city names: both "Tiranë" and "Tirana", "Shkodër" and "Shkodra" are used.
The three largest and deepest tectonic lakes of the Balkan Peninsula are partly located in Albania. Lake Shkodër in the country's northwest has a surface which can vary between and 530 km2, out of which one third belongs to Albania and rest to Montenegro. The Albanian shoreline of the lake is. Ohrid Lake is situated in the country's southeast and is shared between Albania and Republic of Macedonia. It has a maximal depth of 289 meters and a variety of unique flora and fauna can be found there, including "living fossils" and many endemic species. Because of its natural and historical value, Ohrid Lake is under the protection of UNESCO.
Over a third of the territory of Albania – about 10,000 square kilometers (2.5 million acres) – is forested and the country was very rich in flora. About 3.000 different species of plants grow in Albania, many of which are used for medicinal purposes. Phytogeographically, Albania belongs to the Boreal Kingdom and is shared between the Adriatic and East Mediterranean provinces of the Mediterranean Region and the Illyrian province of the Circumboreal Region. According to the World Wide Fund for Nature and Digital Map of European Ecological Regions by the European Environment Agency, the territory of Albania can be subdivided into three ecoregions: the Illyrian deciduous forests, Pindus Mountains mixed forests and Dinaric Alpine mixed forests. The forests are home to a wide range of mammals, including wolves, bears, wild boars and chamois. Lynx, wildcats, pine martens and polecats are rare, but survive in some parts of the country.
Climate.
With its coastline facing the Adriatic and Ionian seas, its highlands backed upon the elevated Balkan landmass, and the entire country lying at a latitude subject to a variety of weather patterns during the winter and summer seasons, Albania has a high number of climatic regions for so small an area. The coastal lowlands have typically Mediterranean weather; the highlands have a Mediterranean continental climate. In both the lowlands and the interior, the weather varies markedly from north to south.
The lowlands have mild winters, averaging about. Summer temperatures average. In the southern lowlands, temperatures average about higher throughout the year. The difference is greater than during the summer and somewhat less during the winter.
Inland temperatures are affected more by differences in elevation than by latitude or any other factor. Low winter temperatures in the mountains are caused by the continental air mass that dominates the weather in Eastern Europe and the Balkans. Northerly and northeasterly winds blow much of the time. Average summer temperatures are lower than in the coastal areas and much lower at higher elevations, but daily fluctuations are greater. Daytime maximum temperatures in the interior basins and river valleys are very high, but the nights are almost always cool.
Average precipitation is heavy, a result of the convergence of the prevailing airflow from the Mediterranean Sea and the continental air mass. Because they usually meet at the point where the terrain rises, the heaviest rain falls in the central uplands. Vertical currents initiated when the Mediterranean air is uplifted also cause frequent thunderstorms. Many of these storms are accompanied by high local winds and torrential downpours.
When the continental air mass is weak, Mediterranean winds drop their moisture farther inland. When there is a dominant continental air mass, cold air spills onto the lowland areas, which occurs most frequently in the winter. Because the season's lower temperatures damage olive trees and citrus fruits, groves and orchards are restricted to sheltered places with southern and western exposures, even in areas with high average winter temperatures.
Lowland rainfall averages from 1,000 millimeters to more than 1,500 millimeters annually, with the higher levels in the north. Nearly 95% of the rain falls in the winter.
Rainfall in the upland mountain ranges is heavier. Adequate records are not available, and estimates vary widely, but annual averages are probably about 1,800 millimeters and are as high as 2,550 millimeters in some northern areas. The western Albanian Alps (valley of Boga) are among the most wet areas in Europe, receiving some of rain annually. The seasonal variation is not quite as great in the coastal area.
The higher inland mountains receive less precipitation than the intermediate uplands. Terrain differences cause wide local variations, but the seasonal distribution is the most consistent of any area.
Flora and Fauna.
Although a small country, Albania is distinguished for its rich biological diversity. The variation of geomorphology, climate and terrain create favorable conditions for a number of endemic and sub-endemic species with 27 endemic and 160 subendemic vascular plants present in the country. The total number of plants is over 3250 species, approximately 30% of the entire flora species found in Europe. Coastal regions and lowlands have typical Mediterranean macchia vegetation, whereas oak forests and vegetation are found on higher altitudes. Vast forests of black pine, beech and fir are found on higher mountains and alpine grasslands grow at altitudes above 1800 meters a.s.l.
There are around 760 vertebrate species found so far in Albania. Among these there are over 350 bird species, 330 freshwater and marine fish and 80 mammal species. There are some 91 globally threatened species found within the country, among which the Dalmatian pelican, Pygmy cormorant, and the European sea sturgeon. Rocky coastal regions in the south provide good habitats for the endangered Mediterranean monk seal. Some of the most significant bird species found in the country include the golden eagle – known as the national symbol of Albania – vulture species, capercaillie and numerous waterfowl. The Albanian forests still maintain significant communities of large mammals such as the brown bear, gray wolf, chamois and wild boar. The north and eastern mountains of the country are home to the last remaining Balkan lynx – a critically endangered population of the Eurasian lynx.
Economy.
Albania remains a poor country by Western European standards. Its GDP per capita (expressed in PPS—Purchasing Power Standards) stood at 25 percent of the EU average in 2008. Still, Albania has shown potential for economic growth, as more and more businesses relocate there and consumer goods are becoming available from emerging market traders as part of the current massive global cost-cutting exercise. Albania and Cyprus are the only countries in Europe that recorded economic growth in the first quarter of 2009. In its latest report, the International Monetary Fund (IMF) said Albania and Cyprus recorded increases of 0.4% and 0.3%, respectively. However, the country is still of low interest for major foreign investors due to frequent power shortages, occasional lack of water supplies and ubiquitous illegal activities.
Albania and Croatia have discussed the possibility of jointly building a nuclear power plant at Lake Shkoder, close to the border with Montenegro, a plan that has gathered criticism from the latter due to seismicity in the area. In addition, there is some doubt whether Albania would be able to finance a project of such a scale with a total national budget of less than $ 5 billion. However, in February 2009 Italian company Enel announced plans to build an 800 MW coal-fired power plant in Albania, to diversify electricity sources. Nearly 100% of the electricity is generated by ageing hydroelectric power plants, which are becoming more ineffective due to increasing droughts.
The country has some deposits of petroleum and natural gas, but produces only 6,425 barrels of oil per day. Natural gas production, estimated at about 30 million cubic meters, is sufficient to meet consumer demands. Other natural resources include coal, bauxite, copper and iron ore.
Agriculture is the most significant sector, employing some 58% of the labor force and generating about 21% of GDP. Albania produces significant amounts of wheat, corn, tobacco, figs (13th largest producer in the world) and olives.
Transport.
In the early 1990s, the rock-strewn roadways, unstable rail lines and obsolete telephone network crisscrossing Albania represented the remnants of the marked improvements that were made after World War II. Enver Hoxha's xenophobia and lust for control had kept Albania isolated, however, as the communications revolution transformed the wider world into a global village. Even internal travel amounted to something of a luxury for many Albanians during communism's ascendancy.
Highways.
Currently the major cities of the country are linked with first class national roads. There is a four lane highway connecting the city of Durrës with Tirana and the city of Durrës with the city of Lushnje. Albania is partaking in the construction of what it sees as three major corridors of transportation. The major priority as of present is the construction of the four lane Durrës-Pristina highway which will link Kosovo with Albania's Adriatic coast. The portion of the highway which links Albania's north east border with Kosovo was completed in June 2009, as a result, cutting the time it takes to get from Kosovo to Durrës from six hours to two. Indeed the roads in northwestern Albania remain in poor condition as of summer 2009. It takes approximately 1H30 to drive the from the border of Montenegro to Shkodër. It is also worth noting that there are no road signs and no traffic lights within and around this city. The second priority is the construction of European corridor 8 linking Albania with the Republic of Macedonia and Greece. The third priority for the government is the construction of the north-south axis of the country; it is sometimes referred to as the Adriatic–Ionian motorway as it is part of a larger regional highway connecting Croatia with Greece along the Adriatic and Ionian coasts. By the end of the decade it is expected that the majority of the sections of these three corridors will have been built. When all three corridors are completed Albania will have an estimated 759 kilometers of highway linking it with its neighbors.
There has been much discussion, debate and interest in the small Durrës–Kukës–Morinë Highway Albanian highway to Kosovo, which is intended to create a new, super-fast connection between Durrës on the Adriatic coast to Morinë at the border of Kosovo.
The current drive time between Kukës and Durrës is 6–7 hours, but once the new highway is completed the drive time will only be two hours. The whole road will be around, when completed to Pristina. The objective for constructing the road, according to the transport ministry, is to reduce transport costs and accidents, and improve traffic flow. It is the biggest, most expensive infrastructure project undertaken in Albania. There has been much controversy and scandal surrounding this project as well, due to the spiralling cost of construction leading to various corruption allegations. Originally the highway was forecast to cost around EUR400 million, and now the cost appears to have breached EUR800 million, although the exact cost for the total highway has yet to be confirmed by the government. Currently there is a display in Tirana's centre on Bvld Dëshmorët e Kombit.
Aviation.
The civil air transport in Albania marked its beginnings in November 1924, when the Republic of Albania signed a Governmental Agreement with German Air Company Lufthansa. On the basis of a ten-year concession agreement, the Albanian Airlines with the name Adria Aero Lloyd Company was established. In the spring of 1925, the first domestic flights from Tirana to Shkoder and Vlora began.
In August 1927, the office of Civil Aviation of Air Traffic Ministry of Italy purchased Adria Aero Lloyd. The company, now in Italian hands, expanded its flights to other cities, such as Elbasan, Korça, Kukësi, Peshkopia and Gjirokastra, and opened up international lines to Rome, Milan, Thessaloniki, Sofia, Belgrade, and Podgorica.
The construction of a more modern airport construction in present Lapraka) started in 1934 and was completed by the end of 1935. This new airport, which was later officially named "Airport of Tirana", was constructed in conformity with optimal technological parameters of that time, with reinforced concrete runway of 1200, and complemented with technical equipment and appropriate buildings.
During 1955–1957, the Rinasi Airport was constructed for military purposes. Later, its administration was shifted to the Ministry of Transport. On 25 January 1957 the State-owned Enterprise of International Air Transport (Albtransport) established its headquarters in Tirana. Aeroflot, Jat, Malev, Tarom and Interflug were the air companies that started to have flights with Albania until 1960.
During 1960–1978, several airlines ceased to operate in Albania due to the impact of the politics, resulting to a decrease of influx of flights and passengers. In 1977 Albania's government signed an agreement with Greece to open the country's first air links with non-communist Europe. As a result, Olympic Airways was the first non-communist airline to commercially fly into Albania after WWII. By 1991 Albania had air links with many major European cities, including Paris, Rome, Zürich, Vienna and Budapest, but no regular domestic air service.
A French-Albanian joint venture Ada Air, was launched in Albania's as the first private airline, in 1991. The company offered flights in a thirty-six-passenger airplane four days each week between Tirana and Bari, Italy and a charter service for domestic and international destinations.
From 1989 to 1991, because of political changes in the Eastern European countries, Albania adhered to the International Civil Aviation Organization (ICAO), opened its air space to international flights, and had its duties of Air Traffic Control defined. As premises of these developments, conditions were created to separate the activities of air traffic control from Albtransport. Instead, the National Agency of Air Traffic (NATA) was established as an independent enterprise. In addition, during these years, governmental agreements of civil air transport were established with Bulgaria, Germany, Slovenia, Italy, Russia, Austria, England, Macedonia, etc. The Directory General of Civil Aviation (DGCA) was established on 3 February 1991, to cope with the development required by the time.
As of 2007 Albania has one international airport: Tirana International Airport Nënë Tereza. The airport is linked to 29 destinations by 14 airlines. It has seen a dramatic rise in terms of passenger numbers and aircraft movements since the early 1990s. The data for 2009 is 1.3 million passengers served and an average of 44 landings and takeoffs per day.
Railways.
The railway system was extensively promoted by the totalitarian regime of Enver Hoxha, during which time the use of private transport was effectively prohibited. Since the collapse of the former regime, there has been a considerable increase in car ownership and bus usage. Whilst some of the country's roads are still in a very poor condition, there have been other developments (such as the construction of a motorway between Tirana and Durrës) which have taken much traffic away from the railways. The railways in Albania are administered by the national railway company "Hekurudha Shqiptare" (HSH) (which means "Albanian Railways"). It operates a gauge (standard gauge) rail system in Albania. All trains are hauled by Czech-built ČKD diesel-electric locomotives.
Demographics.
The Albanian population is considered a very young population, with an average age of 28.9 years. After 1990 the Albanian population has
faced new phenomena like migration, which greatly affected the distribution by districts and prefectures. Districts in the North have seen a decreasing population, while Tirana and Durrës districts have increased their population. Albania's population was 3,152,600 on 1 January 2007 and 3,170,048 on 1 January 2008. Alternative sources estimate the population in July 2009 at 3,639,453 with an annual growth rate of 0.546%. Albania is a largely ethnically homogeneous country with only small minorities. The vast majority of the population is ethnically Albanian (98.6%). Minorities include Greeks 1.17% and others 0.23% (Vlachs, Macedonians, Serbs, Bulgarians, Balkan Egyptians, Roma and former Yugoslavians). The size of the Greek minority is contentious, with the Albanian government claiming it is only 60,000, while the Greek government is claiming 300,000. Most Western sources put the size of the Greek minority at around 200,000, or ~6% of the population, while the CIA Factbook estimates the Greek minority at 3% of the total population.
The dominant language is Albanian, with two main dialects, Gheg and Tosk. Many Albanians are also fluent in English, Italian, Greek, Turkish or German.
Religion.
Estimates of the religious allegiance of the population of Albania vary, with some sources suggesting that the majority do not follow any religion. A second study of religion in Albania under the "International Religious Freedom Report 2009", performed by the Bureau of Democracy, Human Rights, and Labor of the United States's State Department, found that a majority of Albania's population is nonreligious.
A recent study by the Pew Research Center puts the percentage of nominal Muslims in Albania at 79.9%, with the remaining 20% consisting of Christians. The CIA World Factbook gives a distribution of 70% Muslims, 20% Eastern Orthodox, and 10% Roman Catholics. According to the World Christian Encyclopedia, roughly 39% of Albanians are Muslim, and 35% Christian
The Albanians first appear in the historical record in Byzantine sources of the late-11th century. At this point, they are already fully Christianised. Christianity was later overshadowed by Islam, which kept the scepter of the major religion during the period of Ottoman Turkish rule from the 15th century until year 1912. After independence (1912) from the Ottoman Empire, the Albanian republican, monarchic and later communist regimes followed a systematic policy of separating religion from official functions and cultural life. Albania never had an official state religion either as a republic or as a kingdom. In the 20th century, the clergy of all faiths was weakened under the monarchy, and ultimately eradicated during the 1940s and 1950s, under the state policy of obliterating all organised religion from Albanian territories.
The Communist regime that took control of Albania after World War II suppressed religious observance and institutions and entirely banned religion to the point where Albania was officially declared to be the world's first atheist state. Religious freedom has returned to Albania since the regime's change in 1992. Albanian Muslim populations (mainly secular and of the Sunni rite) are found throughout the country whereas Orthodox Christians are concentrated in the south and Roman Catholics are found in the north of the country. No reliable data are available on active participation in formal religious services, and estimates range from 25% to 40%.
There are about 4,000 active Jehovah's witnesses in Albania.
Among other religious organizations making inroads into this nation is The Church of Jesus Christ of Latter-day Saints (LDS or 'Mormons'). The Church's involvement in Albania began with Humanitarian Aid during the 1990s. The first missionaries were sent in 1992 with the Albania Tirana Mission being opened in 1996. As of 2008, there were nearly 2,000 members of the Church in Albania, spread throughout ten branches with two purpose-built Chapels and one Family History Center.
Music and folklore.
Albanian folk music falls into three sylistic groups, with other important music areas around Shkodër and Tirana; the major groupings are the Ghegs of the north and southern Labs and Tosks. The northern and southern traditions are contrasted by the "rugged and heroic" tone of the north and the "relaxed, gentle and exceptionally beautiful" form of the south. These disparate styles are unified by "the intensity that both performers and listeners give to their music as a medium for patriotic expression and as a vehicle carrying the narrative of oral history", as well as certain characteristics like the use of obscure rhythms such as 3/8, 5/8 and 10/8. The first compilation of Albanian folk music was made by Pjetër Dungu in 1940.
Albanian folk songs can be divided into major groups, the heroic epics of the north, and the sweetly melodic lullabies, love songs, wedding music, work songs and other kinds of song. The music of various festivals and holidays is also an important part of Albanian folk song, especially those that celebrate St. Lazarus Day ("the llazore"), which inaugurates the springtime. Lullabies and laments are very important kinds of Albanian folk song, and are generally performed by solo women.
Albanian language and literature.
Albanian was proven to be an Indo-European language in 1854 by the German philologist Franz Bopp. The Albanian language comprises its own branch of the Indo-European language family.
Some scholars believe that Albanian derives from Illyrian while others, claim that it derives from Daco-Thracian. (Illyrian and Daco-Thracian, however, might have been closely related languages; see Thraco-Illyrian.)
Establishing longer relations, Albanian is often compared to Balto-Slavic on the one hand and Germanic on the other, both of which share a number of isoglosses with Albanian. Moreover, Albanian has undergone a vowel shift in which stressed, long "o" has fallen to "a", much like in the former and opposite the latter. Likewise, Albanian has taken the old relative "jos" and innovatively used it exclusively to qualify adjectives, much in the way Balto-Slavic has used this word to provide the definite ending of adjectives.
The cultural resistance was first of all expressed through the elaboration of the Albanian language in the area of church texts and publications, mainly of the Catholic confessional region in the North, but also of the Orthodox in the South. The Protestant reforms invigorated hopes for the development of the local language and literary tradition when cleric Gjon Buzuku brought into the Albanian language the Catholic liturgy, trying to do for the Albanian language what Luther did for German.
"Meshari" (The Missal) by Gjon Buzuku, published by him in 1555, is considered to date as the first literary work of written Albanian. The refined level of the language and the stabilised orthography must be a result of an earlier tradition of writing Albanian, a tradition that is not known. But there are some fragmented evidence, dating earlier than Buzuku, which indicate that Albanian was written at least since 14th century AD. The first known evidence dates from 1332 AD and deals with the French Dominican Guillelmus Adae, Archbishop of Antivari, who in a report in Latin writes that Albanians use Latin letters in their books although their language is quite different from Latin. Of special importance in supporting this are: a baptizing formula ("Unte paghesont premenit Atit et Birit et spertit senit") of 1462, written in Albanian within a text in Latin by the Bishop of Durrës, Pal Engjëlli; a glossary with Albanian words of 1497 by Arnold von Harff, a German who had travelled through Albania, and a 15th century fragment from the Bible from the Gospel of Matthew, also in Albanian, but in Greek letters.
Albanian writings of these centuries must not have been religious texts only, but historical chronicles too. They are mentioned by the humanist Marin Barleti, who, in his book "Rrethimi i Shkodrës" (The Siege of Shkodër) (1504), confirms that he leafed through such chronicles written in the language of the people ("in vernacula lingua"). Despite the obstacles generated by the Counter-Reformation which was opposed to the development of national languages in Christian liturgy, this process went on uninterrupted. During the 16th to 17th centuries, the catechism "E mbësuame krishterë" (Christian Teachings) (1592) by Lekë Matrënga, "Doktrina e krishterë" (The Christian Doctrine) (1618) and "Rituale romanum" (1621) by Pjetër Budi, the first writer of original Albanian prose and poetry, an apology for George Castriot (1636) by Frang Bardhi, who also published a dictionary and folklore creations, the theological-philosophical treaty "Cuneus Prophetarum" (The Band of Prophets) (1685) by Pjetër Bogdani, the most universal personality of Albanian Middle Ages, were published in Albanian. The most famous Albanian writer is probably Ismail Kadare.
Education.
Before the Communist regime, Albania's illiteracy rate was as high as 85%. Schools were scarce between World War I and World War II. When the Communist regime over took the country in 1944, the regime wanted to wipe out illiteracy. The regulations became so strict that anyone between the ages of 12 and 40 who could not read or write was mandated to attend classes to learn. Since these times of struggle the country's literacy rate has improved remarkably. Today the overall literacy rate in Albania is 98.7%, the male literacy rate is 99.2% and female literacy rate is 98.3%. Since the rather large population movements in the 1990s to urban areas, education has moved as well. Thousands of teachers moved to urban areas to follow students. The University of Tirana is the first university in Albania and was founded in October 1957.
Administrative divisions.
Albania is divided into 12 administrative divisions called (Albanian: official "qark"/"qarku", but often "prefekturë"/"prefektura") Counties, 36 districts and 351 municipalities. Each region has its Regional Council and is composed of a number of Municipalities and Communes, which are the first level of local governance responsible for local needs and law enforcement.
Sport.
Football (soccer) is the most popular sport in Albania, both at a participatory and spectator level. The sport is governed by the Football Association of Albania (, F.SH.F.).
Entertainment.
Radio Televizioni Shqiptar, (RTSH), is Albania's leading television network. RTSH runs a national television station "TVSH", (standing for "Televizioni Shqiptar"), and two national radio stations, using the name "Radio Tirana". An international service broadcasts radio programmes in Albanian and seven other languages via medium wave (AM) and short wave (SW). The international service has used the theme from the song "Keputa një gjethe dafine" as its signature tune. Since 1999, RTSH has been a member of the European Broadcasting Union. Since 1993, RTSH has also run an international television service via satellite, aimed at Albanian language communities in Kosovo, Macedonia, Montenegro and Greece, plus the Albanian diaspora in the rest of Europe.
According the National Council of Radio and Television Albania has an estimated 257 media outlets, including 66 radio stations and 65 television stations, with three national and 62 local stations.
Health.
Health care has been in a steep decline after the collapse of socialism in the country, but a process of modernization has been taking place since 2000. As of the early 2000s, there were 51 hospitals in the country, including a military hospital and specialist facilities. Albania has successfully removed diseases such as malaria.
Life expectancy is estimated at 77.43 years, ranking 51st worldwide, and outperforming a number of European Union countries, such as Hungary and the Czech Republic. The most common causes of death are circulatory disease followed by cancerous illnesses.
The medical school, Faculty of Medicine at Tirana University, is in Tirana. There are also nursing schools in many other cities.
Cuisine.
The cuisine of Albania – as with most Mediterranean and Balkan nations – is strongly influenced by its long history. At different times, the territory which is now Albania has been claimed or occupied by Greece, Italy and the Ottoman Turks and each group has left its mark on Albanian cuisine. The main meal of the Albanians is lunch, and it is usually accompanied by a salad of fresh vegetables, such as tomatoes, cucumbers, green peppers and olives with olive oil, vinegar and salt. Lunch also includes a main dish of vegetables and meat. Seafood specialties are also common in the coastal areas of Durrës, Vlorë and Sarandë.
---END.OF.DOCUMENT---

Allah.
Allah (, ',) is the standard Arabic word for God. While the term is best known in the West for its use by Muslims as a reference to God, it is used by Arabic-speakers of all Abrahamic faiths, including Christians and Jews, in reference to "God". The term was also used by pagan Meccans as a reference to the creator-god, possibly the supreme deity in pre-Islamic Arabia.
The concepts associated with the term "Allah" (as a deity) differ among the traditions. In pre-Islamic Arabia amongst pagan Arabs, "Allah" was not considered the sole divinity, having associates and companions, sons and daughters - a concept which Islam thoroughly and resolutely abrogated. In Islam, the name Allah is the supreme and all-comprehensive divine name. All other divine names are believed to refer back to Allah. "Allah" is unique, the only Deity, creator of the universe and omnipotent. Arab Christians today use terms such as "Allāh al-ʼAb" (الله الأب, "God the Father") to distinguish their usage from Muslim usage. There are both similarities and differences between the concept of God as portrayed in the Qur'an and the Hebrew Bible.
Unicode has a codepoint reserved for "Allāh", = U+FDF2. Many Arabic type fonts feature special ligatures for Allah.
Etymology.
The term "Allāh" is derived from a contraction of the Arabic definite article "al-" "the" and ' "deity, god" to ' meaning "the [sole] deity, God" ("ho theos monos").
Cognates of the name "Allāh" exist in other Semitic languages, including Hebrew and Aramaic. The corresponding Aramaic form is אֱלָהָא ' in Biblical Aramaic and ܐܰܠܳܗܳܐ ' or ' in Syriac.
The contraction of "al-" and ' in forming the term "Allāh" ("the god", masculine form) parallels the contraction of "al-" and ' in forming the term "Allāt" ("the goddess", feminine form).
Pre-Islamic Arabia.
In pre-Islamic Arabia, Allah was used by Meccans as a reference to the creator-god, possibly the supreme deity.
Allah was not considered the sole divinity; however, Allah was considered the creator of the world and the giver of rain. The notion of the term may have been vague in the Meccan religion. Allah was associated with companions, whom pre-Islamic Arabs considered as subordinate deities. Meccans held that a kind of kinship existed between Allah and the jinn. Allah was thought to have had sons and that the local deities of al-ʻUzzá, Manāt and al-Lāt were His daughters. The Meccans possibly associated angels with Allah. Allah was invoked in times of distress. Muhammad's father's name was meaning the “servant of Allāh.” or "the slave of Allāh"
Islam.
According to Islamic belief, Allah is the proper name of God, and humble submission to His Will, Divine Ordinances and Commandments is the pivot of the Muslim faith. "He is the only God, creator of the universe, and the judge of humankind." "He is unique ("wahid") and inherently one ("ahad"), all-merciful and omnipotent." The Qur'an declares "the reality of Allah, His inaccessible mystery, His various names, and His actions on behalf of His creatures."
In Islamic tradition, there are 99 Names of God ("al-asma al-husna" lit. meaning: "The best names") each of which evoke a distinct characteristic of Allah. All these names refer to Allah, the supreme and all-comprehensive divine name. Among the 99 names of God, the most famous and most frequent of these names are "the Merciful" ("al-rahman") and "the Compassionate" ("al-rahim").
Most Muslims use the untranslated Arabic phrase "insha' Allah" (meaning "God willing") after references to future events. Muslim discursive piety encourages beginning things with the invocation of "bismillah"(meaning "In the name of God").
There are certain phrases in praise of God that are favored by Muslims, including "Subhan-Allah" (Holiness be to God), "Alhamdulillah" (Praise be to God), "La-il-la-ha-illa-Allah" (There is no deity but God) and "Allāhu Akbar" (God is great) as a devotional exercise of remembering God (zikr). In a Sufi practice known as "zikr Allah" (lit. remembrance of God), the Sufi repeats and contemplates on the name "Allah" or other divine names while controlling his or her breath.
Some scholars have suggested that Muhammad used the term Allah in addressing both pagan Arabs and Jews or Christians in order to establish a common ground for the understanding of the name for God, a claim Gerhard Böwering says is doubtful. According to Böwering, in contrast with Pre-Islamic Arabian polytheism, God in Islam does not have associates and companions nor is there any kinship between God and jinn. Pre-Islamic pagan Arabs believed in a blind, powerful, inexorable and insensible fate over which man had no control. This was replaced with the Islamic notion of a powerful but provident and merciful God.
According to Francis Edwards Peters, "The Qur'an insists, Muslims believe, and historians affirm that Muhammad and his followers worship the same God as the Jews (). The Quran's Allah is the same Creator God who covenanted with Abraham". Peters states that the Qur'an portrays Allah as both more powerful and more remote than Yahweh, and as a universal deity, unlike Yahweh who closely follows Israelites.
Christianity.
Arabic-speakers of all Abrahamic faiths, including Christians and Jews, use the word "Allah" to mean "God". The Christian Arabs of today have no other word for 'God' than 'Allah'. (Even the Arabic-descended Maltese language of Malta, whose population is almost entirely Roman Catholic, uses "Alla" for 'God'.) Arab Christians for example use terms "Allāh al-ʼab" (الله الأب) meaning God the Father, "Allāh al-ibn" (الله الابن) mean God the Son, and ' (الله الروح القدس) meaning God the Holy Spirit (See God in Christianity for the Christian concept of God).
Arab Christians have used two forms of invocations that were affixed to the beginning of their written works. They adopted the Muslim "basm-Allah", and also created their own Trinitized "basm-Allah" as early as the eight century CE. The Muslim "basm-Allah" reads: "In the name of God, the Compassionate, the Merciful." The Trinitized "basm-Allah" reads: "In the name of Father and the Son and the Holy Spirit, One God." The Syriac, Latin and Greek invocations do not have the words "One God" at the end. This addition was made to emphasize the monotheistic aspect of Trinitian belief and also to make it more palatable to Muslims.
According to Marshall Hodgson, it seems that in the pre-Islamic times, some Arab Christians made pilgrimage to the Kaaba, a pagan temple at that time, honoring Allah there as God the Creator.
English and other European languages.
The history of the word "Allāh" in English was probably influenced by the study of comparative religion in 19th century; for example, Thomas Carlyle (1840) sometimes used the term Allah but without any implication that Allah was anything different from God. However, in his biography of Muhammad (1934), Tor Andræ always used the term Allah, though he allows that this 'conception of God' seems to imply that it is different from that of the Jewish and Christian theologies. By this time Christians were also becoming accustomed to retaining the Hebrew term "Yahweh" untranslated (it was previously translated as 'the Lord').
Languages which may not commonly use the term "Allah" to denote a deity may still contain popular expressions which use the word. For example, because of the centuries long Muslim presence in the Iberian Peninsula, the word ojalá in the Spanish language and oxalá in the Portuguese language exist today, borrowed from Arabic (Arabic: إن شاء الله). This word literally means "God willing" (in the sense of "I hope so").
Some Muslims leave the name "Allāh" untranslated in English. Sometimes this comes from a zeal for the Arabic text of the Qur'an and sometimes with a more or less conscious implication that the Jewish and Christian concept of God is not completely true in its details. Conversely, the usage of the term Allah by English speaking non-Muslims in reference to the God in Islam, Marshall G. S. Hodgson says, can imply that Muslims are worshiping a mythical god named 'Allah' rather than God, the creator. This usage is therefore appropriate, Hodgson says, only for those who are prepared to accept its theological implications.
Malay and Indonesian language.
Christians in Indonesia and Malaysia also use "Allah" to refer to God in the Malay language and Indonesian Language (both language although different referred to as "Bahasa").
Mainstream Bible translations in Bahasa use "Allah" as the translation of Hebrew Elohim (translated in English Bibles as "God"). This goes back to early Bahasa translation work by Francis Xavier in the 16th century.
The government of Malaysia in 2007 outlawed usage of the term "Allah" in any other but Muslim contexts, but the High Court in 2009 revoked the law, ruling that it was unconstitutional.
While "Allah" had been used for the Christian God in Malay for more than four centuries, the contemporary controversy was triggered by usage of "Allah" by the Roman Catholic newspaper "The Herald".
The government has in turn appealed the court ruling, and the High Court has suspended implementation of its verdict until the appeal is heard.
Typography.
The word "Allāh" is always written without an alif to spell the "ā" vowel. This is because the spelling was settled before Arabic spelling started habitually using alif to spell "ā". However, in vocalized spelling, a small diacritic "alif" is added on top of the "shaddah" to indicate the pronunciation.
One exception may be in the pre-Islamic Zabad inscription, where it ends with an ambiguous sign that may be a lone-standing "h" with a lengthened start, or may be a non-standard conjoined "l-h":-
Unicode.
Unicode has a codepoint reserved for "Allāh", = U+FDF2.
This character according to the official Unicode specification is a ligature of "alif-lām-lām-shadda-(superscript alif)-hā" (U+0627 U+0644 U+0644 U+0651 U+0670 U+0647).
There is, however some confusion arising from the fact that Arabic typography usually features a ' glyph without the preceding alif, which only occurs phrase-initially (or with in Qur'anic orthography). Consequently, the majority of Arabic Unicode fonts do not conform with the specification and have a glyph without the alif at this position (e.g. those provided by, the great majority of those licensed to or developed by, those of, SIL's and the fonts of developed in Pakistan),
while others have the prescribed form with alif (e.g., distributed with the Middle-Eastern version
of the, Arial Unicode MS, and, distributed with and with).
The calligraphic variant of the word used as the Coat of arms of Iran is encoded in Unicode, in the Miscellaneous Symbols range, at codepoint U+262B ().
---END.OF.DOCUMENT---

Azerbaijan.
Azerbaijan (;), formally the Republic of Azerbaijan (), is a country in the Caucasus region of Eurasia. Located at the crossroads of Eastern Europe and Western Asia, it is bounded by the Caspian Sea to the east, Russia to the north, Georgia to the northwest, Armenia to the west, and Iran to the south. The exclave of Nakhichevan is bounded by Armenia to the north and east, Iran to the south and west, while having a short borderline with Turkey to the northwest. The majority-Armenian populated Nagorno-Karabakh region in the southwest of Azerbaijan declared itself independent from Azerbaijan in 1991, but it is not diplomatically recognised by any nation and is still considered a "de-jure" part of Azerbaijan.
Azerbaijan, a nation with a majority Turkic and Shi‘ite Muslim population, is a secular and a unitary republic with an ancient and historic cultural heritage. Azerbaijan was the first successful attempt to establish a democratic and secular republic in the Muslim world. Azerbaijan is one of the founder members of GUAM and the Organisation for the Prohibition of Chemical Weapons, and joined the Commonwealth of Independent States in September 1993. A Special Envoy of the European Commission is present in the country, which is also a member of the United Nations, the OSCE, the Council of Europe, and the NATO Partnership for Peace (PfP) program.
Etymology of the name.
The name of Azerbaijan derives from "Atropates", a satrap of Persia under the Achaemenid Empire, that was later reinstated as the satrap of Media under Alexander of Macedonia. The original etymology of this name is thought to have its roots in the ancient Iranian religion of Zoroastrianism. In Avestan Frawardin Yasht ("Hymn to the Guardian Angels"), there is a mention of "âterepâtahe ashaonô fravashîm ýazamaide", which literally translates from Old Persian as "we worship the Fravashi of the holy Atare-pata".
Atropates ruled over the region of Atropatene (present-day Iranian Azerbaijan). The name "Atropates" itself is the Greek transliteration of an Old-Iranian, probably Median, compounded name with the meaning "Protected by the (Holy) Fire". The Greek name is mentioned by Diodorus Siculus and Strabo, and it is continued as "ādurbādagān" in the Pahlavi geographical text Shahrestānihā i Erānshahr. The word is translatable as both "the treasury" and "the treasurer" of fire in Modern Persian.
Antiquity.
The earliest evidence of human settlement in the territory of Azerbaijan dates to the late Stone Age and is related to the Guruchay culture of the Azykh Cave, where archeological evidences promoted the inclusion of Azerbaijan into the map of the ascent man sites of Europe. The Upper Paleolithic and late Bronze Age cultures are attested in the caves of Tağılar, Damcılı, Zar, Yataq-yeri and in the necropolises of Leylatepe and Saraytepe. The area was conquered by the Achaemenids around 550 B.C.E., leading to the spread of Zoroastrianism. Later it became part of Alexander the Great's Empire and its successor Seleucid Empire. Caucasian Albanians, the original inhabitants of the area, established an independent kingdom around the fourth century B.C.E.
Early Iranian settlements included the Scythians in the ninth century BC. Following the Scythians, Iranian Medes came to dominate the area to the south of the Aras. The Medes forged a vast empire between 900–700 BC, which was integrated into the Achaemenids Empire around 550 BC.
During this period, Zoroastrianism spread in the Caucasus and Atropatene. Ancient Azaris spoke Ancient Azari language, which belonged to Iranian branch of Indo-European languages.
Middle Ages.
In 252 C.E., the Sassanids turned it into a vassal state, while King Urnayr officially adopted Christianity as the state religion in the fourth century. Despite numerous conquests by the Sassanids and Byzantines, Albania remained an entity in the region until the ninth century. The Islamic Umayyad Caliphate repulsed both the Sassanids and Byzantines from the region and turned Caucasian Albania into a vassal state after the Christian resistance, led by Prince Javanshir, was suppressed in 667.
The power vacuum left by the decline of the Abbasid Caliphate was filled by numerous local dynasties such as the Sallarids, Sajids, Shaddadids, Rawadids and Buyids. At the beginning of the eleventh century, the territory was gradually seized by waves of Turkic Oghuz tribes from Central Asia. The first of these Turkic dynasties was the Ghaznavids, which entered the area now known as Azerbaijan by 1030.
Turkification of Azaris was completed only By the late 1800s. The old Iranic speakers found solely in tiny isolated recesses of the mountains or other remote areas (such as Harzand, Galin Guya, Shahrud villages in Khalkhal and Anarjan). Today, this Turkic speaking population is also known as Azeris.
Locally, the possessions of the subsequent Seljuq Empire were ruled by atabegs, who were technically vassals of the Seljuq sultans, being sometimes de facto rulers themselves. Under the Seljuq Turks, local poets such as Nizami Ganjavi and Khagani Shirvani gave rise to a blossoming of Persian literature on the territory of present-day Azerbaijan. The next ruling state of the Jalayirids was short-lived and fell under the conquests of Timur.
The local dynasty of Shirvanshahs became a vassal state of Timur's Empire and assisted him in his war with the ruler of the Golden Horde Tokhtamysh. Following Timur's death two independent and rival states emerged: Kara Koyunlu and Ak Koyunlu. The Shirvanshahs returned, maintaining a high degree of autonomy as local rulers and vassals from 861 until 1539. During their persecution by the Safavids, the last dynasty imposed Shia Islam upon the formerly Sunni population, as it was battling against the Sunni Ottoman Empire.
Modern Era.
After the Safavids, the area was ruled by the Iranian dynasties of Afshar and Zand and briefly by the Qajars. However, while under Persian sovereignty de facto self-ruling khanates emerged in the area, especially following the collapse of the Zand dynasty and in the early Qajar era. The brief and successful Russian campaign of 1812 was concluded with the Treaty of Gulistan, in which the shah's claims to some of the Khanates of the Caucasus were dismissed by Russia on the ground that they had been de facto independent long before their Russian occupation. The khanates exercised control over their affairs via international trade route between Central Asia and the West. Engaged in constant warfare, these khanates were eventually incorporated into the Russian Empire in 1813, following two Russo-Persian Wars. The area to the North of the river Arax, amongst which the territory of the contemporary republic of Azerbaijan were Iranian territory until they were occupied by Russia.
Under the Treaty of Turkmenchay, Persia recognized Russian sovereignty over the Erivan Khanate, the Nakhchivan Khanate and the remainder of the Lankaran Khanate.
Azerbaijan Democratic Republic.
After the collapse of the Russian Empire during World War I, Azerbaijan, together with Armenia and Georgia became part of the short-lived Transcaucasian Democratic Federative Republic. When the republic dissolved in May 1918, Azerbaijan declared independence as the Azerbaijan Democratic Republic (ADR). The ADR was the first modern parliamentary republic in the Muslim World.
Among the important accomplishments of the Parliament was the extension of suffrage to women, making Azerbaijan the first Muslim nation to grant women equal political rights with men. In this accomplishment, Azerbaijan also preceded the United Kingdom and the United States. Another important accomplishment of ADR was the establishment of Baku State University, which was the first modern-type university founded in Muslim East.
By March 1920, it was obvious that Soviet Russia would attack the much-needed Baku. Vladimir Lenin said that the invasion was justified as Soviet Russia could not survive without Baku oil. Independent Azerbajian lasted only 23 months until the Bolshevik 11th Soviet Red Army invaded it and establishing the Azerbaijan SSR on April 28, 1920.
Although the bulk of the newly formed Azerbaijani army was engaged in putting down an Armenian revolt that had just broken out in Karabakh, Azeris did not surrender their brief independence of 1918–20 quickly or easily. As many as 20,000 Azerbaijani soldiers died resisting what was effectively a Russian reconquest.
Despite existing for only two short years, the multi party Azerbaijani Parliamentary republic and the coalition governments managed to achieve a number of measures on national and state building, education, creation of an army, independent financial and economic systems, international recognition of the ADR as a de facto state pending de jure recognition, official recognitions and diplomatic relations with a number of states, and preparing of a Constitution, equal rights for all. This has laid an important foundation for the re-establishment of independence in 1991.
Soviet Azerbaijan.
In October 13, 1921, the Soviet republics of Russia, Armenia, Azerbaijan, and Georgia signed an agreement with Turkey known as the Treaty of Kars. The previously independent Naxicivan SSR would also become autonomous ASSR within Azerbaijan by the treaty of Kars. On the other hand, Armenia was awarded the region of Zhangezur and Turkey agreed to return Alexandropol (Gymri).
In March 12, 1922, under heavy pressure from Moscow, the leaders of Azerbaijan, Armenian, and Georgian Soviet Socialist Republics established a union known as the Transcaucasian SFSR. This was the first attempt at a union of Soviet republics, preceding the USSR. The Union Council of TSFSR consisted of the representatives of the three republics – Nariman Narimanov (Azerbaijan), Polikarp Mdivani (Georgia), and Aleksandr Fyodorovich Miasnikyan (Armenia). The First Secretary of the Transcaucasian Communist Party was Sergo Ordzhonikidze. In 1936, TSFSR was dissolved and Azerbaijan SSR became one of the constituent member states of the Soviet Union.
During World War II, Azerbaijan played a crucial role in the strategic energy policy of Soviet Union, much of the Soviet Union's oil on the Eastern Front was supplied by Baku. By the Decree of the Supreme Soviet of the USSR in February 1942, the commitment of more than 500 workers and employees of the oil industry of Azerbaijan was awarded orders and medals.
Operation Edelweiss carried out by the German Wehrmacht targeted Baku because of its importance as the energy (petroleum) dynamo of the USSR.
Some 800,000 Azerbaijanis fought well in the ranks of the Soviet Army of which 400,000 died and Azeri Major-General Azi Aslanov was awarded twice Hero of the Soviet Union.
Restoration of independence.
Following the politics of "glasnost", initiated by Mikhail Gorbachev, civil unrest and ethnic strife grew in various regions of the Soviet Union, including Nagorno-Karabakh, a region of the Azerbaijan SSR. The disturbances in Azerbaijan, in response to Moscow's indifference to already heated conflict, resulted in calls for independence and secession, then led to the
Pogrom of Armenians in Baku, and subsequently culminated in the events of Black January in Baku. At this time, Ayaz Mütallibov was appointed as the First Secretary of the Azerbaijan Communist Party.
Later in 1990, the Supreme Council of the Azerbaijan SSR dropped the words "Soviet Socialist" from the title, adopted the Declaration of Sovereignty of the Azerbaijan Republic and restored the modified flag of the Azerbaijan Democratic Republic as a state flag. On 8 September 1991, Ayaz Mütallibov was elected president in nationwide elections in which he was the only candidate.
On 18 October 1991, the Supreme Council of Azerbaijan adopted a Declaration of Independence which was affirmed by a nationwide referendum in December 1991, when the Soviet Union was officially dissolved. The early years of independence were overshadowed by the Nagorno-Karabakh War with neighboring Armenia. By the end of hostilities in 1994, Azerbaijan lost control of up to 16% of its territory, including Nagorno-Karabakh itself. An estimated 30,000 people had been killed and more than a million had been displaced.
Four United Nations Security Council Resolutions (822, 853, 874, and 884) called for "the withdrawal of occupying forces from occupied areas of the Azerbaijani Republic". In 1993, democratically elected president Abülfaz Elçibay was overthrown by a military insurrection led by Colonel Surat Huseynov, which resulted in the rise to power of the former leader of Soviet Azerbaijan, Heydar Aliyev.
In 1994, Surat Huseynov, by that time a prime minister, attempted another military coup against Heydar Aliyev, but Huseynov was arrested and charged with treason. In 1995, another coup attempt against Aliyev, by the commander of the OMON Militsiya special unit, Rovshan Javadov, was averted, resulting in the killing of the latter and disbanding of Azerbaijan's OMON units.
Although during his presidency Aliyev managed to reduce the country's unemployment, reined in criminal groups, established the fundamental institutions of independent statehood, and brought stability, peace and major foreign investment, the country was tainted by rampant corruption in the governing bureaucracy. In October 1998, Aliyev was reelected for a second term.
Despite the much improved economy, particularly with the exploitations of Azeri-Chirag-Guneshli oil field and Shah Deniz gas field, Aliyev's presidency became unpopular due to vote fraud, widespread corruption and objection to his autocratic regime. The same harsh criticism followed the elections of former Prime Minister Ilham Aliyev, the second leader of New Azerbaijan Party after the death of his father Heydar.
Geography.
Azerbaijan is in the South Caucasus region of Eurasia, straddling Western Asia and Eastern Europe. Three physical features dominate Azerbaijan: the Caspian Sea, whose shoreline forms a natural boundary to the east; the Greater Caucasus mountain range to the north; and the extensive flatlands at the country's center.
The total length of Azerbaijan's land borders is, of which 1007 are with Armenia, 756 with Iran, 480 with Georgia, 390 with Russia and 15 with Turkey. The coastline stretches for, and the length of the widest area of the Azerbaijani section of the Caspian Sea is. The territory of Azerbaijan extends from north to south, and from west to east.
The three mountain ranges are the Greater and Lesser Caucasus, and the Talysh Mountains, together covering approximately 40% of the country. The highest peak of Azerbaijan is mount Bazardüzü (4,466 m), while the lowest point lies in the Caspian Sea (−28 m). Nearly half of all the mud volcanoes on Earth are concentrated in Azerbaijan.
The main water sources are the surface waters. However, only 24 of the 8,350 rivers are greater than in length. All the rivers drain into the Caspian Sea in the east of the country. The largest lake is Sarysu (67 km²), and the longest river is Kur (1,515 km), which is transboundary. Azerbaijan's four main islands in the Caspian Sea have a combined area of over thirty square kilometres.
Orography.
Azerbaijan is home to a vast variety of landscapes. Over half of Azerbaijan's land mass consists of mountain ridges, crests, yailas and plateaus which rise up to hypsometric levels of 400–1000 meters (including the Middle and Lower lowlands), in some places (Talis, Jeyranchol-Ajinohur and Langabiz-Alat foreranges) up to 100–120 metres, and others from 0 – 50 meters and up (Qobustan, Absheron). The rest of Azerbaijan's terrain consist of plains and lowlands. Hypsometric marks within the Caucasus region vary from about −28 metres at the Caspian Sea shoreline up to 4466 metres, (Bazardüzü peak).
Climate.
The formation of climate in Azerbaijan is influenced particularly by cold arctic air masses of Scandinavian anticyclone, temperate of Siberian anticyclone, and Central Asian anticyclone. Azerbaijan's diverse landscape affects the ways air masses enter the country.
The Greater Caucasus protects the country from direct influences of cold air masses coming from the north. That leads to the formation of subtropical climate on most foothills and plains of the country. Meanwhile, plains and foothills are characterized by high solar radiation rates.
Nine out of eleven existing climate zones are present in Azerbaijan. Both the absolute minimum temperature () and the absolute maximum temperature () were observed in Julfa and Ordubad. The maximum annual precipitation falls in Lankaran (1,600 to 1,800 mm) and the minimum in Absheron (200 to 350 mm).
Flora.
Azerbaijan has a very rich flora, more than 4,500 species of higher plants have been registed in the country. Due the unique climate in Azerbaijan, the flora is much richer in the number of species than the flora of the other republics of the South Caucasus.
Fauna.
The first reports on the richness and diversity of animal life in Azerbaijan can be found in travel notes of Eastern travelers. Animal carvings on architectural monuments, ancient rocks and stones survived up to the present times. The first information on the animal kingdom of Azerbaijan was collected during the visits of naturalists to Azerbaijan in 17th century. Unlike fauna, the concept of animal kingdom covers not only the types of animals, but also the number of individual species.
There are 106 species of mammals, 97 species of fish, 363 species of birds, 10 species of amphibians and 52 species of reptiles which have been recorded and classified in Azerbaijan.
The symbol of Fauna in Azerbaijan is the Karabakh horse which is a mountain-steppe racing and riding horse which can only be found in Azerbaijan. The Karabakh horse has a reputation for its good temper, speed, elegance and intelligence. It is one of the oldest breeds, with ancestry dating to the ancient world. The horse was originally developed in the Azerbaijani Karabakh region in the 5th century and is named after it.
Rivers and lakes.
Rivers and lakes form the principal part of the water systems of Azerbaijan, they were formed over a long geological timeframe and changed significantly throughout that period. This is particularly evidenced by remnants of ancient rivers found throughout the country. The country's water systems are continually changing under the influence of natural forces and human introduced industrial activities. Artificial rivers (canals) and ponds are a part of Azerbaijan's water systems.
There are 8,359 rivers of various lengths within Azerbaijan. Of them 8,188 rivers are less than 25 kilometers in length. Only 24 rivers are over 100 kilometers long.
The Kura and Aras are the most popular rivers in Azerbaijan, they run through the Kura-Aras Lowland. The rivers that directly flow into the Caspian Sea, originate mainly from the north-eastern slope of the Major Caucasus and Talysh Mountains and run along the Samur-Devechi and Lenkeran lowlands.
From the water supply point, Azerbaijan is below the average in the world with approximately 100,000 m³/year of water per km². All big water reservoirs are built on Kur. The hydrography of Azerbaijan basically belongs to the Caspian Sea basin.
Protection.
Since the independence of Azerbaijan in 1991, the Azerbaijani government has taken drastic measures to preserve the environment of Azerbaijan. But national protection of the environment started to truly improve after 2001 when the state budget increased due to new revenues provided by the Baku-Tbilisi-Ceyhan pipeline. Within four years protected areas doubled and now make up eight percent of the country's territory.
Since 2001 the government has set up seven large reserves and almost doubled the sector of the budget earmarked for environmental protection.
Administrative divisions.
Azerbaijan is divided into 59 rayons ("rayonlar", singular "rayon"), 11 city districts ("şəhərlər", singular "şəhər"), and one autonomous republic ("muxtar respublika") of Nakhchivan, which subdivides into 7 rayons and a city. The President of Azerbaijan appoints the governors of these units, while the government of Nakhchivan is elected and approved by the parliament of Nakhchivan Autonomous Republic.
Government and politics.
The structural formation of Azerbaijan's political system was completed by the adoption of the new Constitution on 12 November 1995. According to the Article 23 of Constitution, the state symbols of the Azerbaijan Republic are the flag, the coat of arms and the national anthem. The state power in Azerbaijan is limited only by law for internal issues, but for international affairs is additionally limited by the provisions of international agreements.
The government of Azerbaijan is based on the separation of powers among the legislative, executive and judicial branches. The legislative power is held by the unicameral National Assembly and the Supreme National Assembly in the Nakhchivan Autonomous Republic. Parliamentary elections are held every five years, on the first Sunday of November. The accuracy of the election results is checked and confirmed by the Constitutional Court. The laws enacted by the National Assembly, unless specified otherwise, go into effect on the day of their publication.
The executive power is held by the President, who is elected for a 5-year term by direct elections. The president is authorized to form the Cabinet, an inferior executive body, subordinated to him. The Cabinet of Azerbaijan consists primarily of the Prime Minister, his Deputies and Ministers. The president does not have the right to dissolve the National Assembly, but he has the right to veto its decisions. To override the presidential veto, the parliament must have a majority of 95 votes. The judicial power is vested in the Constitutional Court, Supreme Court and the Economic Court. The President nominates the judges in these courts.
The Security Council is the deliberative body under the president, and he organizes it according to the Constitution. It was established on 10 April 1997. The administrative department is not a part of the president's office but manages the financial, technical and pecuniary activities of both the president and his office.
Although Azerbaijan has held several elections since regaining its independence and it has many of the formal institutions of democracy, it remains classified as "not free" (on border with "partly free") in Freedom House's Freedom in the World 2009 survey.
Foreign relations.
The short-lived Azerbaijan Democratic Republic succeeded in establishing diplomatic relations with six countries, sending diplomatic representatives to Germany and Finland. The process of international recognition of Azerbaijan's independence from the collapsing Soviet Union lasted roughly one year. The most recent country to recognize Azerbaijan was Bahrain, on 6 November 1996. Full diplomatic relations, including mutual exchanges of missions, were first established with Turkey, Pakistan, the United States, Iran and Israel.
Azerbaijan has diplomatic relations with 158 countries so far and holds membership in 38 international organizations. It holds observer status in the Non-Aligned Movement and World Trade Organization and is a correspondent at the International Telecommunication Union. The Azerbaijani diaspora is found in 36 countries, and in turn there are dozens of centers for ethnic minorities inside Azerbaijan, including the (German cultural society "Karelhaus", Slavic cultural center, Azerbaijani-Israeli community, Kurdish cultural center, International Talysh Association, Lezgin national center "Samur", Azerbaijani-Tatar community, Crimean Tatars society, etc.). On 9 May 2006 Azerbaijan was elected to membership in the newly established Human Rights Council by the United Nations General Assembly. The term of office began on 19 June 2006.
Foreign policy priorities of Azerbaijan include: first of all, the restoration of its territorial integrity; elimination of the consequences of the loss of Nagorno-Karabakh and seven other regions of Azerbaijan; development of good-neighbourly and mutually advantageous relations with neighbouring countries; promotion of security and stability in the region; integration into European and Transatlantic security and cooperation structures; and promotion of transregional economic, energy and transportation projects.
The Azeri Government, in late 2007, stated that the long-standing dispute over the Armenian-occupied territory of Nagorno-Karabakh is almost certain to spark a new war if it remains unresolved. The Government is in the process of increasing its military budget, as its oil and gas revenues bring a torrent of cash into its coffers. Furthermore, economic sanctions by Turkey to the west and by Azerbaijan itself to the east have combined to greatly erode Armenia's economy, leading to steep increases in prices for basic commodities and a great decline in the Armenian state revenues.
Azerbaijan is an active member of international coalitions fighting international terrorism. The country is contributing to peacekeeping efforts in Kosovo, Afghanistan and Iraq. Azerbaijan is an active member of NATO's “Partnership for Peace” program. It also maintains good relations with the European Union and could potentially one day apply for membership.
Military.
The history of the modern Azerbaijan army dates back to Azerbaijan Democratic Republic in 1918, when the National Army of the newly formed Azerbaijan Democratic Republic was created on 26 June 1918. When Azerbaijan gained independence after the collapse of the Soviet Union, the Armed Forces of the Republic of Azerbaijan were created according to the Law on the Armed Forces of 9 October 1991. The original date of the establishment of the short-lived National Army is celebrated as Army Day (26 June) in today's Azerbaijan.
Initially, the equipment and facilities of Azerbaijan's army were those of the Soviet 4th Army. The Armed Forces have three branches, according to the CIA World Fact Book: Land Forces, Air Force and Air Defence Force (a united branch), Navy. Besides the Armed Forces there are several military sub-groups that can be involved in state defence when needed. These are the Internal Troops of the Ministry of Internal Affairs and forces of the State Border Service, which includes the Coast Guard as well. The Azerbaijan National Guard is a further paramilitary force. It operates as a semi-independent entity of the Special State Protection Service, an agency subordinate to the President.
Azerbaijan adheres to the Treaty on Conventional Armed Forces in Europe and has signed all major international arms and weapons treaties. Azerbaijan closely cooperates with NATO in programs such as Partnership for Peace and Individual Partnership Action Plan. Azerbaijan has deployed 151 of its Peacekeeping Forces in Iraq and another 184 in Afghanistan.
The military expenditures of Azerbaijan for 2009 are set at $2.46 billion USD. Azerbaijan has its own Defense Industry, which manufactures small arms, artillery systems, tanks, armors and noctovision devices, aviation bombs, pilotless vehicles, various military vehicles and military planes and helicopters. Azerbaijan's Armed Forces have a training cooperation partnership with the Oklahoma Army National Guard.
Economy.
After gaining independence in 1991, Azerbaijan became a member of the International Monetary Fund, the World Bank, the European Bank for Reconstruction and Development, the Islamic Development Bank and the Asian Development Bank. The banking system of Azerbaijan consists of the Central Bank of Azerbaijan, commercial banks and non-banking credit organizations. The National (now Central) Bank was created in 1992 based on the Azerbaijan State Savings Bank, an affiliate of the former State Savings Bank of the USSR. The Central Bank serves as Azerbaijan's central bank, empowered to issue the national currency, the Azerbaijani manat, and to supervise all commercial banks. Two major commercial banks are the state-owned International Bank of Azerbaijan and the United Universal Joint-Stock Bank.
Pushed up by spending and demand growth, the 2007 Q1 inflation rate reached 16.6%. Nominal incomes and monthly wages climbed 29% and 25% respectively against this figure, but price increases in non-oil industry encouraged inflation in the country. Azerbaijan shows some signs of the so-called "Dutch disease" because of the fast growing energy sector, which causes inflation and makes non-energy exports more expensive.
Two thirds of Azerbaijan is rich in oil and natural gas. The region of the Lesser Caucasus accounts for most of the country's gold, silver, iron, copper, titanium, chromium, manganese, cobalt, molybdenum, complex ore and antimony. In September 1994, a 30-year contract was signed between the State Oil Company of Azerbaijan Republic (SOCAR) and 13 oil companies, among them Amoco, BP, Exxon, LUKoil and Statoil. As Western oil companies are able to tap deepwater oilfields untouched by the Soviet exploitation, Azerbaijan is considered one of the most important spots in the world for oil exploration and development. Meanwhile the State Oil Fund was established as an extra-budgetary fund to ensure the macroeconomic stability, transparency in the management of oil revenue, and the safeguarding of resources for future generations.
At the beginning of 2007 there were 4,755,100 hectares of utilized agricultural area. In the same year the total wood resources counted 136 million m³. Azerbaijan's agricultural scientific research institutes are focused on meadows and pastures, horticulture and subtropical crops, green vegetables, viticulture and wine-making, cotton growing and medicinal plants. In some lands it is profitable to grow grain, potatoes, sugar beets, cotton and tobacco. The Caspian fishing industry is concentrated on the dwindling stocks of sturgeon and beluga. In 2002 the Azerbaijani merchant marine had 54 ships.
Some portions of most products that were previously imported from abroad have begun to be produced locally (among them are Coca Cola by Coca Cola Bottlers LTD, beer by Baki-Kastel, parquet by Nehir and oil pipes by EUPEC Pipe Coating Azerbaijan).
Azerbaijan is also an important economic hub in the transportation of raw materials. The Baku-Tbilisi-Ceyhan pipeline (BTC) became operational in May 2006 and extends more than 1,774 kilometers through the territories of Azerbaijan (440 km), Georgia (260 km) and Turkey (1114 km). The BTC is designed to transport up to 50 million tons of crude oil annually and carries oil from the Caspian Sea oilfields to global markets. The South Caucasus Pipeline, also stretching through the territory of Azerbaijan, Georgia and Turkey, became operational at the end of 2006 and offers additional gas supplies to the European market from the Shah Deniz gas field. It is expected to produce up to 296 billion cubic metres of natural gas per year. Azerbaijan also plays a major role in the EU-sponsored Silk Road Project.
Azeriqaz, a sub-company of the State Oil Company of Azerbaijan, intends to ensure full gasification of the country by 2021.
Tourism.
Tourism is an important part of the economy of Azerbaijan. The country's large abundance of natural and cultural attractions make it an attractive destination of visitors. The country was a well-known tourist spot in the 1980s, yet, the Nagorno-Karabakh War during the 1990s crippled the tourist industry and negatively impacted the image of Azerbaijan as a tourist destination.
It was not until 2000s that the tourism industry began to recover, and the country has since experienced a high rate of growth in the number of tourist visits and overnight stays.
The Government of Azerbaijan has set the development of Azerbaijan as an elite tourist destination a top priority. It is a national strategy to make tourism a major, if not the single largest, contributor to the Azerbaijani economy.
Transportation and communications.
The convenient location of Azerbaijan on the crossroad of major international traffic arteries, such as the Silk Road and the South-North corridor, highlights the strategic importance of transportation sector for the country’s economy.
In 2002 the Azerbaijani government established the Ministry of Transport with a broad range of policy and regulatory functions. In the same year, the country became a member of the Vienna Convention on Road Traffic. The highest priority being; upgrading the transport network and transforming transportation services into one of the key comparative advantages of the country, as this would be highly conducive to the development of other sectors of the economy.
Broad gauge railways in 2005 stretched for and electrified railways numbered. By 2006, there were 36 airports and one heliport.
The transport sector in Azerbaijan includes roads, railways, aviation, and maritime transport.
The economy of Azerbaijan has been markedly stronger in recent years and, not surprisingly, the country has been making progress in developing its telecoms sector. Nonetheless, it still faces problems. These include poor infrastructure and an immature telecom regulatory regime. The Ministry of Communications & Information Technologies (MCIT), as well as being an operator through its role in Aztelekom, is both a policy-maker and regulator. A boom in oil and gas exports has boosted the economy, reducing the country’s dependence on international aid.
In 2002 Azerbaijan led the way in per capita mobile phone use within the CIS. Public pay phones are available for local calls and require the purchase of a token from the telephone exchange or some shops and kiosks. Tokens allow a call of indefinite duration. As of 2005, there were 1,091,400 main telephone lines and 1,036,000 internet users. There are three GSM: Azerfon (Nar Mobile), Bakcell and Azercell mobile network operators and one CDMA.
Demographics.
From the total population of about 8 million people as of April 2006, there were 4,380,000 (nearly 51%) city dwellers and a rural population of 4,060,000 (49%). 51% of the total population were female. The sex ratio for total population in that year was therefore 0.94 males per female.
The 2006 population growth rate was 0.66%, compared to 1.14% worldwide. A significant factor restricting the population growth is rather a high level of migration. As many as 3 million Azeris, many of them guest workers, live in Russia. In 2006 Azerbaijan saw migration of −4.38/1,000 persons.
The highest morbidity in 2005 was from respiratory diseases (806.9 diseases per 10,000 of total population).
In 2005, the highest morbidity for infectious and parasitic diseases was noted among influenza and acute respiratory infections (4168,2 per 100,000 population). 2007 estimate for total life expectancy is 66 years, 70.7 years for women and 61.9 for men. With 800,000 refugees and IDPs, Azerbaijan has the largest internally displaced population in the region, and, as of 2006, had the highest per capita IDP population in the world.
The ethnic composition of the population according to the 1999 population census: 90.6% Azeris, 2.2% Lezgins, 1.8% Russians, 1.5% Armenians (Almost all live in the break-away region of Nagorno-Karabakh), 1.0% Talysh (disputed as too low by Talysh nationalists), 0.6% Avars, 0.5% Turks, 0.4% Tatars, 0.4% Ukrainians, 0.2% Tsakhur, 0.2% Georgians, 0.13% Kurds, 0.13% Tats, 0.1% Jews, 0.05% Udins, other 0.2%. Many Russians left Azerbaijan during the 1990s. According to the 1989 census, there were 392,000 ethnic Russians in Azerbaijan, or 5.6% of the population. According to the statistics, about 390,000 Armenians lived in Azerbaijan in 1989.
Although Azerbaijani (also called "Azeri") is the most widely spoken language in the country and is spoken by about a quarter of the population of Iran, there are 13 other languages spoken natively in the country. Some of these languages are very small communities, others are more vital. Azerbaijani is a Turkic language which belongs to the Altaic family and is mutually intelligible with Turkish. The language is written with a modified Latin alphabet today, but was earlier written in the Arabic alphabet (until 1929), in the Uniform Turkic Alphabet (1929–1939), and in the Cyrillic alphabet (1939–1991). The changes in alphabet have been largely molded by religious and political forces.
Iranian Azeris are the largest minority in Iran. The CIA World Factbook estimates Iranian Azeris as comprising nearly 16 million, or 24% of Iran's population.
Religion.
Approximately 95% of the population of Azerbaijan is Muslim. There are many other faiths practiced among the different ethnic groups within the country. By article 48 of its Constitution, Azerbaijan is a secular state and ensures religious freedom. Of the nation's religious minorites, Christians comprise 3% to 4% of the population, of whom most are Russian, Georgian and Armenian Orthodox (Almost all Armenians live in the break-away region of Nagorno-Karabakh).
In 2003 there were 250 Roman Catholics. Other Christian denominations as of 2002 include Lutherans, Baptists and Molokans. There are also Jewish, Bahá'í, Hare Krishna and Jehovah's Witnesses communities, as well as adherents of the Nehemiah Church, Star in the East Church and the Cathedral of Praise Church. Zoroastrianism had a long history in Azerbaijan, evident in sites such as the Fire Temple of Baku, and along with Manichean. It is estimated that the Zoroastrian community of Azerbaijan numbers around 2,000.
According to the recent Gallup Poll Azerbaijan is one of the most irreligious countries in the world with about 50% of respondents indicating the importance of religion in their life as little or none. Even so, religious tolerance has been threatened in Azerbaijan, though it continues a signatory to the Convention for the Protection of Human Rights and Fundamental Freedoms. A number of nationals who are Jehovah's Witnesses have been harassed, detained, jailed and in some cases physically assaulted by police because of their religious activity. Jehovah's Witnesses are entitled to protection of freedom of religion under Articles 9, 10, and 11 of the aforementioned Convention. In some cases the defendants have been cleared of all charges.
Culture.
Azerbaijani culture has developed as a result of many influences. Today, Western influences, including globalized consumer culture, are strong.
Azerbaijan folk consists of Azerbaijanis, the representative part of society, as well as of nations and ethnic groups, compactly living in various areas of the country. Azerbaijani national and traditional dresses are the Chokha and Papakhi. There are radio broadcasts in Russian, Armenian, Georgian, Kurdish, Lezgin and Talysh languages, which are financed from the state budget. Some local radio stations in Balakən and Xaçmaz organize broadcasts in Avar and Tat. In Baku several newspapers are published in Russian, Kurdish ("Dengi Kurd"), Lezgin ("Samur") and Talysh languages. Jewish society "Sokhnut" publishes the newspaper "Aziz".
Architecture.
Azerbaijani architecture typically combines elements of East and West. Many ancient architectural treasures such as the Maiden Tower and Palace of the Shirvanshahs in the Walled City of Baku survive in modern Azerbaijan. Entries submitted on the UNESCO World Heritage tentative list include the Gobustan State Reserve, the Fire Temple of Baku, the Momine Khatun Mausoleum and the Palace of Shaki Khans in Shaki.
Among other medieval architectural treasures reflecting the influence of several schools are the Shirvan Shahs' palace in Baku, the palace of the Shaki Khan's in the town of Shaki in north-central Azerbaijan, the Surakhany Temple on the Absheron Peninsula, a number of bridges spanning the Aras River, and several mausoleums. In the nineteenth and early twentieth centuries, little monumental architecture was created, but distinctive residences were built in Baku and elsewhere. Among the most recent architectural monuments, the Baku subways are noted for their lavish decor.
Cinema.
The film industry in Azerbaijan dates back to 1898. In fact, Azerbaijan was among the first countries involved in cinematography. When the Lumière brothers of France premiered their first motion picture footage in Paris on December 28, 1895, little did they know how rapidly it would ignite a new age of photographic documentation. These ingenuous brothers invented an apparatus, patented in February 1895, which they called the "Cinématographe" (from which the word "cinematography" is derived).
It's not surprising that this apparatus soon showed up in Baku – at the turn of the 19th century, this bay town on the Caspian was producing more than 50 percent of the world's supply of oil. Just like today, the oil industry attracted foreigners eager to invest and to work.
In 1919, during the Azerbaijan Democratic Republic, a documentary called The Celebration of the Anniversary of Azerbaijani Independence was filmed on Azerbaijan's independence day, May 28, and premiered in June 1919 at several theatres in Baku.
After the Soviet power was established in 1920, Nariman Narimanov, Chairman of the Revolutionary Committee of Azerbaijan, signed a decree nationalizing Azerbaijan's cinema.
In 1991, after Azerbaijan gained its independence from the Soviet Union, first Baku International Film Festival East-West was held in Baku.
Cuisine.
Azerbaijani cuisine, throughout the centuries, has been influenced by the foods of different cultures due to political and economic processes in Azerbaijan. Still, today's Azerbaijani cuisine has distinctive and unique features. Many foods that are indigenous to the country can now be seen in the cuisines of other cultures. For the Azerbaijanis, food is an important part of the country's culture and is deeply rooted in the history, traditions and values of the nation.
Azerbaijani cuisine is an important part of the country's culture. Climatic diversity and fertility of the land are reflected in the national dishes, which are based on fish from the Caspian Sea, local meat (mainly mutton and beef), and an abundance of seasonal vegetables and greens. Saffron-rice plov is the flagship food in Azerbaijan and black tea is the national beverage.
Folk dance.
There are a number of Azerbaijani dances, these folk dances of the Azerbaijani people are old and extremely melodious. It is performed at formal celebrations and the dancers wear festival clothes or Chokha cloaks. It has a very fast rhythm, so the dancer must have inherent skill.
Azerbaijan’s national dance shows the characteristics of the Azerbaijani nation. These dances differ from other dances with its quick temp and optimism. And this talks about nation’s braveness.
The national clothes of Azerbaijan are well preserved within the national dances.
Azerbaijan is a country where national traditions are well preserved. In Azerbaijan where are a lot of traditions. Novruz holiday (novruz is translated as "a new day") is the most ancient and cherished holiday of a New Year and spring. It is celebrated on the day of vernal equinox – March 21–22. Novruz is the symbol of nature renewal and fertility. Agrarian peoples of Middle East have been celebrating Novruz since ancient times.
Folk art.
The Azeris have a rich and distinctive culture, a major part of which is decorative and applied art. This form of art is represented by a wide range of handicrafts, such as chasing, jeweler, engraving in metal, carving in wood, stone and bone, carpet-making, lasing, pattern weaving and printing, knitting and embroidery. Each of these types of decorative art, evidence of the and endowments of the Azerbaijan nation, is very much in favor here. Many interesting facts pertaining to the development of arts and crafts in Azerbaijan were reported by numerous merchants, travelers and diplomats who had visited these places at different times.
Music.
Music of Azerbaijan builds on folk traditions that reach back nearly 1,000 years. For centuries Azerbaijani music has evolved under the badge of monody, producing rhythmically diverse melodies. Azerbaijani music has a branchy mode system, where chromatisation of major and minor scales is of great importance.
According to The Grove Dictionary of Music and Musicians "In terms of ethnicity, culture and religion the Azeri are musically much closer to Iran than Turkey."
"Mugham", "Meykhana" and "Ashik art" are one of the many musical traditions of Azerbaijan. Mugham is usually a suite with poetry and instrumental interludes. When performing Mugam, the singers have to transform their emotions into singing and music. Mugham singer Alim Qasimov is revered as one of the five best singers of all time.
In contrast to the "mugam" traditions of Central Asian countries, Azeri "mugam" is more free-form and less rigid; it is often compared to the improvised field of jazz.
UNESCO proclaimed the Azerbaijani "mugam" tradition a Masterpiece of the Oral and Intangible Heritage of Humanity on 7 November 2003.
Meykhana is a kind of traditional Azeri distinctive folk unaccompanied song, usually performed by several people improvising on a particular subject.
Among national musical instruments there are fourteen string instruments, eight percussion instruments and six wind instruments.
Ashik is a mystic troubadour or traveling bard who sings and plays the saz. This tradition has its origin in the Shamanistic beliefs of ancient Turkic peoples. Ashiks' songs are semi-improvised around common bases. Azerbaijan’s ashik art was included in the list of Intangible Cultural Heritage by the UNESCO on September 30, 2009.
Azerbaijan made its debut appearance at the Eurovision Song Contest 2008, and placed 8th among 43 contestants.
The country's entry in the Eurovision Song Contest 2009 by AySel and Arash won the 3rd place.
Sport.
Sport in Azerbaijan has ancient roots, and even now, both traditional and modern sports are still practiced. Freestyle wrestling has been traditionally regarded as Azerbaijan's national sport, however today, the most popular sports in Azerbaijan are football (soccer) and chess.
Backgammon, a game that has ancient roots in Persian Empire, plays a major role in Azerbaijani culture. This game is very popular in Azerbaijan and is widely played among the local public. There are also different variations of backgammon developed and analysed by Azerbaijani experts.
Azerbaijan is known as one of the chess superpowers; despite the collapse of the Soviet Union, chess is still extremely popular. Notable Azerbaijani chess players include Teimour Radjabov, Shahriyar Mammadyarov, Vladimir Makogonov, Gary Kasparov, Vugar Gashimov and Zeinab Mamedyarova. Azerbaijan has also hosted many international chess tournaments and competitions and became European Team Chess Championship winners in 2009.
Futsal is another popular sport in Azerbaijan. Azerbaijan national futsal team reached the fourth place in 2010 UEFA Futsal Championship, while domestic club Araz Naxçivan also advanced to the semi-finals of UEFA Futsal Cup in 2010.
Other Azerbaijani well-known athletes are Namig Abdullayev, Rovshan Bayramov, Mariya Stadnik and Farid Mansurov in wrestling, Ramil Guliyev in athletics, Elnur Mammadli and Movlud Miraliyev in judo, Rafael Aghayev in karate, Valeriya Korotenko and Natalya Mammadova in volleyball and K-1 fighter Zabit Samedov.
---END.OF.DOCUMENT---

Amateur astronomy.
Amateur astronomy, also called backyard astronomy, is a hobby whose participants enjoy watching the night sky (and the day sky too, for sunspots, eclipses, etc.), and the plethora of objects found in it, mainly with portable telescopes and binoculars. Even though scientific research is not their main goal, many amateur astronomers make a contribution to astronomy by monitoring variable stars, tracking asteroids and discovering transient objects, such as comets. Such efforts are one of the relatively few ways interested amateurs can still make useful contributions to scientific knowledge.
Overview.
The typical amateur astronomer is one who does not depend on the field of astronomy as a primary source of income or support, and does not have a professional degree or advanced academic training in the subject. Many amateurs are beginners, while others have a high degree of experience in astronomy and often assist and work alongside professional astronomers.
Amateur astronomy is usually associated with viewing the night sky when most celestial objects and events are visible, but sometimes amateur astronomers also operate during the day for events such as sunspots and solar eclipses. Amateur astronomers often look at the sky using nothing more than their eyes, but common tools for amateur astronomy include portable telescopes and binoculars.
People have studied the sky throughout history in an amateur framework, without any formal method of funding. It is only within about the past century, however, that amateur astronomy has become an activity clearly distinguished from professional astronomy, and other related activities.
Amateur astronomy objectives.
Collectively, amateur astronomers observe a variety of celestial objects and phenomena. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep sky objects such as star clusters, galaxies, and nebulae. Many amateurs like to specialise in observing particular objects, types of objects, or types of events which interest them. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Astrophotography has become more popular for amateurs in recent times, as relatively sophisticated equipment, such as high quality CCD cameras, has become more affordable.
Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. The pioneer of amateur radio astronomy was Karl Jansky who started observing the sky at radio wavelengths in the 1930s, and interest has increased over time. Non-visual amateur astronomy includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. Some amateur astronomers use home-made radio telescopes, while others use radio telescopes that were originally built for astronomy research but have since been made available for use by amateurs. The One-Mile Telescope is one such example.
Common tools.
Amateur astronomers use a range of instruments to study the sky, depending on a combination of their interests and resources. Methods include simply looking at the night sky with the naked eye, using binoculars, and using a variety of optical telescopes of varying power and quality, as well as additional sophisticated equipment, such as cameras, to study light from the sky in both the visual and non-visual parts of the spectrum. Commercial telescopes are available new and used, but in some places it is also common for amateur astronomers to build (or commission the building of) their own custom telescope. Some people even focus on amateur telescope making as their primary interest within the hobby of amateur astronomy.
Although specialized and experienced amateur astronomers tend to acquire more specialized and more powerful equipment over time, relatively simple equipment is often preferred for certain tasks. Binoculars, for instance, although generally of lower power than the majority of telescopes, also tend to provide a wider field of view, which is preferable for looking at some objects in the night sky.
Amateur astronomers also use star charts that, depending on experience and intentions, may range from simple planispheres through to detailed charts of very specific areas of the night sky. A range of astronomy software is available and used by amateur astronomers, including software that generates maps of the sky, software to assist with astrophotography, observation scheduling software, and software to perform various calculations pertaining to astronomical phenomena.
Amateur astronomers often like to keep records of their observations, which usually takes the form of an observing log. Observing logs typically record details about which objects were observed and when, as well as describing the details that were seen. Sketching is sometimes used within logs, and photographic records of observations have also been used in recent times.
The Internet is an essential tool of amateur astronomers. Almost all astronomy clubs, even those with very few members, have a web site. The popularity of CCD imaging among amateurs has led to large numbers of web sites being written by individuals about their images and equipment. Much of the social interaction of amateur astronomy occurs on mailing lists or discussion groups. Discussion group servers host numerous astronomy lists. A great deal of the commerce of amateur astronomy, the buying and selling of equipment, occurs online. Many amateurs use online tools to plan their nightly observing sessions using tools such as the Clear Sky Chart.
Common techniques.
While a number of interesting celestial objects are readily identified by the naked eye, sometimes with the aid of a star chart, many others are so faint or inconspicuous that technical means are necessary to locate them. Many methods are used in amateur astronomy, but most are variations of a few specific techniques.
Star hopping.
Star hopping is a method often used by amateur astronomers with low-tech equipment such as binoculars or a manually driven telescope. It involves the use of maps (or memory) to locate known landmark stars, and "hopping" between them, often with the aid of a finderscope. Because of its simplicity, star hopping is a very common method for finding objects that are close to naked-eye stars.
More advanced methods of locating objects in the sky include telescope mounts with "setting circles", which assist with pointing telescopes to positions in the sky that are known to contain objects of interest, and "GOTO telescopes", which are fully automated telescopes that are capable of locating objects on demand (having first been calibrated).
Setting circles.
Setting circles are angular measurement scales that can be placed on the two main rotation axes of some telescopes. Since the widespread adoption of digital setting circles, any classical engraved setting circle is now specifically identified as an "analog setting circle" (ASC). By knowing the coordinates of an object (usually given in equatorial coordinates), the telescope user can use the setting circle to align the telescope in the appropriate direction before looking through its eyepiece. A computerized setting circle is called a "digital setting circle" (DSC). Although digital setting circles can be used to display a telescope's RA and Dec coordinates, they are not simply a digital read-out of what can be seen on the telescope's analog setting circles. As with go-to telescopes, digital setting circle computers (commercial names include Argo Navis, Sky Commander, and NGC Max) contain databases of tens of thousands of celestial objects and projections of planet positions.
To find an object, such as globular cluster NGC 6712, one does not need to look up the RA and Dec coordinates in a book, and then move the telescope to those numerical readings. Rather, the object is chosen from the database and arrow markers appear in the display which indicate the direction to move the telescope. The telescope is moved until the distance value reaches zero. When both the RA and Dec axes are thus "zeroed out", the object should be in the eyepiece. The user therefore does not have to go back and forth from some other database (such as a book or laptop) to match the desired object's listed coordinates to the coordinates on the telescope. However, many DSCs, and also go-to systems, can work in conjunction with laptop sky programs.
Computerized systems provide the further advantage of computing coordinate precession. Traditional printed sources are subtitled by the "epoch" year, which refers to the positions of celestial objects at a given time to the nearest year (e.g., J2005, J2007). Most such printed sources have been updated for intervals of only about every fifty years (e.g., J1900, J1950, J2000). Computerized sources, on the other hand, are able to calculate the right ascension and declination of the "epoch of date" to the exact instant of observation.
GoTo telescopes.
GOTO telescopes have become more popular since the 1980s as technology has improved and prices have been reduced. With these computer-driven telescopes, the user typically enters the name of the item of interest and the mechanics of the telescope point the telescope towards that item automatically. They have several notable advantages for amateur astronomers intent on research. For example, GOTO telescopes tend to be faster for locating items of interest than star hopping, allowing more time for studying of the object. GOTO also allows manufacturers to add equatorial tracking to mechanically simpler alt-azimuth telescope mounts, allowing them to produce an over all less expensive product.
Imaging techniques.
Amateur astronomers engage in many imaging techniques including film and CCD astrophotography. Because CCD imagers are linear, image processing may be used to subtract away the effects of light pollution, which has increased the popularity of astrophotography in urban areas.
Scientific research.
Scientific research is most often not the "main" goal for many amateur astronomers, unlike professional astronomy. Work of scientific merit is possible, however, and many amateurs successfully contribute to the knowledge base of professional astronomers. Astronomy is sometimes promoted as one of the few remaining sciences for which amateurs can still contribute useful data. To recognize this, the Astronomical Society of the Pacific annually gives Amateur Achievement Awards for significant contributions to astronomy by amateurs.
The majority of scientific contributions by amateur astronomers are in the area of data collection. In particular, this applies where large numbers of amateur astronomers with small telescopes are more effective than the relatively small number of large telescopes that are available to professional astronomers. Several organizations, such as the, exist to help coordinate these contributions.
Amateur astronomers often contribute toward activities such as monitoring the changes in brightness of variable stars, helping to track asteroids, and observing occultations to determine both the shape of asteroids and the shape of the terrain on the apparent edge of the Moon as seen from Earth. With more advanced equipment, but still cheap in comparison to professional setups, amateur astronomers can measure the light spectrum emitted from astronomical objects, which can yield high-quality scientific data if the measurements are performed with due care. A relatively recent role for amateur astronomers is searching for overlooked phenomena (e.g., Kreutz Sungrazers) in the vast libraries of digital images and other data captured by Earth and space based observatories, much of which is available over the Internet.
In the past and present, amateur astronomers have played a major role in discovering new comets. Recently however, funding of projects such as the Lincoln Near-Earth Asteroid Research and Near Earth Asteroid Tracking projects has meant that "most" comets are now discovered by automated systems, long before it is possible for amateurs to see them.
Societies.
There is a large number of amateur astronomical societies around the world that serve as a meeting point for those interested in amateur astronomy, whether they be people who are actively interested in observing or "armchair astronomers" who may be simply interested in the topic. Societies range widely in their goals, depending on a variety of factors such as geographic spread, local circumstances, size, and membership. For instance, a local society in the middle of a large city may have regular meetings with speakers, focusing less on observing the night sky if the membership is less able to observe due to factors such as light pollution.
It is common for local societies to hold regular meetings, which may include activities such as star parties or presentations. Societies are also a meeting point for people with particular interests, such as amateur telescope making.
---END.OF.DOCUMENT---

Aikido.
is a Japanese martial art developed by Morihei Ueshiba as a synthesis of his martial studies, philosophy, and religious beliefs. Aikido is often translated as "the Way of unifying (with) life energy" or as "the Way of harmonious spirit." Ueshiba's goal was to create an art that practitioners could use to defend themselves while also protecting their attacker from injury.
Aikido is performed by blending with the motion of the attacker and redirecting the force of the attack rather than opposing it head-on. This requires very little physical energy, as the "aikidōka" (aikido practitioner) "leads" the attacker's momentum using entering and turning movements. The techniques are completed with various throws or joint locks. Aikido can be categorized under the general umbrella of grappling arts.
Aikido derives mainly from the martial art of Daitō-ryū Aiki-jūjutsu, but began to diverge from it in the late 1920s, partly due to Ueshiba's involvement with the Ōmoto-kyō religion. Ueshiba's early students' documents bear the term "aiki-jūjutsu". Many of Ueshiba's senior students have different approaches to aikido, depending on when they studied with him. Today aikido is found all over the world in a number of styles, with broad ranges of interpretation and emphasis. However, they all share techniques learned from Ueshiba and most have concern for the well-being of the attacker.
Etymology and basic philosophy.
The term 'aiki' does not readily appear in the Japanese language outside the scope of Budo. This has led to many possible interpretations of the word.
合　is mainly used in compounds to mean 'combine, unite, join together, meet' examples being 合同(combined/united)　合成(composition)　結合(unite/combine/join together)　連合(union/alliance/association)　統合(combine/unify)　合意(mutual agreement). As well as an idea of reciprocalality,　知り合う(to get to know one another)　話し合い(talk/discussion/negotiation) 待ち合わせる(meet by appointment).
気　is often used as a feeling as in 気がする('I feel', as in terms of thinking but with less cognitive reasoning) 気持ち(feeling/sensation) 気分(mood/morale). Also Energy or force.　電気(electricity) 磁気 (magnetism).
The term connects the practice of aikido with the philosophical concept of "Tao", which can be found in martial arts such as judo and kendo, and in the more peaceful arts such as Japanese calligraphy (), flower arranging () and tea ceremony ().
Therefore from a purely linguistic point of view, we could say Aikido is 'Way of combining forces'. The term refers to the martial arts principle or tactic of blending with an attacker's movements for the purpose of controlling their actions with minimal effort. One applies by understanding the rhythm and intent of the attacker to find the optimal position and timing to apply a counter-technique.
This then is very similar to the principles expressed by Kano Jigoro, when he founded Judo.
Of an interesting note, these kanji are identical to the Korean versions of the characters that form the word hapkido, a Korean martial art. Although there are no known direct connections between the two arts, it is suspected that the founders of both arts trained in Daitō-ryū Aiki-jūjutsu.
History.
Aikido was created by Morihei Ueshiba (, 14 December 1883–26 April 1969), referred to by some aikido practitioners as ("Great Teacher"). Ueshiba envisioned aikido not only as the synthesis of his martial training, but also an expression of his personal philosophy of universal peace and reconciliation. During Ueshiba's lifetime and continuing today, aikido has evolved from the "koryū" (old-style martial arts) that Ueshiba studied into a wide variety of expressions by martial artists throughout the world.
Initial development.
Ueshiba developed aikido primarily during the late 1920s through the 1930s through the synthesis of the older martial arts that he had studied. The core martial art from which aikido derives is Daitō-ryū aiki-jūjutsu, which Ueshiba studied directly with Takeda Sokaku, the reviver of that art. Additionally, Ueshiba is known to have studied Tenjin Shin'yō-ryū with Tozawa Tokusaburō in Tokyo in 1901, Gotōha Yagyū Shingan-ryū under Nakai Masakatsu in Sakai from 1903 to 1908, and judo with Kiyoichi Takagi (, 1894–1972) in Tanabe in 1911.
The art of Daitō-ryū is the primary technical influence on aikido. Along with empty-handed throwing and joint-locking techniques, Ueshiba incorporated training movements with weapons, such as those for the spear (), short staff (), and perhaps the. However, aikido derives much of its technical structure from the art of swordsmanship ().
Ueshiba moved to Hokkaidō in 1912, and began studying under Takeda Sokaku in 1915. His official association with Daitō-ryū continued until 1937. However, during the latter part of that period, Ueshiba had already begun to distance himself from Takeda and the Daitō-ryū. At that time Ueshiba was referring to his martial art as "Aiki Budō". It is unclear exactly when Ueshiba began using the name "aikido", but it became the official name of the art in 1942 when the Greater Japan Martial Virtue Society () was engaged in a government sponsored reorganization and centralization of Japanese martial arts.
Religious influences.
After Ueshiba left Hokkaidō in 1919, he met and was profoundly influenced by Onisaburo Deguchi, the spiritual leader of the Ōmoto-kyō religion (a neo-Shinto movement) in Ayabe. One of the primary features of Ōmoto-kyō is its emphasis on the attainment of utopia during one's life. This was a great influence on Ueshiba's martial arts philosophy of extending love and compassion especially to those who seek to harm others. Aikido demonstrates this philosophy in its emphasis on mastering martial arts so that one may receive an attack and harmlessly redirect it. In an ideal resolution, not only is the receiver unharmed, but so is the attacker.
In addition to the effect on his spiritual growth, the connection with Deguchi gave Ueshiba entry to elite political and military circles as a martial artist. As a result of this exposure, he was able to attract not only financial backing but also gifted students. Several of these students would found their own styles of aikido.
International dissemination.
Aikido was first brought to the rest of the world in 1951 by Minoru Mochizuki with a visit to France where he introduced aikido techniques to judo students. He was followed by Tadashi Abe in 1952 who came as the official Aikikai Hombu representative, remaining in France for seven years. Kenji Tomiki toured with a delegation of various martial arts through fifteen continental states of the United States in 1953. Later in that year, Koichi Tohei was sent by Aikikai Hombu to Hawaii, for a full year, where he set up several dojo. This was followed up by several further visits and is considered the formal introduction of aikido to the United States. The United Kingdom followed in 1955; Italy in 1964; Germany and Australia in 1965. Designated "Official Delegate for Europe and Africa" by Morihei Ueshiba, Masamichi Noro arrived in France in September 1961. Today there are aikido dojo available throughout the world.
Proliferation of independent organisations.
The biggest aikido organisation is the Aikikai Foundation which remains under the control of the Ueshiba family. However, aikido has many styles, mostly formed by Morihei Ueshiba's major students.
The earliest independent styles to emerge were Yoseikan Aikido, begun by Minoru Mochizuki in 1931, Yoshinkan Aikido founded by Gozo Shioda in 1955, and Shodokan Aikido, founded by Kenji Tomiki in 1967. The emergence of these styles pre-dated Ueshiba's death and did not cause any major upheavals when they were formalized. Shodokan Aikido, however, was controversial, since it introduced a unique rule-based competition that some felt was contrary to the spirit of aikido.
After Ueshiba's death in 1969, two more major styles emerged. Significant controversy arose with the departure of the Aikikai Hombu Dojo's chief instructor Koichi Tohei, in 1974. Tohei left as a result of a disagreement with the son of the founder, Kisshomaru Ueshiba, who at that time headed the Aikikai Foundation. The disagreement was over the proper role of "ki" development in regular aikido training. After Tohei left, he formed his own style, called Shin Shin Toitsu Aikido, and the organization which governs it, the Ki Society ("Ki no Kenkyūkai").
A final major style evolved from Ueshiba's retirement in Iwama, Ibaraki, and the teaching methodology of long term student Morihiro Saito. It is unofficially referred to as the "Iwama style", and at one point a number of its followers formed a loose network of schools they called Iwama Ryu. Although Iwama style practitioners remained part of the Aikikai until Saito's death in 2002, followers of Saito subsequently split into two groups; one remaining with the Aikikai and the other forming the independent organization the Shinshin Aikishuren Kai, in 2004 around Saito's son Hitohiro Saito.
Today, the major styles of aikido are each run by a separate governing organization, have their own in Japan, and have an international breadth.
Training.
In aikido, as in virtually all Japanese martial arts, there are both physical and mental aspects of training. The physical training in aikido is diverse, covering both general physical fitness and conditioning, as well as specific techniques. Because a substantial portion of any aikido curriculum consists of throws, the first thing most students learn is how to safely fall or roll. The specific techniques for attack include both strikes and grabs; the techniques for defense consist of throws and pins. After basic techniques are learned, students study freestyle defense against multiple opponents, and in certain styles, techniques with weapons.
Fitness.
Physical training goals pursued in conjunction with aikido include controlled relaxation, flexibility, and endurance, with less emphasis on strength training. In aikido, pushing or extending movements are much more common than pulling or contracting movements. This distinction can be applied to general fitness goals for the aikido practitioner.
Certain anaerobic fitness activities, such as weight training, emphasize contracting movements. In aikido, specific muscles or muscle groups are not isolated and worked to improve tone, mass, and power. Aikido-related training emphasizes the use of coordinated whole-body movement and balance similar to yoga or pilates. For example, many dojo begin each class with, which may include stretching and break falls.
Roles of "uke" and "tori".
Aikido training is based primarily on two partners practicing pre-arranged forms ("kata") rather than freestyle practice. The basic pattern is for the receiver of the technique ("uke") to initiate an attack against the person who applies the technique - the 取り "tori", or "shite", (depending on aikido style) also referred to as ("nage" (when applying a throwing technique), who neutralises this attack with an aikido technique.
Both halves of the technique, that of "uke" and that of "nage", are considered essential to aikido training. Both are studying aikido principles of blending and adaptation. "Nage" learns to blend with and control attacking energy, while "uke" learns to become calm and flexible in the disadvantageous, off-balance positions in which "nage" places them. This "receiving" of the technique is called "ukemi". "Uke" continuously seeks to regain balance and cover vulnerabilities (e.g., an exposed side), while "nage" uses position and timing to keep "uke" off-balance and vulnerable. In more advanced training, "uke" will sometimes apply to regain balance and pin or throw "nage".
refers to the act of receiving a technique. Good "ukemi" involves a parry or breakfall that is used to avoid pain or injury, such as joint dislocations.
Initial attacks.
Aikido techniques are usually a defense against an attack; therefore, to practice aikido with their partner, students must learn to deliver various types of attacks. Although attacks are not studied as thoroughly as in striking-based arts, "honest" attacks (a strong strike or an immobilizing grab) are needed to study correct and effective application of technique.
Basic techniques.
The following are a sample of the basic or widely practiced throws and pins. The precise terminology for some may vary between organisations and styles, so what follows are the terms used by the Aikikai Foundation. Note that despite the names of the first five techniques listed, they are not universally taught in numeric order.
Implementations.
Aikido makes use of body movement ("tai sabaki") to blend with "uke". For example, an "entering" ("irimi") technique consists of movements inward towards "uke", while a technique uses a pivoting motion.
Additionally, an technique takes place in front of "uke", whereas an technique takes place to his side; a technique is applied with motion to the front of "uke", and a version is applied with motion towards the rear of "uke", usually by incorporating a turning or pivoting motion. Finally, most techniques can be performed while in a seated posture ("seiza"). Techniques where both "uke" and "nage" are sitting are called "suwari-waza", and techniques performed with "uke" standing and "nage" sitting are called "hanmi handachi".
Thus, from fewer than twenty basic techniques, there are thousands of possible implementations. For instance, "ikkyō" can be applied to an opponent moving forward with a strike (perhaps with an "ura" type of movement to redirect the incoming force), or to an opponent who has already struck and is now moving back to reestablish distance (perhaps an "omote-waza" version). Specific aikido "kata" are typically referred to with the formula "attack-technique(-modifier)". For instance, "katate-dori ikkyō" refers to any "ikkyō" technique executed when "uke" is holding one wrist. This could be further specified as "katate-dori ikkyō omote", referring to any forward-moving "ikkyō" technique from that grab.
"Atemi" () are strikes (or feints) employed during an aikido technique. Some view "atemi" as attacks against "vital points" meant to cause damage in and of themselves. For instance, Gōzō Shioda described using "atemi" in a brawl to quickly down a gang's leader. Others consider "atemi", especially to the face, to be methods of distraction meant to enable other techniques. A strike, whether or not it is blocked, can startle the target and break his or her concentration. The target may also become unbalanced in attempting to avoid the blow, for example by jerking the head back, which may allow for an easier throw.
Many sayings about "atemi" are attributed to Morihei Ueshiba, who considered them an essential element of technique.
Weapons.
Weapons training in aikido traditionally includes the short staff ("jō"), wooden sword ("bokken"), and knife ("tantō"). Today, some schools also incorporate firearms-disarming techniques. Both weapon-taking and weapon-retention are sometimes taught, to integrate armed and unarmed aspects, although some schools of aikido do not train with weapons at all. Others, such as the Iwama style of Morihiro Saito, usually spend substantial time with "bokken" and "jō", practised under the names "aiki-ken", and "aiki-jō", respectively. The founder developed much of empty handed aikido from traditional sword and spear movements, so the practice of these movements is generally for the purpose of giving insight into the origin of techniques and movements, as well as vital practice of these basic building blocks.
Multiple attackers and "randori".
One feature of aikido is training to defend against multiple attackers, often called "taninzudori", or "taninzugake". Freestyle ("randori", or "jiyūwaza") practice with multiple attackers is a key part of most curricula and is required for the higher level ranks. "Randori" exercises a person's ability to intuitively perform techniques in an unstructured environment. Strategic choice of techniques, based on how they reposition the student relative to other attackers, is important in "randori" training. For instance, an "ura" technique might be used to neutralise the current attacker while turning to face attackers approaching from behind.
In Shodokan Aikido, "randori" differs in that it is not performed with multiple persons with defined roles of defender and attacker, but between two people, where both participants attack, defend, and counter at will. In this respect it resembles judo "randori".
Injuries.
In applying a technique during training, it is the responsibility of "nage" to prevent injury to "uke" by employing a speed and force of application that is commensurate with their partner's proficiency in "ukemi". Injuries (especially those to the joints), when they do occur in aikido, are often the result of "nage" misjudging the ability of "uke" to receive the throw or pin.
A study of injuries in the martial arts showed that while the type of injuries varied considerably from one art to the other, the differences in overall rates of injury were much less pronounced. Soft tissue injuries are one of the most common types of injuries found within aikido although a few deaths from repetitive "shihōnage" have been reported.
Mental training.
Aikido training is mental as well as physical, emphasizing the ability to relax the mind and body even under the stress of dangerous situations. This is necessary to enable the practitioner to perform the bold enter-and-blend movements that underlie aikido techniques, wherein an attack is met with confidence and directness. Morihei Ueshiba once remarked that one "must be willing to receive 99% of an opponent's attack and stare death in the face" in order to execute techniques without hesitation. As a martial art concerned not only with fighting proficiency but also with the betterment of daily life, this mental aspect is of key importance to aikido practitioners.
Criticisms.
The most common criticism of aikido is that it suffers from a lack of realism in training. The attacks initiated by "uke" (and which "nage" must defend against) have been criticized as being "weak," "sloppy," and "little more than caricatures of an attack."
Weak attacks from "uke" cause a conditioned response from "nage", and result in underdevelopment of the strength and conditioning needed for the safe and effective practice of both partners. To counteract this, some styles allow students to become less compliant over time but, in keeping with the core philosophies, this is after having demonstrated proficiency in being able to protect themselves and their training partners. Shodokan Aikido addresses the issue by practising in a competitive format. Such adaptations are debated between styles, with some maintaining that there is no need to adjust their methods because either the criticisms are unjustified, or that they are not training for self-defence or combat effectiveness, but spiritual, fitness or other reasons.
Another criticism is that after the end of Ueshiba's seclusion in Iwama from 1942 to the mid 1950s, he increasingly emphasized the spiritual and philosophical aspects of aikido. As a result, strikes to vital points by "nage", entering ("irimi") and initiation of techniques by "nage", the distinction between "omote" (front side) and "ura" (back side) techniques, and the practice of weapons, were all deemphasized or eliminated from practice. Lack of training in these areas is thought to lead to an overall loss of effectiveness by some aikido practitioners.
Alternately, there are some who criticize aikido practitioners for not placing enough importance on the spiritual practices emphasized by Ueshiba. The premise of this criticism is that "O-Sensei’s aikido was not a continuation and extension of the old and has a distinct discontinuity with past martial and philosophical concepts." That is, that aikido practitioners who focus on aikido's roots in traditional jujutsu or "kenjutsu" are diverging from what Ueshiba taught. Such critics urge practitioners to embrace the assertion that "[Ueshiba's] transcendence to the spiritual and universal reality was the fundamentals ["sic"] of the paradigm that he demonstrated."
Ki.
The study of "ki" is a critical component of aikido, and its study defies categorization as either "physical" or "mental" training, as it encompasses both. The original "kanji" for "ki" was (shown right), and is a symbolic representation of a lid covering a pot full of rice; the "nourishing vapors" contained within are "ki".
The character for "ki" is used in everyday Japanese terms, such as, or. "Ki" is most often understood as unified physical and mental intention, however in traditional martial arts it is often discussed as "life energy". Gōzō Shioda's Yoshinkan Aikido, considered one of the "hard styles," largely follows Ueshiba's teachings from before World War II, and surmises that the secret to "ki" lies in timing and the application of the whole body's strength to a single point. In later years, Ueshiba's application of "ki" in aikido took on a softer, more gentle feel. This was his Takemusu Aiki and many of his later students teach about "ki" from this perspective. Koichi Tohei's Ki Society centers almost exclusively around the study of the empirical (albeit subjective) experience of "ki" with students ranked separately in aikido techniques and "ki" development.
Uniforms and ranking.
Aikido practitioners (commonly called "aikidōka" outside of Japan) generally progress by promotion through a series of "grades" ("kyū"), followed by a series of "degrees" ("dan"), pursuant to formal testing procedures. Most aikido organisations use only white and black belts to distinguish rank, but some use various belt colors. Testing requirements vary, so a particular rank in one organization is not always comparable or interchangeable with the rank of another. Some dojos do not allow students to take the test to obtain a dan unless they are 16 or older.
The uniform worn for practicing aikido ("aikidōgi") is similar to the training uniform ("keikogi") used in most other modern martial arts; simple trousers and a wraparound jacket, usually white. Both thick ("judo-style"), and thin ("karate-style") cotton tops are used. Aikido-specific tops are also available with shorter sleeves which reach to just below the elbow.
Most aikido systems also add a pair of wide pleated black or indigo trousers called a "hakama". In many styles its use is reserved for practitioners with black belt ("dan") ranks or for instructors, while others allow all practitioners or female practitioners to wear a "hakama" regardless of rank.
---END.OF.DOCUMENT---

Art.
Art is the process or product of deliberately arranging elements in a way to affect the senses or emotions. It encompasses a diverse range of human activities, creations, and modes of expression, including music, literature, film, sculpture, and paintings. The meaning of art is explored in a branch of philosophy known as aesthetics.
The definition and evaluation of art has become especially problematic since the early 20th century. Richard Wollheim distinguishes three approaches: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans. An object may be characterized by the intentions, or lack thereof, of its creator, regardless of its apparent purpose. A cup, which ostensibly can be used as a container, may be considered art if intended solely as an ornament, while a painting may be deemed craft if mass-produced.
Traditionally, the term "art" was used to refer to any skill or mastery. This conception changed during the Romantic period, when art came to be seen as "a special faculty of the human mind to be classified with religion and science". Generally, art is made with the intention of stimulating thoughts and emotions.
The nature of art has been described by Richard Wollheim as "one of the most elusive of the traditional problems of human culture". It has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as "mimesis" or representation. Leo Tolstoy identified art as a use of indirect means to communicate from one person to another. Benedetto Croce and R.G. Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator. The theory of art as form has its roots in the philosophy of Immanuel Kant, and was developed in the early twentieth century by Roger Fry and Clive Bell. Art as "mimesis" or representation has deep roots in the philosophy of Aristotle. More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation.
Definition of the term.
Britannica Online defines "art" as "the use of skill and imagination in the creation of aesthetic objects, environments, or experiences that can be shared with others." By this definition of the word, artistic works have existed for almost as long as humankind: from early pre-historic art to contemporary art; however, some theories restrict the concept to modern Western societies. Adorno said in 1970, "It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist." The first and broadest sense of "art" is the one that has remained closest to the older Latin meaning, which roughly translates to "skill" or "craft." A few examples where this meaning proves very broad include "artifact", "artificial", "artifice", "medical arts", and "military arts". However, there are many other colloquial uses of the word, all with some relation to its etymology.
The second and more recent sense of the word "art" is as an abbreviation for "creative art" or "fine art". Fine art means that a skill is being used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of the "finer" things. Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it will be considered Commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference. However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically-, spiritually-, or philosophically-motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent.
Art can describe several things: a study of creative skill, a process of using the creative skill, a product of the creative skill, or the audience's experience with the creative skill. The creative arts ("art" as discipline) are a collection of disciplines ("arts") that produce "artworks" ("art" as objects) that are compelled by a personal drive (art as activity) and echo or reflect a message, mood, or symbolism for the viewer to interpret (art as experience). Artworks can be defined by purposeful, creative interpretations of limitless concepts or ideas in order to communicate something to another person. Artworks can be explicitly made for this purpose or interpreted based on images or objects. Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. It is also an expression of an idea and it can take many different forms and serve many different purposes. Although the application of scientific knowledge to derive a new scientific theory involves skill and results in the "creation" of something new, this represents science only and is not categorized as art.
History.
Sculptures, cave paintings, rock paintings, and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found, but the precise meaning of such art is often disputed because so little is known about the cultures that produced them. The oldest art objects in the world—a series of tiny, drilled snail shells about 75,000 years old—were discovered in a South African cave.
Many great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec. Each of these centers of early civilization developed a unique and characteristic style in their art. Because of the size and duration these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times. Some also have provided the first records of how artists worked. For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions.
In Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of Biblical and not material truths, and used styles which showed the higher unseen glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms. Nevertheless a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe.
Renaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three dimensional picture space.
In the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture. Further east, religion dominated artistic styles and forms too. India and Tibet saw emphasis on painted sculptures and dance with religious painting borrowing many conventions from sculpture and tending to bright contrasting colors with emphasis on outlines. China saw many art forms flourish, jade carving, bronzework, pottery (including the stunning terracotta army of Emperor Qin), poetry, calligraphy, music, painting, drama, fiction, etc. Chinese styles vary greatly from era to era and are traditionally named after the ruling dynasty. So, for example, Tang Dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming Dynasty paintings are busy, colorful, and focus on telling stories via setting and composition. Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting. Woodblock printing became important in Japan after the 17th century.
The western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer, or David's propagandistic paintings. This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe. The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others.
The history of twentieth century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next. Thus the parameters of Impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc cannot be maintained very much beyond the time of their invention. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art, such as Pablo Picasso being influenced by African sculpture. Japanese woodblock prints (which had themselves been influenced by Western Renaissance draftsmanship) had an immense influence on Impressionism and subsequent development. Later, African sculptures were taken up by Picasso and to some extent by Matisse. Similarly, the west has had huge impacts on Eastern art in 19th and 20th century, with originally western ideas like Communism and Post-Modernism exerting powerful influence on artistic styles.
Modernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability. Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with irony. Furthermore the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than regional cultures.
Characteristics.
Art tends to facilitate intuitive rather than rational understanding, and is usually consciously created with this intention. Fine art intentionally serves no other purpose. As a result of this impetus, works of art are elusive, refractive to attempts at classification, because they can be appreciated in more than one way, and are often susceptible to many different interpretations. In the case of Géricault's "Raft of the Medusa", special knowledge concerning the shipwreck that the painting depicts is not a prerequisite to appreciating it, but allows the appreciation of Géricault's political intentions in the piece. Even art that superficially depicts a mundane event or object, may invite reflection upon elevated themes.
Traditionally, the highest achievements of art demonstrate a high level of ability or fluency within a medium. This characteristic might be considered a point of contention, since many modern artists (most notably, conceptual artists) do not themselves create the works they conceive, or do not even create the work in a conventional, demonstrative sense. Art has a transformative capacity: confers particularly appealing or aesthetically satisfying structures or forms upon an original set of unrelated, passive constituents.
Forms, genres, media, and styles.
The creative arts are often divided into more specific categories that are related to their technique, or medium, such as decorative arts, plastic arts, performing arts, or literature. Unlike scientific fields, art is one of the few subjects that is academically organized according to technique. An artistic medium is the substance or material the artistic work is made from, and may also refers to the technique used. For example, paint is the medium used in painting, paper is a medium used in drawing.
An "art form" is the specific "shape", or quality an artistic expression takes. The media used often influences the form. For example, the form of a sculpture must exist in space in three-dimensions, and respond to gravity. The constraints and limitations of a particular medium are thus called its "formal qualities". To give another example, the formal qualities of painting are the canvas texture, color, and brush texture. The formal qualities of video games are non-linearity, interactivity and virtual presence.
The "form" of a particular work of art is determined by both the formal qualities of the media, and the intentions of the artist.
A "genre" is a set of conventions and styles within a particular media. For instance, well recognized genres in film are western, horror and romantic comedy. Genres in music include death metal and trip hop. Genres in painting include still life, and pastoral landscape. A particular work of art may bend or combine genres but each genre has a recognizable group of conventions, clichés and tropes. (One note: the word genre has a second older meaning within painting; genre painting was a phrase used in the 17th to 19th century to refer specifically to paintings of scenes of everyday life and can still be used in this way.)
An artwork, artist's, or movement's "style" is the distinctive method and form that art takes. Any loose brushy, dripped or poured abstract painting is called "expressionistic". Often these styles are linked with a particular historical period, set of ideas, and particular artistic movement. So Jackson Pollock is called an Abstract Expressionist.
Because a particular style may have a specific cultural meanings, it is important to be sensitive to differences in technique. Roy Lichtenstein's (1923–1997) paintings are not pointillist, despite his uses of dots, because they are not aligned with the original proponents of Pointillism. Lichtenstein used Ben-Day dots: they are evenly-spaced and create flat areas of color. These types of dots, used in halftone printing, were originally used in comic strips and newspapers to reproduce color. Lichtenstein thus uses the dots as a style to question the "high" art of painting with the "low" art of comics - to comment on class distinctions in culture. Lichtenstein is thus associated with the American Pop art movement (1960s). Pointillism is a technique in late Impressionism (1880s), developed especially by the artist Georges Seurat, that employs dots that are spaced in a way to create variation in color and depth in an attempt to paint images that were closer to the way people really see color. Both artists use dots, but the particular style and technique relates to the artistic movement these artists were a part of.
These are all ways of beginning to define a work of art, to narrow it down. "Imagine you are an art critic whose mission is to compare the meanings you find in a wide range of individual artworks. How would you proceed with your task? One way to begin is to examine the materials each artist selected in making an object, image video, or event. The decision to cast a sculpture in bronze, for instance, inevitably effects its meaning; the work becomes something different than if it had been cast in gold or plastic or chocolate, even if everything else about the artwork remained the same. Next, you might examine how the materials in each artwork have become an arrangement of shapes, colors, textures, and lines. These, in turn, are organized into various patterns and compositional structures. In your interpretation, you would comment on how salient features of the form contribute to the overall meaning of the finished artwork. [But in the end] the meaning of most artworks... is not exhausted by a discussion of materials, techniques, and form. Most interpretations also include a discussion of the ideas and feelings the artwork engenders."
Skill and craft.
Art can connote a sense of trained ability or mastery of a medium. Art can also simply refer to the developed and efficient use of a language to convey meaning with immediacy and or depth. Art is an act of expressing feelings, thoughts, and observations. There is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes.
A common view is that the epithet "art", particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability or an originality in stylistic approach such as in the plays of Shakespeare, or a combination of these two. Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill. Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity. At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency, yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled.
A common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object. In conceptual art, Marcel Duchamp's "Fountain" is among the first examples of pieces wherein the artist used found objects ("ready-made") and exercised no traditionally recognised set of skills. Tracey Emin's "My Bed", or Damien Hirst's "The Physical Impossibility of Death in the Mind of Someone Living" follow this example and also manipulate the mass media. Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art. Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans. Hirst's celebrity is founded entirely on his ability to produce shocking concepts. The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects. However there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating "hands on" works of art.
Value judgment.
Somewhat in relation to the above, the word "art" is also used to apply judgments of value, as in such expressions like "that meal was a work of art" (the cook is an artist), or "the art of deception", (the highly attained level of skill of the deceiver is praised). It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity.
Making judgments of value requires a basis for criticism. At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered "art" is whether it is perceived to be attractive or repulsive. Though perception is always colored by experience, and is necessarily subjective, it is commonly taken that - that which is not aesthetically satisfying in some fashion cannot be art. However, "good" art is not always or even regularly aesthetically appealing to a majority of viewers. In other words, an artist's prime motivation need not be the pursuit of the aesthetic. Also, art often depicts terrible images made for social, moral, or thought-provoking reasons. For example, Francisco Goya's painting depicting the Spanish shootings of 3rd of May 1808 is a graphic depiction of a firing squad executing several pleading civilians. Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage. Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'.
The assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of that which is aesthetically appealing. Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself. Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium in order to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the "zeitgeist".
Communication.
Art is often intended to appeal and connect with human emotion. It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings. Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously. Art explores what is commonly termed as "the human condition"; that is, essentially what it is to be human.
Effective art often brings about some new insight concerning the human condition either singly or en-mass, which is not necessarily always positive, or necessarily widens the boundaries of collective human ability. The degree of skill that the artist has, will affect their ability to trigger an emotional response and thereby provide new insights, the ability to manipulate them at will shows exemplary skill and determination.
Purpose of art.
Art has had a great number of different functions throughout its history, making its purpose difficult to abstract or quantify to any single concept. This does not imply that the purpose of Art is "vague", but that it has had many unique, different, reasons for being created. Some of these functions of Art are provided in the following outline. The different purposes of art may be grouped according to those which are non-motivated, and those which are motivated (Levi-Strauss).
Non-motivated functions of art.
The non-motivated purposes of art are those which are integral to being human, transcend the individual, or do not fulfill a specific external purpose. Aristotle has said, "Imitation, then, is one instinct of our nature." In this sense, Art, as creativity, is something which humans must do by their very nature (i.e. no other species creates art), and is therefore beyond utility.
Motivated functions of art.
The purposes of art which are motivated refer to intentional, conscious actions on the part of the artists or creator. These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) to sell a product, or simply as a form of communication.
The functions of art described above are not mutually exclusive, as many of them may overlap. For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game.
Controversial art.
Théodore Géricault's "Raft of the Medusa (c. 1820), was a social commentary on a current event, unprecedented at the time. Édouard Manet's "Le Déjeuner sur l'Herbe" (1863), was considered scandalous not because of the nude woman, but because she is seated next to men fully dressed in the clothing of the time, rather than in robes of the antique world. John Singer Sargent's "Madame Pierre Gautreau (Madam X)" (1884), caused a huge uproar over the reddish pink used to color the woman's ear lobe, considered far too suggestive and supposedly ruining the high-society model's reputation.
In the twentieth century, Pablo Picasso's "Guernica" (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town. Leon Golub's "Interrogation III" (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing. Andres Serrano's "Piss Christ" (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine. The resulting uproar led to comments in the United States Senate about public funding of the arts.
Art theories.
In the nineteenth century, artists were primarily concerned with ideas of "truth" and "beauty". The aesthetic theorist John Ruskin, who championed what he saw as the naturalism of J. M. W. Turner, saw art's role as the communication by artifice of an essential truth that could only be found in nature.
The definition and evaluation of art has become especially problematic since the 20th century. Richard Wollheim distinguishes three approaches: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans.
After Greenberg, several important art theorists emerged, such as Michael Fried, T. J. Clark, Rosalind Krauss, Linda Nochlin and Griselda Pollock among others. Though only originally intended as a way of understanding a specific set of artists, Greenberg's definition of modern art is important to many of the ideas of art within the various art movements of the 20th century and early 21st century.
Pop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world. Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond "high art" to all cultural image-making, including fashion images, comics, billboards and pornography.
Classification disputes.
Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art.
Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's "Fountain", the movies, superlative imitations of banknotes, Conceptual art, and Video games.
Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem. Rather, "the passionate concerns and interests that humans vest in their social life" are "so much a part of all classificatory disputes about art" (Novitz, 1996). According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper. For example, when the Daily Mail criticized Hirst's and Emin's work by arguing "For 1,000 years art has been one of our great civilising forces. Today, pickled sheep and soiled beds threaten to make barbarians of us all" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work. In 1998, Arthur Danto, suggested a thought experiment showing that "the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities. Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood."
Anti-art is a label for art which intentionally challenges the established parameters and values of art; it is term associated with Dadaism and attributed to Marcel Duchamp just before World War I, when he was making art from found objects. One of these, "Fountain" (1917), an ordinary urinal, has achieved considerable prominence and influence on art. Anti-art is a feature of work by Situationist International, the lo-fi Mail art movement, and the Young British Artists, though it is a form still rejected by the Stuckists, who describe themselves as anti-anti-art.
Art, class, and value.
Art has been perceived by some as belonging to some social classes and often excluding others. In this context, art is seen as an upper-class activity associated with wealth, the ability to purchase art, and the leisure required to pursue or enjoy it. For example, the palaces of Versailles or the Hermitage in St. Petersburg with their vast collections of art, amassed by the fabulously wealthy royalty of Europe exemplify this view. Collecting such art is the preserve of the rich, or of governments and institutions.
"Fine" and expensive goods have been popular markers of status in many cultures, and they continue to be so today. There has been a cultural push in the other direction since at least 1793, when the Louvre, which had been a private palace of the Kings of France, was opened to the public as an art museum during the French Revolution. Most modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone. Museums in the United States tend to be gifts from the very rich to the masses (The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.) But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status.
There have been attempts by artists to create art that can not be bought by the wealthy as a status object. One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold. It is "necessary to present something more than mere objects" said the major post war German artist Joseph Beuys. This time period saw the rise of such things as performance art, video art, and conceptual art. The idea was that if the artwork was a performance that would leave nothing behind, or was simply an idea, it could not be bought and sold. "Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s. Artists broadly identified under the heading of Conceptual art... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form... [have] endeavored to undermine the art object qua object."
In the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works, invitations to exclusive performance art pieces, and the objects left over from conceptual pieces. Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art. The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity. "With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors."
---END.OF.DOCUMENT---

Agnostida.
Agnostida is an order of trilobite which first developed near the end of the Early Cambrian period and thrived during the Middle Cambrian. They are present in the lower Cambrian fossil record along with trilobites from the Redlichiida, Corynexochida, and Ptychopariida orders. The last agnostids went extinct in the Late Ordovician.
The Agnostida are divided into two suborders — Agnostina and Eodiscina — that are then divided into a number of families. As a group, agnostids have "pygidia" (tails) that are so similar in size and shape to their "cephalons" (heads) that it is difficult to distinguish which end is which. Most agnostid species were eyeless.
The systematic position of Order Agnostida within Class Trilobita remains uncertain, and there has been continuing debate whether they are trilobites or a stem group. The challenge to the status has focused on the Agnostina partly because juveniles of one genus have been found with legs greatly different from those of adult trilobites, suggesting they are separately descended from Crustaceans. Other researchers have suggested, based on cladistic analyses, that Eodiscina and Agnostida are closely united, and that the Eodiscina descended from the trilobite Order Ptychopariida.
Scientists have long debated whether the agnostids lived a pelagic or a benthic lifestyle. Their lack of eyes, a morphology not well-suited for swimming, and their fossils found in association with other benthic trilobites all suggest a benthic (bottom-dwelling) mode of life. They likely lived on areas of the ocean floor that received little or no light and fed on detritus that descended from upper layers of the sea to the bottom. In contrast, their wide geographic dispersion in the fossil record is uncharacteristic of benthic animals, suggesting a pelagic existence. The thoracic segment appears to form a hinge between the head and pygidium allowing for a bivalved ostracodan-type lifestyle. Furthermore, the orientation of the thoracic appendages appears ill suited for benthic living.
Agnostina are generally referred to simply as "agnostids" even though they probably should be called "agnostines".
---END.OF.DOCUMENT---

Abortion.
Abortion is the termination of a pregnancy by the removal or expulsion from the uterus of a fetus or embryo, resulting in or caused by its death. An abortion can occur spontaneously due to complications during pregnancy or can be induced, in humans and other species. In the context of human pregnancies, an abortion induced to preserve the health of the gravida (pregnant female) is termed a "therapeutic abortion", while an abortion induced for any other reason is termed an "elective abortion". The term "abortion" most commonly refers to the induced abortion of a human pregnancy, while spontaneous abortions are usually termed miscarriages.
Abortion has a long history and has been induced by various methods including herbal abortifacients, the use of sharpened tools, physical trauma and other traditional methods. Contemporary medicine utilizes medications and surgical procedures to induce abortion. The legality, prevalence, and cultural views on abortion vary substantially around the world. In many parts of the world there is prominent and divisive public controversy over the ethical and legal issues of abortion. Abortion and abortion-related issues feature prominently in the national politics in many nations, often involving the opposing "pro-life" and "pro-choice" worldwide social movements. Incidence of abortion has declined worldwide, as access to family planning education and contraceptive services has increased. Abortion incidence in the United States declined 8% from 1996 to 2003.
Spontaneous abortion.
Spontaneous abortion (also known as miscarriage) is the expulsion of an embryo or fetus due to accidental trauma or natural causes before approximately the 22nd week of gestation; the definition by gestational age varies by country. Most miscarriages are due to incorrect replication of chromosomes; they can also be caused by environmental factors. A pregnancy that ends before 37 weeks of gestation resulting in a live-born infant is known as a "premature birth". When a fetus dies in utero after about 22 weeks, or during delivery, it is usually termed "stillborn". Premature births and stillbirths are generally not considered to be miscarriages although usage of these terms can sometimes overlap.
Between 10% and 50% of pregnancies end in clinically apparent miscarriage, depending upon the age and health of the pregnant woman. Most miscarriages occur very early in pregnancy, in most cases, they occur so early in the pregnancy that the woman is not even aware that she was pregnant. One study testing hormones for ovulation and pregnancy found that 61.9% of conceptuses were lost prior to 12 weeks, and 91.7% of these losses occurred subclinically, without the knowledge of the once pregnant woman.
The risk of spontaneous abortion decreases sharply after the 10th week from the last menstrual period (LMP). One study of 232 pregnant women showed "virtually complete [pregnancy loss] by the end of the embryonic period" (10 weeks LMP) with a pregnancy loss rate of only 2 percent after 8.5 weeks LMP.
The most common cause of spontaneous abortion during the first trimester is chromosomal abnormalities of the embryo/fetus, accounting for at least 50% of sampled early pregnancy losses. Other causes include vascular disease (such as lupus), diabetes, other hormonal problems, infection, and abnormalities of the uterus. Advancing maternal age and a patient history of previous spontaneous abortions are the two leading factors associated with a greater risk of spontaneous abortion. A spontaneous abortion can also be caused by accidental trauma; intentional trauma or stress to cause miscarriage is considered induced abortion or feticide.
Induced abortion.
An abortion is referred to as elective when it is performed at the request of the woman "for reasons other than maternal health or fetal disease."
Medical.
"Medical abortions" are non-surgical abortions that use pharmaceutical drugs, and are only effective in the first trimester of pregnancy. Medical abortions comprise 10% of all abortions in the United States and Europe. Combined regimens include methotrexate or mifepristone, followed by a prostaglandin (either misoprostol or gemeprost: misoprostol is used in the U.S.; gemeprost is used in the UK and Sweden.) When used within 49 days gestation, approximately 92% of women undergoing medical abortion with a combined regimen completed it without surgical intervention. Misoprostol can be used alone, but has a lower efficacy rate than combined regimens. In cases of failure of medical abortion, vacuum or manual aspiration is used to complete the abortion surgically.
Surgical.
In the first 12 weeks, suction-aspiration or vacuum abortion is the most common method. "Manual Vacuum aspiration" (MVA) abortion consists of removing the fetus or embryo, placenta and membranes by suction using a manual syringe, while "electric vacuum aspiration" (EVA) abortion uses an electric pump. These techniques are comparable, and differ in the mechanism used to apply suction, how early in pregnancy they can be used, and whether cervical dilation is necessary. MVA, also known as "mini-suction" and "menstrual extraction", can be used in very early pregnancy, and does not require cervical dilation. Surgical techniques are sometimes referred to as 'Suction (or surgical) Termination Of Pregnancy' (STOP). From the 15th week until approximately the 26th, dilation and evacuation (D&E) is used. D&E consists of opening the cervix of the uterus and emptying it using surgical instruments and suction.
"Dilation and curettage" (D&C), the second most common method of abortion, is a standard gynecological procedure performed for a variety of reasons, including examination of the uterine lining for possible malignancy, investigation of abnormal bleeding, and abortion. "Curettage" refers to cleaning the walls of the uterus with a curette. The World Health Organization recommends this procedure, also called "sharp curettage," only when MVA is unavailable. The term "D and C", or sometimes "suction curette", is used as a euphemism for the first trimester abortion procedure, whichever the method used.
Other techniques must be used to induce abortion in the second trimester. Premature delivery can be induced with prostaglandin; this can be coupled with injecting the amniotic fluid with hypertonic solutions containing saline or urea. After the 16th week of gestation, abortions can be induced by intact dilation and extraction (IDX) (also called intrauterine cranial decompression), which requires surgical decompression of the fetus' head before evacuation. IDX is sometimes called "partial-birth abortion," which has been federally banned in the United States. A hysterotomy abortion is a procedure similar to a caesarean section and is performed under general anesthesia. It requires a smaller incision than a caesarean section and is used during later stages of pregnancy.
From the 20th to 23rd week of gestation, an injection to stop the fetal heart can be used as the first phase of the surgical abortion procedure to ensure that the fetus is not born alive.
Other methods.
Historically, a number of herbs reputed to possess abortifacient properties have been used in folk medicine: tansy, pennyroyal, black cohosh, and the now-extinct silphium (see history of abortion). The use of herbs in such a manner can cause serious—even lethal—side effects, such as multiple organ failure, and is not recommended by physicians.
Abortion is sometimes attempted by causing trauma to the abdomen. The degree of force, if severe, can cause serious internal injuries without necessarily succeeding in inducing miscarriage. Both accidental and deliberate abortions of this kind can be subject to criminal liability in many countries. In Southeast Asia, there is an ancient tradition of attempting abortion through forceful abdominal massage. One of the bas reliefs decorating the temple of Angkor Wat in Cambodia depicts a demon performing such an abortion upon a woman who has been sent to the underworld.
Reported methods of unsafe, self-induced abortion include misuse of misoprostol, and insertion of non-surgical implements such as knitting needles and clothes hangers into the uterus. These methods are rarely seen in developed countries where surgical abortion is legal and available.
Health risks.
Early-term surgical abortion is a simple procedure which is safer than childbirth when performed before the 21st week. Abortion methods, like most minimally invasive procedures, carry a small potential for serious complications. The risk of complications can increase depending on how far pregnancy has progressed.
Women typically experience minor pain during first-trimester abortion procedures. In a 1979 study of 2,299 patients, 97% reported experiencing some degree of pain. Patients rated the pain as being less than earache or toothache, but more than headache or backache. Local and general anesthetics are used during surgical procedures.
Mental health.
The relationship between induced abortion and mental health is an area of controversy. No scientific research has demonstrated a direct causal relationship between abortion and poor mental health, though some studies have noted that there may be a statistical correlation. Pre-existing factors in a woman's life, such as emotional attachment to the pregnancy, lack of social support, pre-existing psychiatric illness, and conservative views on abortion increase the likelihood of experiencing negative feelings after an abortion.
In a 1990 review, the American Psychological Association (APA) found that "severe negative reactions [after abortion] are rare and are in line with those following other normal life stresses." The APA revised and updated its findings in August 2008 to account for the accumulation of new evidence, and again concluded that induced abortion did not lead to increased mental health problems. A 2008 review by a group from the Johns Hopkins Bloomberg School of Public Health concluded that the highest quality studies found few, if any, mental health differences between women who had abortions and their comparison groups, whereas studies with the most flaws reported negative mental health consequences of abortion. As of August 2008, the United Kingdom Royal College of Psychiatrists is also performing a systematic review of the medical literature to update their position statement on the subject.
Some proposed negative psychological effects of abortion have been referred to by pro-life advocates as a separate condition called "post-abortion syndrome." However, the existence of "post-abortion syndrome" is not recognized by any medical or psychological organization, and some physicians and pro-choice advocates have argued that the effort to popularize the idea of a "post-abortion syndrome" is a tactic used by pro-life advocates for political purposes.
Incidence of induced abortion.
The incidence and reasons for induced abortion vary regionally. It has been estimated that approximately 46 million abortions are performed worldwide every year. Of these, 26 million are said to occur in places where abortion is legal; the other 20 million happen where the procedure is illegal. Some countries, such as Belgium (11.2 per 100 known pregnancies) and the Netherlands (10.6 per 100), have a low rate of induced abortion, while others like Russia (62.6 per 100) and Vietnam (43.7 per 100) have a comparatively high rate. The world ratio is 26 induced abortions per 100 known pregnancies.
By gestational age and method.
Abortion rates also vary depending on the stage of pregnancy and the method practiced. In 2003, from data collected in those areas of the United States that sufficiently reported gestational age, it was found that 88.2% of abortions were conducted at or prior to 12 weeks, 10.4% from 13 to 20 weeks, and 1.4% at or after 21 weeks. 90.9% of these were classified as having been done by "curettage" (suction-aspiration, Dilation and curettage, Dilation and evacuation), 7.7% by "medical" means (mifepristone), 0.4% by "intrauterine instillation" (saline or prostaglandin), and 1.0% by "other" (including hysterotomy and hysterectomy). The Guttmacher Institute estimated there were 2,200 intact dilation and extraction procedures in the U.S. during 2000; this accounts for 0.17% of the total number of abortions performed that year. Similarly, in England and Wales in 2006, 89% of terminations occurred at or under 12 weeks, 9% between 13 to 19 weeks, and 1.5% at or over 20 weeks. 64% of those reported were by vacuum aspiration, 6% by D&E, and 30% were medical. Later abortions are more common in China, India, and other developing countries than in developed countries.
By personal and social factors.
A 1998 aggregated study, from 27 countries, on the reasons women seek to terminate their pregnancies concluded that common factors cited to have influenced the abortion decision were: desire to delay or end childbearing, concern over the interruption of work or education, issues of financial or relationship stability, and perceived immaturity. A 2004 study in which American women at clinics answered a questionnaire yielded similar results. In Finland and the United States, concern for the health risks posed by pregnancy in individual cases was not a factor commonly given; however, in Bangladesh, India, and Kenya health concerns were cited by women more frequently as reasons for having an abortion. 1% of women in the 2004 survey-based U.S. study became pregnant as a result of rape and 0.5% as a result of incest. Another American study in 2002 concluded that 54% of women who had an abortion were using a form of contraception at the time of becoming pregnant while 46% were not. Inconsistent use was reported by 49% of those using condoms and 76% of those using the combined oral contraceptive pill; 42% of those using condoms reported failure through slipping or breakage. The Guttmacher Institute estimated that "most abortions in the United States are obtained by minority women" because minority women "have much higher rates of unintended pregnancy."
Some abortions are undergone as the result of societal pressures. These might include the stigmatization of disabled persons, preference for children of a specific sex, disapproval of single motherhood, insufficient economic support for families, lack of access to or rejection of contraceptive methods, or efforts toward population control (such as China's one-child policy). These factors can sometimes result in compulsory abortion or sex-selective abortion.
History of abortion.
Induced abortion can be traced to ancient times. There is evidence to suggest that, historically, pregnancies were terminated through a number of methods, including the administration of abortifacient herbs, the use of sharpened implements, the application of abdominal pressure, and other techniques.
The Hippocratic Oath, the chief statement of medical ethics for Hippocratic physicians in Ancient Greece, forbade doctors from helping to procure an abortion by pessary. Soranus, a second-century Greek physician, suggested in his work "Gynaecology" that women wishing to abort their pregnancies should engage in energetic exercise, energetic jumping, carrying heavy objects, and riding animals. He also prescribed a number of recipes for herbal baths, pessaries, and bloodletting, but advised against the use of sharp instruments to induce miscarriage due to the risk of organ perforation. It is also believed that, in addition to using it as a contraceptive, the ancient Greeks relied upon silphium as an abortifacient. Such folk remedies, however, varied in effectiveness and were not without risk. Tansy and pennyroyal, for example, are two poisonous herbs with serious side effects that have at times been used to terminate pregnancy.
During the medieval period, physicians in the Islamic world documented detailed and extensive lists of birth control practices, including the use of abortifacients, commenting on their effectiveness and prevalence. They listed many different birth control substances in their medical encyclopedias, such as Avicenna listing 20 in "The Canon of Medicine" (1025) and Muhammad ibn Zakariya ar-Razi listing 176 in his "Hawi" (10th century). This was unparalleled in European medicine until the 19th century.
During the Middle Ages, abortion was tolerated because there were no laws against it. A medieval female physician, Trotula of Salerno, administered a number of remedies for the “retention of menstrua,” which was sometimes a code for early abortifacients. Pope Sixtus V (1585–1590) is noted as the first Pope to declare that abortion is homicide regardless of the stage of pregnancy. Abortion in the 19th century continued, despite bans in both the United Kingdom and the United States, as the disguised, but nonetheless open, advertisement of services in the Victorian era suggests.
In the 20th century the Soviet Union (1919), Iceland (1935) and Sweden (1938) were among the first countries to legalize certain or all forms of abortion. In 1935 Nazi Germany, a law was passed permitting abortions for those deemed "hereditarily ill," while women considered of German stock were specifically prohibited from having abortions.
Sex-selective abortion and female infanticide.
Sonography and amniocentesis allow parents to determine sex before birth. The development of this technology has led to sex-selective abortion, or the targeted termination of female fetuses.
It is suggested that sex-selective abortion might be partially responsible for the noticeable disparities between the birth rates of male and female children in some places. The preference for male children is reported in many areas of Asia, and abortion used to limit female births has been reported in Mainland China, Taiwan, South Korea, and India.
In India, the economic role of men, the costs associated with dowries, and a Hindu tradition which dictates that funeral rites must be performed by a male relative have led to a cultural preference for sons. The widespread availability of diagnostic testing, during the 1970s and '80s, led to advertisements for services which read, "Invest 500 rupees [for a sex test] now, save 50,000 rupees [for a dowry] later." In 1991, the male-to-female sex ratio in India was skewed from its biological norm of 105 to 100, to an average of 108 to 100. Researchers have asserted that between 1985 and 2005 as many as 10 million female fetuses may have been selectively aborted. The Indian government passed an official ban of pre-natal sex screening in 1994 and moved to pass a complete ban of sex-selective abortion in 2002.
In the People's Republic of China, there is also a historic son preference. The implementation of the one-child policy in 1979, in response to population concerns, led to an increased disparity in the sex ratio as parents attempted to circumvent the law through sex-selective abortion or the abandonment of unwanted daughters. Sex-selective abortion might be an influence on the shift from the baseline male-to-female birth rate to an elevated national rate of 117:100 reported in 2002. The trend was more pronounced in rural regions: as high as 130:100 in Guangdong and 135:100 in Hainan. A ban upon the practice of sex-selective abortion was enacted in 2003.
Unsafe abortion.
Women seeking to terminate their pregnancies sometimes resort to unsafe methods, particularly where and when access to legal abortion is being barred.
The World Health Organization (WHO) defines an unsafe abortion as being "a procedure... carried out by persons lacking the necessary skills or in an environment that does not conform to minimal medical standards, or both." Unsafe abortions are sometimes known colloquially as "back-alley" abortions. This can include a person without medical training, a professional health provider operating in sub-standard conditions, or the woman herself.
Unsafe abortion remains a public health concern today due to the higher incidence and severity of its associated complications, such as incomplete abortion, sepsis, hemorrhage, and damage to internal organs. WHO estimates that 19 million unsafe abortions occur around the world annually and that 68,000 of these result in the woman's death. Complications of unsafe abortion are said to account, globally, for approximately 13% of all maternal mortalities, with regional estimates including 12% in Asia, 25% in Latin America, and 13% in sub-Saharan Africa. A 2007 study published in the "The Lancet" found that, although the global rate of abortion declined from 45.6 million in 1995 to 41.6 million in 2003, unsafe procedures still accounted for 48% of all abortions performed in 2003.
Health education, access to family planning, and improvements in health care during and after abortion have been proposed to address this phenomenon.
Abortion debate.
In the history of abortion, induced abortion has been the source of considerable debate, controversy, and activism. An individual's position on the complex ethical, moral, philosophical, biological, and legal issues is often related to his or her value system. The main positions are the pro-choice position, which argues in favor of access to abortion, and the pro-life position, which argues against access to abortion. Opinions of abortion may be described as being a combination of beliefs on its morality, and beliefs on the responsibility, ethical scope, and proper extent of governmental authorities in public policy. Religious ethics also has an influence upon both personal opinion and the greater debate over abortion (see religion and abortion).
Abortion debates, especially pertaining to abortion laws, are often spearheaded by groups advocating one of these two positions. In the United States, those in favor of greater legal restrictions on, or even complete prohibition of abortion, most often describe themselves as pro-life while those against legal restrictions on abortion describe themselves as pro-choice. Generally, the pro-life position argues that a human fetus is a human being with the right to live making abortion tantamount to murder. The pro-choice position argues that a woman has certain reproductive rights, especially the choice whether or not to carry a pregnancy to term.
In both public and private debate, arguments presented in favor of or against abortion focus on either the moral permissibility of an induced abortion, or justification of laws permitting or restricting abortion.
Debate also focuses on whether the pregnant woman should have to notify and/or have the consent of others in distinct cases: a minor, her parents; a legally married or common-law wife, her husband; or a pregnant woman, the biological father. In a 2003 Gallup poll in the United States, 79% of male and 67% of female respondents were in favor of legalized mandatory spousal notification; overall support was 72% with 26% opposed.
Public opinion.
A number of opinion polls around the world have explored public opinion regarding the issue of abortion. Results have varied from poll to poll, country to country, and region to region, while varying with regard to different aspects of the issue.
A May 2005 survey examined attitudes toward abortion in 10 European countries, asking polltakers whether they agreed with the statement, "If a woman doesn't want children, she should be allowed to have an abortion". The highest level of approval was 81% (in the Czech Republic); the lowest was 47% (in Poland).
In North America, a December 2001 poll surveyed Canadian opinion on abortion, asking Canadians in what circumstances they believe abortion should be permitted; 32% responded that they believe abortion should be legal in all circumstances, 52% that it should be legal in certain circumstances, and 14% that it should be legal in no circumstances. A similar poll in April 2009 surveyed people in the United States about U.S. opinion on abortion; 18% said that abortion should be "legal in all cases", 28% said that abortion should be "legal in most cases", 28% said abortion should be "illegal in most cases" and 16% said abortion should be "illegal in all cases". A November 2005 poll in Mexico found that 73.4% think abortion should not be legalized while 11.2% think it should.
Of attitudes in South America, a December 2003 survey found that 30% of Argentines thought that abortion in Argentina should be allowed "regardless of situation", 47% that it should be allowed "under some circumstances", and 23% that it should not be allowed "regardless of situation". A March 2007 poll regarding the abortion law in Brazil found that 65% of Brazilians believe that it "should not be modified", 16% that it should be expanded "to allow abortion in other cases", 10% that abortion should be "decriminalized", and 5% were "not sure". A July 2005 poll in Colombia found that 65.6% said they thought that abortion should remain illegal, 26.9% that it should be made legal, and 7.5% that they were unsure.
Breast cancer hypothesis.
The abortion-breast cancer hypothesis posits that induced abortion increases the risk of developing breast cancer. This position contrasts with the scientific consensus that abortion does "not" cause breast cancer.
In early pregnancy, levels of estrogen increase, leading to breast growth in preparation for lactation. The hypothesis proposes that if this process is interrupted by an abortion before full maturity in the third trimester then more relatively vulnerable immature cells could be left than there were prior to the pregnancy, resulting in a greater potential risk of breast cancer. The hypothesis mechanism was first proposed and explored in rat studies conducted in the 1980s.
Fetal pain debate.
Fetal pain, its existence, and its implications are part of a larger debate about abortion. Many researchers in the area of fetal development believe that a fetus is unlikely to feel pain until after the seventh month of pregnancy. Others disagree. However, legislation has been proposed by pro-life advocates requiring abortion providers to tell a woman that the fetus may feel pain during an abortion procedure.
A review by researchers from the University of California, San Francisco in "JAMA" concluded that data from dozens of medical reports and studies indicate that fetuses are unlikely to feel pain until the third trimester of pregnancy. However a number of medical critics have since disputed these conclusions. At the end of the 20th century there was an emerging consensus among developmental neurobiologists that the establishment of thalamocortical connections (at about 26 weeks) is a critical event with regard to fetal perception of pain. Other researchers such as Anand and Fisk have challenged this late date, positing that pain can be felt around 20 weeks. Because pain can involve sensory, emotional and cognitive factors, it may be "impossible to know" when painful experiences are perceived, even if it is known when thalamocortical connections are established. In any case, one of the first steps in second-trimester and third-trimester abortions is to anesthetize the fetus or stop its heart to prevent fetal pain.
Effect upon crime rate.
A theory attempts to draw a correlation between the United States' unprecedented nationwide decline of the overall crime rate during the 1990s and the decriminalization of abortion 20 years prior.
The suggestion was brought to widespread attention by a 1999 academic paper, "The Impact of Legalized Abortion on Crime", authored by the economists Steven D. Levitt and John Donohue. They attributed the drop in crime to a reduction in individuals said to have a higher statistical probability of committing crimes: unwanted children, especially those born to mothers who are African-American, impoverished, adolescent, uneducated, and single. The change coincided with what would have been the adolescence, or peak years of potential criminality, of those who had not been born as a result of "Roe v. Wade" and similar cases. Donohue and Levitt's study also noted that states which legalized abortion before the rest of the nation experienced the lowering crime rate pattern earlier, and those with higher abortion rates had more pronounced reductions.
Fellow economists Christopher Foote and Christopher Goetz criticized the methodology in the Donohue-Levitt study, noting a lack of accommodation for statewide yearly variations such as cocaine use, and recalculating based on incidence of crime per capita; they found no statistically significant results. Levitt and Donohue responded to this by presenting an adjusted data set which took into account these concerns and reported that the data maintained the statistical significance of their initial paper.
Such research has been criticized by some as being utilitarian, discriminatory as to race and socioeconomic class, and as promoting eugenics as a solution to crime. Levitt states in his book "Freakonomics" that they are neither promoting nor negating any course of action—merely reporting data as economists.
Mexico City Policy.
The Mexico City policy, also known as the "Global Gag Rule" required any non-governmental organization receiving US Government funding to refrain from performing or promoting abortion services in other countries. This had a significant effect on the health policies of many nations across the globe. The Mexico City Policy was instituted under President Reagan, suspended under President Clinton, reinstated by President George W. Bush, and suspended again by President Barack Obama on January 24, 2009.
Religious views.
Each faith has many varying views on the moral implications of abortion with each side citing their own textual proof. Often times, these views can be in direct opposition to each other.
Abortion law.
Before the scientific discovery in the nineteenth century that human development begins at fertilization, English common law forbade abortions after "quickening”, that is, after “an infant is able to stir in the mother's womb.” There was also an earlier period in England when abortion was prohibited "if the foetus is already formed" but not yet quickened. Both pre- and post-quickening abortions were criminalized by "Lord Ellenborough's Act" in 1803. In 1861, the Parliament of the United Kingdom passed the "Offences against the Person Act 1861", which continued to outlaw abortion and served as a model for similar prohibitions in some other nations.
The Soviet Union, with legislation in 1920, and Iceland, with legislation in 1935, were two of the first countries to generally allow abortion. The second half of the 20th century saw the liberalization of abortion laws in other countries. The "Abortion Act 1967" allowed abortion for limited reasons in the United Kingdom (except Northern Ireland). In the 1973 case, "Roe v. Wade", the United States Supreme Court struck down state laws banning abortion, ruling that such laws violated an implied right to privacy in the United States Constitution. The Supreme Court of Canada, similarly, in the case of "R. v. Morgentaler", discarded its criminal code regarding abortion in 1988, after ruling that such restrictions violated the security of person guaranteed to women under the "Canadian Charter of Rights and Freedoms". Canada later struck down provincial regulations of abortion in the case of "R. v. Morgentaler (1993)." By contrast, abortion in Ireland was affected by the addition of an amendment to the Irish Constitution in 1983 by popular referendum, recognizing "the right to life of the unborn".
Other countries, in which abortion is normally illegal, will allow one to be performed in the case of rape, incest, or danger to the pregnant woman's life or health.
In places where abortion is illegal or carries heavy social stigma, pregnant women may engage in medical tourism and travel to countries where they can terminate their pregnancy. Women without the means to travel can resort to providers of illegal abortions or try to do it themselves.
In the USA, about 8% of abortions are performed on women who travel from another state. However, that is driven at least partly by differing limits on abortion according to gestational age or the scarcity of doctors trained and willing to do later abortions.
In other animals.
Spontaneous abortion occurs in various animals. For example, in sheep, it may be caused by crowding through doors, or being chased by dogs. In cows, abortion may be caused by contagious disease, such as Brucellosis or Campylobacter, but can often be controlled by vaccination. Additionally, many other diseases are known to increase the risk of miscarriage in humans and other animals.
Abortion may also be induced in animals, in the context of animal husbandry. For example, abortion may be induced in mares that have been mated improperly, or that have been purchased by owners who did not realize the mares were pregnant, or that are pregnant with twin foals.
Feticide can occur in horses and zebras due to male harassment of pregnant mares or forced copulation, although the frequency in the wild has been questioned. Male Gray langur monkeys may attack females following male takeover, causing miscarriage.
---END.OF.DOCUMENT---

Abstract (law).
In law, an abstract is a brief statement that contains the most important points of a long legal document or of several related legal papers.
Abstract of title.
The Abstract of Title, used in real estate transactions, is the more common form of abstract. An abstract of title lists all the owners of a piece of land, a house, or a building before it came into possession of the present owner. The abstract also records all deeds, wills, mortgages, and other documents that affect ownership of the property. An abstract describes a chain of transfers from owner to owner and any agreements by former owners that are binding on later owners.
Clear Title.
A Clear Title to property is one that clearly states any obligation in the deed to the property. It reveals no breaks in the chain of legal ownership. After the records of the property have been traced and the title has been found clear, it is sometimes guaranteed, or insured. In a few states, a different system of insuring title of real properties provides for registration of a clear title with public authorities. After this is accomplished, no abstract of title is necessary.
Patent law.
In the context of patent law and specifically in prior art searches, searching through abstracts is a common way to find relevant prior art document to question to novelty or inventive step (or non-obviousness in United States patent law) of an invention. Under United States patent law, the abstract may be called "Abstract of the Disclosure".
Administrative process.
Certain government bureaucracies, such as a "department of motor vehicles" will issue an abstract of a completed transaction or an updated record intended to serve as a proof of compliance with some administrative requirement. This is often done in advance of the update of reporting databases and/or the issuance of official documents.
---END.OF.DOCUMENT---

American Revolutionary War.
The American Revolutionary War (1775–1783) or American War of Independence began as a war between the Kingdom of Great Britain and thirteen former British colonies in North America, and concluded in a global war between several European great powers.
The war was the culmination of the political American Revolution, whereby many of the colonists rejected the legitimacy of the Parliament of Great Britain to govern them without representation, claiming that this violated the Rights of Englishmen. The First Continental Congress met in 1774 to coordinate relations with Great Britain and the thirteen now self-governing and individual provinces, petitioning George III for intervention with Parliament, organizing a boycott of British goods, while affirming loyalty to the British Crown. Their pleas ignored, and with British combat troops billeted in Boston, Massachusetts, by 1775 the Provincial Congresses formed the Second Continental Congress and authorized a Continental Army. Additional petitions to the king to intervene with Parliament resulted in the following year with Congress being declared traitors and the states to be in rebellion. The Americans responded in 1776 by formally declaring their independence as one new nation — the United States of America — claiming their own sovereignty and rejecting any allegiance to the British monarchy.
France provided supplies, ammunition and weapons to the rebels from 1776, and the Continentals' capture of a British army in 1777 led France to enter the war in early 1778, which evened the military strength with Britain. Spain and the Dutch Republic – French allies – also went to war with Britain over the next two years, threatening an invasion of England and severely testing British military strength with campaigns in Europe — including attacks on Minorca and Gibraltar — and an escalating global naval war. Spain's involvement culminated in the expulsion of British armies from West Florida, securing the American colonies' southern flank.
Throughout the war, the British were able to use their naval superiority to capture and occupy American coastal cities, but control of the countryside (where 90% of the population lived) largely eluded them because of the relatively small size of their land army. French involvement proved decisive, with a French naval victory in the Chesapeake leading at Yorktown in 1781 to the surrender of a second British army. In 1783, the Treaty of Paris ended the war and recognized the sovereignty of the United States over the territory bounded by what is now Canada to the north, Florida to the south, and the Mississippi River to the west.
American armies and militias.
At the outset of the war, the thirteen colonies lacked a professional army or navy. Each colony provided for its own defenses with local militia. Militiamen were lightly armed, slightly trained, and usually did not have uniforms. Their units served for only a few weeks or months at a time, were reluctant to go very far from home, and were thus generally unavailable for extended operations. Militia lacked the training and discipline of soldiers with more experience, but were more numerous and could overwhelm regular troops as at the battles of Concord, Bennington and Saratoga, and the siege of Boston. Both sides used partisan warfare but the Americans were particularly effective at suppressing Loyalist activity when British regulars were not in the area.
Seeking to coordinate military efforts, the Continental Congress established (on paper) a regular army in June 1775, and appointed George Washington as commander-in-chief. The development of the Continental Army was always a work in progress, and Washington used both his regulars and state militia throughout the war. The United States Marine Corps traces its institutional roots to the Continental Marines of the war, formed at Tun Tavern in Philadelphia, by a resolution of the Continental Congress on November 10, 1775, a date regarded and celebrated as the birthday of the Marine Corps. At the beginning of 1776, Washington's army had 20,000 men, with two-thirds enlisted in the Continental Army and the other third in the various state militias. At the end of the American Revolution in 1783, both the Continental Navy and Continental Marines were disbanded. About 250,000 men served as regulars or as militiamen for the Revolutionary cause in the eight years of the war, but there were never more than 90,000 total men under arms at one time. Armies were small by European standards of the era, largely attributable to limitations such as lack of powder and other logistical capabilities on the American side. By comparison, Duffy notes that Frederick the Great usually commanded from 23,000 to 50,000 in battle. Both figures pale in comparison to the armies that would be fielded in the early nineteenth century, where troop formations approached or exceeded 100,000 men.
Loyalists.
Historians have estimated that approximately 40–45% of the colonists actively supported the rebellion while 15–20% of the population of the thirteen colonies remained loyal to the British Crown. The remaining 35–45% attempted to remain neutral.
At least 25,000 Loyalists fought on the side of the British. Thousands served in the Royal Navy. On land, Loyalist forces fought alongside the British in most battles in North America. Many Loyalists fought in partisan units, especially in the Southern theater.
The British military met with many difficulties in maximizing the use of Loyalist factions. British historian Jeremy Black wrote, “In the American war it was clear to both royal generals and revolutionaries that organized and significant Loyalist activity would require the presence of British forces.” In the South, the use of Loyalists presented the British with “major problems of strategic choice” since while it was necessary to widely disperse troops in order to defend Loyalist areas, it was also recognized that there was a need for “the maintenance of large concentrated forces able” to counter major attacks from the American forces. In addition, the British were forced to ensure that their military actions would not “offend Loyalist opinion”, eliminating such options as attempting to “live off the country’, destroying property for intimidation purposes, or coercing payments from colonists (“laying them under contribution”).
British armies and auxiliaries.
Early in 1775, the British Army consisted of about 36,000 men worldwide, but wartime recruitment steadily increased this number. Great Britain had a difficult time appointing general officers, however. General Thomas Gage, in command of British forces in North America when the rebellion started, was criticized for being too lenient (perhaps influenced by his American wife). General Jeffrey Amherst, 1st Baron Amherst turned down an appointment as commander in chief due to an unwillingness to take sides in the conflict. Similarly, Admiral Augustus Keppel turned down a command, saying "I cannot draw the sword in such a cause." The Earl of Effingham very publicly resigned his commission when his 22nd Regiment of foot was posted to America, and William Howe and John Burgoyne were both members of parliament who opposed military solutions to the American rebellion. Howe and Henry Clinton both made statements that they were not willing participants in the war, but were following orders.
Over the course of the war, Great Britain signed treaties with various German states, which supplied about 30,000 soldiers. Germans made up about one-third of the British troop strength in North America. Hesse-Kassel contributed more soldiers than any other state, and German soldiers became known as "Hessians" to the Americans. Revolutionary speakers called German soldiers "foreign mercenaries," and they are scorned as such in the Declaration of Independence. By 1779, the number of British and German troops stationed in North America was over 60,000, although these were spread from Canada to Florida. About 10,000 Loyalist Americans under arms for the British are included in these figures.
African Americans.
African Americans—slave and free—served on both sides during the war. The British actively recruited slaves belonging to Patriot masters. Because of manpower shortages, George Washington lifted the ban on black enlistment in the Continental Army in January 1776. Small all-black units were formed in Rhode Island and Massachusetts; many slaves were promised freedom for serving. Another all-black unit came from Haiti with French forces. At least 5,000 black soldiers fought for the Revolutionary cause and almost 1,000 black soldiers fought on the British side (although more than 20,000 blacks were with the British at war's end).
Native Americans.
Most Native Americans east of the Mississippi River were affected by the war, and many communities were divided over the question of how to respond to the conflict. Though a few tribes were on friendly terms with the Americans, most Native Americans opposed the United States as a potential threat to their territory. Approximately 13,000 Native Americans fought on the British side, with the largest group coming from the Iroquois tribes, who fielded around 1,500 men. The powerful Iroquois Confederacy was shattered as a result of the conflict; the Mohawk, Seneca, Onondaga, and Cayuga sided with the British, while many Tuscarora and Oneida sided with the colonists. The Continental Army sent the Sullivan Expedition to cripple the Iroquois tribes which had sided with the British. Both during and after the war friction between the Mohawks Joseph Louis Cook and Joseph Brant, who had sided with the Americans and the British respectively, further exacerbated the split.
Massachusetts.
Before the war, Boston had been the scene of much revolutionary activity, leading to the Massachusetts Government Act that ended home rule as a punishment in 1774. Popular resistance to these measures, however, compelled the newly appointed royal officials in Massachusetts to resign or to seek refuge in Boston. Lieutenant General Thomas Gage, the British North American commander-in chief, commanded four regiments of British regulars (about 4,000 men) from his headquarters in Boston, but the countryside was in the hands of the Revolutionaries.
On the night of April 18, 1775, General Gage sent 700 men to seize munitions stored by the colonial militia at Concord, Massachusetts. Riders including Paul Revere alerted the countryside, and when British troops entered Lexington on the morning of April 19, they found 77 minutemen formed up on the village green. Shots were exchanged, killing several minutemen. The British moved on to Concord, where a detachment of three companies was engaged and routed at the North Bridge by a force of 500 minutemen. As the British retreated back to Boston, thousands of militiamen attacked them along the roads, inflicting great damage before timely British reinforcements prevented a total disaster. With the Battles of Lexington and Concord, the war had begun.
The militia converged on Boston, bottling up the British in the city. About 4,500 more British soldiers arrived by sea, and on June 17, 1775, British forces under General William Howe seized the Charlestown peninsula at the Battle of Bunker Hill. The Americans fell back, but British losses were so heavy that the attack was not followed up. The siege was not broken, and Gage was soon replaced by Howe as the British commander-in-chief.
In July 1775, newly appointed General Washington arrived outside Boston to take charge of the colonial forces and to organize the Continental Army. Realizing his army's desperate shortage of gunpowder, Washington asked for new sources. Arsenals were raided and some manufacturing was attempted; 90% of the supply (2 million pounds) was imported by the end of 1776, mostly from France.
The standoff continued throughout the fall and winter. In early March 1776, heavy cannons that the patriots had captured at Fort Ticonderoga were brought to Boston by Colonel Henry Knox, and placed on Dorchester Heights. Since the artillery now overlooked the British positions, Howe's situation was untenable, and the British fled on March 17, 1776, sailing to their naval base at Halifax, Nova Scotia. Washington then moved most of the Continental Army to fortify New York City.
Quebec.
Three weeks after the siege of Boston began, a troop of militia volunteers led by Ethan Allen and Benedict Arnold captured Fort Ticonderoga, a strategically important point on Lake Champlain between New York and the Province of Quebec. After that action they also raided Fort St. John's, not far from Montreal, which alarmed the population and the authorities there. In response, Quebec's governor Guy Carleton began fortifying St. John's, and opened negotiations with the Iroquois and other Native American tribes for their support. These actions, combined with lobbying by both Allen and Arnold and the fear of a British attack from the north, eventually persuaded the Congress to authorize an invasion of Quebec, with the goal of driving the British military from that province. (Quebec was then frequently referred to as "Canada", as most of its territory included the former French Province of Canada.)
Two Quebec-bound expeditions were undertaken. On September 28, 1775, Brigadier General Richard Montgomery marched north from Fort Ticonderoga with about 1,700 militiamen, besieging and capturing Fort St. Jean on November 2 and then Montreal on November 13. General Carleton escaped to Quebec City and began preparing that city for an attack. The second expedition, led by Colonel Arnold, went through the wilderness of what is now northern Maine. It was a logistical nightmare, with 300 men turning back, and another 200 perishing due to the difficult conditions. By the time Arnold reached Quebec City in early November, he had but 600 of his original 1,100 men. Montgomery's force joined Arnold's, and they attacked Quebec City on December 31, but were defeated by Carleton in a battle that ended with Montgomery dead, Arnold wounded, and over 400 Americans taken prisoner. The remaining Americans held on outside Quebec City until the spring of 1776, suffering from poor camp conditions and smallpox, and then withdrew when a squadron of British ships under Captain Charles Douglas arrived to relieve the siege.
Another attempt was made by the Americans to push back towards Quebec, but they failed at Trois-Rivières on June 8, 1776. Carleton then launched his own invasion and defeated Arnold at the Battle of Valcour Island in October. Arnold fell back to Fort Ticonderoga, where the invasion had begun. While the invasion ended as a disaster for the Americans, Arnold's efforts in 1776 delayed a full-scale British counteroffensive until the Saratoga campaign of 1777.
The invasion cost the Americans their base of support in British public opinion, "So that the violent measures towards America are freely adopted and countenanced by a majority of individuals of all ranks, professions, or occupations, in this country." It gained them at best limited support in the population of Quebec, which, while somewhat supportive early in the invasion, became less so later during the occupation, when American policies against suspected Loyalists became harsher, and the army's hard currency ran out. Two small regiments of Canadiens were recruited during the operation, and they were with the army on its retreat back to Ticonderoga.
New York and New Jersey.
Having withdrawn his army from Boston, General Howe now focused on capturing New York City. To defend the city, General Washington divided his 20,000 soldiers between Long Island and Manhattan. While British troops were assembling on Staten Island for the campaign, Washington had the newly issued Declaration of American Independence read to his men. No longer was there any possibility of compromise. On August 27, 1776, after landing about 22,000 men on Long Island, the British drove the Americans back to Brooklyn Heights, securing a decisive British victory in the largest battle of the entire Revolution. Howe then laid siege to fortifications there. In a feat considered by many historians to be one of his most impressive actions as Commander in Chief, Washington personally directed the withdrawal of his entire remaining army and all their supplies across the East River in one night without discovery by the British or the loss of a single man or any materiel.
On September 15, Howe landed about 12,000 men on lower Manhattan, quickly taking control of New York City. The Americans withdrew to Harlem Heights, where they skirmished the next day but held their ground. When Howe moved to encircle Washington's army in October, the Americans again fell back, and a battle at White Plains was fought on October 28. Again Washington retreated, and Howe returned to Manhattan and captured Fort Washington in mid November, taking about 2,000 prisoners (with an additional 1,000 having been captured during the battle for Long Island). Thus began the infamous "prison ships" system the British maintained in New York for the remainder of the war, in which more American soldiers and sailors died of neglect than died in every battle of the entire war, combined.
General Lord Cornwallis continued to chase Washington's army through New Jersey, until the Americans withdrew across the Delaware River into Pennsylvania in early December. With the campaign at an apparent conclusion for the season, the British entered winter quarters. Although Howe had missed several opportunities to crush the diminishing American army, he had killed or captured over 5,000 Americans.
The outlook of the Continental Army was bleak. "These are the times that try men's souls," wrote Thomas Paine, who was with the army on the retreat. The army had dwindled to fewer than 5,000 men fit for duty, and would be reduced to 1,400 after enlistments expired at the end of the year. Congress had abandoned Philadelphia in despair, although popular resistance to British occupation was growing in the countryside.
Washington decided to take the offensive, stealthily crossing the Delaware on Christmas night and capturing nearly 1,000 Hessians at the Battle of Trenton on December 26, 1776. Cornwallis marched to retake Trenton but was outmaneuvered by Washington, who successfully attacked the British rearguard at Princeton on January 3, 1777. Washington then entered winter quarters at Morristown, New Jersey, having given a morale boost to the American cause. New Jersey militia continued to harass British and Hessian forces throughout the winter, forcing the British to retreat to their base in and around New York City.
At every stage the British strategy assumed a large base of Loyalist supporters would rally to the King given some military support. In February 1776 Clinton took 2,000 men and a naval squadron to invade North Carolina, which he called off when he learned the Loyalists had been crushed at the Battle of Moore's Creek Bridge. In June he tried to seize Charleston, South Carolina, the leading port in the South, hoping for a simultaneous rising in South Carolina. It seemed a cheap way of waging the war but it failed as the naval force was defeated by the forts and because no local Loyalists attacked the
town from behind. The loyalists were too poorly organized to be effective, but as late as 1781 senior officials in London, misled by Loyalist exiles, placed their confidence in their rising.
Saratoga and Philadelphia.
When the British began to plan operations for 1777, they had two main armies in North America: Carleton's army in Quebec, and Howe's army in New York. In London, Lord George Germain approved campaigns for these armies which, because of miscommunication, poor planning, and rivalries between commanders, did not work in conjunction. Although Howe successfully captured Philadelphia, the northern army was lost in a disastrous surrender at Saratoga. Both Carleton and Howe resigned after the 1777 campaign.
Saratoga campaign.
The first of the 1777 campaigns was an expedition from Quebec led by General John Burgoyne. The goal was to seize the Lake Champlain and Hudson River corridor, effectively isolating New England from the rest of the American colonies. Burgoyne's invasion had two components: he would lead about 10,000 men along Lake Champlain towards Albany, New York, while a second column of about 2,000 men, led by Barry St. Leger, would move down the Mohawk River Valley and link up with Burgoyne in Albany, New York.
Burgoyne set off in June, and recaptured Fort Ticonderoga in early July. Thereafter, his march was slowed by Americans who literally knocked down trees in his path. A detachment was sent out to seize supplies but was decisively defeated in the Battle of Bennington by American militia in August, depriving Burgoyne of nearly 1,000 men.
Meanwhile, St. Leger—half of his force Native Americans led by Sayenqueraghta—had laid siege to Fort Stanwix. American militiamen and their Native American allies marched to relieve the siege but were ambushed and scattered at the Battle of Oriskany. When a second relief expedition approached, this time led by Benedict Arnold, St. Leger's Indian support abandoned him, forcing him to break off the siege and return to Quebec.
Burgoyne's army had been reduced to about 6,000 men by the loss at Bennington and the need to garrison Ticonderoga, and he was running short on supplies. Despite these setbacks, he determined to push on towards Albany. An American army of 8,000 men, commanded by the General Horatio Gates, had entrenched about 10 miles (16 km) south of Saratoga, New York. Burgoyne tried to outflank the Americans but was checked at the first battle of Saratoga in September. Burgoyne's situation was desperate, but he now hoped that help from Howe's army in New York City might be on the way. It was not: Howe had instead sailed away on his expedition to capture Philadelphia. American militiamen flocked to Gates' army, swelling his force to 11,000 by the beginning of October. After being badly beaten at the second battle of Saratoga, Burgoyne surrendered on October 17.
Saratoga was the turning point of the war. Revolutionary confidence and determination, suffering from Howe's successful occupation of Philadelphia, was renewed. What is more important, the victory encouraged France to make an open alliance with the Americans, after two years of semi-secret support. For the British, the war had now become much more complicated.
Philadelphia campaign.
Having secured New York City in 1776, General Howe concentrated on capturing Philadelphia, the seat of the Revolutionary government, in 1777. He moved slowly, landing 15,000 troops in late August at the northern end of Chesapeake Bay. Washington positioned his 11,000 men between Howe and Philadelphia but was driven back at the Battle of Brandywine on September 11, 1777. The Continental Congress again abandoned Philadelphia, and on September 26, Howe finally outmaneuvered Washington and marched into the city unopposed. Washington unsuccessfully attacked the British encampment in nearby Germantown in early October and then retreated to watch and wait.
After repelling a British attack at White Marsh, Washington and his army encamped at Valley Forge in December 1777, about 20 miles (32 km) from Philadelphia, where they stayed for the next six months. Over the winter, 2,500 men (out of 10,000) died from disease and exposure. The next spring, however, the army emerged from Valley Forge in good order, thanks in part to a training program supervised by Baron von Steuben, who introduced the most modern Prussian methods of organization and tactics.
General Clinton replaced Howe as British commander-in-chief. French entry into the war had changed British strategy, and Clinton abandoned Philadelphia to reinforce New York City, now vulnerable to French naval power. Washington shadowed Clinton on his withdrawal and forced a strategic victory at the battle at Monmouth on June 28, 1778, the last major battle in the north. Clinton's army went to New York City in July, arriving just before a French fleet under Admiral d'Estaing arrived off the American coast. Washington's army returned to White Plains, New York, north of the city. Although both armies were back where they had been two years earlier, the nature of the war had now changed.
An international war, 1778–1783.
In 1778, the war over the rebellion in North America became international, spreading not only to Europe but to European colonies in the West Indies and in India. From 1776 France had informally been involved, with French admiral Latouche Tréville having provided supplies, ammunition and guns from France to the United States after Thomas Jefferson had encouraged a French alliance, and guns such as de Valliere type were used, playing an important role in such battles as the Battle of Saratoga. George Washington wrote about the French supplies and guns in a letter to General Heath on 2 May 1777. After learning of the American victory at Saratoga, France signed the Treaty of Alliance with the United States on February 6, 1778, formalizing the Franco-American alliance negotiated by Benjamin Franklin. Spain entered the war as an ally of France in June 1779, a renewal of the Bourbon Family Compact. Unlike France, Spain initially refused to recognize the independence of the United States, because Spain was not keen on encouraging similar anti-colonial rebellions in the Spanish Empire. Both countries had quietly provided assistance to the Americans since the beginning of the war, hoping to dilute British power. So too had the Dutch Republic, which was formally brought into the war at the end of 1780.
In London King George III gave up hope of subduing America by more armies while Britain had a European war to fight. "It was a joke," he said, "to think of keeping Pennsylvania." There was no hope of recovering New England. But the King was determined "never to acknowledge the independence of the Americans, and to punish their contumacy by the indefinite prolongation of a war which promised to be eternal." His plan was to keep the 30,000 men garrisoned in New York, Rhode Island, Quebec, and Florida; other forces would attack the French and Spanish in the West Indies. To punish the Americans the King planned to destroy their coasting-trade, bombard their ports; sack and burn towns along the coast (as Benedict Arnold did to New London, Connecticut in 1781), and turn loose the Native Americans to attack civilians in frontier settlements. These operations, the King felt, would inspire the Loyalists; would splinter the Congress; and "would keep the rebels harassed, anxious, and poor, until the day when, by a natural and inevitable process, discontent and disappointment were converted into penitence and remorse" and they would beg to return to his authority. The plan meant destruction for the Loyalists and loyal Native Americans, an indefinite prolongation of a costly war, and the risk of disaster as the French and Spanish assembled an armada to invade the British Isles. The British planned to re-subjugate the rebellious colonies after dealing with the Americans' European allies.
Widening of the naval war.
When the war began, the British had overwhelming naval superiority over the American colonists. The Royal Navy had over 100 ships of the line and many frigates and smaller craft, although this fleet was old and in poor condition, a situation which would be blamed on Lord Sandwich, the First Lord of the Admiralty. During the first three years of the war, the Royal Navy was primarily used to transport troops for land operations and to protect commercial shipping. The American colonists had no ships of the line, and relied extensively on privateering to harass British shipping. The privateers caused worry disproportionate to their material success, although those operating out of French channel ports before and after France joined the war caused significant embarrassment to the Royal Navy and inflamed Anglo-French relations. About 55,000 American sailors served aboard the privateers during the war. The American privateers had almost 1,700 ships, and they captured 2,283 enemy ships. The Continental Congress authorized the creation of a small Continental Navy in October 1775, which was primarily used for commerce raiding. John Paul Jones became the first great American naval hero, capturing HMS "Drake" on April 24, 1778, the first victory for any American military vessel in British waters.
France's formal entry into the war meant that British naval superiority was now contested. The Franco-American alliance began poorly, however, with failed operations at Rhode Island in 1778 and Savannah, Georgia, in 1779. Part of the problem was that France and the United States had different military priorities: France hoped to capture British possessions in the West Indies before helping to secure American independence. While French financial assistance to the American war effort was already of critical importance, French military aid to the Americans would not show positive results until the arrival in July 1780 of a large force of soldiers led by the Comte de Rochambeau.
Spain entered the war as a French ally with the goal of recapturing Gibraltar and Minorca, which it had lost to the British in 1704. Gibraltar was besieged for more than three years, but the British garrison stubbornly resisted and was resupplied twice: once after Admiral Rodney's victory over Juan de Lángara in the 1780 "Moonlight Battle", and again after Admiral Richard Howe fought Luis de Córdova y Córdova to a draw in the Battle of Cape Spartel. Further Franco-Spanish efforts to capture Gibraltar were unsuccessful. One notable success took place on February 5, 1782, when Spanish and French forces captured Minorca, which Spain retained after the war. Ambitious plans for an invasion of England in 1779 had to be abandoned.
West Indies and Gulf Coast.
There was much action in the West Indies, with several islands changing hands, especially in the Lesser Antilles. At the Battle of the Saintes in April 1782, a victory by Rodney's fleet over the French Admiral de Grasse frustrated the hopes of France and Spain to take Jamaica and other colonies from the British.
On May 8, 1782, Count Bernardo de Gálvez, the Spanish governor of Louisiana, captured the British naval base at New Providence in the Bahamas. On the Gulf Coast, Gálvez quickly removed the British from their outposts on the lower Mississippi River in 1779 in actions at Manchac and Baton Rouge in British West Florida. Gálvez then captured Mobile in 1780 and stormed and captured the British citadel and capital of Pensacola in 1781. His actions led to the Spanish acquisition East and West Florida in the peace settlement. Gálvez' success denied the British the opportunity of encircling the American rebels from the south, and kept open a vital conduit for supplies. George Washington took him to his right in the parade of July 4 and the American Congress cited Gálvez for his aid during the Revolution.
Central America was also subject to conflict between Britain and Spain, as Britain sought to expand its influence beyond coastal logging and fishing communities in present-day Belize, Honduras, and Nicaragua. Expeditions against San Fernando de Omoa in 1779 and San Juan in 1780 (the latter famously led by a young Horatio Nelson) met with only temporary success before being abandoned due to disease. The Spanish colonial leaders, in turn, were unable to completely eliminate British influences along the Mosquito Coast. Except for the French acquisition of Tobago, sovereignty in the West Indies was returned to the "status quo ante bellum" in the peace of 1783.
India and the Netherlands.
When word reached India in 1778 that France had entered the war, British military forces moved quickly to capture French colonial outposts there, capturing Pondicherry after two months of siege. The capture of the French-controlled port of Mahé on India's west coast motivated Mysore's ruler Hyder Ali (who was already upset at other British actions, and benefited from trade through the port) to open the Second Anglo-Mysore War in 1780. Ali, and later his son Tipu Sultan, almost drove the British from southern India but was frustrated by weak French support, and the war ended "status quo ante bellum" with the 1784 Treaty of Mangalore. French opposition was led in 1782 and 1783 by Admiral the Baillie de Suffren, who recaptured Trincomalee from the British and fought five celebrated, but largely inconclusive, naval engagements against British Admiral Sir Edward Hughes. France's Indian colonies were returned after the war.
The Dutch Republic, nominally neutral, had been trading with the Americans, exchanging Dutch arms and munitions for American colonial wares (in contravention of the British "Navigation Acts"), primarily through activity based in St. Eustatius, before the French formally entered the war. The British considered this trade to include contraband military supplies and had attempted to stop it, at first diplomatically by appealing to previous treaty obligations, interpretation of whose terms the two nations disagreed on, and then by searching and seizing Dutch merchant ships. The situation escalated when the British seized a Dutch merchant convoy sailing under Dutch naval escort in December 1779, prompting the Dutch to join the League of Armed Neutrality. Britain responded to this decision by declaring war on the Dutch in December 1780, sparking the Fourth Anglo-Dutch War. The war was a military and economic disaster for the Dutch Republic. Paralyzed by internal political divisions, it was unable to effectively respond to British blockades of its coast and the capture of many of its colonies. In the 1784 peace treaty between the two nations, the Dutch lost the Indian port of Negapatam and were forced to make trade concessions. The Dutch Republic signed a friendship and trade agreement with the United States in 1782, and was the second country (after France) to formally recognize the United States.
Southern theater.
During the first three years of the American Revolutionary War, the primary military encounters were in the north, although some attempts to organize Loyalists were defeated, a British attempt at Charleston, South Carolina failed, and a variety of efforts to attack British forces in East Florida failed. After French entry into the war, the British turned their attention to the southern colonies, where they hoped to regain control by recruiting Loyalists. This southern strategy also had the advantage of keeping the Royal Navy closer to the Caribbean, where the British needed to defend economically important possessions against the French and Spanish.
On December 29, 1778, an expeditionary corps from Clinton's army in New York captured Savannah, Georgia. An attempt by French and American forces to retake Savannah failed on October 9, 1779. Clinton then besieged Charleston, capturing it and most of the southern Continental Army on May 12, 1780. With relatively few casualties, Clinton had seized the South's biggest city and seaport, paving the way for what seemed like certain conquest of the South.
The remnants of the southern Continental Army began to withdraw to North Carolina but were pursued by Lt. Colonel Banastre Tarleton, who defeated them at the Waxhaws on May 29, 1780. With these events, organized American military activity in the region collapsed, though the war was carried on by partisans such as Francis Marion. Cornwallis took over British operations, while Horatio Gates arrived to command the American effort. On August 16, 1780, Gates was defeated at the Battle of Camden, setting the stage for Cornwallis to invade North Carolina.
Cornwallis' victories quickly turned, however. One wing of his army was utterly defeated at the Battle of Kings Mountain on October 7, 1780, and Tarleton was decisively defeated by Daniel Morgan at the Battle of Cowpens on January 17, 1781. General Nathanael Greene, who replaced General Gates, proceeded to wear down the British in a series of battles, each of them tactically a victory for the British but giving no strategic advantage to the victors. Greene summed up his approach in a motto that would become famous: "We fight, get beat, rise, and fight again." By March, Greene's army had grown to the point were he felt that he could face Cornwallis directly. In the key Battle of Guilford Court House, Cornwallis defeated Greene, but at tremendous cost, and without breaking Greene's army. He retreated to Wilmington, North Carolina for resupply and reinforcement, after which he moved north into Virginia, leaving the Carolinas and Georgia open to Greene.
In March 1781, General Washington dispatched General Lafayette to defend Virginia, and in April, a British force under the recently-turned Benedict Arnold landed there. Arnold moved through the Virginia countryside, destroying supply depots, mills, and other economic targets, before joining his army with that of Cornwallis. Lafayette skirmished with Cornwallis, avoiding a decisive battle while gathering reinforcements. Cornwallis was unable to trap Lafayette, and so he moved his forces to Yorktown, Virginia, in July so the Royal Navy could return his army to New York.
Northern and western frontier.
West of the Appalachian Mountains and along the border with Quebec, the American Revolutionary War was an "Indian War". Most Native Americans supported the British. Like the Iroquois Confederacy, tribes such as the Cherokees and the Shawnees split into factions.
The British supplied their native allies with muskets and gunpowder and advised raids against civilian settlements, especially in New York, Kentucky, and Pennsylvania. Joint Iroquois-Loyalist attacks in the Wyoming Valley and at Cherry Valley in 1778 provoked Washington to send the Sullivan Expedition into western New York during the summer of 1779. There was little fighting as Sullivan systematically destroyed the Native American winter food supplies, forcing them to flee permanently to British bases in Quebec and the Niagara Falls area.
In the Ohio Country and the Illinois Country, the Virginia frontiersman George Rogers Clark attempted to neutralize British influence among the Ohio tribes by capturing the outposts of Kaskaskia and Cahokia and Vincennes in the summer of 1778. When General Henry Hamilton, the British commander at Detroit, retook Vincennes, Clark returned in a surprise march in February 1779 and captured Hamilton himself.
In March 1782, Pennsylvania militiamen killed about a hundred neutral Native Americans in the Gnadenhütten massacre. In one of the last major encounters of the war, a force of 200 Kentucky militia was defeated at the Battle of Blue Licks in August 1782.
Yorktown and the Surrender of Cornwallis.
The northern, southern, and naval theaters of the war converged in 1781 at Yorktown, Virginia. In early September, French naval forces defeated a British fleet at the Battle of the Chesapeake, cutting off Cornwallis' escape. Washington hurriedly moved American and French troops from New York, and a combined Franco-American force of 17,000 men commenced the Siege of Yorktown in early October. For several days, the French and Americans bombarded the British defenses. Cornwallis' position quickly became untenable, and he surrendered his entire army of 7,000 men on October 19, 1781.
With the surrender at Yorktown, King George lost control of Parliament to the peace party, and there were no further major military activities on land. The British had 30,000 garrison troops occupying New York City, Charleston, and Savannah. The war continued at sea between the British and the French fleets in the West Indies.
Treaty of Paris.
In London, as political support for the war plummeted after Yorktown, Prime Minister Lord North resigned in March 1782. In April 1782, the Commons voted to end the war in America. Preliminary peace articles were signed in Paris at the end of November, 1782; the formal end of the war did not occur until the Treaty of Paris and Treaties of Versailles were signed on September 3, 1783. The last British troops left New York City on November 25, 1783, and the United States Congress of the Confederation ratified the Paris treaty on January 14, 1784.
Britain negotiated the Paris peace treaty without consulting her Native American allies and ceded all Native American territory between the Appalachian Mountains and the Mississippi River to the United States. Full of resentment, Native Americans reluctantly confirmed these land cessions with the United States in a series of treaties, but the fighting would be renewed in conflicts along the frontier in the coming years, the largest being the Northwest Indian War.
Advantages/disadvantages of the opposing sides.
During the war the Americans benefited greatly from international assistance. In addition, Britain had significant military disadvantages. Distance was a major problem: most troops and supplies had to be shipped across the Atlantic Ocean. The British usually had logistical problems whenever they operated away from port cities, while the Americans had local sources of manpower and food and were more familiar with (and accustomed to) the territory. Additionally, ocean travel meant that British communications were always about two months out of date: by the time British generals in America received their orders from London, the military situation had usually changed.
Suppressing a rebellion in America also posed other problems. Since the colonies covered a large area and had not been united before the war, there was no central area of strategic importance. In Europe, the capture of a capital often meant the end of a war; in America, when the British seized cities such as New York and Philadelphia, the war continued unabated. Furthermore, the large size of the colonies meant that the British lacked the manpower to control them by force. Once any area had been occupied, troops had to be kept there or the Revolutionaries would regain control, and these troops were thus unavailable for further offensive operations. The British had sufficient troops to defeat the Americans on the battlefield but not enough to simultaneously occupy the colonies. This manpower shortage became critical after French and Spanish entry into the war, because British troops had to be dispersed in several theaters, where previously they had been concentrated in America.
The British also had the difficult task of fighting the war while simultaneously retaining the allegiance of Loyalists. Loyalist support was important, since the goal of the war was to keep the colonies in the British Empire, but this imposed numerous military limitations. Early in the war, the Howe brothers served as peace commissioners while simultaneously conducting the war effort, a dual role which may have limited their effectiveness. Additionally, the British could have recruited more slaves and Native Americans to fight the war, but this would have alienated many Loyalists, even more so than the controversial hiring of German mercenaries. The need to retain Loyalist allegiance also meant that the British were unable to use the harsh methods of suppressing rebellion they employed in Ireland and Scotland. Even with these limitations, many potentially neutral colonists were nonetheless driven into the ranks of the Revolutionaries because of the war. This combination of factors led ultimately to the downfall of British rule in America and the rise of the revolutionaries' own independent nation, the United States of America.
Casualties.
The total loss of life resulting from the American Revolutionary War is unknown. As was typical in the wars of the era, disease claimed more lives than battle. Between 1775 and 1782, a smallpox epidemic raged across much of North America, killing more than 130,000 people. Historian Joseph Ellis suggests that Washington's decision to have his troops inoculated against the smallpox epidemic was one of his most important decisions.
Approximately 25,000 American Revolutionaries died during active military service. About 8,000 of these deaths were in battle; the other 17,000 deaths were from disease, including about 8,000 – 12,000 who died while prisoners of war, most in rotting prison ships in New York. The number of Revolutionaries seriously wounded or disabled by the war has been estimated from 8,500 to 25,000. The total American military casualty figure was therefore as high as 50,000.
About 171,000 sailors served for the British during the war; about 25 to 50 percent of them had been pressed into service. About 1,240 were killed in battle, while 18,500 died from disease. The greatest killer was scurvy, a disease known at the time to be easily preventable by issuing lemon juice to sailors. About 42,000 British sailors deserted during the war.
Approximately 1,200 Germans were killed in action and 6,354 died from illness or accident. About 16,000 of the remaining German troops returned home, but roughly 5,500 remained in the United States after the war for various reasons, many eventually becoming American citizens. No reliable statistics exist for the number of casualties among other groups, including Loyalists, British regulars, Native Americans, French and Spanish troops, and civilians.
Financial costs.
The British spent about £80 million and ended with a national debt of £250 million, which it easily financed at about £9.5 million a year in interest. The French spent 1.3 billion livres (about £56 million). Their total national debt was £187 million, which they could not easily finance; over half the French national revenue went to debt service in the 1780s. The debt crisis became a major enabling factor of the French Revolution as the government was unable to raise taxes without public approval. The United States spent $37 million at the national level plus $114 million by the states. This was mostly covered by loans from France and the Netherlands, loans from Americans, and issuance of an increasing amount of paper money (which became "not worth a continental.") The U.S. finally solved its debt and currency problems in the 1790s when Alexander Hamilton spearheaded the establishment of the First Bank of the United States.
Notes.
To avoid duplication, notes for sections with a link to a "Main article" will be found in the linked article.
Further reading == .
These are some of the standard works about the war in general which are not listed above; books about specific campaigns, battles, units, and individuals can be found in those articles.
---END.OF.DOCUMENT---

Ampere.
The ampere (symbol: A) is the SI unit of electric current and is one of the seven SI base units. It is named after André-Marie Ampère (1775–1836), French mathematician and physicist, considered the father of electrodynamics. In practice, its name is often shortened to amp.
In practical terms, the ampere is a measure of the amount of electric charge passing a point per unit time. Around 6.242 × 1018 electrons passing a given point each second constitutes one ampere. (Since electrons have negative charge, they flow in the opposite direction to the conventional current.)
Definition.
Ampère's force law states that there is an attractive force between two parallel wires carrying an electric current. This force is used in the formal definition of the ampere which states that it is "the constant current which will produce an attractive force of 2 × 10–7 newtons per metre of length between two straight, parallel conductors of infinite length and negligible circular cross section placed one metre apart in a vacuum".
In terms of Ampère's force law,
The SI unit of charge, the coulomb, "is the quantity of electricity carried
That is, in general, charge "Q" is determined by steady current "I" flowing for a time "t" as "Q" = "It".
History.
The ampere was originally defined as one tenth of the CGS system electromagnetic unit of current (now known as the abampere), the amount of current which generates a force of two dynes per centimetre of length between two wires one centimetre apart. The size of the unit was chosen so that the units derived from it in the MKSA system would be conveniently sized.
The "international ampere" was an early realisation of the ampere, defined as the current that would deposit grams of silver per second from a silver nitrate solution. Later, more accurate measurements revealed that this current is 0.99985 A.
Realisation.
The ampere is most accurately realised using a watt balance, but is in practice maintained via Ohm's Law from the units of electromotive force and resistance, the volt and the ohm, since the latter two can be tied to physical phenomena that are relatively easy to reproduce, the Josephson junction and the quantum Hall effect, respectively.
At present, techniques to establish the realisation of an ampere have a relative uncertainty of approximately a few parts in 107, and involve realisations of the watt, the ohm and the volt.
Proposed future definition.
Rather than a definition in terms of the force between two current-carrying wires, it has been proposed to define the ampere in terms of the rate of flow of elementary charges.
Since a coulomb is approximately equal to elementary charges, one ampere is approximately equivalent to elementary charges, such as electrons, moving past a boundary in one second. The proposed change would define 1 A as being the current in the direction of flow of a particular number of elementary charges per second. In 2005, the International Committee for Weights and Measures (CIPM) agreed to study the proposed change, and, depending on the outcome of experiments over the next few years, to formally propose the change at the 24th General Conference on Weights and Measures (CGPM) in 2011.
---END.OF.DOCUMENT---

Algorithm.
In mathematics, computer science, and related subjects, an algorithm is an effective method for solving a problem using a finite sequence of instructions. Algorithms are used for calculation, data processing, and many other fields.
Each algorithm is a list of well-defined instructions for completing a task. Starting from an initial state, the instructions describe a computation that proceeds through a well-defined series of successive states, eventually terminating in a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate randomness.
A partial formalization of the concept began with attempts to solve the Entscheidungsproblem (the "decision problem") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define "effective calculability" or "effective method"; those formalizations included the Gödel-Herbrand-Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's "Formulation 1" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939.
Etymology.
Al-Khwārizmī, Persian astronomer and mathematician, wrote a treatise in the arabic language in 825 AD, "On Calculation with Hindu–Arabic numeral system". (See algorism). It was translated from arabic into Latin in the 12th century as "Algoritmi de numero Indorum" (al-Daffa 1977), whose title is supposedly likely intended to mean "Algoritmi on the numbers of the Indians", where "Algoritmi" was the translator's rendition of the author's name; but people misunderstanding the title treated "Algoritmi" as a Latin plural and this led to the word "algorithm" (Latin "algorismus") coming to mean "calculation method". The intrusive "th" is most likely due to a false cognate with the Greek ("arithmos") meaning "numbers".
Why algorithms are necessary: an informal definition.
While there is no generally accepted "formal" definition of "algorithm," an informal definition could be "a process that performs some sequence of operations." For some people, a program is only an algorithm if it stops eventually. For others, a program is only an algorithm if it stops before a given number of calculation steps.
A prototypical example of an algorithm is Euclid's algorithm to determine the maximum common divisor of two integers.
No human being can write fast enough, or long enough, or small enough† (†"smaller and smaller without limit...you'd be trying to write on molecules, on atoms, on electrons") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the "n"th member of the set, for arbitrary finite "n". Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols
The concept of "algorithm" is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related with our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of "algorithm" that suits both concrete (in some sense) and abstract usage of the term.
Formalization.
Minsky: "But we will also maintain, with Turing... that any procedure which could "naturally" be called effective, can in fact be realized by a (simple) machine. Although this may seem extreme, the arguments... in its favor are hard to refute".
Gurevich: "...Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine... according to Savage [1987], an algorithm is a computational process defined by a Turing machine".
Typically, when an algorithm is associated with processing information, data is read from an input source, written to an output device, and/or stored for further processing. Stored data is regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.
For any such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).
Because an algorithm is a precise list of precise steps, the order of computation will always be critical to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting "from the top" and going "down to the bottom", an idea that is described more formally by "flow of control".
So far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, "mechanical" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of "memory" as a scratchpad. There is an example below of such an assignment.
For some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.
Termination.
Some writers restrict the definition of "algorithm" to procedures that eventually finish. In such a category Kleene places the "decision procedure" or "decision method" or "algorithm" for the question". Others, including Kleene, include procedures that could run forever without stopping; such a procedure has been called a "computational method" or "calculation procedure" or "algorithm" (and hence a "calculation problem") in relation to a general question which requires for an answer, not yes or no, but the exhibiting of some object".
But if the length of the process isn't known in advance, then "trying" it may not be decisive, because if the process does go on forever — then at no time will we ever be sure of the answer.
As it happens, no other method can do any better, as was shown by Alan Turing with his celebrated result on the undecidability of the so-called halting problem. There is no algorithmic procedure for determining of arbitrary algorithms whether or not they terminate from given starting states. The analysis of algorithms for their likelihood of termination is called termination analysis.
We normally require auxiliary evidence for this [that the algorithm correctly defines a mu recursive function], e.g., in the form of an inductive proof that, for each argument value, the computation terminates with a unique value.
Expressing algorithms.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements, while remaining independent of a particular implementation language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.
There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite state machine and state transition table), as flowcharts (see more at state diagram), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see more at Turing machine).
Sometimes it is helpful in the description of an algorithm to supplement small "flow charts" (state diagrams) with natural-language and/or arithmetic expressions written inside "block diagrams" to summarize what the "flow charts" are accomplishing.
Computer algorithms.
Note that CASE 3 includes two possibilities: (i) the document is NOT located at 'D:/My Documents' AND there's paper in the printer OR (ii) the document is NOT located at 'D:/My Documents' AND there's NO paper in the printer.
These examples' logic grants precedence to the instance of "NO document at 'D:/My Documents' ". Also observe that in a well-crafted CASE statement or sequence of IF-THEN-ELSE statements the number of distinct actions—4 in these examples: do nothing, print the document, display 'document not found', display 'out of paper' -- equals the number of cases.
Given unlimited memory, a computational machine with the ability to execute either a set of CASE statements or a sequence of IF-THEN-ELSE statements is Turing complete. Therefore, anything that is computable can be computed by this machine. This form of algorithm is fundamental to computer programming in all its forms (see more at McCarthy formalism).
Implementation.
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
Example.
Input: A non-empty list of numbers "L".
Output: The "largest" number in the list "L".
for each "item" in the list "L≥1", do
if the "item" > "largest", then
For a more complex example of an algorithm, see Euclid's algorithm for the greatest common divisor, one of the earliest algorithms known.
Algorithmic analysis.
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the algorithm above has a time requirement of O("n"), using the big O notation with "n" as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore it is said to have a space requirement of "O(1)", if the space required to store the input numbers is not counted, or O("n") if it is counted.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm will usually outperform a brute force sequential search when used for table lookups on sorted lists.
Formal versus empirical.
The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code.
Empirical testing is useful because it may uncover unexpected interactions that affect performance. For instance an algorithm that has no locality of reference may have much poorer performance than predicted because it 'thrashes the cache'. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Classification.
There are various ways to classify algorithms, each with its own merits.
By implementation.
One way to classify algorithms is by implementation means.
By field of study.
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.
Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry, but is now used in solving a broad range of problems in many fields.
By complexity.
Algorithms can be classified by the amount of time they need to complete compared to their input size. There is a wide variety: some algorithms complete in linear time relative to input size, some do so in an exponential amount of time or even worse, and some never halt. Additionally, some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.
By computing power.
Another way to classify algorithms is by computing power. This is typically done by considering some collection (class) of algorithms. A recursive class of algorithms is one that includes algorithms for all Turing computable functions. Looking at classes of algorithms allows for the possibility of restricting the available computational resources (time and memory) used in a computation. A subrecursive class of algorithms is one in which not all Turing computable functions can be obtained. For example, the algorithms that run in polynomial time suffice for many important types of computation but do not exhaust all Turing computable functions. The class of algorithms implemented by primitive recursive functions is another subrecursive class.
Burgin (2005, p. 24) uses a generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes a function must be determined after a finite number of steps. He defines a super-recursive class of algorithms as "a class of algorithms in which it is possible to compute functions not computable by any Turing machine" (Burgin 2005, p. 107). This is closely related to the study of methods of hypercomputation.
Legal issues.
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However, practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.
Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
Origin of the word.
The word "algorithm" comes from the name of the 9th century Persian mathematician Muhammad ibn Mūsā al-Khwārizmī whose works introduced Indian numerals and algebraic concepts. He worked in Baghdad at the time when it was the centre of scientific studies and trade. The word "algorism" originally referred only to the rules of performing arithmetic using Arabic numerals but evolved via European Latin translation of al-Khwarizmi's name into "algorithm" by the 18th century. The word evolved to include all definite procedures for solving problems or performing tasks.
Discrete and distinguishable symbols.
Tally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks, or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post-Turing machine computations.
Mechanical contrivances with discrete states.
The clock: Bolter credits the invention of the weight-driven clock as “The key invention [of Europe in the Middle Ages]", in particular the verge escapement that provides us with the tick and tock of a mechanical clock. “The accurate automatic machine” led immediately to "mechanical automata" beginning in the thirteenth century and finally to “computational machines" – the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace.
Logical machines 1870 -- Stanley Jevons' "logical abacus" and "logical machine": The technical problem was to reduce Boolean equations when presented in a form similar to what are now known as Karnaugh maps. Jevons (1880) describes first a simple "abacus" of "slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically... More recently however I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine" His machine came equipped with "certain moveable wooden rods" and "at the foot are 21 keys like those of a piano [etc]...". With this machine he could analyze a "syllogism or any other simple logical argument".
This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 "Symbolic Logic", turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's "abacus"... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine... but I suppose that it could do very completely all that can be rationally expected of any logical machine".
Jacquard loom, Hollerith punch cards, telegraphy and telephony — the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and “telephone switching technologies” were the roots of a tree leading to the development of the first computers. By the mid-1800s the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as “dots and dashes” a common sound. By the late 1800s the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the Teletype (ca. 1910) with its punched-paper use of Baudot code on tape.
Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the “burdensome’ use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".
Mathematics during the 1800s up to the mid-1900s.
Symbols and rules: In rapid succession the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's "The principles of arithmetic, presented by a new method" (1888) was "the first attempt at an axiomatization of mathematics in a symbolic language".
But Heijenoort gives Frege (1879) this kudos: Frege’s is "perhaps the most important single work ever written in logic.... in which we see a " 'formula language', that is a "lingua characterica", a language written with special symbols, "for pure thought", that is, free from rhetorical embellishments... constructed from specific symbols that are manipulated according to definite rules". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).
The paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel’s paper (1931) — he specifically cites the paradox of the liar — that completely reduces rules of recursion to numbers.
Effective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an "effective method" or "effective calculation" or "effective calculability" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser's λ-calculus a finely-honed definition of "general recursion" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his "a- [automatic-] machine" -- in effect almost identical to Post's "formulation", J. Barkley Rosser's definition of "effective method" in terms of "a machine". S. C. Kleene's proposal of a precursor to "Church thesis" that he called "Thesis I", and a few years later Kleene's renaming his Thesis "Church's Thesis" and proposing "Turing's Thesis".
Emil Post (1936) and Alan Turing (1936-7, 1939).
Here is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations — and they yield virtually identical definitions.
Alan Turing’s work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing’s biographer believed that Turing’s use of a typewriter-like model derived from a youthful interest: “Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter; and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'". Given the prevalence of Morse code and telegraphy, ticker tape machines, and Teletypes we might conjecture that all were influences.
Turing — his model of computation is now called a Turing machine — begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.
J. B. Rosser (1939) and S. C. Kleene (1943).
Rosser's footnote #5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his "An Unsolvable Problem of Elementary Number Theory" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper "On Formally Undecidable Propositions of Principia Mathematica and Related Systems I" (1931); and (3) Post (1936) and Turing (1936-7) in their mechanism-models of computation.
History after 1950.
A number of efforts have been directed toward further refinement of the definition of "algorithm", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church-Turing Thesis) and philosophy of mind (especially arguments around artificial intelligence). For more, see Algorithm characterizations.
---END.OF.DOCUMENT---

Annual plant.
An annual plant is a plant that usually germinates, flowers, and dies in a year or season. True annuals will only live longer than a year if they are prevented from setting seed. Some seedless plants can also be considered annuals even though they do not grow a flower.
In gardening, annual often refers to a plant grown outdoors in the spring and summer and surviving just for one growing season.
Many food plants are, or are grown as, annuals, including most domesticated grains. Some perennials and biennials are grown in gardens as annuals for convenience, particularly if they are not considered cold hardy for the local climate. Carrot, celery and parsley are true biennials that are usually grown as annual crops for their edible roots, petioles and leaves, respectively. Tomato, sweet potato and bell pepper are tender perennials usually grown as annuals.
Ornamental annualer perennials commonly grown as annuals are impatiens, wax begonia, snapdragon, "Pelargonium", coleus and petunia. Some biennials that can be grown as annuals are pansy and hollyhock.
One seed-to-seed life cycle for an annual can occur in as little as a month in some species, though most last several months. Oilseed rapa can go from seed-to-seed in about five weeks under a bank of fluorescent lamps in a school classroom. Many desert annuals are therophytes, because their seed-to-seed life cycle is only weeks and they spend most of the year as seeds to survive dry conditions.
Examples of true annuals include corn, lettuce, pea, cauliflower, watermelon, bean, zinnia and marigold.
Summer annuals.
Summer annuals sprout, flower and die within the same spring/summer/fall. The lawn weed, crabgrass, is a summer annual. Annual plants die off in about a year.
Winter annuals.
Winter Annuals are plants that have an annual life span but tend to germinate in the fall or winter and bloom in late autumn/fall, winter or early spring. The plants grow and bloom during the cool season when most other plants are dormant or other annuals are in seed form waiting for warmer weather to germinate. Winter annuals die after flowering and setting seed, the seeds wait to germinate until the soil temperature is cool again in the fall or winter. Winter annuals typically grow low to the ground, where they are usually sheltered from the coldest nights by snow cover, and make use of warm periods in winter for growth when the snow melts. Some common winter annuals include henbit, deadnettle, chickweed, and winter cress. Winter annuals are important ecologically, as they provide vegetative cover that prevents soil erosion during winter and early spring when no other cover exists and they provide fresh vegetation for animals and birds that feed on them.
Although they are often considered to be weeds in gardens, this viewpoint is not always necessary, as most of them die when the soil temperature warms up again in early to late spring when other plants are still dormant and have not yet leafed out.
Even though they do not compete directly with cultivated plants, sometimes winter annuals are considered a pest in commercial agriculture, because they can be hosts for insect pests or fungal diseases (ovary smut - Microbotryum sp) which attack crops being cultivated. Ironically, the property that they prevent the soil from drying out can also be problematic for commercial agriculture.
Molecular genetics.
In 2008, it was discovered that the inactivation of only two genes in an annual plant leads to the conversion into a perennial plant. Researchers deactivated the SOC1 and FUL genes in "Arabidopsis thaliana", which control flowering time. This switch established phenotypes common in perennial plants, such as wood formation.
---END.OF.DOCUMENT---

Anthophyta.
The anthophytes were thought to be a clade comprising plants bearing flower-like structures. The group contained the angiosperms - the extant flowering plants - as well as the Gnetales and the extinct Bennettitales.
Detailed morphological and molecular studies have shown that the group is not actually monophyletic. This makes it easier to reconcile molecular clock data that suggests that the angiosperms diverged from the gymnosperms around.
Some more recent studies have used the word anthophyte to describe a group which includes the angiosperms and a variety of fossils (glossopterids, "Pentoxylon", Bennettitales, and "Caytonia"), but not the Gnetales.
---END.OF.DOCUMENT---

Atlas (disambiguation).
An atlas is a collection of maps.
---END.OF.DOCUMENT---

Mouthwash.
Mouthwash or mouth rinse is a product used to enhance oral hygiene. Antiseptic and anti-plaque mouth rinse claims to kill the bacterial plaque causing caries, gingivitis, and bad breath. Anti-cavity mouth rinse uses fluoride to protect against tooth decay. It is, however, generally agreed that the use of mouthwash does not eliminate the need for both brushing and flossing. As per the American Dental Association, regular brushing and proper flossing are enough in most cases (in addition to regular dental check-ups) and mouthwash should only be used as a short-term solution.
Mouthwash may also be used to help remove mucus and food particles deeper down in the throat. Alcoholic and strongly flavored mouthwash may cause coughing when used for this purpose.
History.
The first known references to mouth rinsing is in Ayurveda and Chinese medicine, about 2700 BC, for treatment of gingivitis. Later, in the Greek and Roman periods, mouthrinsing following mechanical cleansing became common among the upper classes, and Hippocrates recommended a mixture of salt, alum, and vinegar. The Jewish Talmud, dating back about 1800 years, suggests a cure for gum ailments containing "dough water" and olive oil.
Anton van Leeuwenhoek, the famous 17th century microscopist, discovered living organisms (living, because they were motile) in deposits on the teeth (what we now call dental plaque). He also found organisms in water from the canal next to his home in Delft. He experimented with samples by adding vinegar or brandy and found that this resulted in the immediate immobilization or killing of the organisms suspended in water. Next he tried rinsing the mouth of himself and somebody else with a rather foul mouthwash containing vinegar or brandy and found that living organisms remained in the dental plaque. He concluded—correctly—that the mouthwash either did not reach, or was not present long enough, to kill the plaque organisms.
That remained the state of affairs until the late 1960s when Harald Loe (at the time a professor at the Royal Dental College in Aarhus, Denmark) demonstrated that a chlorhexidine compound could prevent the build-up of dental plaque. The reason for chlorhexidine effectiveness is that it strongly adheres to surfaces in the mouth and thus remains present in effective concentrations for many hours.
Since then commercial interest in mouthwashes has been intense and several newer products claim effectiveness in reducing the build-up in dental plaque and the associated severity of gingivitis (inflammation of the gums), in addition to fighting bad breath. Many of these solutions aim to control the Volatile Sulfur Compound (VSC)-creating anaerobic bacteria that live in the mouth and excrete substances that lead to bad breath and unpleasant mouth taste.
Usage.
Common use involves rinsing the mouth with about 20ml (2/3 fl oz) of mouthwash two times a day after brushing. The wash is typically swished or gargled for about half a minute and then spat out. In some brands, the expectorate is stained, so that one can see the bacteria and debris. It is probably advisable to use mouthwash at least an hour after brushing with toothpaste when the toothpaste contains sodium lauryl sulfate, since the anionic compounds in the SLS toothpaste can deactivate cationic agents present in the mouthrinse.
Active ingredients.
Active ingredients in commercial brands of mouthwash can include thymol, eucalyptol, hexetidine, methyl salicylate, menthol, chlorhexidine gluconate, benzalkonium chloride, cetylpyridinium chloride, methylparaben, hydrogen peroxide, domiphen bromide and sometimes fluoride, enzymes and calcium. Ingredients also include water, sweeteners such as sorbitol, sucralose, sodium saccharine, and xylitol (which doubles as a bacterial inhibitor).
Sometimes a significant amount of alcohol (up to 27% vol) is added, as a carrier for the flavor, to provide "bite", and to contribute an antibacterial effect. Because of the alcohol content, it is possible to fail a breathalyzer test after rinsing; in addition, alcohol is a drying agent and may worsen chronic bad breath. Furthermore, it is possible for alcoholics to abuse mouthwash. Recently, the possibility that the alcohol used in mouthrinses acts as a carcinogen was raised, but there is to date no scientific consensus on the issue. Commercial mouthwashes usually contain a preservative such as sodium benzoate to preserve freshness once the container has been opened. Many newer brands are alcohol-free and contain odor-elimination agents such as oxidizers, as well as odor-preventing agents such as zinc ion to keep future bad breath from developing.
Alternative mouthwash ingredients.
A salt mouthwash is a home treatment for mouth infections and/or injuries, or post extraction, and is made by dissolving a teaspoon of salt in a cup of warm water.
Recently, the use of herbal mouthwashes such as persica is increasing, due to the perceived discoloration effects and unpleasant taste of Chlorhexidine. Research has also indicated that sesame and sunflower oils as cheap alternatives compared to chlorhexidine.
Other products like hydrogen peroxide have been tried out as stand-alone and in combination with chlorhexidine, due to some inconsistent results regarding its usefulness.
Another study has demonstrated that daily use of an alum-containing mouthrinse was safe and produced a significant effect on plaque that supplemented the benefits of daily toothbrushing.
However, many studies acknowledge that Chlorhexidine remains the most effective mouthwash when used on an already clean tooth surface or immediately after surgery. As chlorhexidine has difficulty in penetrating plaque biofilm, other mouthwashes may be more effective where pre-existing plaque is present. One side-effect as noted on the label is the staining of the teeth will occur for prolong usage.
Compounding.
Custom mouthwashes, called "magic mouthwash" may be prescribed by dentists for post oral surgeries. Variations are common, and some are done with over-the-counter products.
Health risks.
In January 2009 a report published in the "Dental Journal of Australia" concluded there is "sufficient evidence" that "alcohol-containing mouthwashes contribute to the increased risk of development of oral cancer". However, this claim has been disputed by Yinka Ebo of Cancer Research UK, concluding that "there is still not enough evidence to suggest that using mouthwash that contains alcohol will increase the risk of mouth cancer". More recent studies have shown that the risk of acquiring cancer rises almost five times in mouthwash users who neither smoke nor drink (with a higher rate of increase in those who do). The same study also highlighted side effects from several mainstream mouthwashes that included dental erosion and accidental poisoning of children.
---END.OF.DOCUMENT---

Alexander the Great.
Alexander III of Macedon (356–323 BC), popularly known as Alexander the Great (, "Mégas Aléxandros"), was a Greek king (basileus) of Macedon. He is the most celebrated member of the Argead Dynasty and created one of the largest empires in ancient history. Born in Pella in 356 BC, Alexander received a classical Greek education under the tutorship of famed philosopher Aristotle, succeeded his father Philip II of Macedon to the throne in 336 BC after the King was assassinated, and died thirteen years later at the age of 32. Although both Alexander's reign and empire were short-lived, the cultural impact of his conquests lasted for centuries. Alexander is one of the most famous figures of antiquity, and is remembered for his tactical ability, his conquests, and for spreading Greek culture into the East (marking the beginning of Hellenistic civilization).
Philip had brought most of the city-states of mainland Greece under Macedonian hegemony, using both military and diplomatic means. Upon Philip's death, Alexander inherited a strong kingdom and an experienced army. He succeeded in being awarded the generalship of Greece and, with his authority firmly established, launched the military plans for expansion left by his father. He invaded Persian-ruled Asia Minor, and began a series of campaigns lasting ten years. Alexander repeatedly defeated the Persians in battle; marched through Syria, Egypt, Mesopotamia, Persia, and Bactria; and in the process he overthrew the Persian king Darius III and conquered the entirety of the Persian Empire. Following his desire to reach the "ends of the world and the Great Outer Sea", he invaded India, but was eventually forced to turn back by the near-mutiny of his troops, who were tired of war.
Alexander died in Babylon in 323 BC, before realizing a series of planned campaigns that would have begun with an invasion of Arabia. In the years following Alexander's death, a series of civil wars tore his empire apart, which resulted in the formation of a number of states ruled by Macedonian aristocracy (the Diadochi). Remarkable though his conquests were, Alexander's lasting legacy was not his reign, but the cultural diffusion his conquests engendered. Alexander's importation of Greek colonists and culture to the East resulted in a new "Hellenistic" culture, aspects of which were still evident in the traditions of the Byzantine Empire until the mid-15th century. Alexander became legendary as a classical hero in the mould of Achilles, and features prominently in the history and myth of Greek and non-Greek cultures. He became the measure against which generals, even to this day, compare themselves, and military academies throughout the world still teach his tactical exploits.
Lineage and childhood.
Alexander was born on 20 (or 21) July 356 BC, in Pella, the capital of the Kingdom of Macedon. He was the son of Philip II, the King of Macedon.
His mother was Philip's fourth wife Olympias, the daughter of Neoptolemus I, the king of the northern Greek state of Epirus. Although Philip had either seven or eight wives, Olympias was his principal wife for a time.
As a member of the Argead dynasty, Alexander claimed patrilineal descent from Heracles through Caranus of Macedon. From his mother's side and the Aeacids, he claimed descent from Neoptolemus, son of Achilles; Alexander was a second cousin of the celebrated general Pyrrhus of Epirus, who was ranked by Hannibal as, depending on the source, either the best or second-best (after Alexander) commander the world had ever seen.
According to the ancient Greek historian Plutarch, Olympias, on the eve of the consummation of her marriage to Philip, dreamed that her womb was struck by a thunder bolt, causing a flame that spread "far and wide" before dying away. Some time after the wedding, Philip was said to have seen himself, in a dream, sealing up his wife's womb with a seal upon which was engraved the image of a lion. Plutarch offers a variety of interpretations of these dreams: that Olympia was pregnant before her marriage, indicated by the sealing of her womb; or that Alexander's father was Zeus. Ancient commentators were divided as to whether the ambitious Olympias promulgated the story of Alexander's divine parentage, some claiming she told Alexander, others that she dismissed the suggestion as impious.
On the day that Alexander was born, Philip was preparing himself for his siege on the city of Potidea on the peninsula of Chalkidiki. On the same day, Philip received news that his general Parmenion had defeated the combined Illyrian and Paeonian armies, and that his horses had won at the Olympic Games. It was also said that on this day, the Temple of Artemis in Ephesus—one of the Seven Wonders of the World—burnt down, leading Hegesias of Magnesia to say that it burnt down because Artemis was attending the birth of Alexander.
In his early years, Alexander was raised by his nurse, Lanike, the sister of Alexander's future friend and general Cleitus the Black. Later on in his childhood, Alexander was tutored by the strict Leonidas, a relative of his mother, and by Lysimachus.
When Alexander was ten years old, a horse trader from Thessaly brought Philip a horse, which he offered to sell for thirteen talents. The horse refused to be mounted by anyone, and Philip ordered it to be taken away. Alexander, however, detected the horse's fear of his own shadow and asked for a turn to tame the horse, which he eventually managed. According to Plutarch, Philip, overjoyed at this display of courage and ambition, kissed him tearfully, declaring: "My boy, you must find a kingdom big enough for your ambitions. Macedon is too small for you", and bought the horse for him. Alexander would name the horse Bucephalus, meaning 'ox-head'. Bucephalus would be Alexander's companion throughout his journeys as far as India. When Bucephalus died (due to old age, according to Plutarch, for he was already thirty), Alexander named a city after him (Bucephala).
Adolescence and education.
When Alexander was thirteen years old, Philip decided that Alexander needed a higher education, and he began to search for a tutor. Many people were passed over including Isocrates and Speusippus, Plato's successor at the Academy, who offered to resign to take up the post. In the end, Philip offered the job to Aristotle, who accepted, and Philip gave them the Temple of the Nymphs at Mieza as their classroom. In return for teaching Alexander, Philip agreed to rebuild Aristotle's hometown of Stageira, which Philip had razed, and to repopulate it by buying and freeing the ex-citizens who were slaves, or pardoning those who were in exile.
Mieza was like a boarding school for Alexander and the children of Macedonian nobles, such as Ptolemy, Hephaistion, and Cassander. Many of the pupils who learned by Alexander's side would become his friends and future generals, and are often referred to as the 'Companions'. At Mieza, Aristotle educated Alexander and his companions in medicine, philosophy, morals, religion, logic, and art. From Aristotle's teaching, Alexander developed a passion for the works of Homer, and in particular the Iliad; Aristotle gave him an annotated copy, which Alexander was to take on his campaigns.
Regency and ascent of Macedon.
When Alexander became sixteen years old, his tutorship under Aristotle came to an end. Philip, the king, departed to wage war against Byzantium, and Alexander was left in charge as regent of the kingdom. During Philip's absence, the Thracian Maedi revolted against Macedonian rule. Alexander responded quickly; he crushed the Maedi insurgence, driving them from their territory, colonised it with Greeks, and founded a city named Alexandropolis.
After Philip's return from Byzantium, he dispatched Alexander with a small force to subdue certain revolts in southern Thrace. During another campaign against the Greek city of Perinthus, Alexander is reported to have saved his father's life. Meanwhile, the city of Amphissa began to work lands that were sacred to Apollo near Delphi, a sacrilege that gave Philip the opportunity to further intervene in the affairs of Greece. Still occupied in Thrace, Philip ordered Alexander to begin mustering an army for a campaign in Greece. Concerned with the possibility of other Greek states intervening, Alexander made it look as if he were preparing to attack Illyria instead. During this turmoil, the Illyrians took the opportunity to invade Macedonia, but Alexander repelled the invaders.
Philip joined Alexander with his army in 338 BC, and they marched south through Thermopylae, which they took after a stubborn resistance from its Theban garrison. They went on to occupy the city of Elatea, a few days march from both Athens and Thebes. Meanwhile, the Athenians, led by Demosthenes, voted to seek an alliance with Thebes in the war against Macedonia. Both Athens and Philip sent embassies to try to win Thebes's favour, with the Athenians eventually succeeding. Philip marched on Amphissa (theoretically acting on the request of the Amphicytonic League), captured the mercenaries sent there by Demosthenes, and accepted the city's surrender. Philip then returned to Elatea and sent a final offer of peace to Athens and Thebes, which was rejected.
As Philip marched south, he was blocked near Chaeronea, Boeotia by the forces of Athens and Thebes. During the ensuing Battle of Chaeronea, Philip commanded the right, and Alexander the left wing, accompanied by a group of Philip's trusted generals. According to the ancient sources, the two sides fought bitterly for a long time. Philip deliberately commanded the troops on his right wing to backstep, counting on the untested Athenian hoplites to follow, thus breaking their line. On the left, Alexander was the first to break into the Theban lines, followed by Philip's generals. Having achieved a breach in the enemy's cohesion, Philip ordered his troops to press forward and quickly routed his enemy. With the rout of the Athenians, the Thebans were left to fight alone; surrounded by the victorious enemy, they were crushed.
After the victory at Chaeronea, Philip and Alexander marched unopposed into the Peloponnese welcomed by all cities; however, when they reached Sparta, they were refused, and they simply left. At Corinth, Philip established a "Hellenic Alliance" (modeled on the old anti-Persian alliance of the Greco-Persian Wars), with the exception of Sparta. Philip was then named "Hegemon" (often translated as 'Supreme Commander') of this league (known by modern historians as the League of Corinth). He then announced his plans for a war of revenge against the Persian Empire, which he would command.
Exile and return.
After returning to Pella, Philip fell in love with and married Cleopatra Eurydice, the niece of one of his generals, Attalus. This marriage made Alexander's position as heir to the throne less secure, since if Cleopatra Eurydice bore Philip a son, there would be a fully Macedonian heir, while Alexander was only half Macedonian. During the wedding banquet, a drunken Attalus made a speech praying to the gods that the union would produce a legitimate heir to the Macedonian throne. Alexander shouted to Attalus, "What am I then, a bastard?" and he threw his goblet at him. Philip, who was also drunk, drew his sword and advanced towards Alexander before collapsing, leading Alexander to say, "See there, the man who makes preparations to pass out of Europe into Asia, overturned in passing from one seat to another."
Alexander fled from Macedon taking his mother with him, whom he dropped off with her brother in Dodona, capital of Epirus. He carried on to Illyria, where he sought refuge with the Illyrian King and was treated as a guest by the Illyrians, despite having defeated them in battle a few years before. Alexander returned to Macedon after six months in exile due to the efforts of a family friend, Demaratus the Corinthian, who mediated between the two parties.
The following year, the Persian satrap (governor) of Caria, Pixodarus, offered the hand of his eldest daughter to Alexander's half-brother, Philip Arrhidaeus. Olympias and several of Alexander's friends suggested to Alexander that this move showed that Philip intended to make Arrhidaeus his heir. Alexander reacted by sending an actor, Thessalus of Corinth, to tell Pixodarus that he should not offer his daughter's hand to an illegitimate son, but instead to Alexander. When Philip heard of this, he scolded Alexander for wishing to marry the daughter of a Carian. Philip had four of Alexander's friends, Harpalus, Nearchus, Ptolemy and Erigyius exiled, and had the Corinthians bring Thessalus to him in chains.
Accession.
In 336 BC, whilst at Aegae, attending the wedding of his daughter by Olympias, Cleopatra, to Olympias's brother, Alexander I of Epirus, Philip was assassinated by the captain of his bodyguard, Pausanias. As Pausanias tried to escape, he tripped over a vine and was killed by his pursuers, including two of Alexander's companions, Perdiccas and Leonnatus. Alexander was proclaimed king by the Macedonian army and by the Macedonian noblemen at the age of 20.
Power consolidation.
Alexander began his reign by having his potential rivals to the throne murdered. He had his cousin, the former Amyntas IV, executed, as well as having two Macedonian princes from the region of Lyncestis killed, while a third, Alexander Lyncestes, was spared. Olympias had Cleopatra Eurydice and her daughter by Philip, Europa, burned alive. When Alexander found out about this, he was furious with his mother. Alexander also ordered the murder of Attalus, who was in command of the advance guard of the army in Asia Minor. Attalus was at the time in correspondence with Demosthenes, regarding the possibility of defecting to Athens. Regardless of whether Attalus actually intended to defect, he had already severely insulted Alexander, and having just had Attalus's daughter and grandchildren murdered, Alexander probably felt Attalus was too dangerous to leave alive. Alexander spared the life of Arrhidaeus, who was by all accounts mentally disabled, possibly as a result of poisoning by Olympias.
News of Philip's death roused many states into revolt, including Thebes, Athens, Thessaly, and the Thracian tribes to the north of Macedon. When news of the revolts in Greece reached Alexander, he responded quickly. Though his advisors advised him to use diplomacy, Alexander mustered the Macedonian cavalry of 3,000 men and rode south towards Thessaly, Macedon's neighbor to the south. When he found the Thessalian army occupying the pass between Mount Olympus and Mount Ossa, he had the men ride over Mount Ossa. When the Thessalians awoke the next day, they found Alexander in their rear, and promptly surrendered, adding their cavalry to Alexander's force, as he rode down towards the Peloponnesus.
Alexander stopped at Thermopylae, where he was recognized as the leader of the Amphictyonic League before heading south to Corinth. Athens sued for peace and Alexander received the envoy and pardoned anyone involved with the uprising. At Corinth, he was given the title "Hegemon", and like Philip, appointed commander of the forthcoming war against Persia. While at Corinth, he heard the news of the Thracian rising to the north.
Balkan campaign.
Before crossing to Asia, Alexander wanted to safeguard his northern borders; and, in the spring of 335 BC, he advanced to suppress several apparent revolts. Starting from Amphipolis, he first went east into the country of the "Independent Thracians"; and at Mount Haemus, the Macedonian army attacked and defeated a Thracian army manning the heights. The Macedonians marched on into the country of the Triballi, and proceeded to defeat the Triballian army near the Lyginus river (a tributary of the Danube). Alexander then advanced for three days on to the Danube, encountering the Getae tribe on the opposite shore. Surprising the Getae by crossing the river at night, he forced the Getae army to retreat after the first cavalry skirmish, leaving their town to the Macedonian army. News then reached Alexander that Cleitus, King of Illyria, and King Glaukias of the Taulanti were in open revolt against Macedonian authority. Marching west into Illyria, Alexander defeated each in turn, forcing Cleitus and Glaukias to flee with their armies, leaving Alexander's northern frontier secure.
While he was triumphantly campaigning north, the Thebans and Athenians rebelled once more. Alexander reacted immediately, but, while the other cities once again hesitated, Thebes decided to resist with the utmost vigor. However, the resistance was useless, as the city was razed to the ground amid great bloodshed, and its territory was divided between the other Boeotian cities. The end of Thebes cowed Athens into submission, leaving all of Greece at least outwardly at peace with Alexander.
Asia Minor.
Alexander's army crossed the Hellespont in 334 BC with approximately 42,000 soldiers from Macedon and various Greek city-states, mercenaries, and feudally-raised soldiers from Thrace, Paionia, and Illyria. After an initial victory against Persian forces at the Battle of the Granicus, Alexander accepted the surrender of the Persian provincial capital and treasury of Sardis and proceeded down the Ionian coast. At Halicarnassus, Alexander successfully waged the first of many sieges, eventually forcing his opponents, the mercenary captain Memnon of Rhodes and the Persian satrap of Caria, Orontobates, to withdraw by sea. Alexander left the government of Caria to Ada, who adopted Alexander as her son.
From Halicarnassus, Alexander proceeded into mountainous Lycia and the Pamphylian plain, asserting control over all coastal cities. He did this to deny the Persians naval bases. Since Alexander had no reliable fleet of his own, defeating the Persian fleet required land control. From Pamphylia onward, the coast held no major ports and so Alexander moved inland. At Termessos, Alexander humbled but did not storm the Pisidian city. At the ancient Phrygian capital of Gordium, Alexander 'undid' the hitherto unsolvable Gordian Knot, a feat said to await the future "king of Asia". According to the most vivid story, Alexander proclaimed that it did not matter how the knot was undone, and he hacked it apart with his sword.
The Levant and Syria.
After spending the winter campaigning in Asia Minor, Alexander's army crossed the Cilician Gates in 333 BC, and defeated the main Persian army under the command of Darius III at the Battle of Issus in November. Darius was forced to flee the battle after his army broke, and in doing so left behind his wife, his two daughters, his mother Sisygambis, and a fabulous amount of treasure. He afterward offered a peace treaty to Alexander, the concession of the lands he had already conquered, and a ransom of 10,000 talents for his family. Alexander replied that since he was now king of Asia, it was he alone who decided territorial divisions.
Alexander proceeded to take possession of Syria, and most of the coast of the Levant. However, the following year, 332 BC, he was forced to attack Tyre, which he eventually captured after a famous siege. After the capture of Tyre, Alexander crucified all the men of military age, and sold the women and children into slavery.
Egypt.
When Alexander destroyed Tyre, most of the towns on the route to Egypt quickly capitulated, with the exception of Gaza. The stronghold at Gaza was built on a hill and was heavily fortified. At the beginning of the Siege of Gaza, Alexander utilized the engines he had employed against Tyre. After three unsuccessful assaults, the stronghold was finally taken by force, but not before Alexander received a serious shoulder wound. When Gaza was taken, the male population was put to the sword and the women and children were sold into slavery.
Jerusalem, on the other hand, opened its gates in surrender, and according to Josephus, Alexander was shown the book of Daniel's prophecy, presumably chapter 8, where a mighty Greek king would subdue and conquer the Persian Empire. Thereupon, Alexander spared Jerusalem and pushed south into Egypt.
Alexander advanced on Egypt in later 332 BC, where he was regarded as a liberator. He was pronounced the new "master of the Universe" and son of the deity of Amun at the Oracle of Siwa Oasis in the Libyan desert. Henceforth, Alexander often referred to Zeus-Ammon as his true father, and subsequent currency depicted him adorned with ram horns as a symbol of his divinity. During his stay in Egypt, he founded Alexandria-by-Egypt, which would become the prosperous capital of the Ptolemaic kingdom after his death.
Assyria and Babylonia.
Leaving Egypt in 331 BC, Alexander marched eastward into Mesopotamia (now northern Iraq) and defeated Darius once more at the Battle of Gaugamela. Once again, Darius was forced to leave the field, and Alexander chased him as far as Arbela. Darius fled over the mountains to Ecbatana (modern Hamedan), but Alexander instead marched to and captured Babylon.
Persia.
From Babylon, Alexander went to Susa, one of the Achaemenid capitals, and captured its legendary treasury. Sending the bulk of his army to the Persian ceremonial capital of Persepolis via the Royal Road, Alexander himself took selected troops on the direct route to the city. However, the pass of the Persian Gates (in the modern Zagros Mountains) had been blocked by a Persian army under Ariobarzanes, and Alexander had to storm the pass. Alexander then made a dash for Persepolis before its garrison could loot the treasury. At Persepolis, Alexander stared at the crumbled statue of Xerxes and decided to leave it on the ground. During their stay at the capital, a fire broke out in the eastern palace of Xerxes and spread to the rest of the city. Theories abound as to whether this was the result of a drunken accident, or a deliberate act of revenge for the burning of the Acropolis of Athens during the Second Persian War.
Fall of the Empire and the East.
Alexander then set off in pursuit of Darius again, first into Media, and then Parthia. The Persian king was no longer in control of his destiny, having been taken prisoner by Bessus, his Bactrian satrap and kinsman. As Alexander approached, Bessus had his men fatally stab the Great King and then declared himself Darius' successor as Artaxerxes V, before retreating into Central Asia to launch a guerrilla campaign against Alexander. Darius' remains were buried by Alexander next to his Achaemenid predecessors in a full regal funeral. Alexander claimed that, while dying, Darius had named him as his successor to the Achaemenid throne. The Achaemenid Empire is normally considered to have fallen with the death of Darius.
Alexander, now considering himself the legitimate successor to Darius, viewed Bessus as a usurper to the Achaemenid throne, and set out to defeat him. This campaign, initially against Bessus, turned into a grand tour of central Asia, with Alexander founding a series of new cities, all called Alexandria, including modern Kandahar in Afghanistan, and Alexandria Eschate ("The Furthest") in modern Tajikistan. The campaign took Alexander through Media, Parthia, Aria (West Afghanistan), Drangiana, Arachosia (South and Central Afghanistan), Bactria (North and Central Afghanistan), and Scythia.
Bessus was betrayed in 329 BC by Spitamenes, who held an undefined position in the satrapy of Sogdiana. Spitamenes handed over Bessus to Ptolemy, one of Alexander's trusted companions, and Bessus was executed. However, when, at some point later, Alexander was on the Jaxartes, Spitamenes raised Sogdiana in revolt. Alexander launched a campaign and defeated him in the Battle of Gabai; after the defeat, Spitamenes was killed by his own men, who then sued for peace.
Problems and plots.
During this time, Alexander took the Persian title "King of Kings" ("Shahanshah") and adopted some elements of Persian dress and customs at his court, notably the custom of "proskynesis", either a symbolic kissing of the hand, or prostration on the ground, that Persians paid to their social superiors. The Greeks regarded the gesture as the province of deities and believed that Alexander meant to deify himself by requiring it. This cost him much in the sympathies of many of his countrymen. A plot against his life was revealed, and one of his officers, Philotas, was executed for failing to bring the plot to his attention. The death of the son necessitated the death of the father, and thus Parmenion, who had been charged with guarding the treasury at Ecbatana, was assassinated by command of Alexander, so he might not make attempts at vengeance. Most infamously, Alexander personally slew the man who had saved his life at Granicus, Cleitus the Black, during a drunken argument at Maracanda. Later, in the Central Asian campaign, a second plot against his life was revealed, this one instigated by his own royal pages. His official historian, Callisthenes of Olynthus (who had fallen out of favor with the king by leading the opposition to his attempt to introduce "proskynesis"), was implicated in the plot; however, there has never been consensus among historians regarding his involvement in the conspiracy.
Invasion of the Indian subcontinent.
After the death of Spitamenes and his marriage to Roxana (Roshanak in Bactrian) to cement his relations with his new Central Asian satrapies, Alexander was finally free to turn his attention to the Indian subcontinent. Alexander invited all the chieftains of the former satrapy of Gandhara, in the north of what is now Pakistan, to come to him and submit to his authority. Omphis (whose actual name is Ambhi), ruler of Taxila, whose kingdom extended from the Indus to the Hydaspes, complied, but the chieftains of some hill clans, including the Aspasioi and Assakenoi sections of the Kambojas (known in Indian texts also as Ashvayanas and Ashvakayanas), refused to submit.
In the winter of 327/326 BC, Alexander personally led a campaign against these clans; the Aspasioi of Kunar valleys, the Guraeans of the Guraeus valley, and the Assakenoi of the Swat and Buner valleys. A fierce contest ensued with the Aspasioi in which Alexander himself was wounded in the shoulder by a dart but eventually the Aspasioi lost the fight. Alexander then faced the Assakenoi, who fought bravely and offered stubborn resistance to Alexander in the strongholds of Massaga, Ora and Aornos. The fort of Massaga could only be reduced after several days of bloody fighting in which Alexander himself was wounded seriously in the ankle. According to Curtius, "Not only did Alexander slaughter the entire population of Massaga, but also did he reduce its buildings to rubbles". A similar slaughter then followed at Ora, another stronghold of the Assakenoi. In the aftermath of Massaga and Ora, numerous Assakenians fled to the fortress of Aornos. Alexander followed close behind their heels and captured the strategic hill-fort after the fourth day of a bloody fight.
After Aornos, Alexander crossed the Indus and fought and won an epic battle against a local ruler Porus, who ruled a region in the Punjab, in the Battle of Hydaspes in 326 BC. Alexander was greatly impressed by Porus for his bravery in battle, and therefore made an alliance with him and appointed him as satrap of his own kingdom, even adding land he did not own before. Additional reasons were probably political since, to control lands so distant from Greece required local assistance and co-operation. Alexander named one of the two new cities that he founded, Bucephala, in honor of the horse that had brought him to India, and had died during the battle.
Revolt of the army.
East of Porus' kingdom, near the Ganges River, was the powerful Nanda Empire of Magadha and Gangaridai Empire of Bengal. Fearing the prospects of facing other powerful Indian armies and exhausted by years of campaigning, his army mutinied at the Hyphasis River, refusing to march further east. This river thus marks the easternmost extent of Alexander's conquests.
As for the Macedonians, however, their struggle with Porus blunted their courage and stayed their further advance into India. For having had all they could do to repulse an enemy who mustered only twenty thousand infantry and two thousand horse, they violently opposed Alexander when he insisted on crossing the river Ganges also, the width of which, as they learned, was thirty-two furlongs, its depth a hundred fathoms, while its banks on the further side were covered with multitudes of men-at-arms and horsemen and elephants. For they were told that the kings of the Ganderites and Praesii were awaiting them with eighty thousand horsemen, two hundred thousand footmen, eight thousand chariots, and six thousand war elephants.
Alexander spoke to his army and tried to persuade them to march further into India but Coenus pleaded with him to change his opinion and return, the men, he said, "longed to again see their parents, their wives and children, their homeland". Alexander, seeing the unwillingness of his men, eventually agreed and turned south. Along the way his army conquered the Malli clans (in modern day Multan), and other Indian tribes.
Return.
Alexander sent much of his army to Carmania (modern southern Iran) with his general Craterus, and commissioned a fleet to explore the Persian Gulf shore under his admiral Nearchus, while he led the rest of his forces back to Persia through the more difficult southern route along the Gedrosian Desert and Makran (now part of southern Iran and Pakistan).
Last years in Persia.
Discovering that many of his satraps and military governors had misbehaved in his absence, Alexander executed a number of them as examples, on his way to Susa. As a gesture of thanks, he paid off the debts of his soldiers, and announced that he would send those over-aged and disabled veterans back to Macedon under Craterus. But, his troops misunderstood his intention and mutinied at the town of Opis, refusing to be sent away and bitterly criticizing his adoption of Persian customs and dress, and the introduction of Persian officers and soldiers into Macedonian units. Alexander executed the ringleaders of the mutiny, but forgave the rank and file. In an attempt to craft a lasting harmony between his Macedonian and Persian subjects, he held a mass marriage of his senior officers to Persian and other noblewomen at Susa, but few of those marriages seem to have lasted much beyond a year. Meanwhile, upon his return, Alexander learned some men had desecrated the tomb of Cyrus the Great, and swiftly executed them, because they were put in charge of guarding the tomb Alexander held in honor.
After Alexander traveled to Ecbatana to retrieve the bulk of the Persian treasure, his closest friend and possibly lover Hephaestion died of an illness, or possibly of poisoning. According to Plutarch, Alexander, distraught over the death of his longtime companion, sacked a nearby town, and put all of its inhabitants to the sword, as a sacrifice to Hephaestion's ghost. Arrian finds great diversity and casts doubts on the accounts of Alexander's displays of grief, although he says that they all agree that Hephaestion's death devastated him, and that he ordered the preparation of an expensive funeral pyre in Babylon, as well as a decree for the observance of a public mourning.
Back in Babylon, Alexander planned a series of new campaigns, beginning with an invasion of Arabia, but he would not have a chance to realize them.
Final days.
On either 10 or 11 June 323 BC, Alexander died in the palace of Nebuchadnezzar II, in Babylon at the age of 32. Plutarch gives a lengthy account of the circumstances of his death, echoed (without firm dates) by Arrian. Roughly 14 days before his death, Alexander entertained his admiral Nearchus, and then, instead of going to bed, spent the night and next day drinking with Medius of Larissa. After this, and by 18 Daesius (a Macedonian month) he had developed a fever, which then grew steadily worse. By 25 Daesius, he was unable to speak. By 26 Daesius, the common soldiers had become anxious about his health, or thought he was already dead. They demanded to see him, and Alexander's generals acquiesced. The soldiers slowly filed past him, whilst Alexander raised his right hand in greeting, still unable to speak. Two days later, on 28 Daesius (although Aristobolus's account says it was 30 Daesius), Alexander was dead. Conversely, Diodorus recounts that Alexander was struck down with pain after downing a large bowl of unmixed wine in honour of Hercules, and (rather mysteriously) died after some agony, which is also mentioned as an alternative by Arrian, but Plutarch specifically refutes this claim.
Poison.
Given the propensity of the Macedonian aristocracy to assassination, it is scarcely surprising that allegations of foul play have been made about the death of Alexander. Diodorus, Plutarch, Arrian and Justin all mention the theory that Alexander was poisoned. Plutarch dismisses it as a fabrication, but both Diodorus and Arrian say that they only mention it for the sake of completeness. The accounts are nevertheless fairly consistent in designating Antipater, recently removed from the position of Macedonian viceroy, and at odds with Olympias, as the head of the alleged plot. Perhaps taking his summons to Babylon as a death sentence in waiting, and having seen the fate of Parmenion and Philotas, Antipater arranged for Alexander to be poisoned by his son Iollas, who was Alexander's wine-pourer. There is even a suggestion that Aristotle may have had a hand in the plot. Conversely, the strongest argument against the poison theory is the fact that twelve days had passed between the start of his illness and his death; in the ancient world, such long-acting poisons were probably not available.
Natural causes.
Several natural causes (diseases) have been suggested as the cause of Alexander's death; malaria or typhoid fever are obvious candidates. A 1998 article in the "New England Journal of Medicine" attributed his death to typhoid fever complicated by bowel perforation and ascending paralysis, whereas another recent analysis has suggested pyrogenic spondylitis or meningitis as the cause. Other illnesses could have also been the culprit, including acute pancreatitis or the West Nile virus. Natural-cause theories also tend to emphasise that Alexander's health may have been in general decline after years of heavy drinking and his suffering severe wounds (including one in India that nearly claimed his life). Furthermore, the anguish that Alexander felt after Hephaestion's death may have contributed to his declining health.
The most probable cause of Alexanders death is however, the result of overdosing on medicine made from Hellebore, deadly in large doses. The very few things we do know about his death, can today be explained only with accidental hellebore-poisoning.
Fate after death.
Alexander's body was placed in a gold anthropoid sarcophagus, which was in turn placed in a second gold casket. According to Aelian, a seer called Aristander foretold that the land where Alexander was laid to rest "would be happy and unvanquishable forever". Perhaps more likely, the successors may have seen possession of the body as a symbol of legitimacy (it was a royal prerogative to bury the previous king). At any rate, Ptolemy stole the funeral cortege, and took it to Memphis. His successor, Ptolemy II Philadelphus, transferred the sarcophagus to Alexandria, where it remained until at least Late Antiquity. Ptolemy IX Lathyros, one of the last successors of Ptolemy I, replaced Alexander's sarcophagus with a glass one so he could melt the original down for issues of his coinage. Pompey, Julius Caesar and Augustus all visited the tomb in Alexandria, the latter allegedly accidentally knocking the nose off the body. Caligula was said to have taken Alexander's breastplate from the tomb for his own use. In c. AD 200, Emperor Septimius Severus closed Alexander's tomb to the public. His son and successor, Caracalla, was a great admirer of Alexander, and visited the tomb in his own reign. After this, details on the fate of the tomb are sketchy.
The so-called "Alexander Sarcophagus", discovered near Sidon and now in the Istanbul Archaeology Museum, is so named not because it was thought to have contained Alexander's remains, but because its bas-reliefs depict Alexander and his companions hunting and in battle with the Persians. It was originally thought to have been the sarcophagus of Abdalonymus (died 311 BC), the king of Sidon appointed by Alexander immediately following the battle of Issus in 331. However, more recently, it has been suggested that it may date from earlier than Abdalonymus' death.
Division of the Empire.
Alexander had no obvious or legitimate heir, his son Alexander IV by Roxane being born after Alexander's death. This left the huge question as to who would rule the newly-conquered, and barely-pacified Empire. According to Diodorus, Alexander's companions asked him when he was on his deathbed to whom he bequeathed his kingdom; his laconic reply was "tôi kratistôi"—"to the strongest". Given that Arrian and Plutarch have Alexander speechless by this point, it is possible that this is an apocryphal story. Diodorus, Curtius and Justin also have the more plausible story of Alexander passing his signet ring to Perdiccas, one of his bodyguard and leader of the companion cavalry, in front of witnesses, thereby possibly nominating Perdiccas as his successor.
In any event, Perdiccas initially avoided explicitly claiming power, instead suggesting that Roxane's baby would be king, if male; with himself, Craterus, Leonnatus and Antipater as guardians. However, the infantry, under the command of Meleager, rejected this arrangement since they had been excluded from the discussion. Instead, they supported Alexander's half-brother Philip Arrhidaeus. Eventually, the two sides reconciled, and after the birth of Alexander IV, he and Philip III were appointed joint kings of the Empire—albeit in name only.
It was not long, however, before dissension and rivalry began to afflict the Macedonians. The satrapies handed out by Perdiccas at the Partition of Babylon became power bases each general could use to launch his own bid for power. After the assassination of Perdiccas in 321 BC, all semblance of Macedonian unity collapsed, and 40 years of war between "The Successors" ("Diadochi") ensued before the Hellenistic world settled into four stable power blocks: the Ptolemaic kingdom of Egypt, the Seleucid Empire in the east, the Kingdom of Pergamon in Asia Minor, and Macedon. In the process, both Alexander IV and Philip III were murdered.
Testament.
Diodorus relates that Alexander had given detailed written instructions to Craterus some time before his death. Although Craterus had already started to carry out some of Alexander's commands, the successors chose not to further implement them, on the grounds they were impractical and extravagant.
Physical appearance.
Green provides a description of Alexander's appearance, based on ancient sources:Physically, Alexander was not prepossessing. Even by Macedonian standards he was very short, though stocky and tough. His beard was scanty, and he stood out against his hirsute Macedonian barons by going clean-shaven. His neck was in some way twisted, so that he appeared to be gazing upward at an angle. His eyes (one blue, one brown) revealed a dewy, feminine quality. He had a high complexion and a harsh voice.
Many descriptions and statues portray Alexander with the aforementioned gaze looking upward and outward. Both his father Philip II and his brother Philip Arrhidaeus also suffered from physical deformities, which had led to the suggestion that Alexander suffered from a congenital scoliotic disorder (familial neck and spinal deformity). Furthermore, it has been suggested that this may have contributed to his death.
During his last years, sculptor Lysippus sculpted an image of Alexander. Lysippus had captured in the stone Alexander's appearance characteristics; slightly left-turned neck and peculiar gaze. Lysippus' sculpture, which is opposite to his often vigorous portrayal, especially in coinage of the time, is thought to be the most faithful depiction of Alexander.
Personality.
Alexander's personality is well described by the ancient sources. Some of his strongest personality traits formed in response to his parents. His mother had huge ambitions for Alexander, and encouraged him to believe it was his destiny to conquer the Persian Empire. Indeed, Olympias may have gone to the extent of poisoning Philip Arrhidaeus so as to disable him, and prevent him being a rival for Alexander. Olympias's influence instilled huge ambition and a sense of destiny in Alexander, and Plutarch tells us that his ambition "kept his spirit serious and lofty in advance of his years". Alexander's relationship with his father generated the competitive side of his personality; he had a need to out-do his father, as his reckless nature in battle suggests. While Alexander worried that his father would leave him "no great or brilliant achievement to be displayed to the world", he still attempted to downplay his father's achievements to his companions.
Alexander's most evident personality traits were his violent temper and rash, impulsive nature, which undoubtedly contributed to some of his decisions during his life. Plutarch thought that this part of his personality was the cause of his weakness for alcohol. Although Alexander was stubborn and did not respond well to orders from his father, he was easier to persuade by reasoned debate. Indeed, set beside his fiery temperament, there was a calmer side to Alexander; perceptive, logical, and calculating. He had a great desire for knowledge, a love for philosophy, and was an avid reader. This was no doubt in part due to his tutelage by Aristotle; Alexander was intelligent and quick to learn. The tale of his "solving" the Gordian knot neatly demonstrates this. He had great self-restraint in "pleasures of the body", contrasting with his lack of self control with alcohol. The intelligent and rational side to Alexander is also amply demonstrated by his ability and success as a general.
Alexander was undoubtedly erudite, and was a patron to both the arts and sciences. However, he had little interest in sports, or the Olympic games (unlike his father), seeking only the Homeric ideals of glory and fame. He had great charisma and force of personality, characteristics, which made him a great leader. This is further emphasised by the inability of any of his generals to unite the Macedonians and retain the Empire after his death only Alexander had the personality to do so.
Megalomania.
During his final years, and especially after the death of Hephaestion, Alexander began to exhibit signs of megalomania and paranoia. His extraordinary achievements, coupled with his own ineffable sense of destiny and the flattery of his companions, may have combined to produce this effect. His delusions of grandeur are readily visible in the testament that he ordered Craterus to fulfil, and in his desire to conquer all non-Greek peoples.
He seems to have come to believe himself a deity, or at least sought to deify himself. Olympias always insisted to him that he was the son of Zeus, a theory apparently confirmed to him by the oracle of Amun at Siwa. He began to identify himself as the son of Zeus-Ammon. Alexander adopted some elements of Persian dress and customs at his court, notably the custom of "proskynesis", a practice of which the Macedonians disapproved, and were loathe to perform. Such behaviour cost him much in the sympathies of many of his countrymen.
Personal relationships.
The greatest emotional relationship of Alexander's life was with his friend, general, and bodyguard Hephaestion, the son of a Macedonian noble. Hephaestion's death devastated Alexander, sending him into a period of grieving. This event may have contributed to Alexander's failing health, and detached mental state during his final months. Alexander married twice: Roxana, daughter of a Bactrian nobleman, Oxyartes, out of love; and Stateira, a Persian princess and daughter of Darius III of Persia out of political interest. He apparently had two sons, Alexander IV of Macedon of Roxana and, possibly, Heracles of Macedon from his mistress Barsine; and lost another child when Roxana miscarried at Babylon.
Alexander's sexuality has been the subject of speculation and controversy. Nowhere in the ancient sources is it stated that Alexander had homosexual relationships, or that Alexander's relationship with Hephaestion was sexual. Aelian, however, writes of Alexander's visit to Troy where "Alexander garlanded the tomb of Achilles and Hephaestion that of Patroclus, the latter riddling that he was a beloved of Alexander, in just the same way as Patroclus was of Achilles". Noting that the word "eromenos" (ancient Greek for beloved) does not necessarily bear sexual meaning, Alexander may indeed have been bisexual, which in his time was not ethically controversial.
Green argues that there is little evidence in the ancient sources Alexander had much interest in women, particularly since he did not produce an heir until the very end of his life. However, he was relatively young when he died, and Ogden suggests that Alexander's matrimonial record is more impressive than his father's at the same age. Apart from wives, Alexander had many more female companions. Alexander had accumulated a harem in the style of Persian kings but he used it rather sparingly; showing great self-control in "pleasures of the body". It is possible that Alexander was simply not a highly-sexed person. Nevertheless, Plutarch describes how Alexander was infatuated by Roxanne while complimenting him on not forcing himself on her. Green suggests that, in the context of the period, Alexander formed quite strong friendships with women, including Ada of Caria, who adopted Alexander, and even Darius's mother Sisygambis, who supposedly died from grief when Alexander died.
Hellenistic Kingdoms.
Alexander's most obvious legacy was the introduction of Macedonian rule to huge new swathes of Asia. Many of these areas would remain in Macedonian hands, or under Greek influence for the next 200–300 years. The successor states that emerged were, at least initially, dominant forces during this epoch, and these 300 years are often referred to as the Hellenistic Period.
The eastern borders of Alexander's empire began to collapse even during his lifetime. However, the power vacuum he left in the northwest of the Indian subcontinent directly gave rise to one of the most powerful Indian dynasties in history. Taking advantage of the neglect shown to this region by the successors, Chandragupta Maurya (referred to in European sources as Sandrokotto), of relatively humble origin, took control of the Punjab, and then with that power base proceeded to conquer the Nanda Empire of northern India. In 305 BC, Seleucus, one of the successors, marched to India to reclaim the territory; instead, he ceded the area to Chandragupta in return for 500 war elephants. These in turn played a pivotal role in the Battle of Ipsus, the result of which did much to settle the division of the Empire.
Hellenization.
Hellenization is a term coined by the German historian Johann Gustav Droysen to denote the spread of Greek language, culture, and population into the former Persian empire after Alexander's conquest. That this export took place is undoubted, and can be seen in the great Hellenistic cities of, for instance, Alexandria (one of around twenty towns founded by Alexander), Antioch and Seleucia (south of modern Baghdad). However, exactly how widespread and deeply permeating this was, and to what extent it was a deliberate policy, is debatable. Alexander certainly made deliberate efforts to insert Greek elements into Persian culture and in some instances he attempted to hybridize Greek and Persian culture, culminating in his aspiration to homogenise the populations of Asia and Europe. However, the successors explicitly rejected such policies after his death. Nevertheless, Hellenization occurred throughout the region, and moreover, was accompanied by a distinct and opposite 'Orientalization' of the Successor states.
The core of Hellenistic culture was essentially Athenian by origin. The Athenian koine dialect had been adopted long before Philip II for official use and was thus spread throughout the Hellenistic world, becoming the lingua franca through Alexander's conquests. Furthermore, town planning, education, local government, and art current in the Hellenistic period were all based on Classical Greek ideals, evolving though into distinct new forms commonly grouped as Hellenistic. Aspects of the Hellenistic culture were still evident in the traditions of the Byzantine Empire up until the mid-15th century.
Some of the most unusual effects of Hellenization can be seen in India, in the region of the relatively late-arising Indo-Greek kingdoms. There, isolated from Europe, Greek culture apparently hybridised with Indian, and especially Buddhist, influences. The first realistic portrayals of the Buddha appeared at this time; they are modelled on Greek statues of Apollo. Several Buddhist traditions may have been influenced by the ancient Greek religion: the concept of Boddhisatvas is reminiscent of Greek divine heroes, and some Mahayana ceremonial practices (burning incense, gifts of flowers, and food placed on altars) are similar to those practiced by the ancient Greeks. Zen Buddhism draws in part on the ideas of Greek stoics, such as Zeno. One Greek king, Menander I, probably became Buddhist, and is immortalized in Buddhist literature as 'Milinda'.
Influence on Rome.
Alexander and his exploits were admired by many Romans who wanted to associate themselves with his achievements. Polybius started his "Histories" by reminding Romans of his role, and thereafter subsequent Roman leaders saw him as his inspirational role model. Julius Caesar reportedly wept in Spain at the sight of Alexander's statue, because he thought he had achieved so little by the same age that Alexander had conquered the world. Pompey the Great searched the conquered lands of the east for Alexander's 260-year-old cloak, which he then wore as a sign of greatness. In his zeal to honor Alexander, Augustus accidentally broke the nose off the Macedonian's mummified corpse while laying a wreath at the Alexander's tomb Alexandria. The Macriani, a Roman family that in the person of Macrinus briefly ascended to the imperial throne, kept images of Alexander on their persons, either on jewelry, or embroidered into their clothes.
In the summer of 1995, a statue of Alexander was recovered in an excavation of a Roman house in Alexandria, which was richly decorated with mosaic and marble pavements and probably was constructed in the 1st century AD and occupied until the 3rd century.
Legend.
There are many legendary accounts surrounding the life of Alexander the Great, with a relatively large number deriving from his own lifetime, probably encouraged by Alexander himself. His court historian Callisthenes portrayed the sea in Cilicia as drawing back from him in proskynesis. Writing shortly after Alexander's death, another participant, Onesicritus, went so far as to invent a tryst between Alexander and Thalestris, queen of the mythical Amazons. When Onesicritus read this passage to his patron, Alexander's general and later King Lysimachus reportedly quipped, "I wonder where I was at the time."
In the first centuries after Alexander's death, probably in Alexandria, a quantity of the more legendary material coalesced into a text known as the "Alexander Romance", later falsely ascribed to the historian Callisthenes and therefore known as "Pseudo-Callisthenes". This text underwent numerous expansions and revisions throughout Antiquity and the Middle Ages.
The Alexander legend is also believed to extend to Alexander the Great in the Qur'an, where he appears as a man called Dhul-Qarnayn.
In ancient and modern culture.
Alexander the Great's accomplishments and legacy have been preserved and depicted in many ways. Alexander has figured in works of both high and popular culture from his own era to the modern day.
In Punjab, the land of his final conquest, the name "Secunder" is commonly given to children even today. This is both due to respect and admiration for Alexander and also as a momento to the fact that fighting the people of Punjab fatigued his army to the point that they revolted against him.
A common proverb in the Punjab, reads "jit jit key jung, secunder jay haar", translation, "alexander wins so many battles that he loses the war" used to address anyone who is good at winning but never taking advantage of those wins.
Sources.
Texts written by people who actually knew Alexander or who gathered information from men who served with Alexander are all lost apart from a few inscriptions and fragments. Contemporaries who wrote accounts of his life include Alexander's campaign historian Callisthenes; Alexander's generals Ptolemy and Nearchus; Aristobulus, a junior officer on the campaigns; and Onesicritus, Alexander's chief helmsman. These works have been lost, but later works based on these original sources survive. The five main surviving accounts are by Arrian, Curtius, Plutarch, Diodorus, and Justin.
---END.OF.DOCUMENT---

Alfred Korzybski.
Alfred Habdank Skarbek Korzybski () (July 3, 1879 – March 1, 1950) was a Polish-American philosopher and scientist. He is most remembered for developing the theory of general semantics.
Early life and career.
He was born in Warsaw, Russian Empire. He came from an aristocratic polish family whose members had worked as mathematicians, scientists, and engineers for generations. He learned Polish at home and Russian in the schools; and having a French governess and a German governess, he became fluent in these four languages as a child.
Korzybski was educated at the Warsaw University of Technology in engineering. During the First World War Korzybski served as an intelligence officer in the Russian Army. After being wounded in his leg and suffering other injuries, he came to North America in 1916 (first to Canada, then the United States) to coordinate the shipment of artillery to the war front. He also lectured to Polish-American audiences about the conflict, promoting the sale of war bonds. Following the War, he decided to remain in the United States, becoming a naturalized citizen in 1940. His first book, "Manhood of Humanity", was published in 1921. In the book, he proposed and explained in detail a new theory of humankind: mankind as a time-binding class of life.
General semantics.
Korzybski's work culminated in the founding of a discipline that he called general semantics (GS). As Korzybski explicitly said, GS should not be confused with semantics, a different subject. The basic principles of general semantics, which include time-binding, are outlined in "Science and Sanity", published in 1933. In 1938 Korzybski founded the Institute of General Semantics and directed it until his death in Lakeville, Connecticut, USA.
Korzybski's work held a view that human beings are limited in what they know by (1) the structure of their nervous systems, and (2) the structure of their languages. Human beings cannot experience the world directly, but only through their "abstractions" (nonverbal impressions or "gleanings" derived from the nervous system, and verbal indicators expressed and derived from language). Sometimes our perceptions and our languages actually mislead us as to the "facts" with which we must deal. Our understanding of what is going on sometimes lacks "similarity of structure" with what is actually going on. He stressed training in awareness of abstracting, using techniques that he had derived from his study of mathematics and science. He called this awareness, this goal of his system, "consciousness of abstracting". His system included modifying the way we approach the world, e.g., with an attitude of "I don't know; let's see," to better discover or reflect its realities as shown by modern science. One of these techniques involved becoming inwardly and outwardly quiet, an experience that he called, "silence on the objective levels".
Korzybski and "to be".
Many supporters and critics of Korzybski reduced his rather complex system to a simple matter of what he said about the verb 'to be.' His system, however, is based primarily on such terminology as the different 'orders of abstraction,' and formulations such as 'consciousness of abstracting.' It is often said that Korzybski "opposed" the use of the verb "to be," an unfortunate exaggeration (see 'Criticisms' below). He thought that "certain uses" of the verb "to be", called the "is of identity" and the "is of predication", were faulty in structure, e.g., a statement such as, "Joe is a fool" (said of a person named 'Joe' who has done something that we regard as foolish). In Korzybski's system, one's assessment of Joe belongs to a higher order of abstraction than Joe himself. Korzybski's remedy was to "deny" identity; in this example, to be continually aware that 'Joe' is "not" what we "call" him. We find Joe not in the verbal domain, the world of words, but the nonverbal domain (the two, he said, amount to different orders of abstraction). This was expressed in Korzybski's most famous premise, "the map is not the territory". Note that this premise uses the phrase "is not", a form of "to be"; this and many other examples show that he did not intend to abandon "to be" as such. In fact, he expressly said that there were no structural problems with the verb "to be" when used as an auxiliary verb or when used to state existence or location. It was even 'OK' sometimes to use the faulty forms of the verb 'to be,' as long as one was aware of their structural limitations. This was developed into E-prime by one of his students 15 years after his death.
Anecdote about Korzybski.
One day, Korzybski was giving a lecture to a group of students, and he suddenly interrupted the lesson in order to retrieve a packet of biscuits, wrapped in white paper, from his briefcase. He muttered that he just had to eat something, and he asked the students on the seats in the front row, if they would also like a biscuit. A few students took a biscuit. "Nice biscuit, don't you think," said Korzybski, while he took a second one. The students were chewing vigorously. Then he tore the white paper from the biscuits, in order to reveal the original packaging. On it was a big picture of a dog's head and the words "Dog Cookies." The students looked at the package, and were shocked. Two of them wanted to throw up, put their hands in front of their mouths, and ran out of the lecture hall to the toilet. "You see, ladies and gentlemen," Korzybski remarked, "I have just demonstrated that people don't just eat food, but also words, and that the taste of the former is often outdone by the taste of the latter." Apparently his prank aimed to illustrate how some human suffering originates from the confusion or conflation of linguistic representations of reality and reality itself.
Criticisms.
See the criticism section of the main General Semantics article.
Impact.
Korzybski's work influenced Gestalt Therapy, Rational Emotive Behavior Therapy, and Neuro-linguistic programming (especially the Meta model, Korzybski's critique of cause-effect thinking, and ideas behind human modeling for performance). As reported in the Third Edition of "Science and Sanity", The U.S. Army in World War II used his system to treat battle fatigue in Europe under the supervision of Dr. Douglas M. Kelley, who also became the psychiatrist in charge of the Nazi prisoners at Nuremberg. Other individuals influenced by Korzybski include Kenneth Burke, William S. Burroughs, Frank Herbert, Albert Ellis, Gregory Bateson, John Grinder, Buckminster Fuller, Douglas Engelbart, Stuart Chase, Alvin Toffler, Robert A. Heinlein (Korzybski is mentioned in the 1940 short story "Blowups Happen" and the 1949 novella "Gulf"), L.Ron Hubbard, A. E. van Vogt, Robert Anton Wilson, Alan Watts, entertainer Steve Allen, and Tommy Hall (lyricist for the 13th Floor Elevators); and scientists such as William Alanson White (psychiatry), physicist P. W. Bridgman, and researcher W. Horsley Gantt (a former student and colleague of Pavlov). He also influenced the Belgian surrealist writer of comics Jan Bucquoy in the seventh part of the comics series "Jaunes": "Labyrinthe", with explicit reference in the plot to Korzybski's "the map is not the territory."
In part the General Semantics tradition was upheld by Samuel I. Hayakawa, who did have a falling out with Korzybski. When asked over what, Hayakawa is said to have replied: "Words."
---END.OF.DOCUMENT---

Asteroids (video game).
"Asteroids" is a video arcade game released in 1979 by Atari Inc. It was one of the most popular and influential games of the Golden Age of Arcade Games. "Asteroids" uses vector graphics and a two-dimensional view that wraps around in both screen axes (a toroidal topology). The player controls a spaceship in an asteroid field which is periodically traversed by flying saucers. The object of the game is to shoot and destroy asteroids and saucers while not colliding with either, or being hit by the saucers' counter-fire.
Description.
The game was conceived by Lyle Rains and programmed and designed by Ed Logg. "Asteroids" was a hit in the United States and became Atari's best selling game of all time. Atari had been in the process of releasing another vector game, "Lunar Lander", but demand for "Asteroids" was so high they stopped further production of "Lunar Lander" so they could begin building "Asteroids". The first 200 "Asteroids" machines were sent out in "Lunar Lander" cabinets. "Asteroids" was so popular that video arcade owners sometimes had to install larger boxes to hold the amount of coins that were spent by players.
"Asteroids" is also the first game to use Atari's "QuadraScan" vector-refresh system. A full-color version known as "Color-QuadraScan" was later developed for games such as "Space Duel" and "Tempest".
Gameplay.
The objective of "Asteroids" is to score as many points as possible by destroying asteroids and flying saucers. The player controls a triangular-shaped ship that can rotate left and right, fire shots straight forward, and thrust forward. As the ship moves, momentum is not conserved — the ship eventually comes to a stop again when not thrusting. The player can also send their ship into hyperspace, causing it to disappear and reappear in a random location on the screen (with the risk of self-destructing or appearing on top of an asteroid).
Each stage starts with a few asteroids drifting in random directions on the screen. Objects wrap around screen edges — for instance, an asteroid that drifts off the top edge of the screen reappears at the bottom and continues moving in the same direction. As the player shoots asteroids, they break into smaller asteroids that frequently move faster and are more difficult to hit. Smaller asteroids also score higher points. Periodically, a flying saucer appears on one side of the screen and moves across to the other before disappearing again. The saucers are of two kinds: Large saucers fire in random directions, while small saucers aim at the player's ship.
The minimalist soundtrack features a memorable deep-toned electronic "heartbeat", which quickens as the asteroid density is reduced by the player's fire.
Once the screen has been cleared of all asteroids and flying saucers, a new set of large asteroids appears. The number of asteroids increases each round up to a maximum of twelve. The game is over when the player has lost all of his/her lives.
Like many games of its time, "Asteroids" contains several bugs that were mostly the result of the original programmers underestimating the game's popularity or the skill of its players. The maximum possible score in this game is 99,990 points, after which it "rolls over" back to zero. Also, an oversight in the small saucer's programming gave rise to a popular strategy known as "lurking" — because the saucer could only shoot directly at the player's position on the screen, the player could "hide" at the opposite end of the screen and shoot across the screen boundary, while remaining relatively safe. This led to experienced players being able to play indefinitely on a single credit. This oversight was addressed in the game's sequel, "Asteroids Deluxe", and led to significant changes in the way game developers designed and tested their games in the future.
On some early versions of the game, it was also possible to hide the ship in the score area indefinitely without being hit by asteroids.
Technical description.
The "Asteroids" arcade machine is a vector game. This means that the game graphics are composed entirely of lines which are drawn on a vector monitor. The hardware consists primarily of a standard MOS 6502 CPU, which executes the game program, and the Digital Vector Generator (DVG), vector processing circuitry developed by Atari themselves. As the 6502 by itself was too slow to control both the game play and the vector hardware at the same time, the latter task was delegated to the DVG.
The original design concepts of the DVG came out of Atari's off-campus research lab in Grass Valley, CA, in 1978. The prototype was given to engineer Howard Delman, who refined it, produced it, and then added additional features for Atari's first vector game, "Lunar Lander". When it was decided that "Asteroids" would be a vector game as well, Delman modified a "Lunar Lander" circuit board for Ed Logg. More memory was added, as was the circuitry for the many sounds in the game. That original "Asteroids" prototype board still exists, and is currently in Delman's personal collection.
For each picture frame, the 6502 writes graphics commands for the DVG into a defined area of RAM (the vector RAM), and then asks the DVG to draw the corresponding vector image on the screen. The DVG reads the commands and generates appropriate signals for the vector monitor. There are DVG commands for positioning the cathode ray, for drawing a line to a specified destination, calling a subroutine with further commands, and so on.
"Asteroids" also features various sound effects, each of which is implemented by its own circuitry. There are seven distinct audio circuits, designed by Howard Delman. The CPU activates these audio circuits (and other hardware components) by writing to special memory addresses (memory mapped ports). The inputs from the player's controls (buttons) are also mapped into the CPU address space
The main "Asteroids" game program uses only 6 KB of ROM code. Another 2 KB of vector ROM contains the descriptions of the main graphical elements (rocks, saucer, player's ship, explosion pictures, letters, and digits) in the form of DVG commands.
Legacy.
Due to the game's success, a sequel followed in 1980 dubbed "Asteroids Deluxe". Though similar to the original game, several changes and additions occurred, with the onscreen objects now tinted blue and a shield that depleted with use replacing the hyperspace feature. In addition a new enemy dubbed a "killer satellite" was added to the game, and would break apart into two smaller ships that homed on the player's position if shot. Another two sequels followed this, "Space Duel" in 1982 and "Blasteroids" in 1987.
The Killer List of Videogames (KLOV) credits this game as one of the "Top 100 Videogames." Readers of the KLOV credit it as the seventh most popular game.
The gameplay in "Asteroids" was imitated by many games that followed. For example, one of the objects of "Sinistar" is to shoot asteroids to get them to release resources which the player needs to collect.
Ports and follow-ups.
"Asteroids" has been ported to multiple systems, including many of Atari's systems (Atari 2600, 7800, Atari Lynx) and many others. The 2600 port was the first game to utilize a bank-switched cartridge, doubling available ROM space. A port was in development for the 5200 and advertised as a launch title but never officially released, although an unofficial release was produced by AtariAge. 1993 saw a release for PCs with Windows 3.1 as part of the original "Microsoft Arcade" package. Also, a new version of "Asteroids" was developed for PlayStation, Nintendo 64, Windows, and the Game Boy Color in the late 1990s. A port was also included on Atari's Cosmos system, but the system never saw release. Many of the recent TV Games series of old Atari games have included either the 2600 or arcade versions of "Asteroids". Atari has also used the game for its other late '90s and 2000's anthology series.
"Asteroids Hyper 64" is an update to the 1979 arcade shooter "Asteroids" released for the Nintendo 64 on December 14, 1999. It includes fully 3D environments, new weapons, over 50 levels, and a 2 player split-screen mode; including a Versus mode, a Co-op mode, and a Team mode.
In 2001, Infogrames released "Atari Anniversary Edition" for the Sega Dreamcast and PC compatibles which included emulated versions of Asteroids and other classics.
In 2004, "Asteroids" (Including both the Atari 2600 port and the arcade original, along with "Asteroids Deluxe") were included as part of "Atari Anthology" for both Xbox and PlayStation 2, using Digital Eclipse's emulation technology. (This package was released for the PC a year earlier under the title "Atari: 80 Classic Games in One".)
"Asteroids" was released via Xbox Live Arcade for the Xbox 360 on November 28, 2007, with an option for special revamped HD graphics and a high-speed "throttle monkey" mode.
Glu Mobile released a licensed cellular phone version of "Asteroids" that includes the original game as well as updated gameplay, skins, and modes.
Also, a port for Rockbox was released, named "Spacerocks".
Clones and bootlegs.
There have been countless unofficial ports of "Asteroids" produced. These include near-copies such as Acornsoft's "Meteors" and Ambrosia Software's Maelstrom, as well as those with expanded gameplay and background, such as "Astrogeddon", "Stardust", "Spheres of Chaos" and "Astro Fire". The Vectrex had a built-in similar game called "Minestorm".
Highest score.
On November 13, 1982, 15 year old Scott Safran of Cherry Hill, NJ, set a world record of 41,336,440 points on the arcade game "Asteroids". He beat the 40,101,910 point score set by Leo Daniels of Carolina Beach on February 6, 1982. To congratulate Safran on his accomplishment, the Twin Galaxies Intergalactic Scoreboard searched for him for four years until 2002, when it was discovered that he had died in an accident in 1989. In a ceremony in Philadelphia on April 27, 2002, Walter Day of Twin Galaxies presented an award to the surviving members of Safran's family, commemorating the Asteroid Champion's achievement.
Film.
In July 2009, it was revealed that Universal Studios had won the rights to adapt the game into a film. Matthew Lopez will write the script and Lorenzo di Bonaventura will be producing.
---END.OF.DOCUMENT---

Asparagales.
Asparagales is an order of flowering plants. The order has always included the family Asparagaceae, but other families included in the order have varied markedly between different classifications. It is supposed that this group of plants evolved between the late and early Cretaceous, but given the difficult classification of the families involved, this is not entirely certain.
APG II system.
Note: "+..." = optional segregate family, that may be split off from the preceding family.
APG II has consolidated some of the families in the earlier APG system, while recognizing an alternative, that allows smaller families to be segregated and still follow the 'APG system'. Under the new classification system a taxonomist could, for example, correctly choose to include the daylilies ("Hemerocallis") in family Hemerocallidaceae, or in family Xanthorrhoeaceae.
Other systems.
The Cronquist system did not recognise the order, and placed many of the plants involved in order Liliales (in subclass Liliidae in class Liliopsida [= monocotyledons]). Some genera were even included in family Liliaceae.
The Wettstein system, last revision of 1935, did not recognise such an order, and placed many of the plants involved in order Liliiflorae in class Monocotyledones.
---END.OF.DOCUMENT---

Alismatales.
Alismatales is an order of flowering plants. The order will of necessity contain the family Alismataceae.
Taxonomy.
Thus circumscribed, the order contains about 165 genera in 14 families, with a cosmopolitan distribution. Most of the families are composed of herbaceous plants, commonly found in aquatic environments. The flowers are usually arranged in inflorescences, and the mature seeds lack endosperm.
The biggest departure from earlier systems (see below) is the inclusion of family Araceae. By its inclusion the order has grown enormously in number of species. The family Araceae alone accounts for about a hundred genera, totalling over two thousand species. The rest of families together contain just about five hundred species.
The Cronquist subclass Alismatidae conformed fairly closely to the order Alismatales as circumscribed by APG, minus the family Araceae.
The Dahlgren superorder Alismatanae conformed fairly closely to the order Alismatales as circumscribed by APG, minus the family Araceae.
The Wettstein system, last version in 1935, and the Engler system, update in 1964, used the name Helobiae for the order.
---END.OF.DOCUMENT---

Apiales.
The Apiales are an order of flowering plants. The families given at right are typical of newer classifications, though there is some slight variation, and in particular the Torriceliaceae may be divided. These families are placed within the asterid group of eudicots as circumscribed by the APG II system. Within the asterids, Apiales belongs to an unranked group called the campanulids.
Under this definition well-known members include carrots, celery, parsley, and ivy.
Under the Cronquist system, only the Apiaceae and Araliaceae were included here, and the restricted order was placed among the rosids rather than the asterids. The Pittosporaceae were placed within the Rosales, and many of the other forms within the family Cornaceae. Pennantia was in the family Icacinaceae.
---END.OF.DOCUMENT---

Asterales.
Asterales is an order of dicotyledonous flowering plants that includes the composite family (Asteraceae) and its related families.
The order is a cosmopolite, and includes mostly herbaceous species, although a small number of trees (such as some members of the genus "Lobelia") and shrubs are also present.
The Asterales can be characterized on the morphological and molecular level. Synapomorphies include the oligosaccharide inulin, a nutrient storage molecule, and unique stamen morphology. The stamens are usually found around the style, either aggregated densely or fused into a tube, probably an adaptation in association with the plunger (or secondary) pollination that is common among the families of the order.
Families.
The order Asterales includes about eleven families, the largest of which are the Asteraceae, with about 25,000 species, and the Campanulaceae, with about 2,000 species. The remaining families count together for less than 500 species. The two large families are cosmopolitan, with many of their species found in the northern hemisphere, and the smaller families are usually confined to Australia and the adjacent areas, or sometimes South America.
Under the Cronquist system, Asteraceae was the only family in the group, but newer systems (e. g. APG II) have expanded it.
Evolution and biogeography.
The Asterales order probably originated in the Cretaceous on the supercontinent Gondwana, in the area that is now Australia and Asia. Although most extant species are herbaceous, the examination of the basal families in the order suggests that the common ancestor of the order was an arborescent plant.
Fossil evidence of the Asterales is rare and belongs to rather recent epochs, so the precise estimation of the order's age is quite difficult. An Oligocene pollen is known for Asteraceae and Goodeniaceae, and seeds from Oligocene and Miocene are known for Menyanthaceae and Campanulaceae respectively.
Economical importance.
The Asteraceae include some species grown for food, including sunflower ("Helianthus annuus"), lettuce ("Lactuca sativa") and chicory ("Cichorium"). Many spices and medicinal herbs are also present.
Of horticultural importance are the Asteraceae (e. g. chrysanthemum) and Campanulaceae.
---END.OF.DOCUMENT---

Asteroid.
Asteroids, sometimes called minor planets or planetoids, are small Solar System bodies in orbit around the Sun, especially in the inner Solar System; they are smaller than planets but larger than meteoroids. The term "asteroid" has historically been applied primarily to minor planets of the inner Solar System, as the outer Solar System was poorly known when it came into common usage. The distinction between asteroids and comets is made on visual appearance: Comets show a perceptible coma while asteroids do not.
Terminology == .
Traditionally, small bodies orbiting the Sun were classified as asteroids, comets or meteoroids, with anything smaller than ten metres across being called a meteoroid. The term "asteroid" is somewhat ill-defined. It never had a formal definition, with the broader term minor planet being preferred by the International Astronomical Union until 2006, when the term "small Solar System body" (SSSB) was introduced to cover both minor planets and comets. The 2006 definition of SSSB says that they "include most of the Solar System asteroids, most Trans-Neptunian Objects (TNOs), comets, and other small bodies". Other languages prefer "planetoid" (Greek for "planet-like"), and this term is occasionally used in English for the larger asteroids. The word "planetesimal" has a similar meaning, but refers specifically to the small building blocks of the planets that existed at the time the Solar System was forming. The term "planetule" was coined by the geologist William Daniel Conybeare to describe minor planets, but is not in common use.
When found, asteroids were seen as a class of objects distinct from comets, and there was no unified term for the two until "small Solar System body" was coined in 2006. The main difference between an asteroid and a comet is that a comet shows a coma due to sublimation of near surface ices by solar radiation. A few objects have ended up being dual-listed because they were first classified as minor planets but later showed evidence of cometary activity. Conversely, some (perhaps all) comets are eventually depleted of their surface volatile ices and become asteroids. A further distinction is that comets typically have more eccentric orbits than most asteroids; most "asteroids" with notably eccentric orbits are probably dormant or extinct comets.
For almost two centuries, from the discovery of the first asteroid, Ceres, in 1801 until the discovery of the first centaur, 2060 Chiron, in 1977, all known asteroids spent most of their time at or within the orbit of Jupiter, though a few such as 944 Hidalgo ventured far beyond Jupiter for part of their orbit. When astronomers started finding additional small bodies that permanently resided further out than Jupiter, now called centaurs, they numbered them among the traditional asteroids, though there was debate over whether they should be classified as asteroids or as a new type of object. Then, when the first trans-Neptunian object, 1992 QB1, was discovered in 1992, and especially when large numbers of similar objects started turning up, new terms were invented to sidestep the issue: Kuiper Belt object (KBO), trans-Neptunian object (TNO), scattered-disc object (SDO), and so on. These inhabit the cold outer reaches of the Solar System where ices remain solid and comet-like bodies are not expected to exhibit much cometary activity; if centaurs or TNOs were to venture close to the Sun, their volatile ices would sublimate, and traditional approaches would classify them as comets rather than asteroids.
The innermost of these are the Kuiper Belt Objects (KBOs), called "objects" partly to avoid the need to classify them as asteroids or comets. KBOs are believed to be predominantly comet-like in composition, though some may be more akin to asteroids. Furthermore, most do not have the highly eccentric orbits associated with comets, and the ones so far discovered are very much larger than traditional comet nuclei. (The much more distant Oort cloud is hypothesized to be the main reservoir of dormant comets.) Other recent observations, such as the analysis of the cometary dust collected by the Stardust probe, are increasingly blurring the distinction between comets and asteroids, suggesting "a continuum between asteroids and comets" rather than a sharp dividing line.
The minor planets beyond Jupiter's orbit are rarely directly referred to as "asteroids", but all are commonly lumped together under the term "asteroid" in popular presentations. For instance, a joint NASA-JPL public-outreach website states,
It is, however, becoming increasingly common for the term "asteroid" to be restricted to minor planets of the inner Solar System, and therefore this article will restrict itself for the most part to the classical asteroids: objects of the main asteroid belt, Jupiter trojans, and near-Earth objects.
When the IAU introduced the class small solar system bodies in 2006 to include most objects previously classified as minor planets and comets, they created the class of dwarf planets for the largest minor planets—those which have sufficient mass to have become ellipsoidal under their own gravity. According to the IAU, "the term 'minor planet' may still be used, but generally the term 'small solar system body' will be preferred." Currently only the largest object in the asteroid belt, Ceres, at about across, has been placed in the dwarf planet category, although there are several large asteroids (Vesta, Pallas, and Hygiea) that may be classified as dwarf planets when their shapes are better known.
Formation.
It is believed that planetesimals in the main asteroid belt evolved much like the rest of the Solar Nebula until Jupiter neared its current mass, at which point excitation from orbital resonances with Jupiter ejected over 99% of planetesimals in the belt. Both simulations and a discontinuity in spin rate and spectral properties suggest that asteroids larger than approximately in diameter accreted during that early era, whereas smaller bodies are fragments from collisions between asteroids during or after the Jovian disruption. At least two asteroids, Ceres and Vesta, grew large enough to melt and differentiate, with heavy metallic elements sinking to the core, leaving rocky minerals in the crust.
In the Nice model, a large number of Kuiper Belt objects are captured in the outer Main Belt, at distances greater than 2.6 AU. Most were subsequently ejected by Jupiter, but those that remained may be the D-type asteroids, and possibly include Ceres.
Characteristics.
Objects in the main asteroid belt vary greatly in size, from a diameter of 950 kilometres for the dwarf planet Ceres and over 500 kilometres for the asteroids 2 Pallas and 4 Vesta down to rocks just tens of metres across. The three largest are very much like miniature planets: they are roughly spherical, have at least partially differentiated interiors, and indeed are thought to be surviving protoplanets. The vast majority, however, are much smaller and are irregularly shaped; they are thought to be either surviving planetesimals or fragments of larger bodies.
The physical composition of asteroids is varied and in most cases poorly understood. Ceres appears to be composed of a rocky core covered by an icy mantle, whereas Vesta is thought to have a nickel-iron core, olivine mantle, and basaltic crust. 10 Hygiea, on the other hand, which appears to have a uniformly primitive composition of carbonaceous chondrite, is thought to be the largest undifferentiated asteroid. Many, perhaps most, of the smaller asteroids are piles of rubble held together loosely by gravity. Some have moons or are co-orbiting binary asteroids. The rubble piles, moons, binaries, and scattered asteroid families are believed to be the results of collisions which disrupted a parent asteroid.
Asteroids contain traces of amino-acids and other organic compounds, and some speculate that asteroid impacts may have seeded the early Earth with the chemicals necessary to initiate life, or may have even brought life itself to Earth. (See also panspermia.)
Only one asteroid, 4 Vesta, which has a particularly reflective surface, is normally visible to the naked eye, and this only in very dark skies when it is favorably positioned. Very rarely, small asteroids passing close to Earth may be naked-eye visible for a short period of time.
The orbits of asteroids are often influenced by the gravity of other bodies in the solar system or the Yarkovsky effect.
Distribution within the Solar System.
The vast majority of known asteroids orbit within the main asteroid belt between the orbits of Mars and Jupiter, generally in relatively low-eccentricity (i.e., not very elongated) orbits. This belt is currently estimated to contain between 1.1 and 1.9 million asteroids larger than in diameter, and millions of smaller ones. It is thought that these asteroids are remnants of the protoplanetary disk, and in this region the accretion of planetesimals into planets during the formative period of the solar system was prevented by large gravitational perturbations by Jupiter. Although fewer Trojan asteroids sharing Jupiter's orbit are currently known, it is thought that there are as many as there are asteroids in the main belt.
The dwarf planet Ceres is the largest object in the asteroid belt, with a diameter of over. The next largest are the asteroids 2 Pallas and 4 Vesta, both with diameters of over. Normally Vesta is the only main belt asteroid that can, on occasion, become visible to the naked eye. However, on some very rare occasions, a near-Earth asteroid may briefly become visible without technical aid; see 99942 Apophis.
The mass of all the objects of the Main asteroid belt, lying between the orbits of Mars and Jupiter, is estimated to be about 3.0-3.6 kg, or about 4 percent of the mass of the Moon. Of this, Ceres comprises 0.95 kg, some 32 percent of the total. Adding in the next three most massive objects, Vesta (9%), Pallas (7%), and Hygiea (3%), brings this figure up to 51%; while the three after that, 511 Davida (1.2%), 704 Interamnia (1.0%), and 52 Europa (0.9%), only add another 3% to the total mass. The number of asteroids then increases rapidly as their individual masses decrease.
Various classes of asteroid have been discovered outside the main asteroid belt. Near-Earth asteroids have orbits in the vicinity of Earth's orbit. Trojan asteroids are gravitationally locked into synchronisation with Jupiter, either leading or trailing the planet in its orbit. A couple trojans have been found orbiting with Mars. A group of asteroids called Vulcanoids are hypothesised by some to lie very close to the Sun, within the orbit of Mercury, but none has so far been found.
Classification.
Asteroids are commonly classified according to two criteria: the characteristics of their orbits, and features of their reflectance spectrum.
Orbit groups and families.
Many asteroids have been placed in groups and families based on their orbital characteristics. Apart from the broadest divisions, it is customary to name a group of asteroids after the first member of that group to be discovered. Groups are relatively loose dynamical associations, whereas families are much tighter and result from the catastrophic break-up of a large parent asteroid sometime in the past. Families have only been recognized within the main asteroid belt. They were first recognised by Kiyotsugu Hirayama in 1918 and are often called Hirayama families in his honor.
About 30% to 35% of the bodies in the main belt belong to dynamical families each thought to have a common origin in a past collision between asteroids. A family has also been associated with the plutoid dwarf planet.
Quasi-satellites and horseshoe objects.
Some asteroids have unusual horseshoe orbits that are co-orbital with the Earth or some other planet. Examples are 3753 Cruithne and. The first instance of this type of orbital arrangement was discovered between Saturn's moons Epimetheus and Janus.
Sometimes these horseshoe objects temporarily become quasi-satellites for a few decades or a few hundred years, before returning to their prior status. Both Earth and Venus are known to have quasi-satellites.
Such objects, if associated with Earth or Venus or even hypothetically Mercury, are a special class of Aten asteroids. However, such objects could be associated with outer planets as well.
Spectral classification ===.
In 1975, an asteroid taxonomic system based on colour, albedo, and spectral shape was developed by Clark R. Chapman, David Morrison, and Ben Zellner. These properties are thought to correspond to the composition of the asteroid's surface material. The original classification system had three categories: C-types for dark carbonaceous objects (75% of known asteroids), S-types for stony (silicaceous) objects (17% of known asteroids) and U for those that did not fit into either C or S. This classification has since been expanded to include a number of other asteroid types. The number of types continues to grow as more asteroids are studied.
The two most widely used taxonomies currently used are the Tholen classification and SMASS classification. The former was proposed in 1984 by David J. Tholen, and was based on data collected from an eight-color asteroid survey performed in the 1980s. This resulted in 14 asteroid categories. In 2002, the Small Main-Belt Asteroid Spectroscopic Survey resulted in a modified version of the Tholen taxonomy with 24 different types. Both systems have three broad categories of C, S, and X asteroids, where X consists of mostly metallic asteroids, such as the M-type. There are also a number of smaller classes.
Note that the proportion of known asteroids falling into the various spectral types does not necessarily reflect the proportion of all asteroids that are of that type; some types are easier to detect than others, biasing the totals.
Problems with spectral classification.
Originally, spectral designations were based on inferences of an asteroid's composition. However, the correspondence between spectral class and composition is not always very good, and there are a variety of classifications in use. This has led to significant confusion. While asteroids of different spectral classifications are likely to be composed of different materials, there are no assurances that asteroids within the same taxonomic class are composed of similar materials.
At present, the spectral classification based on several coarse resolution spectroscopic surveys in the 1990s is still the standard. Scientists have been unable to agree on a better taxonomic system, largely due to the difficulty of obtaining detailed measurements consistently for a large sample of asteroids (e.g. finer resolution spectra, or non-spectral data such as densities would be very useful).
Discovery.
The first named minor planet, Ceres, was discovered in 1801 by Giuseppe Piazzi, and was originally considered a new planet. This was followed by the discovery of other similar bodies, which with the equipment of the time appeared to be points of light, like stars, showing little or no planetary disc (though readily distinguishable from stars due to their apparent motions). This prompted the astronomer Sir William Herschel to propose the term "asteroid", from Greek "αστεροειδής", "asteroeidēs" = star-like, star-shaped, from ancient Greek "Aστήρ", "astēr" = star. In the early second half of the nineteenth century, the terms "asteroid" and "planet" (not always qualified as "minor") were still used interchangeably; for example, the, page 316, reads "Professor J. Watson has been awarded by the Paris Academy of Sciences, the astronomical prize, Lalande foundation, for the discovery of 8 new asteroids in one year. The planet Lydia (No. 110), discovered by M. Borelly at the Marseilles Observatory [...] M. Borelly had previously discovered 2 planets bearing the numbers 91 and 99 in the system of asteroids revolving between Mars and Jupiter" (emphasis added).
Historical methods.
Asteroid discovery methods have dramatically improved over the past two centuries.
In the last years of the 18th century, Baron Franz Xaver von Zach organized a group of 24 astronomers to search the sky for the missing planet predicted at about 2.8 AU from the Sun by the Titius-Bode law, partly as a consequence of the discovery, by Sir William Herschel in 1781, of the planet Uranus at the distance predicted by the law. This task required that hand-drawn sky charts be prepared for all stars in the zodiacal band down to an agreed-upon limit of faintness. On subsequent nights, the sky would be charted again and any moving object would, hopefully, be spotted. The expected motion of the missing planet was about 30 seconds of arc per hour, readily discernible by observers.
The first asteroid, 1 Ceres, was not discovered by a member of the group, but rather by accident in 1801 by Giuseppe Piazzi, director of the observatory of Palermo in Sicily. He discovered a new star-like object in Taurus and followed the displacement of this object during several nights. His colleague, Carl Friedrich Gauss, used these observations to determine the exact distance from this unknown object to the Earth. Gauss' calculations placed the object between the planets Mars and Jupiter. Piazzi named it after Ceres, the Roman goddess of agriculture.
Three other asteroids (2 Pallas, 3 Juno, and 4 Vesta) were discovered over the next few years, with Vesta found in 1807. After eight more years of fruitless searches, most astronomers assumed that there were no more and abandoned any further searches.
However, Karl Ludwig Hencke persisted, and began searching for more asteroids in 1830. Fifteen years later, he found 5 Astraea, the first new asteroid in 38 years. He also found 6 Hebe less than two years later. After this, other astronomers joined in the search and at least one new asteroid was discovered every year after that (except the wartime year 1945). Notable asteroid hunters of this early era were J. R. Hind, Annibale de Gasparis, Robert Luther, H. M. S. Goldschmidt, Jean Chacornac, James Ferguson, Norman Robert Pogson, E. W. Tempel, J. C. Watson, C. H. F. Peters, A. Borrelly, J. Palisa, the Henry brothers and Auguste Charlois.
In 1891, however, Max Wolf pioneered the use of astrophotography to detect asteroids, which appeared as short streaks on long-exposure photographic plates. This dramatically increased the rate of detection compared with previous visual methods: Wolf alone discovered 248 asteroids, beginning with 323 Brucia, whereas only slightly more than 300 had been discovered up to that point. Still, a century later, only a few thousand asteroids were identified, numbered and named. It was known that there were many more, but most astronomers did not bother with them, calling them "vermin of the skies".
Manual methods of the 1900s and modern reporting.
Until 1998, asteroids were discovered by a four-step process. First, a region of the sky was photographed by a wide-field telescope, or Astrograph. Pairs of photographs were taken, typically one hour apart. Multiple pairs could be taken over a series of days. Second, the two films of the same region were viewed under a stereoscope. Any body in orbit around the Sun would move slightly between the pair of films. Under the stereoscope, the image of the body would appear to float slightly above the background of stars. Third, once a moving body was identified, its location would be measured precisely using a digitizing microscope. The location would be measured relative to known star locations.
These first three steps do not constitute asteroid discovery: the observer has only found an apparition, which gets a provisional designation, made up of the year of discovery, a letter representing the half-month of discovery, and finally a letter and a number indicating the discovery's sequential number (example:).
The final step of discovery is to send the locations and time of observations to the Minor Planet Center, where computer programs determine whether an apparition ties together previous apparitions into a single orbit. If so, the object receives a catalogue number and the observer of the first apparition with a calculated orbit is declared the discoverer, and granted the honor of naming the object subject to the approval of the International Astronomical Union.
Computerized methods.
There is increasing interest in identifying asteroids whose orbits cross Earth's, and that could, given enough time, collide with Earth (see Earth-crosser asteroids). The three most important groups of near-Earth asteroids are the Apollos, Amors, and Atens. Various asteroid deflection strategies have been proposed, as early as the 1960s.
The near-Earth asteroid 433 Eros had been discovered as long ago as 1898, and the 1930s brought a flurry of similar objects. In order of discovery, these were: 1221 Amor, 1862 Apollo, 2101 Adonis, and finally 69230 Hermes, which approached within 0.005 AU of the Earth in 1937. Astronomers began to realize the possibilities of Earth impact.
Two events in later decades increased the level of alarm: the increasing acceptance of Walter Alvarez' hypothesis that an impact event resulted in the Cretaceous-Tertiary extinction, and the 1994 observation of Comet Shoemaker-Levy 9 crashing into Jupiter. The U.S. military also declassified the information that its military satellites, built to detect nuclear explosions, had detected hundreds of upper-atmosphere impacts by objects ranging from one to 10 metres across.
The LINEAR system alone has discovered 97,470 asteroids, as of September 18, 2008. Between all of the automated systems, 4711 near-Earth asteroids have been discovered including over 600 more than in diameter. The rate of discovery peaked in 2000, when 38,679 minor planets were numbered, and has been going down steadily since then (719 minor planets were numbered in 2007).
Naming.
A newly discovered asteroid is given a provisional designation (such as) consisting of the year of discovery and an alphanumeric code indicating the half-month of discovery and the sequence within that half-month. Once an asteroid's orbit has been confirmed, it is given a number, and later may also be given a name (e.g. 433 Eros). The formal naming convention uses parentheses around the number (e.g. (433) Eros), but dropping the parentheses is quite common. Informally, it is common to drop the number altogether, or to drop it after the first mention when a name is repeated in running text.
Symbols.
The first few asteroids discovered were assigned symbols like the ones traditionally used to designate Earth, the Moon, the Sun and planets. The symbols quickly became ungainly, hard to draw and recognize. By the end of 1851 there were 15 known asteroids, each (except one) with its own symbol(s).
Johann Franz Encke made a major change in the Berliner Astronomisches Jahrbuch (BAJ, Berlin Astronomical Yearbook) for 1854. He introduced encircled numbers instead of symbols, although his numbering began with Astraea, the first four asteroids continuing to be denoted by their traditional symbols. This symbolic innovation was adopted very quickly by the astronomical community. The following year (1855), Astraea's number was bumped up to 5, but Ceres through Vesta would be listed by their numbers only in the 1867 edition. A few more asteroids (28 Bellona, 35 Leukothea, and 37 Fides) would be given symbols as well as using the numbering scheme. The circle would become a pair of parentheses, and the parentheses sometimes omitted altogether over the next few decades.
Exploration.
Until the age of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes and their shapes and terrain remained a mystery. The best modern ground-based telescopes, as well as the Earth-orbiting Hubble Space Telescope, can resolve a small amount of detail on the surfaces of the very largest asteroids, but even these mostly remain little more than fuzzy blobs. Limited information about the shapes and compositions of asteroids can be inferred from their light curves (their variation in brightness as they rotate) and their spectral properties, and asteroid sizes can be estimated by timing the lengths of star occulations (when an asteroid passes directly in front of a star). Radar imaging can yield good information about asteroid shapes and orbital and rotational parameters, especially for near-Earth asteroids.
The first close-up photographs of asteroid-like objects were taken in 1971 when the Mariner 9 probe imaged Phobos and Deimos, the two small moons of Mars, which are probably captured asteroids. These images revealed the irregular, potato-like shapes of most asteroids, as did subsequent images from the Voyager probes of the small moons of the gas giants.
The first true asteroid to be photographed in close-up was 951 Gaspra in 1991, followed in 1993 by 243 Ida and its moon Dactyl, all of which were imaged by the Galileo probe en route to Jupiter.
The first dedicated asteroid probe was NEAR Shoemaker, which photographed 253 Mathilde in 1997, before entering into orbit around 433 Eros, finally landing on its surface in 2001.
Other asteroids briefly visited by spacecraft en route to other destinations include 9969 Braille (by Deep Space 1 in 1999), and 5535 Annefrank (by Stardust in 2002).
In September 2005, the Japanese Hayabusa probe started studying 25143 Itokawa in detail and may return samples of its surface to earth. The Hayabusa mission has been plagued with difficulties, including the failure of two of its three control wheels, rendering it difficult to maintain its orientation to the sun to collect solar energy. Following that, the next asteroid encounters will involve the European Rosetta probe (launched in 2004), which flew by 2867 Šteins in 2008 and will buzz 21 Lutetia in 2010.
In September 2007, NASA launched the Dawn Mission, which will orbit the dwarf planet Ceres and the asteroid 4 Vesta in 2011-2015, with its mission possibly then extended to 2 Pallas.
It has been suggested that asteroids might be used in the future as a source of materials which may be rare or exhausted on earth (asteroid mining), or materials for constructing space habitats (see Colonization of the asteroids). Materials that are heavy and expensive to launch from earth may someday be mined from asteroids and used for space manufacturing and construction.
In fiction.
Asteroids and asteroid belts are a staple of science fiction stories. Asteroids play several potential roles in science fiction: as places which human beings might colonize; as resources for extracting minerals; as a hazard encountered by spaceships travelling between two other points; and as a threat to life on Earth due to potential impacts.
---END.OF.DOCUMENT---

Allocution.
Generally, to allocute in law means "to speak out formally." In the field of apologetics, allocution is generally done in defense of a belief. In politics, one may allocute before a legislative body in an effort to influence their position on an issue. In law, it is generally meant to state specifically and in detail what one did and for what reason, often in relation to commission of a crime.
In most United States jurisdictions a defendant is allowed the opportunity to allocute—that is, explain himself—before sentence is passed. Some jurisdictions hold this as an absolute right, and in its absence, a sentence may potentially be overturned, with the result that a new sentencing hearing must be held.
Allocution is sometimes required of a defendant who pleads guilty to a crime in a plea bargain in exchange for a reduced sentence. In this instance, allocution can serve to provide closure for victims or their families. In principle, it removes any doubt as to the exact nature of the defendant's guilt in the matter. There have however, been many cases in which the defendant allocuted to a crime that he or she did not commit, often because this was a requirement to receiving a lesser sentence.
The term "allocution" is generally only in use in jurisdictions in the United States, though there are vaguely similar processes in other common law countries.
For example in Australia the term "allocutus" will be used. It will be used by the Clerk of Arraigns or another formal associate of the Court. It will generally be phrased as "Prisoner at the Bar, you have been found Guilty by a jury of your peers of the offense of XYZ. Do you have anything to say as to why the sentence of this Court should not now be passed upon you?". The defense counsel will then make a "plea in mitigation" (also called "submissions on penalty") wherein he or she will attempt to mitigate the relative seriousness of the offense and heavily refer to and rely upon the defendant's previous good character and good works (if any). In Australia, the right to make a plea in mitigation is absolute. If a judge or magistrate were to refuse to hear such a plea, or obviously fail to properly consider it, then the sentence would, without doubt, be overturned on appeal.
In many other jurisdictions it is for the defense lawyer to mitigate on his client's behalf, and the defendant himself will rarely have the opportunity to speak.
In media.
Allocution refers to the one way dissemination of information through a media channel. It assumes that one party has an unlimited amount of information (usually through some kind of expertise) and can act as the ‘information services provider’ (pg 268) while the other party acts as the ‘information services consumer’ (Bordewijk and Kaam, 1986:268)
The term allocution differs from distribution as distribution implies that the original party loses some kind of control over the information. One party can tell many others a piece of information without losing it themselves, the original information store never becomes empty. (Bordewijk and Kaam, 1986:268)
The original party holds all control over the information. They decide when, how and how much information to give to the information services consumer. The consumer has no control over it in this model.
Examples of this type of communication include radio and traditional television programs such as the news.
Bordewijk, Jan L. and van Kaam, Ben (2002) [1986] “Towards a New Classification of Tele-Information Services,” in Denis McQuail (ed.) McQuail’s Reader in Mass Communication Theory, Sage, London, pp. 113–24
Roman Catholic Magisterium.
According to the Catholic Encyclopedia, an Allocution is a solemn form of address or speech from the throne employed by the Pope on certain occasions. It is delivered only in a secret consistory at which the cardinals alone are present. The term allocutio was used by the ancient Romans for the speech made by a commander to his troops, either before a battle or during it, to animate and encourage them. The term when adopted into ecclesiastical usage retained much of its original significance. An allocution of the Pope often takes the place of a manifesto when a struggle between the Holy See and the secular powers has reached an acute stage.
---END.OF.DOCUMENT---

Affidavit.
An affidavit is a formal sworn statement of fact, signed by the author, who is called the affiant or deponent, and witnessed as to the authenticity of the affiant's signature by a taker of oaths, such as a notary public or commissioner of oaths. The name is Medieval Latin for "he has declared upon oath". An affidavit is a type of verified statement or showing, or in other words, it contains a verification, meaning it is under oath or penalty of perjury, and this serves as evidence to its veracity and is required for court proceedings.
If an affidavit is notarized or authenticated, it will also include a caption with a venue and title in reference to judicial proceedings. In some cases, an introductory clause, called a preamble, is added attesting that the affiant personally appeared before the authenticating authority.
United States.
In American jurisprudence, under the rules for hearsay, admission of an unsupported affidavit as evidence is unusual (especially if the affiant is not available for cross-examination) with regard to material facts which may be dispositive of the matter at bar. Affidavits from persons who are dead or otherwise incapacitated, or who cannot be located or made to appear may be accepted by the court, but usually only in the presence of corroborating evidence. An affidavit which reflected a better grasp of the facts close in time to the actual events may be used to refresh a witness' recollection. Materials used to refresh recollection are admissible as evidence. If the affiant is a party in the case, the affiant's opponent may be successful in having the affidavit admitted as evidence, as statements by a party-opponent are not considered hearsay.
Some types of motions will not be accepted by the court unless accompanied by an independent sworn statement or other evidence, in support of the need for the motion. In such a case, a court will accept an affidavit from the filing attorney in support of the motion, as certain assumptions are made, to wit: The affidavit in place of sworn testimony promotes judicial economy. The lawyer is an officer of the court and knows that a false swearing by him, if found out, could be grounds for severe penalty up to and including disbarment. The lawyer if called upon would be able to present independent and more detailed evidence to prove the facts set forth in his affidavit.
The acceptance of an affidavit by one society does not confirm its acceptance as a legal document in other jurisdictions. Equally, the acceptance that a lawyer is an officer of the court (for swearing the affidavit) is not a given. This matter is addressed by the use of the Apostille, a means of certifying the legalization of a document for international use under the terms of the 1961 Hague Convention Abolishing the Requirement of Legalization for Foreign Public Documents. Documents which have been notarized by a notary public, and certain other documents, and then certified with a conformant apostille are accepted for legal use in all the nations that have signed the Hague Convention. Thus most Affidavits now require to be Apostilled if used for cross border issues.
Ireland.
Affidavits are made in a similar way as to England and Wales, although "make oath" is sometimes omitted. A declaration may be substituted for an affidavit in most cases for those opposed to swearing oaths. The person making the affidavit is known as the deponent but does not sign the affidavit. The affidavit concludes in the standard format "sworn (declared) before me, [name of commissioner for oaths/solicitor], a commissioner for oaths (solicitor), on the [date] at [location] in the county/city of [county/city], and I know the deponent (declarant)", and it is signed and stamped by the commissioner for oaths.
---END.OF.DOCUMENT---

Aries (constellation).
Aries (meaning: "ram") is one of the constellations of the zodiac, located between Pisces to the west and Taurus to the east. Its name is Latin for ram, and its symbol is (), representing a ram's horns. Aries was one of the 48 constellations described by the 2nd century astronomer Ptolemy, and remains one of the 88 modern constellations today.
Deep sky objects.
The few deep sky objects in Aries are very dim. They include the galaxies NGC 697 (northwest of β Ari), NGC 772 (southeast of β Ari), NGC 972 (in the constellation's northern corner), and NGC 1156 (northwest of δ Ari).
Visualizations.
Ancient Greek astronomers visualized Aries as a ram lying down with its head turned to the right. The stars α, β and γ Arietis mark the ram’s head and horns, according to Ptolemy's Almagest.
Mythology.
In the ancient Babylonian calendar given in the stone tablets known as the "MUL.APIN", the constellation now known as Aries was the final station along the ecliptic. It was known as, "The Agrarian Worker". It is unclear how the "Agrarian Worker" became the "Ram" of Greek tradition, but John H. Rogers has suggested that it may have been via association with the legendary figure of Dumuzi the Shepherd. Aries only rose to its prominent position in the Neo-Babylonian revision of the Babylonian zodiac as the first point of Aries came to represent vernal equinox, replacing the Pleiades which had had this function during the Bronze Age.
In Greek mythology, the constellation of Aries represents the golden ram that rescued Phrixos, taking him to the land of Colchis. Phrixos sacrificed the ram to the gods and hung its skin in a temple, where it was known as the Golden Fleece.
Astrology.
, the Sun appears in the constellation Aries from April 19 to May 13. In tropical astrology, the Sun is considered to be in the sign Aries from March 21 to April 19, and in sidereal astrology, from April 15 to May 15.
---END.OF.DOCUMENT---

Aquarius (constellation).
Aquarius is a constellation of the zodiac, situated between Capricornus and Pisces. Its name is Latin for "water-bearer" or "cup-bearer", and its symbol is (), a representation of water.
Aquarius is one of the oldest of the recognized constellations along the zodiac (the sun's apparent path). It is found in a region often called the Sea due to its profusion of constellations with watery associations such as Cetus the whale, Pisces the fish and Eridanus the river.
Deep sky objects.
There are three deep sky objects that are on the Messier catalog: the globular clusters Messier 2, Messier 72, and the open cluster Messier 73. Two well-known planetary nebulae are located in Aquarius: the Saturn Nebula (NGC 7009), to the southwest of η Aquarii; and the famous Helix Nebula (NGC 7293), southwest of δ Aquarii.
Illustrations.
Image:Aquariusurania.jpg|thumb|left|300px|Aquarius pours water from a jar into the mouth of the southern fish, as depicted in
In illustrations, the brightest stars of Aquarius are represented as the figure of a man, while the fainter naked eye stars are represented as a vessel from which is pouring a stream of water. The water flows southwards into the mouth of the southern fish, Piscis Austrinus.
Mythology.
Aquarius is identified as "The Great One" in the Babylonian star catalogues and represents the god Ea himself. It contained the winter solstice in the Early Bronze Age. In Old Babylonian astronomy, Ea was the ruler of the southernmost quarter of the Sun's path, the "Way of Ea", corresponding to the period of 45 days on either side of winter solstice.
In the Greek tradition, the constellation became represented as simply a single vase from which a stream poured down to Piscis Austrinus. The name in the Hindu zodiac is likewise "kumbha" "water-pitcher", showing that the zodiac reached India via Greek intermediaries.
Aquarius is sometimes identified with Ganymede, a beautiful youth in Greek mythology with whom Zeus fell in love and, in the disguise of an eagle (represented by the constellation Aquila) carried off to Olympus to be cup-bearer to the gods. The constellation of Crater is sometimes identified as his cup.
Aquarius has also been identified as the pourer of the waters that flooded the Earth in the ancient Greek version of the Great Flood myth. As such, the constellation Eridanus the river is sometimes identified as a river being poured by Aquarius.
Aquarius may also, together with the constellation Pegasus, be part of the origin of the myth of the Mares of Diomedes, which forms one of The Twelve Labours of Heracles. Its association with pouring out rivers, and the nearby constellation of Capricornus, may be the source of the myth of the Augean stable, which forms another of the labours.
Astrology.
, the Sun appears in the constellation Aquarius from February 16 to March 11. In tropical astrology, the Sun is considered to be in the sign Aquarius from January 20 to February 19, and in sidereal astrology, from February 15 to March 14.
---END.OF.DOCUMENT---

Anime.
Anime in English usually refers to a style of animation originating in Japan, heavily influenced by the manga (Japanese comics) style and typically featuring characters with large eyes, big hair and elongated limbs, exaggerated facial expressions, brush-stroked outlines, limited motion and other distinctive features. The term may also be used for other animation connected to Japan or to anime proper, irrespective of style. The word comes from Japanese アニメ "anime", meaning "animation" in general, and is typically pronounced or in English.
While the earliest known Japanese animation dates from 1917,
and many original Japanese cartoons were produced in the ensuing decades, the characteristic anime style developed in the 1960s - notably with the work of Osamu Tezuka - and became known outside Japan in the 1980s.
Anime, like manga, has a large audience in Japan and high recognition throughout the world. Distributors can release anime via television broadcasts, directly to video, or theatrically, as well as online.
Both hand-drawn and computer-animated anime exist. It is used in television series, films, video, video games, commercials, and internet-based releases, and represents most, if not all, genres of fiction. Anime gained early popularity in East and Southeast Asia and has garnered more-recent popularity in the Western World.
History.
Anime began at the start of the 20th century, when Japanese filmmakers experimented with the animation techniques also pioneered in France, Germany, the United States, and Russia. The oldest known anime in existence first screened in 1917 – a two-minute clip of a samurai trying to test a new sword on his target, only to suffer defeat. Early pioneers included Shimokawa Oten, Jun'ichi Kouchi, and Seitarō Kitayama.
By the 1930s animation became an alternative format of storytelling to the live-action industry in Japan. But it suffered competition from foreign producers and many animators, such as Noburō Ōfuji and Yasuji Murata still worked in cheaper cutout not cel animation, although with masterful results. Other creators, such as Kenzō Masaoka and Mitsuyo Seo, nonetheless made great strides in animation technique, especially with increasing help from a government using animation in education and propaganda. The first talkie anime was "Chikara to Onna no Yo no Naka", produced by Masaoka in 1933. The first feature length animated film was "Momotaro's Divine Sea Warriors" directed by Seo in 1945 with sponsorship by the Imperial Japanese Navy.
The success of The Walt Disney Company's 1937 feature film "Snow White and the Seven Dwarfs" influenced Japanese animators. In the 1960s, manga artist and animator Osamu Tezuka adapted and simplified many Disney animation-techniques to reduce costs and to limit the number of frames in productions. He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation-staff.
The 1970s saw a surge of growth in the popularity of manga – many of them later animated. The work of Osamu Tezuka drew particular attention: he has been called a "legend" and the "god of manga".
His work – and that of other pioneers in the field – inspired characteristics and genres that remain fundamental elements of anime today. The giant robot genre (known as "Mecha" outside Japan), for instance, took shape under Tezuka, developed into the Super Robot genre under Go Nagai and others, and was revolutionized at the end of the decade by Yoshiyuki Tomino who developed the Real Robot genre. Robot anime like the "Gundam" and "The Super Dimension Fortress Macross" series became instant classics in the 1980s, and the robot genre of anime is still one of the most common in Japan and worldwide today. In the 1980s, anime became more accepted in the mainstream in Japan (although less than manga), and experienced a boom in production. Following a few successful adaptations of anime in overseas markets in the 1980s, anime gained increased acceptance in those markets in the 1990s and even more at the turn of the 21st century.
Terminology.
Japanese write the English term "animation" in "katakana" as アニメーション ("animēshon", pronounced), and it is widely assumed that the term アニメ ("anime", pronounced in Japanese) emerged in the 1970s as an abbreviation. Others claim that the word derives from the French phrase "dessin animé".
Japanese-speakers use both the original and abbreviated forms interchangeably, but the shorter form occurs more commonly.
The pronunciation of "anime" in Japanese, differs significantly from the Standard English, which has different vowels and stress. (In Japanese each mora carries equal stress.) As with a few other Japanese words such as "saké", "Pokémon", and "Kobo Abé," English-language texts sometimes spell "anime" as "animé" (as in French), with an acute accent over the final "e", to cue the reader to pronounce the letter, not to leave it silent as English orthography might suggest.
Word usage.
In Japan, the term "anime" does not specify an animation's nation of origin or style; instead, it serves as a blanket term to refer to all forms of animation from around the world. English-language dictionaries define "anime" as "a Japanese style of motion-picture animation" or as "a style of animation developed in Japan".
Non-Japanese works that borrow stylization from anime are commonly referred to as "anime-influenced animation" but it is not unusual for a viewer who does not know the country of origin of such material to refer to it as simply "anime". Some works result from co-productions with non-Japanese companies, such as most of the traditionally animated Bass works, the Cartoon Network and Production I.G series "IGPX" or "Ōban Star-Racers"; different viewers may or may not consider these anime.
In English, "anime", when used as a common noun, normally functions as a mass noun (for example: "Do you watch anime?", "How much anime have you collected?"). However, in casual usage the word also appears as a count noun. "Anime" can also be used as a suppletive adjective or classifier noun ("The anime "Guyver" is different from the movie "Guyver").
Synonyms.
English-speakers occasionally refer to anime as "Japanimation", but this term has fallen into disuse. "Japanimation" saw the most usage during the 1970s and 1980s, but the term "anime" supplanted it in the mid-1990s as the material became more widely known in English-speaking countries.
In general, the term now only appears in nostalgic contexts. Although the term was coined outside Japan to refer to animation imported from Japan, it is now used primarily "in" Japan, to refer to domestic animation; since "anime" does not identify the country of origin in Japanese usage, "Japanimation" is used to distinguish Japanese work from that of the rest of the world.
In Japan, "manga" can additionally refer to both animation and comics (although the use of "manga" to refer to animation mostly occurs only among non-fans). Among English speakers, "manga" usually has the stricter meaning of "Japanese comics", in parallel to the usage of "anime" in and outside of Japan. An alternate explanation is that it is due to the prominence of Manga Entertainment, a distributor of anime to the US and UK markets. Because Manga Entertainment originated in the UK, the term occurs commonly outside Japan. The term "ani-manga" has been used to collectively refer to anime and manga, though it is also a term used to describe comics produced from animation cels.
Visual characteristics.
Many commentators refer to anime as an art form. As a visual medium, it can emphasize visual styles. The styles can vary from artist to artist or from studio to studio. Some titles make extensive use of common stylization: "FLCL", for example, has a reputation for wild, exaggerated stylization. Other titles use different methods: "Only Yesterday" or "Jin-Roh" take much more realistic approaches, featuring few stylistic exaggerations; "Pokémon" uses drawings which specifically do not distinguish the nationality of characters.
While different titles and different artists have their own artistic styles, many stylistic elements have become so common that people describe them as definitive of anime in general. However, this does not mean that all modern anime share one strict, common art-style. Many anime have a very different art style from what would commonly be called "anime style", yet fans still use the word "anime" to refer to these titles. Generally, the most common form of anime drawings include "exaggerated physical features such as large eyes, big hair and elongated limbs... and dramatically shaped speech bubbles, speed lines and onomatopoeic, exclamatory typography."
The influences of Japanese calligraphy and Japanese painting also characterize linear qualities of the anime style. The round ink brush traditionally used for writing kanji and for painting, produces a stroke of widely varying thickness.
Anime also tends to borrow many elements from manga, including text in the background and panel layouts. For example, an opening may employ manga panels to tell the story, or to dramatize a point for humorous effect. See for example the anime "Kare Kano".
Proportions.
Body proportions emulated in anime come from proportions of the human body. The height of the head is considered as the base unit of proportion. Head heights can vary as long as the remainder of the body remains proportional. Most anime characters are about seven to eight heads tall, and extreme heights are set around nine heads tall.
Variations to proportion can be modded. Super-deformed characters feature a non-proportionally small body compared to the head. Sometimes specific body parts, like legs, are shortened or elongated for added emphasis. Most super deformed characters are two to four heads tall. Some anime works like "Crayon Shin-chan" completely disregard these proportions, such that they resemble Western cartoons. For exaggeration, certain body features are increased in proportion.
Eye styles.
Many anime and manga characters feature large eyes. Osamu Tezuka, who is believed to have been the first to use this technique, was inspired by the exaggerated features of American cartoon characters such as Betty Boop, Mickey Mouse, and Disney's "Bambi". Tezuka found that large eyes style allowed his characters to show emotions distinctly. When Tezuka began drawing "Ribbon no Kishi", the first manga specifically targeted at young girls, Tezuka further exaggerated the size of the characters' eyes. Indeed, through "Ribbon no Kishi", Tezuka set a stylistic template that later "shōjo" artists tended to follow.
Coloring is added to give eyes, particularly to the cornea, some depth. The depth is accomplished by applying variable color shading. Generally, a mixture of a light shade, the tone color, and a dark shade is used. Cultural anthropologist Matt Thorn argues that Japanese animators and audiences do not perceive such stylized eyes as inherently more or less foreign.
However, not all anime have large eyes. For example, some of the work of Hayao Miyazaki and Toshiro Kawamoto are known for having realistically proportioned eyes, as well as realistic hair colors on their characters. In addition many other productions also have been known to use smaller eyes. This design tends to have more resemblance to traditional Japanese art. Some characters have even smaller eyes, where simple black dots are used. However, many western audiences associate anime with large detailed eyes.
Facial expressions.
Anime characters may employ wide variety of facial expressions to denote moods and thoughts. These techniques are often different in form than their counterparts in western animation.
There are a number of other stylistic elements that are common to conventional anime as well but more often used in comedies. Characters that are shocked or surprised will perform a "face fault", in which they display an extremely exaggerated expression. Angry characters may exhibit a "vein" or "stress mark" effect, where lines representing bulging veins will appear on their forehead. Angry women will sometimes summon a mallet from nowhere and strike someone with it, leading to the concept of Hammerspace and cartoon physics. Male characters will develop a bloody nose around their female love interests (typically to indicate arousal, based on an old wives' tale). Embarrassed characters either produce a massive sweat-drop (which has become one of the most widely recognized motifs of conventional anime) or produce a visibly red blush or set of parallel (sometimes squiggly) lines beneath the eyes, especially as a manifestation of repressed romantic feelings. Some anime, usually with political plots and other more serious subject matters, have abandoned the use of these techniques.
Animation technique.
Like all animation, the production processes of storyboarding, voice acting, character design, cel production and so on still apply. With improvements in computer technology, computer animation increased the efficiency of the whole production process.
Anime is often considered a form of limited animation. That means that stylistically, even in bigger productions the conventions of limited animation are used to fool the eye into thinking there is more movement than there is. Many of the techniques used are comprised with cost-cutting measures while working under a set budget.
Anime scenes place emphasis on achieving three-dimensional views. Backgrounds depict the scenes' atmosphere. For example, anime often puts emphasis on changing seasons, as can be seen in numerous anime, such as "Tenchi Muyo!". Sometimes actual settings have been duplicated into an anime. The backgrounds for the "Melancholy of Haruhi Suzumiya" are based on various locations within the suburb of Nishinomiya, Hyogo, Japan.
Camera angles, camera movement, and lighting play an important role in scenes. Directors often have the discretion of determining viewing angles for scenes, particularly regarding backgrounds. In addition, camera angles show perspective. Directors can also choose camera effects within cinematography, such as panning, zooming, facial closeup, and panoramic.
The large majority of anime uses traditional animation, which better allows for division of labor, pose to pose approach and checking of drawings before they are shot – practices favoured by the anime industry. Other mediums are mostly limited to independently-made short films, examples of which are the silhouette and other cutout animation of Noburō Ōfuji, the stop motion puppet animation of Tadahito Mochinaga, Kihachirō Kawamoto and Tomoyasu Murata and the computer animation of Satoshi Tomioka (most famously "Usavich").
Distribution.
While anime had entered markets beyond Japan in the 1960s, it grew as a major cultural export during its market expansion during the 1980s and 1990s. The anime market for the United States alone is "worth approximately $4.35 billion, according to the Japan External Trade Organization". Anime has also been a commercial success in Asia, Europe and Latin America, where anime has become even more mainstream than in the United States. For example, the "Saint Seiya" video game was released in Europe due to the popularity of the show even years after the series has been off-air.
Anime distribution companies handled the licensing and distribution of anime outside Japan. Licensed anime is modified by distributors through dubbing into the language of the country and adding language subtitles to the Japanese language track. Using a similar global distribution pattern as Hollywood, the world is divided into five regions.
Some editing of cultural references may occur to better follow the references of the non-Japanese culture. Certain companies may remove any objectionable content, complying with domestic law. This editing process was far more prevalent in the past (e.g. "Voltron"), but its use has declined because of the demand for anime in its original form. This "light touch" approach to localization has favored viewers formerly unfamiliar with anime. The use of such methods is evident by the success of "Naruto" and Cartoon Network's Adult Swim programming block, both of which employ minor edits. "Robotech" and "Star Blazers" were the earliest attempts to present anime (albeit still modified) to North American television audiences without harsh censoring for violence and mature themes.
With the advent of DVD, it became possible to include multiple language tracks into a simple product. This was not the case with VHS cassette, in which separate VHS media were used and with each VHS cassette priced the same as a single DVD. The "light touch" approach also applies to DVD releases as they often include both the dubbed audio and the original Japanese audio with subtitles, typically unedited. Anime edited for television is usually released on DVD "uncut", with all scenes intact.
TV networks regularly broadcast anime programming. In Japan, major national TV networks, such as TV Tokyo broadcast anime regularly. Smaller regional stations broadcast anime under the UHF. In the United States, cable TV channels such as Cartoon Network, Disney, Syfy, and others dedicate some of their timeslots to anime. Some, such as the Anime Network and the FUNimation Channel, specifically show anime. Sony-based Animax and Disney's Jetix channel broadcast anime within many countries in the world. AnimeCentral solely broadcasts anime in the UK.
Although it violates copyright laws in many countries, some fans add subtitles to anime on their own. These are distributed as fansubs. The ethical implications of producing, distributing, or watching fansubs are topics of much controversy even when fansub groups do not profit from their activities. Once the series has been licensed outside of Japan, fansub groups often cease distribution of their work. In one case, Media Factory Incorporated requested that no fansubs of their material be made, which was respected by the fansub community. In another instance, Bandai specifically thanked fansubbers for their role in helping to make "The Melancholy of Haruhi Suzumiya" popular in the English speaking world.
The Internet has played a significant role in the exposure of anime beyond Japan. Prior to the 1990s, anime had limited exposure beyond Japan's borders. Coincidentally, as the popularity of the Internet grew, so did interest in anime. Much of the fandom of anime grew through the Internet. The combination of internet communities and increasing amounts of anime material, from video to images, helped spur the growth of fandom. As the Internet gained more widespread use, Internet advertising revenues grew from 1.6 billion yen to over 180 billion yen between 1995 and 2005.
Influence on world culture.
Anime has become commercially profitable in western countries, as early commercially successful western adaptations of anime, such as "Astro Boy", have revealed. The phenomenal success of Nintendo's multi-billion dollar "Pokémon" franchise was helped greatly by the spin-off anime series that, first broadcast in the late 1990s, is still running worldwide to this day. In doing so, anime has made significant impacts upon Western culture. Since the 19th century, many Westerners have expressed a particular interest towards Japan. Anime dramatically exposed more Westerners to the culture of Japan. Aside from anime, other facets of Japanese culture increased in popularity. Worldwide, the number of people studying Japanese increased. In 1984, the Japanese Language Proficiency Test was devised to meet increasing demand. Anime-influenced animation refers to non-Japanese works of animation that emulate the visual style of anime. Most of these works are created by studios in the United States, Europe, and non-Japanese Asia; and they generally incorporate stylizations, methods, and gags described in anime physics, as in the case of '. Often, production crews either are fans of anime or are required to view anime. Some creators cite anime as a source of inspiration with their own series. Furthermore, a French production team for "Ōban Star-Racers" moved to Tokyo to collaborate with a Japanese production team from Hal Film Maker. Critics and the general anime fanbase do not consider them as anime.
Some American animated television-series have singled out anime styling with satirical intent, for example "South Park" (with "Chinpokomon" and with "Good Times with Weapons"). "South Park" has a notable drawing style, itself parodied in "Brittle Bullet", the fifth episode of the anime "FLCL", released several months after "Chinpokomon" aired. This intent on satirizing anime is the springboard for the basic premise of "Kappa Mikey", a Nicktoons Network original cartoon. Even clichés normally found in anime are parodied in some series, such as "Perfect Hair Forever". Anime conventions began to appear in the early 1990s, during the Anime boom, starting with Anime Expo, Animethon, Otakon, and JACON. Currently anime conventions are held annually in various cities across the Americas, Asia, and Europe. Many attendees participate in cosplay, where they dress up as anime characters. Also, guests from Japan ranging from artists, directors, and music groups are invited. In addition to anime conventions, anime clubs have become prevalent in colleges, high schools, and community centers as a way to publicly exhibit anime as well as broadening Japanese cultural understanding.
Anime and American audiences.
The Japanese term "otaku" is used in America as a term for anime fans, more particularly the obsessive ones. The negative connotations associated with the word in Japan have disappeared in its American context, where it instead connotes the pride of the fans. Only in the recent decade or so has there been a more casual viewership outside the devoted "otaku" fan base, which can be attributed highly to technological advances. Also, shows like "Pokémon" and "Dragon Ball Z" provided a pivotal introduction of anime's conventions, animation methods, and Shinto influences to many American children.
Ancient Japanese myths – often deriving from the animistic nature worship of Shinto – have influenced anime greatly, but most American audiences not accustomed to anime know very little of these foreign texts and customs. For example, an average American viewing the live-action TV show "Hercules" will be no stranger to the Greek myths and legends it is based on, while the same person watching the show "Tenchi Muyo!" might not understand that the pleated ropes wrapped around the "space trees" are influenced by the ancient legend of "Amaterasu and Susano".
---END.OF.DOCUMENT---

Ankara.
Ankara is the capital of Turkey and the country's second largest city after Istanbul. The city has a mean elevation of, and as of 2007 the city had a population of 4,751,360, which includes eight districts under the city's administration. Ankara also serves as the capital of Ankara Province.
As with many ancient cities, Ankara has gone by several names over the ages: The Hittites gave it the name "Ankuwash" before 1200 BC. The Galatians and Romans called it "Ancyra". In the classical, Hellenistic, and Byzantine periods it was known as ("Ánkyra", meaning "Anchor") in Greek. The city was also known in the European languages as "Angora" after its conquest by the Seljuk Turks in 1073, and continued to be internationally called with this name until it was officially renamed "Ankara" with the Turkish Postal Service Law of 1930.
Centrally located in Anatolia, Ankara is an important commercial and industrial city. It is the center of the Turkish Government, and houses all foreign embassies. It is an important crossroads of trade, strategically located at the centre of Turkey's highway and railway networks, and serves as the marketing centre for the surrounding agricultural area. The city was famous for its long-haired Angora goat and its prized wool (mohair), a unique breed of cat (Angora cat), white rabbits and their prized wool (Angora wool), pears, honey, and the region's muscat grapes.
The historical center of Ankara is situated upon a steep and rocky hill, which rises above the plain on the left bank of the "Ankara Çayı", a tributary of the Sakarya (Sangarius) river. The city is located at 39°52'30" North, 32°52' East (), about to the southeast of Istanbul, the country's largest city. Although situated in one of the driest places of Turkey and surrounded mostly by steppe vegetation except for the forested areas on the southern periphery, Ankara can be considered a green city in terms of green areas per inhabitant, which is 72 m2 per head.
Ankara is a very old city with various Hittite, Phrygian, Hellenistic, Roman, Byzantine, and Ottoman archaeological sites. The hill which overlooks the city is crowned by the ruins of the old castle, which adds to the picturesqueness of the view, but only a few historic structures surrounding the old citadel have survived to our date. There are, however, many finely preserved remains of Hellenistic, Roman and Byzantine architecture, the most remarkable being the Temple of Augustus and Rome (20 BC) which is also known as the "Monumentum Ancyranum".
History.
The region's history can be traced back to the Bronze Age Hatti civilization, which was succeeded in the 2nd millennium BC by the Hittites, in the 10th century BC by the Phrygians, and later by the Lydians, Persians, Greeks, Galatians, Romans, Byzantines, and Turks (the Seljuk Sultanate of Rûm, the Ottoman Empire and Turkey.)
Ancient history.
The oldest settlements in and around the city centre of Ankara belong to the Hatti civilization which existed during the Bronze Age. The city grew significantly in size and importance under the Phrygians starting around 1000 BC, and experienced a large expansion following the mass migration from Gordion, (the capital of Phrygia), after an earthquake which severely damaged that city around that time. In Phrygian tradition, King Midas was venerated as the founder of Ancyra, but Pausanias mentions that the city was actually far older, which accords with present archaeological knowledge.
Phrygian rule was succeeded first by Lydian and later by Persian rule, though the strongly Phrygian character of the peasantry remained, as evidenced by the gravestones of the much later Roman period. Persian sovereignty lasted until the Persians' defeat at the hands of Alexander the Great who conquered the city in 333 BC. Alexander came from Gordion to Ankara and stayed in the city for a short period. After his death at Babylon in 323 BC and the subsequent division of his empire amongst his generals, Ankara and its environs fell into the share of Antigonus.
Another important expansion took place under the Greeks of Pontos who came there around 300 BC and developed the city as a trading centre for the commerce of goods between the Black Sea ports and Crimea to the north; Assyria, Cyprus, and Lebanon to the south; and Georgia, Armenia and Persia to the east. By that time the city also took its name "Áγκυρα" ("Ànkyra", meaning "Anchor" in Greek) which in slightly modified form provides the modern name of "Ankara".
Celtic history.
In 278 BC, the city, along with the rest of central Anatolia, was occupied by the Celtic race of Galatians, who were the first to make Ankara one of their main tribal centres, the headquarters of the Tectosages tribe. Other centres were Pessinos, today's "Balhisar", for the Trocmi tribe, and Tavium, to the east of Ankara, for the "Tolstibogii" tribe. The city was then known as "Ancyra". The Celtic element was probably relatively small in numbers; a warrior aristocracy which ruled over Phrygian-speaking peasants. However, the Celtic language continued to be spoken in Galatia for many centuries. At the end of the 4th century AD, St. Jerome, a native of Galatia, observed that the language spoken around Ankara was very similar to that being spoken in the northwest of the Roman world near Trier.
Roman history.
The city was subsequently conquered by Augustus in 25 BC and passed under the control of the Roman Empire. Now the capital city of the Roman province of Galatia, Ancyra continued to be a center of great commercial importance. Ankara is also famous for the "Monumentum Ancyranum" ("Temple of Augustus and Rome") which contains the official record of the "Acts of Augustus", known as the "Res Gestae Divi Augusti", an inscription cut in marble on the walls of this temple. The ruins of Ancyra still furnish today valuable bas-reliefs, inscriptions and other architectural fragments.
Augustus decided to make Ancyra one of three main administrative centres in central Anatolia. The town was then populated by Phrygians and Celts—the "Galatians" who spoke a language closely related to Welsh and Gaelic. Ancyra was the center of a tribe known as the "Tectosages", and Augustus upgraded it into a major provincial capital for his empire. Two other Galatian tribal centres, Tavium near Yozgat, and Pessinus (Balhisar) to the west, near Sivrihisar, continued to be reasonably important settlements in the Roman period, but it was Ancyra that grew into a grand metropolis.
An estimated 200,000 people lived in Ancyra in good times during the Roman Empire, a far greater number than was to be the case from after the fall of the Roman Empire until the early twentieth century. A small river, the Ankara Çayı, ran through the centre of the Roman town. It has now been covered over and diverted, but it formed the northern boundary of the old town during the Roman, Byzantine and Ottoman periods. Çankaya, the rim of the majestic hill to the south of the present city center, stood well outside the Roman city, but may have been a summer resort. In the 19th century, the remains of at least one Roman villa or large house were still standing not far from where the Çankaya Presidential Residence stands today. To the west, the Roman city extended until the area of the Gençlik Park and Railway Station, while on the southern side of the hill, it may have extended downwards as far as the site presently occupied by Hacettepe University. It was thus a sizeable city by any standards and much larger than the Roman towns of Gaul or Britannia.
Ancyra's importance rested on the fact was that it was the junction point where the roads in northern Anatolia running north-south and east-west intersected. The great imperial road running east passed through Ankara and a succession of emperors and their armies came this way. They were not the only ones to use the Roman highway network, which was equally convenient for invaders. In the second half of the 3rd century, Ancyra was invaded in rapid succession by the Goths coming from the west (who rode far into the heart of Cappadocia, taking slaves and pillaging) and later by the Arabs. For about a decade, the town was one of the western outposts of one of the most brilliant queens of the ancient world, the Arab empress Zenobia from Palmyra in the Syrian desert, who took advantage of a period of weakness and disorder in the Roman Empire to set up a short-lived state of her own.
The town was reincorporated into the Roman Empire under the Emperor Aurelian in 272. The tetrarchy, a system of multiple (up to four) emperors introduced by Diocletian (284-305), seems to have engaged in a substantial programme of rebuilding and of road construction from Ankara westwards to Germe and Dorylaeum (now Eskişehir).
In its heyday, Roman Ankara was a large market and trading center but it also functioned as a major administrative capital, where a high official ruled from the city's Praetorium, a large administrative palace or office. During the 3rd century, life in Ancyra, as in other Anatolian towns, seems to have become somewhat militarised in response to the invasions and instability of the town. In this period, like other cities of central Anatolia, Ankara was also undergoing Christianisation.
Early martyrs, about whom little is known, included Proklos and Hilarios who were natives of the otherwise unknown village of Kallippi, near Ancyra, and suffered repression under the emperor Trajan (98-117). In the 280s AD we hear of Philumenos, a Christian corn merchant from southern Anatolia, being captured and martyred in Ankara, and Eustathius.
As in other Roman towns, the reign of Diocletian marked the culmination of the persecution of the Christians. In 303, Ancyra was one of the towns where the co-Emperors Diocletian and his deputy Galerius launched their anti-Christian persecution. In Ancyra, their first target was the 38-year-old Bishop of the town, whose name was Clement. Clement's life describes how he was taken to Rome, then sent back, and forced to undergo many interrogations and hardship before he, and his brother, and various companions were put to death. The remains of the church of St. Clement can be found today in a building just off Işıklar Caddesi in the Ulus district. Quite possibly this marks the site where Clement was originally buried. Four years later, a doctor of the town named Plato and his brother Antiochus also became celebrated martyrs under Galerius. Theodotus of Ancyra is also venerated as a saint.
However, the persecution proved unsuccessful and in 314 Ancyra was the center of an important council of the early church; which considered ecclesiastical policy for the reconstruction of the Christian church after the persecutions, and in particular the treatment of 'lapsi'—Christians who had given in and conformed to paganism during these persecutions.
Three councils were held in the former capital of Galatia in Asia Minor, during the 4th century. The first, an orthodox plenary synod, was held in 314, and its 25 disciplinary canons constitute one of the most important documents in the early history of the administration of the Sacrament of Penance. Nine of them deal with conditions for the reconciliation of the lapsi; the others, with marriage, alienations of church property, etc.
Though paganism was probably tottering in Ancyra in Clement's day, it may still have been the majority religion. Twenty years later, Christianity and monotheism had taken its place. Ancyra quickly turned into a Christian city, with a life dominated by monks and priests and theological disputes. The town council or senate gave way to the bishop as the main local figurehead. During the middle of the 4th century, Ancyra was involved in the complex theological disputes over the nature of Christ, and a form of Arianism seems to have originated there.
The synod of 358 was a Semi-Arian conciliabulum, presided over by Basil of Ancyra. It condemned the grosser Arian blasphemies, but set forth an equally heretical doctrine in the proposition that the Son was in all things similar to the Father, but not identical in substance.
In 362-363, the Emperor Julian the Apostate passed through Ancyra on his way to an ill-fated campaign against the Persians, and according to Christian sources, engaged in a persecution of various holy men. The stone base for a statue, with an inscription describing Julian as "Lord of the whole world from the British Ocean to the barbarian nations", can still be seen, built into the eastern side of the inner circuit of the walls of Ankara Castle. The Column of Julian which was erected in honor of the emperor's visit to the city in 362 still stands today. In 375, Arian bishops met at Ancyra and deposed several bishops, among them St. Gregory of Nyssa. The modern Ankara, also known in some Western texts as "Angora", remains a Roman Catholic titular see in the former Roman province of Galatia in Asia Minor, suffragan of Laodicea. Its episcopal list is given in Gams, "Series episc. Eccl. cath."; also that of another Ancyra in Phrygia Pacatiana.
In the later 4th century Ancyra became something of an imperial holiday resort. After Constantinople became the East Roman capital, emperors in the 4th and 5th centuries would retire from the humid summer weather on the Bosporus to the drier mountain atmosphere of Ancyra. Theodosius II (408-450) kept his court in Ancyra in the summers. Laws issued in Ancyra testify to the time they spent there. The city's military as well as logistical significance lasted well into the long Byzantine rule. Although Ancyra temporarily fell into the hands of several Arab Muslim armies numerous times after the seventh century, it remained an important crossroads polis within the Byzantine Empire until the late 11th century. It was also the capital of the powerful Opsician Theme, and after ca. 750 of the Bucellarian Theme.
Turkish history.
In 1071, the Turkish Seljuk Sultan Alparslan conquered much of eastern and central Anatolia after his victory at the Battle of Manzikert (Malazgirt). He then annexed Ankara, an important location for military transportation and natural resources, to his territory in 1073. After Battle of Kösedağ in 1243 which Mongols defeated Seljuks, most of Anatolia became dominion of Mongols. Taking advantage of Seljuk decline, a semi religious cast of craftsmen and trade people named "Ahiler" chose Ankara as their independent city state in 1290. Orhan I, the second Bey of the Ottoman Empire, captured the city in 1356. Timur defeated the Ottomans at the Battle of Ankara in 1402 and took the city, but in 1403 Ankara was again under Ottoman control.
Following the Ottoman defeat at World War I, the Ottoman capital Istanbul and much of Anatolia were occupied by the Allies, who planned to share these lands between Armenia, France, Greece, Italy and the United Kingdom, leaving for the Turks the core piece of land in central Anatolia. In response, the leader of the Turkish nationalist movement, Mustafa Kemal Atatürk, established the headquarters of his resistance movement in Ankara in 1920 (see the Treaty of Sèvres and the Turkish War of Independence.) After the War of Independence was won and the Treaty of Sèvres was superseded by the Treaty of Lausanne, the Turkish nationalists replaced the Ottoman Empire with the Republic of Turkey on 29 October 1923. A few days earlier, Ankara had officially replaced Istanbul (formerly Constantinople) as the new Turkish capital city, on 13 October 1923.
After Ankara became the capital of the newly founded Republic of Turkey, new development divided the city into an old section, called "Ulus", and a new section, called "Yenişehir". Ancient buildings reflecting Roman, Byzantine, and Ottoman history and narrow winding streets mark the old section. The new section, now centered around "Kızılay", has the trappings of a more modern city: wide streets, hotels, theaters, shopping malls, and high-rises. Government offices and foreign embassies are also located in the new section.
Ankara has experienced a phenomenal growth since it was made Turkey's capital. It was "a small town of no importance" when it was made the capital of Turkey. In 1924, the year after the government had moved there, Ankara had about 35,000 residents. By 1927 there were 44,553 residents and by 1950 the population had grown to 286,781.
Climate.
Ankara has a continental climate, with cold, snowy winters and hot, dry summers. Rainfall occurs mostly during the spring and autumn. Under Köppen's climate classification, Ankara features the rare Continental Mediterranean climate (Köppen Csb) due to its elevation, the forementioned cold, snowy winters and hot dry summers and peaks of precipitation during the spring and autumn. It borders on a cold semi-arid climate (Köppen BSk) due to the low average annual precipitation. Because of Ankara's high altitude and its dry summers, nightly temperatures in the summer months are cool. Precipitation levels are low, but precipitation can be observed throughout the year.
Demographics.
Central Ankara has a population of 3,763,591 (2007) of which 1,870,831 are men and 1,892,760 are women. The metropolitan municipality, containing the central part of the city and the remaining balance of the 8 districts under its jurisdiction, had a total population of 3,901,201 the same year.
Ankara Citadel.
The foundations of the citadel or castle were laid by the Galatians on a prominent lava outcrop, and the rest was completed by the Romans. The Byzantines and Seljuks further made restorations and additions. The area around and inside the citadel, being the oldest part of Ankara, contains many fine examples of traditional architecture. There are also recreational areas to relax. Many restored traditional Turkish houses inside the citadel area have found new life as restaurants, serving local cuisine.
The citadel was depicted in various Turkish banknotes during 1927-1952 and 1983-1989.
Roman Theatre.
The remains, the stage, and the backstage can be seen outside the castle. Roman statues that were found here are exhibited in the Museum of Anatolian Civilizations (see above). The seating area is still under excavation.
Temple of Augustus and Rome.
The temple, also known as the Monumentum Ancyranum, was built between 25 BC - 20 BC following the conquest of Central Anatolia by the Roman Empire and the formation of the Roman province of Galatia, with Ancyra (modern Ankara) as its administrative capital. After the death of Augustus in 14 AD, a copy of the text of Res Gestae Divi Augusti was inscribed on the interior of the "pronaos" in Latin, whereas a Greek translation is also present on an exterior wall of the "cella". The temple, on the ancient Acropolis of Ancyra, was enlarged by the Romans in the 2nd century. In the 5th century it was converted into a church by the Byzantines. It is located in the Ulus quarter of the city.
Roman Bath.
This bath has all the typical features of a classical Roman bath: a "frigidarium" (cold room), "tepidarium" (warm room) and "caldarium" (hot room). The bath was built during the reign of Emperor Caracalla in the 3rd century AD to honour Asclepios, the God of Medicine. Today, only the basement and first floors remain. It is situated in the Ulus quarter.
Column of Julian.
The column, popularly known among the locals as the "Belkıs Minaresi" (literally the "Queen of Sheba Column", for reasons unknown), was erected to commemorate a visit to Ancyra by the Roman emperor Julian in A.D. 362. The Corinthian capital dates to the 6th century; the stork's nest, a permanent crowning feature, is of more recent vintage.
Victory Monument.
Erected in 1927 on Zafer Square in the Sıhhiye quarter, it depicts Atatürk in uniform.
Monument to a Secure, Confident Future.
This monument, located in Güven Park near Kızılay Square, was erected in 1935 and bears Atatürk's advice to his people: "Turk! Be proud, work hard, and believe in yourself."
The monument was depicted on the reverse of the Turkish 5 lira banknote of 1937-1952 and of the 1000 lira banknotes of 1939-1946.
Hatti Monument.
Built in the 1970s on Sıhhiye Square, this impressive monument symbolizes the Hatti gods and commemorates Anatolia's earliest known civilization. The symbol derived from this monument has been used as the logo of the city for a long time.
Parks.
Ankara has many parks and open spaces mainly established in the early years of the Republic and well maintained and expanded thereafter. The most important of these parks are: Gençlik Park (houses an amusement park with a large pond for rowing), the Botanical Garden, Seğmenler Park, Anayasa Park, Kuğulu Park (famous for the swans received as a gift from the Chinese government), Abdi İpekçi Park, Güven Park (see above for the monument), Kurtuluş Park (has an ice-skating rink), Altınpark (also a prominent exposition/fair area), Harikalar Diyarı (claimed to be Biggest Park of Europe inside city borders) and Göksu Park.
Gençlik Park was depicted on the reverse of the Turkish 100 lira banknotes of 1952-1976.
Atatürk Forest Farm and Zoo ("Atatürk Orman Çiftliği") is an expansive recreational farming area which houses a zoo, several small agricultural farms, greenhouses, restaurants, a dairy farm and a brewery. It is a pleasant place to spend a day with family, be it for having picnics, hiking, biking or simply enjoying good food and nature. There is also an exact replica of the house where Atatürk was born in 1881, in Thessaloniki, Greece. Visitors to the "Çiftlik" (farm) as it is affectionately called by Ankarans, can sample such famous products of the farm such as old-fashioned beer and ice cream, fresh dairy products and meat rolls/kebaps made on charcoal, at a traditional restaurant ("Merkez Lokantası", Central Restaurant), cafés and other establishments scattered around the farm.
Shopping.
Foreign visitors to Ankara usually like to visit the old shops in "Çıkrıkçılar Yokuşu" (Weavers' Road) near Ulus, where myriad things ranging from traditional fabrics, hand-woven carpets and leather products can be found at bargain prices. "Bakırcılar Çarşısı" (Bazaar of Coppersmiths) is particularly popular, and many interesting items, not just of copper, can be found here like jewelry, carpets, costumes, antiques and embroidery. Up the hill to the castle gate, there are many shops selling a huge and fresh collection of spices, dried fruits, nuts, and other produce.
Modern shopping areas are mostly found in Kızılay, or on Tunalı Hilmi Avenue, including the modern mall of Karum (named after the ancient Assyrian merchant colonies (Karum) that were established in central Anatolia at the beginning of the 2nd millennium BC) which is located towards the end of the Avenue; and in the Atakule Tower at Çankaya, the quarter with the highest elevation in the city, which commands a magnificent view over the whole city and also has a revolving restaurant at the top where the complete panorama can be enjoyed in a more leisurely fashion. The symbol of the Armada Shopping Mall is an anchor, and there's a large anchor monument at its entrance, as a reference to the ancient Greek name of the city, Ἄγκυρα (Ánkyra), which means anchor. Likewise, the anchor is also related with the Spanish name of the mall, Armada, which means naval fleet.
As Ankara started expanding westward in the 1970s, several modern, suburbia-style developments and mini-cities began to rise along the western highway, also known as the Eskişehir Road. The "Armada" and "CEPA" malls on the highway, the "Galleria" in Ümitköy, and a huge mall, "Real" in Bilkent Center, offer North American and European style shopping opportunities (these places can be reached through the Eskişehir Highway.) There is also the newly expanded "Ankamall" at the outskirts, on the Istanbul Highway, which houses most of the well-known international brands. This mall is the largest throughout the Ankara region.
Culture and education.
In addition the city is served by several private theatre companies among which Ankara Sanat Tiyatrosu who have their own stage in the city centre is a notable example.
Transportation.
Esenboğa International Airport, located in the north-east of the city, is the main airport of Ankara.
() is an important part of the bus network which covers every neighbourhood in the city.
The central train station, "Ankara Garı" of the Turkish State Railways (), is an important hub connecting the western and eastern parts of the country. High-speed rail services are to be operated between Ankara and Istanbul, beginning in 2009.
The "Electricity, Gas, Bus General Directorate" (EGO) operates the Ankara Metro and other forms of public transportation. Ankara is currently served by suburban rail and two subway lines with about 300,000 total daily commuters, and three additional subway lines are under construction.
Sports.
As with all other cities of Turkey, football is the most popular sport in Ankara. The city has four football clubs currently competing in the Turkcell Super League: Ankaragücü founded in 1910 is the oldest club in Ankara and associated with Ankara's military arsenal manufacturing company MKE. They were the Turkish Cup winners in 1972 and 1981. Their rival is Gençlerbirliği founded in 1923 known as Ankara Wind or the Poppies because of their colours: red and black. They were the Turkish Cup winners in 1987 and 2001. Gençler's B team, Hacettepe SK (formerly known as Gençlerbirliği OFTAŞ) has been allowed to ascend to the Super League along with its A team as long as they have 2 different chairmen. All these three teams have their home at the Ankara 19 Mayıs Stadium in Ulus, which has a capacity of 21,250 (all-seater). The fourth team is owned by the Municipality, Büyükşehir Belediye Ankaraspor who are nicknamed the Leopards. Their home is the Yenikent Asaş Stadium in the Sincan district of Yenikent, outside the city center.
Ankara has a large number of minor teams, playing at regional levels: Bugsaşspor in Sincan; Etimesgut Şekerspor in Etimesgut; Türk Telekom owned by the phone company in Yenimahalle; Demirspor in Çankaya; Keçiörengücü, Keçiörenspor, Pursaklarspor, Bağlumspor in Keçiören; and Petrol Ofisi Spor.
In the Turkish Basketball League, Ankara is represented by Türk Telekom, whose home is the ASKI Sport Hall, and CASA TED Kolejliler, whose home is the TOBB Sports Hall.
Ankara Buz Pateni Sarayı is where the ice skating and ice hockey competitions take place in the city.
There are many popular spots for skateboarding which is active in the city since the 1980s. Skaters in Ankara usually meet in the park near the Grand National Assembly of Turkey.
Angora cat.
Ankara is home to a world famous cat breed — the Turkish Angora, called "Ankara kedisi" (Ankara cat) in Turkish. It is a breed of domestic cat. Turkish Angoras are one of the ancient, naturally-occurring cat breeds, having originated in Ankara and its surrounding region in central Anatolia.
They mostly have a white, silky, medium to long length coat, no undercoat and a fine bone structure. There seems to be a connection between the Angora Cats and Persians, and the Turkish Angora is also a distant cousin of the Turkish Van. Although they are known for their shimmery white coat, currently there are more than twenty varieties including black, blue and reddish fur. They come in tabby and tabby-white, along with smoke varieties, and are in every color other than pointed, lavender, and cinnamon (all of which would indicate breeding to an outcross.)
Eyes may be blue, green, or amber, or even one blue and one amber or green. The W gene which is responsible for the white coat and blue eye is closely related to the hearing ability, and the presence of a blue eye can indicate that the cat is deaf to the side the blue eye is located. However, a great many blue and odd-eyed white cats have normal hearing, and even deaf cats lead a very normal life if kept indoors.
Ears are pointed and large, eyes are almond shaped and the head is massive with a two plane profile. Another characteristic is the tail, which is often kept parallel to the back.
Angora rabbit.
The Angora rabbit () is a variety of domestic rabbit bred for its long, soft hair. The Angora is one of the oldest types of domestic rabbit, originating in Ankara and its surrounding region in central Anatolia, along with the Angora cat and Angora goat. The rabbits were popular pets with French royalty in the mid 1700s, and spread to other parts of Europe by the end of the century. They first appeared in the United States in the early 1900s. They are bred largely for their long Angora wool, which may be removed by shearing, combing, or plucking (gently pulling loose wool.)
Angoras are bred mainly for their wool because it is silky and soft. They have a humorous appearance, as they oddly resemble a fur ball. Most are calm and docile but should be handled carefully. Grooming is necessary to prevent the fiber from matting and felting on the rabbit. A condition called "wool block" is common in Angora rabbits and should be treated quickly. Sometimes they are shorn in the summer as the long fur can cause the rabbits to overheat.
Angora goat.
The Angora goat () is a breed of domestic goat that originated in Ankara and its surrounding region in central Anatolia.
This breed was first mentioned in the time of Moses, roughly in 1500 BC. The first Angora goats were brought to Europe by Charles V, Holy Roman Emperor, about 1554, but, like later imports, were not very successful. Angora goats were first introduced in the United States in 1849 by Dr. James P. Davis. Seven adult goats were a gift from Sultan Abdülmecid I in appreciation for his services and advice on the raising of cotton.
The fleece taken from an Angora goat is called mohair. A single goat produces between five and eight kilograms of hair per year. Angoras are shorn twice a year, unlike sheep, which are shorn only once. Angoras have high nutritional requirements due to their rapid hair growth. A poor quality diet will curtail mohair development. The United States, Turkey, and South Africa are the top producers of mohair.
For a long period of time, Angora goats were bred for their white coat. In 1998, the Colored Angora Goat Breeders Association was set up to promote breeding of colored Angoras. Now Angora goats produce white, black (deep black to greys and silver), red (the color fades significantly as the goat gets older), and brownish fiber.
Angora goats were depicted on the reverse of the Turkish 50 lira banknotes of 1938-1952.
---END.OF.DOCUMENT---

Arabic language.
Arabic (', () or ') is a Central Semitic language, thus related to and classified alongside other Semitic languages such as Hebrew and the Neo-Aramaic languages. In terms of speakers, Arabic is the largest member of the Semitic language family. It is spoken by more than 280 million people as a first language, most of whom live in the Middle East and North Africa, and by 250 million more as a second language. Arabic has many different, geographically-distributed spoken varieties, some of which are mutually unintelligible. Modern Standard Arabic is widely taught in schools, universities, and used in workplaces, government and the media.
Modern Standard Arabic derives from Classical Arabic, the only surviving member of the Old North Arabian dialect group, attested in Pre-Islamic Arabic inscriptions dating back to the 4th century. Classical Arabic has also been a literary language and the liturgical language of Islam since its inception in the 7th century.
Arabic has lent many words to other languages of the Islamic world. During the Middle Ages, Arabic was a major vehicle of culture in Europe, especially in science, mathematics and philosophy. As a result, many European languages have also borrowed many words from it. Arabic influence is seen in Mediterranean languages, particularly Spanish, Portuguese, and Sicilian, owing to both the proximity of European and Arab civilizations and 700 years of Islamic rule in the Iberian peninsula (see Al-Andalus).
Arabic has also borrowed words from many languages, including Hebrew, Persian and Syriac in early centuries, Turkish in medieval times and contemporary European languages in modern times.
Classical, Modern Standard, and colloquial Arabic.
"Arabic" usually designates one of three main variants: Classical Arabic; Modern Standard Arabic; "colloquial" or "dialectal" Arabic.
Classical Arabic (فصحى "fuṣḥā") is the language found in the Qur'an and used from the period of Pre-Islamic Arabia to that of the Abbasid Caliphate. Classical Arabic is considered normative; modern authors attempt to follow the syntactic and grammatical norms laid down by classical grammarians (such as Sibawayh), and use the vocabulary defined in classical dictionaries (such as the Lisān al-Arab).
Based on Classical Arabic, Modern Standard Arabic (فصحى "fuṣḥā") is the literary language used in most current, printed Arabic publications, spoken by the Arabic media across North Africa and the Middle East, and understood by most educated Arabic speakers. "Literary Arabic" and "Standard Arabic" are less strictly defined terms that may refer to Modern Standard Arabic and/or Classical Arabic.
"Colloquial" or "dialectal" Arabic refers to the many national or regional varieties which constitute the everyday spoken language. Colloquial Arabic has many different regional variants; these sometimes differ enough to be mutually unintelligible and some linguists consider them distinct languages. The varieties are typically unwritten. They are often used in informal spoken media, such as soap operas and talk shows, as well as occasionally in certain forms of written media, such as poetry and printed advertising. The only variety of modern Arabic to have acquired official language status is Maltese, spoken in (predominately Roman Catholic) Malta and written with the Latin alphabet. It is descended from Classical Arabic through Siculo-Arabic and is not mutually intelligible with other varieties of Arabic. Most linguists list it as a separate language rather than as a dialect of Arabic.
The sociolinguistic situation of Arabic in modern times provides a prime example of the linguistic phenomenon of diglossia, which is the normal use of two separate varieties of the same language, usually in different social situations. In the case of Arabic, educated Arabs of any nationality can be assumed to speak both their local dialect and their school-taught Standard Arabic. When educated Arabs of different dialects engage in conversation (for example, a Moroccan speaking with a Lebanese), many speakers code-switch back and forth between the dialectal and standard varieties of the language, sometimes even within the same sentence. Arabic speakers often improve their familiarity with other dialects via music or film.
Like other languages, Modern Standard Arabic continues to evolve. Many modern terms have entered into common usage, in some cases taken from other languages (for example, فيلم "film") or coined from existing lexical resources (for example, هاتف "hātif" "telephone" < "caller"). Structural influence from foreign languages or from the colloquial varieties has also affected Modern Standard Arabic. For example, texts in Modern Standard Arabic sometimes use the format "A, B, C, and D" when listing things, whereas Classical Arabic prefers "A and B and C and D", and subject-initial sentences may be more common in Modern Standard Arabic than in Classical Arabic. For these reasons, Modern Standard Arabic is generally treated separately in non-Arab sources.
Influence of Arabic on other languages.
The influence of Arabic has been most important in Islamic countries. Arabic is a major source of vocabulary for languages such as Amharic, Baluchi, Bengali, Berber, Catalan, Cypriot Greek, Gujarati, Hindustani, Indonesian, Kurdish, Malay, Marathi, Pashto, Persian, Portuguese, Punjabi, Rohingya, Sindhi, Spanish, Swahili, Tagalog, Turkish and Urdu as well as other languages in countries where these languages are spoken. For example, the Arabic word for "book" (/kitāb/) has been borrowed in all the languages listed, with the exception of Spanish, Catalan and Portuguese which use the Latin-derived words "libro","llibre" and "livro", respectively, and Tagalog which uses "aklat". In addition, English has quite a few Arabic loan words, some directly but most through the medium of other Mediterranean languages. Other languages such as Maltese and Kinubi derive from Arabic, rather than merely borrowing vocabulary or grammar rules.
The terms borrowed range from religious terminology (like Berber "prayer" < salat), academic terms (like Uyghur "mentiq" "logic"), economic items (like English "sugar") to placeholders (like Spanish "fulano" "so-and-so") and everyday conjunctions (like Hindustani "lekin" "but", or Spanish "hasta" "until"). Most Berber varieties (such as Kabyle), along with Swahili, borrow some numbers from Arabic. Most Islamic religious terms are direct borrowings from Arabic, such as "salat" 'prayer' and "imam" 'prayer leader.' In languages not directly in contact with the Arab world, Arabic loanwords are often transferred indirectly via other languages rather than being transferred directly from Arabic. For example, most Arabic loanwords in Hindustani entered through Persian, and many older Arabic loanwords in Hausa were borrowed from Kanuri.
Some words in English and other European languages are derived from Arabic, often through other European languages, especially Spanish and Italian. Among them are commonly-used words like "sugar" ("sukkar"), "cotton" (') and "magazine" ("makhzen"). English words more recognizably of Arabic origin include "algebra", "alcohol", "alchemy", "alkali", "zenith" and "nadir". Some words in common use, such as "intention" and "information", were originally calques of Arabic philosophical terms.
Arabic words also made their way into several West African languages as Islam spread across the Sahara. Variants of Arabic words such as "kitaab" (book) have spread to the languages of African groups who had no direct contact with Arab traders.
Arabic was influenced by other languages as well. The most important sources of borrowings into (pre-Islamic) Arabic are Aramaic, which used to be the principal, international language of communication throughout the ancient Near and Middle East, Ethiopic, and to a lesser degree Hebrew (mainly religious concepts).
As Arabic occupied a position similar to Latin (in Europe) throughout the Islamic world many of the Arabic concepts in the field of science, philosophy, commerce etc., were often coined by non-native Arabic speakers, notably by Aramaic and Persian translators. This process of using Arabic roots in notably Turkish and Persian, to translate foreign concepts continued right until the 18th and 19th century, when large swaths of Arab-inhabited lands were under Ottoman rule.
Arabic and Islam.
Arabic is the language of the Qur'an. Arabic is often associated with Islam, but it is also spoken by Arab Christians, Mizrahi Jews and Iraqi Mandaeans.
Most of the world's Muslims do not speak Arabic as their native language but many can read the script and recite the words of religious texts. Some Muslims consider the Arabic language to be "the language chosen by God to speak to mankind" and the original revealed language spoken by man from which all other languages were derived having been corrupted. It is most notably understood by Muslims as being the lingua franca of the afterlife.
History.
The earliest surviving texts in Proto-Arabic, or Ancient North Arabian, are the Hasaean inscriptions of eastern Saudi Arabia, from the 8th century BC, written not in the modern Arabic alphabet, nor in its Nabataean ancestor, but in variants of the epigraphic South Arabian "musnad". These are followed by 6th-century BC Lihyanite texts from southeastern Saudi Arabia and the Thamudic texts found throughout Arabia and the Sinai, and not actually connected with Thamud. Later come the Safaitic inscriptions beginning in the 1st century BC, and the many Arabic personal names attested in Nabataean inscriptions (which are, however, written in Aramaic). From about the 2nd century BC, a few inscriptions from Qaryat al-Faw (near Sulayyil) reveal a dialect which is no longer considered "Proto-Arabic", but Pre-Classical Arabic.
By the fourth century AD, the Arab kingdoms of the Lakhmids in southern Iraq, the Ghassanids in southern Syria the Kindite Kingdom emerged in Central Arabia. Their courts were responsible for some notable examples of pre-Islamic Arabic poetry, and for some of the few surviving pre-Islamic Arabic inscriptions in the Arabic alphabet.
Dialects and descendants.
"Colloquial Arabic" is a collective term for the spoken varieties of Arabic used throughout the Arab world, which differ radically from the literary language. The main dialectal division is between the North African dialects and those of the Middle East, followed by that between sedentary dialects and the much more conservative Bedouin dialects. Speakers of some of these dialects are unable to converse with speakers of another dialect of Arabic. In particular, while Middle Easterners can generally understand one another, they often have trouble understanding North Africans (although the converse is not true, in part due to the popularity of Middle Eastern—especially Egyptian—films and other media).
One factor in the differentiation of the dialects is influence from the languages previously spoken in the areas, which have typically provided a significant number of new words, and have sometimes also influenced pronunciation or word order; however, a much more significant factor for most dialects is, as among Romance languages, retention (or change of meaning) of different classical forms. Thus Iraqi "aku", Levantine "fīh", and North African "kayən" all mean "there is", and all come from Classical Arabic forms ("yakūn", "fīhi", "kā'in" respectively), but now sound very different.
Sounds.
The phonemes below reflect the pronunciation of Modern Standard Arabic. There are minor variations from country to country. Additionally, these dialects can vary from region to region within a country.
Vowels.
Modern Standard Arabic has three vowels, with long and short forms of, and. There are also two diphthongs: and.
Consonants.
See Arabic alphabet for explanations on the IPA phonetic symbols found in this chart.
Arabic has consonants traditionally termed "emphatic", which exhibit simultaneous pharyngealization as well as varying degrees of velarization. This simultaneous articulation is described as "Retracted Tongue Root" by phonologists. In some transcription systems, emphasis is shown by capitalizing the letter, for example, is written ‹D›; in others the letter is underlined or has a dot below it, for example.
Vowels and consonants can be phonologically short or long. Long (geminate) consonants are normally written doubled in Latin transcription (i.e. bb, dd, etc.), reflecting the presence of the Arabic diacritic mark shaddah, which indicates doubled consonants. In actual pronunciation, doubled consonants are held twice as long as short consonants. This consonant lengthening is phonemically contrastive: "qabala" "he accepted" vs. "qabbala" "he kissed."
Syllable structure.
Arabic has two kinds of syllables: open syllables (CV) and (CVV)—and closed syllables (CVC), (CVVC), and (CVCC), the latter two, which are (CVVC) and (CVCC) occurring only at the end of the sentence. Every syllable begins with a consonant. Syllables cannot begin with a vowel. Arabic phonology recognizes the glottal stop as an independent consonant, so in cases where a word begins with a vowel sound, as the definite article "al", for example, the word is recognized in Arabic as beginning with the consonant (glottal stop). When a word ends in a vowel and the following word begins with a glottal stop, then the glottal stop and the initial vowel of the word are in some cases elided, and the following consonant closes the final syllable of the preceding word, for example, "baytu al-mudi:r" "house (of) the director," which becomes.
Stress.
For example: "ki-TAA-bun" "book", "KAA-ti-bun" "writer", "MAK-ta-bun" "desk", "ma-KAA-ti-bu" "desks", "mak-TA-ba-tun" "library", "KA-ta-buu" (Modern Standard Arabic) "they wrote" = "KA-ta-bu" (dialect), "ka-ta-BUU-hu" (Modern Standard Arabic) "they wrote it" = "ka-ta-BUU" (dialect), "ka-TA-ba-taa" (Modern Standard Arabic) "they (dual, fem) wrote", "ka-TAB-tu" (Modern Standard Arabic) "I wrote" = "ka-TABT" (dialect). Doubled consonants count as two consonants: "ma-JAL-la" "magazine", "ma-HALL" "place".
Some dialects have different stress rules. In the Cairo (Egyptian Arabic) dialect, for example, a heavy syllable may not carry stress more than two syllables from the end of a word, hence "mad-RA-sa" "school", "qaa-HI-ra" "Cairo". In the Arabic of Sana, stress is often retracted: "BAY-tayn" "two houses", "MAA-sat-hum" "their table", "ma-KAA-tiib" "desks", "ZAA-rat-hiin" "sometimes", "mad-RA-sat-hum" "their school". (In this dialect, only syllables with long vowels or diphthongs are considered heavy; in a two-syllable word, the final syllable can be stressed only if the preceding syllable is light; and in longer words, the final syllable cannot be stressed.)
Dialectal variations.
In some dialects, there may be more or fewer phonemes than those listed in the chart above. For example, non-Arabic is used in the Maghrebi dialects as well in the written language mostly for foreign names. Semitic became extremely early on in Arabic before it was written down; a few modern Arabic dialects, such as Iraqi (influenced by Persian and Turkish) distinguish between and.
Interdental fricatives (and) are rendered as stops and in some dialects (such as Egyptian, Levantine, and much of the Maghreb); some of these dialects render them as and in "learned" words from the Standard language. Early in the expansion of Arabic, the separate emphatic phonemes and coallesced into a single phoneme, becoming one or the other. "Predictably, dialects without interdental fricatives use exclusively, while dialects with such fricatives use." Again, in "learned" words from the Standard language, is rendered as (in Egypt & the Levant) or (in North Africa) in dialects without interdental fricatives.
Grammar.
Compared with other Semitic language systems, Classical Arabic is distinguished by, "its almost (too perfect) algebraic-looking grammar, i.e. root pattern and morphology." Nouns in Literary Arabic have three grammatical cases (nominative, accusative, and genitive [also used when the noun is governed by a preposition]); three numbers (singular, dual and plural); two genders (masculine and feminine); and three "states" (indefinite, definite, and construct). The cases of singular nouns (other than those that end in long ā) are indicated by suffixed short vowels (/-u/ for nominative, /-a/ for accusative, /-i/ for genitive). The feminine singular is often marked by /-at/, which is reduced to /-ah/ or /-a/ before a pause. Plural is indicated either through endings (the sound plural) or internal modification (the broken plural). Definite nouns include all proper nouns, all nouns in "construct state" and all nouns which are prefixed by the definite article /al-/. Indefinite singular nouns (other than those that end in long ā) add a final /-n/ to the case-marking vowels, giving /-un/, /-an/ or /-in/ (which is also referred to as nunation or tanwīn).
Verbs in Literary Arabic are marked for person (first, second, or third), gender, and number. They are conjugated in two major paradigms (termed perfective and imperfective, or past and non-past); two voices (active and passive); and five moods in the imperfective (indicative, imperative, subjunctive, jussive and energetic). There are also two participles (active and passive) and a verbal noun, but no infinitive. As indicated by the differing terms for the two tense systems, there is some disagreement over whether the distinction between the two systems should be most accurately characterized as tense, aspect or a combination of the two. The perfective aspect is constructed using fused suffixes that combine person, number and gender in a single morpheme, while the imperfective aspect is constructed using a combination of prefixes (primarily encoding person) and suffixes (primarily encoding gender and number). The moods other than imperative are primarily marked by suffixes (/u/ for indicative, /a/ for subjunctive, no ending for jussive, /an/ for energetic). The imperative has the endings of the jussive but lacks any prefixes. The passive is marked through internal vowel changes. Plural forms for the verb are only used when the subject is not mentioned, or precedes it, and the feminine singular is used for all non-human plurals.
Adjectives in Literary Arabic are marked for case, number, gender and state, as for nouns. However, the plural of all non-human nouns is always combined with a singular feminine adjective, which takes the /-ah/ or /-at/ suffix.
Pronouns in Literary Arabic are marked for person, number and gender. There are two varieties, independent pronouns and enclitics. Enclitic pronouns are attached to the end of a verb, noun or preposition and indicate verbal and prepositional objects or possession of nouns. The first-person singular pronoun has a different enclitic form used for verbs (/-ni/) and for nouns or prepositions (/-ī/ after consonants, /-ya/ after vowels).
Nouns, verbs, pronouns and adjectives agree with each other in all respects. However, non-human plural nouns are grammatically considered to be feminine singular. Furthermore, a verb in a verb-initial sentence is marked as singular regardless of its semantic number when the subject of the verb is explicitly mentioned as a noun. Numerals between three and ten show "chiasmic" agreement, in that grammatically masculine numerals have feminine marking and vice versa.
The spoken dialects have lost the case distinctions and make only limited use of the dual (it occurs only on nouns and its use is no longer required in all circumstances). They have lost the mood distinctions other than imperative, but many have since gained new moods through the use of prefixes (most often /bi-/ for indicative vs. unmarked subjunctive). They have also mostly lost the indefinite "nunation" and the internal passive. Modern Standard Arabic maintains the grammatical distinctions of Literary Arabic except that the energetic mood is almost never used; in addition, Modern Standard Arabic sometimes drop the final short vowels that indicate case and mood.
As in many other Semitic languages, Arabic verb formation is based on a (usually) triconsonantal root, which is not a word in itself but contains the semantic core. The consonants, for example, indicate "write", indicate "read", indicate "eat", etc. Words are formed by supplying the root with a vowel structure and with affixes. (Traditionally, Arabic grammarians have used the root, "do", as a template to discuss word formation.) From any particular root, up to fifteen different verbs can be formed, each with its own template; these are referred to by Western scholars as "form I", "form II", and so on through "form XV". These forms, and their associated participles and verbal nouns, are the primary means of forming vocabulary in Arabic. Forms XI to XV are incidental.
Writing system .
The Arabic alphabet derives from the Aramaic script through Nabatean, to which it bears a loose resemblance like that of Coptic or Cyrillic script to Greek script. Traditionally, there were several differences between the Western (North African) and Middle Eastern version of the alphabet—in particular, the "fa" and "qaf" had a dot underneath and a single dot above respectively in the Maghreb, and the order of the letters was slightly different (at least when they were used as numerals). However, the old Maghrebi variant has been abandoned except for calligraphic purposes in the Maghreb itself, and remains in use mainly in the Quranic schools (zaouias) of West Africa. Arabic, like all other Semitic languages (except for the Latin-written Maltese, and the languages with the Ge'ez script), is written from right to left. There are several styles of script, notably Naskh which is used in print and by computers, and Ruq'ah which is commonly used in handwriting.
Calligraphy.
After Khalil ibn Ahmad al Farahidi finally fixed the Arabic script around 786, many styles were developed, both for the writing down of the Qur'an and other books, and for inscriptions on monuments as decoration.
Arabic calligraphy has not fallen out of use as calligraphy has in the Western world, and is still considered by Arabs as a major art form; calligraphers are held in great esteem. Being cursive by nature, unlike the Latin alphabet, Arabic script is used to write down a verse of the Qur'an, a Hadith, or simply a proverb, in a spectacular composition. The composition is often abstract, but sometimes the writing is shaped into an actual form such as that of an animal. One of the current masters of the genre is Hassan Massoudy
Transliteration.
There are a number of different standards of Arabic transliteration: methods of accurately and efficiently representing Arabic with the Latin alphabet. There are multiple conflicting motivations for transliteration. Scholarly systems are intended to accurately and unambiguously represent the phonemes of Arabic, generally making the phonetics more explicit than the original word in the Arabic alphabet. These systems are heavily reliant on diacritical marks such as "š" for the sound equivalently written "sh" in English. In some cases, the "sh" or "kh" sounds can be represented by italicizing or underlining them that way, they can be distinguished from separate "s" and "h" sounds or "k" and "h" sounds, respectively. (Compare "gashouse" to "gash".) At first sight, this may be difficult to recognize. Less scientific systems often use digraphs (like "sh" and "kh"), which are usually more simple to read, but sacrifice the definiteness of the scientific systems. Such systems may be intended to help readers who are neither Arabic speakers nor linguists to intuitively pronounce Arabic names and phrases. An example of such a system is the Bahá'í orthography. A third type of transliteration seeks to represent an equivalent of the Arabic spelling with Latin letters, for use by Arabic speakers when Arabic writing is not available (for example, when using an ASCII communication device).
An example is the system used by the US military, Standard Arabic Technical Transliteration System or SATTS, which represents each Arabic letter with a unique symbol in the ASCII range to provide a one-to-one mapping from Arabic to ASCII and back. This system, while facilitating typing on English keyboards, presents its own ambiguities and disadvantages. During the last few decades and especially since the 1990s, Western-invented text communication technologies have become prevalent in the Arab world, such as personal computers, the World Wide Web, email, Bulletin board systems, IRC, instant messaging and mobile phone text messaging. Most of these technologies originally had the ability to communicate using the Latin alphabet only, and some of them still do not have the Arabic alphabet as an optional feature. As a result, Arabic speaking users communicated in these technologies by transliterating the Arabic text using the Latin script, sometimes known as IM Arabic.
To handle those Arabic letters that cannot be accurately represented using the Latin script, numerals and other characters were appropriated. For example, the numeral "3" may be used to represent the Arabic letter "ع", "ayn". There is no universal name for this type of transliteration, but some have named it Arabic Chat Alphabet. Other systems of transliteration exist, such as using dots or capitalization to represent the "emphatic" counterparts of certain consonants. For instance, using capitalization, the letter "د", or "daal", may be represented by d. Its emphatic counterpart, "ض", may be written as D.
Numerals.
In most of present-day North Africa, the Western Arabic numerals (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) are used. However in Egypt and Arabic-speaking countries to the east of it, the Eastern Arabic numerals () are in use. When representing a number in Arabic, the lowest-valued position is placed on the right, so the order of positions is the same as in left-to-right scripts. Sequences of digits such as telephone numbers are read from left to right, but numbers are spoken in the traditional Arabic fashion, with units and tens reversed from the modern English usage. For example, 24 is said "four and twenty", and 1975 is said "one thousand and nine hundred and five and seventy."
Language-standards regulators.
Academy of the Arabic Language is the name of a number of language-regulation bodies formed in Arab countries. The most active are in Damascus and Cairo. They review language development, monitor new words and approve inclusion of new words into their published standard dictionaries. They also publish old and historical Arabic manuscripts.
Studying Arabic.
Because the Quran is written in Arabic and all Islamic terms are in Arabic, millions of Muslims (both Arab and non-Arab) study the language. Arabic has been taught worldwide in many elementary and secondary schools, especially Muslim schools. Universities around the world have classes that teach Arabic as part of their foreign languages, Middle Eastern studies, and religious studies courses. Arabic language schools exist to assist students in learning Arabic outside of the academic world. Many Arabic language schools are located in the Arab world and other Muslim countries. Software and books with tapes are also important part of Arabic learning, as many of Arabic learners may live in places where there are no academic or Arabic language school classes available. Radio series of Arabic language classes are also provided from some radio stations. A number of websites on the Internet provide online classes for all levels as a means of distance education.
---END.OF.DOCUMENT---

Alfred Hitchcock.
Sir Alfred Joseph Hitchcock, KBE (13 August 1899 – 29 April 1980) was an English filmmaker and producer who pioneered many techniques in the suspense and psychological thriller genres. After a successful career in his native United Kingdom in both silent films and early talkies, Hitchcock moved to Hollywood. In 1956 he became an American citizen while retaining his British citizenship.
Hitchcock directed more than fifty feature films in a career spanning six decades. Often regarded as the greatest British filmmaker, he came first in a 2007 poll of film critics in Britain's "Daily Telegraph" newspaper, which said: "Unquestionably the greatest filmmaker to emerge from these islands, Hitchcock did more than any director to shape modern cinema, which would be utterly different without him. His flair was for narrative, cruelly withholding crucial information (from his characters and from us) and engaging the emotions of the audience like no one else."
Early life.
Hitchcock was born on 13 August 1899 in Leytonstone, London, the second son and youngest of three children of William Hitchcock (1862–1914), a greengrocer and poulterer, and Emma Jane Hitchcock ("née" Whelan; 1863–1942). He was named after his father's brother, Alfred. His family was mostly Roman Catholic, with his mother and paternal grandmother being of Irish extraction. Hitchcock was sent to the Jesuit Classic school St Ignatius' College in Stamford Hill, London. He often described his childhood as being very lonely and sheltered, a situation compounded by his obesity.
Hitchcock said he was sent by his father on numerous occasions to the local police station with a note asking the officer to lock him away for ten minutes as punishment for behaving badly. This idea of being harshly treated or wrongfully accused is frequently reflected in Hitchcock's films.
Hitchcock's mother would often make him address her while standing at the foot of her bed, especially if he behaved badly, forcing him to stand there for hours. These experiences would later be used for the portrayal of the character of Norman Bates in his movie "Psycho".
Hitchcock's father died when he was 14. In the same year, Hitchcock left St. Ignatius to study at the London County Council School of Engineering and Navigation in Poplar, London. After graduating, he became a draftsman and advertising designer with a cable company.
During this period, Hitchcock became intrigued by photography and started working in film production in London, working as a title-card designer for the London branch of what would become Paramount Pictures. In 1920, he received a full-time position at Islington Studios with its American owner, Famous Players-Lasky and their British successor, Gainsborough Pictures, designing the titles for silent movies. His rise from title designer to film director took five years.
Pre-war British career.
Hitchcock's last collaboration with Graham Cutts led him to Germany in 1924. The film "Die Prinzessin und der Geiger" (UK title "The Blackguard", 1925), directed by Cutts and co-written by Hitchcock, was produced in the Babelsberg Studios in Potsdam near Berlin. Hitchcock also worked as an art-director on the set of F. W. Murnau's film "Der letzte Mann" (1924). He was very impressed with Murnau's work and later used many techniques for the set design in his own productions. In his book-length interview with François Truffaut, "Hitchcock/Truffaut" (Simon and Schuster, 1967), Hitchcock also said he was influenced by Fritz Lang's film "Destiny" (1921).
Hitchcock's first few films faced a string of bad luck. His first directing project came in 1922 with the aptly-titled "Number 13". However, the production was canceled due to financial problems and the few scenes that were finished at that point were apparently lost. In 1925, Michael Balcon of Gainsborough Pictures gave Hitchcock another opportunity for a directing credit with "The Pleasure Garden" made at UFA Studios in Germany. Unfortunately, The film was a commercial flop. Next, Hitchcock directed a drama called "The Mountain Eagle" (released under the title "Fear o' God" in the United States). This film was also eventually lost. In 1926, Hitchcock's luck changed with his first thriller, '. The film, released in January 1927, was a major commercial and critical success in the United Kingdom. As with many of his earlier works, this film was influenced by Expressionist techniques Hitchcock had witnessed first-hand in Germany. Some commentators regard this piece as the first truly "Hitchcockian" film, incorporating such themes as the "wrong man".
Following the success of "The Lodger", Hitchcock hired a publicist to help enhance his growing reputation. On 2 December 1926, Hitchcock married his assistant director, Alma Reville at the Brompton Oratory. Their only child, a daughter Patricia, was born on 7 July 1928. Alma was to become Hitchcock's closest collaborator. She wrote some of his screenplays and (though often uncredited) worked with him on every one of his films.
In 1929, Hitchcock began work on his tenth film "Blackmail". While the film was still in production, the studio, British International Pictures (BIP), decided to make it one of the UK's first sound pictures. With the climax of the film taking place on the dome of the British Museum, "Blackmail" began the Hitchcock tradition of using famous landmarks as a backdrop for suspense sequences. In the PBS series "The Men Who Made The Movies", Hitchcock had explained how he used early sound recording as a special element of the film, emphasizing the word "knife" in a conversation with the woman suspected of murder. During this period, Hitchcock directed segments for a BIP musical film revue "Elstree Calling" (1930) and directed a short film featuring two "Film Weekly" scholarship winners, "An Elastic Affair" (1930). Another BIP musical revue, "Harmony Heaven" (1929), reportedly had minor input from Hitchcock, but his name does not appear in the credits.
In 1933, Hitchcock was once again working for Michael Balcon at Gaumont-British Picture Corporation. His first film for the company, "The Man Who Knew Too Much" (1934), was a success and his second, "The 39 Steps" (1935), is often considered one of the best films from his early period. This film was also one of the first to introduce the concept of the "MacGuffin", a plot device around which a whole story seems to revolve, but ultimately has nothing to do with the true meaning or ending of the story. In "The 39 Steps", the Macguffin is a stolen set of design plans. (Hitchcock told French director François Truffaut: "There are two men sitting in a train going to Scotland and one man says to the other, 'Excuse me, sir, but what is that strange parcel you have on the luggage rack above you?', 'Oh', says the other, 'that's a Macguffin.', 'Well', says the first man, 'what's a Macguffin?', The other answers, 'It's an apparatus for trapping lions in the Scottish Highlands.', 'But', says the first man, 'there are no lions in the Scottish Highlands.', 'Well', says the other, 'then that's no Macguffin.'")
Hitchcock's next major success was in 1938 with his film "The Lady Vanishes", a clever and fast-paced film about the search for a kindly old Englishwoman Miss Froi (Dame May Whitty), who disappears while on board a train in the fictional country of Vandrika (a thinly-veiled version of Nazi Germany).
By 1938, Hitchcock had become known for his observation, "Actors are cattle". He once said that he first made this remark as early as the late 1920s, in connection to stage actors who were snobbish about motion pictures. However, Michael Redgrave said that Hitchcock had made the statement during the filming of "The Lady Vanishes". The phrase would haunt Hitchcock for years to come and would result in an incident during the filming of his 1941 production of "Mr. & Mrs. Smith" where Carole Lombard brought some heifers onto the set — with name tags of Lombard, Robert Montgomery, and Gene Raymond, the stars of the film — to surprise the director. Hitchcock said he was misquoted: "I said 'Actors should be "treated" like cattle'."
At the end of the 1930s, David O. Selznick signed Hitchcock to a seven-year contract beginning in March 1939, when the Hitchcocks moved to the United States.
Hollywood.
The suspense and the gallows humor that had become Hitchcock's trademark in film continued to appear in his productions. The working arrangements with Selznick were less than optimal. Selznick suffered from perennial money problems, and Hitchcock was often displeased with Selznick's creative control over his films. In a later interview, Hitchcock summarised the working relationship thus:\
Selznick loaned Hitchcock to the larger studios more often than producing Hitchcock's films himself. In addition, Selznick, as well as fellow independent producer Samuel Goldwyn, made only a few films each year, so Selznick did not always have projects for Hitchcock to direct. Goldwyn had also negotiated with Hitchcock on a possible contract, only to be outbid by Selznick. Hitchcock was quickly impressed with the superior resources of the American studios compared to the financial restrictions he had frequently encountered in England.
Hitchcock's fondness for his homeland resulted in numerous American films set in, or filmed in, the United Kingdom, including his penultimate film, "Frenzy".
With the prestigious Selznick picture "Rebecca" in 1940, Hitchcock made his first American movie, set in England and based on a novel by English author Daphne du Maurier. The film starred Laurence Olivier and Joan Fontaine. This Gothic melodrama explores the fears of a naive young bride who enters a great English country home and must adapt to the extreme formality and coldness she finds there. The film won the Academy Award for Best Picture of 1940. The statuette was given to Selznick, as the film's producer. The film did not win the Best Director award for Hitchcock.
There were additional problems between Selznick and Hitchcock. Selznick was known to impose very restrictive rules upon Hitchcock who was forced to shoot the film as Selznick wanted. At the same time, Selznick complained about Hitchcock's "goddamn jigsaw cutting", which meant that the producer did not have nearly the leeway to create his own film as he liked, but had to follow Hitchcock's vision of the finished product. The film was the fourth longest of Hitchcock's films, at 130 minutes, exceeded only by "The Paradine Case" (132 minutes), "North by Northwest" (136 minutes), and "Topaz" (142 minutes).
Hitchcock's second American film, the European-set thriller "Foreign Correspondent" (1940), based on Vincent Sheean's "Personal History" and produced by Walter Wanger, was nominated for Best Picture that year. The movie was filmed in the first year of World War II and was apparently inspired by the rapidly-changing events in Europe, as fictionally covered by an American newspaper reporter portrayed by Joel McCrea. The film mixed actual footage of European scenes and scenes filmed on a Hollywood back lot. In compliance with Hollywood's Production Code censorship, the film avoided direct references to Germany and Germans.
1940s films.
Hitchcock's films during the 1940s were diverse, ranging from the romantic comedy "Mr. & Mrs. Smith" (1941) to the courtroom drama "The Paradine Case" (1947), to the dark and disturbing film noir "Shadow of a Doubt" (1943).
In September 1940, the Hitchcocks purchased the Cornwall Ranch, located near Scotts Valley in the Santa Cruz Mountains in northern California. The Ranch became the primary residence of the Hitchcocks for the rest of their lives, although they kept their Bel Air home. "Suspicion" (1941) marked Hitchcock's first film as a producer as well as director. Hitchcock used the north coast of Santa Cruz, California for the English coastline sequence. This film was to be actor Cary Grant's first time working with Hitchcock, and it was one of the few times that Grant would be cast in a sinister role. Joan Fontaine won Best Actress Oscar and the New York Film Critics Circle Award for her "outstanding performance in "Suspicion". "Grant plays an irresponsible husband whose actions raise suspicion and anxiety by his wife (Fontaine)". In what critics regard as a classic scene, Hitchcock uses a light bulb to illuminate what might be a fatal glass of milk that Grant is bringing to his wife. In the book upon which the movie is based ("Before the Fact" by Francis Iles), the Grant character is a killer, but Hitchcock and the studio felt Grant's image would be tarnished by that ending. Though a homicide would have suited him better, as he stated to François Truffaut, Hitchcock settled for an ambiguous finale.
"Saboteur" (1942) was the first of two films that Hitchcock made for Universal, a studio where he would continue his career during his later years. Hitchcock was forced to use Universal contract players Robert Cummings and Priscilla Lane, both known for their work in comedies and light dramas. Breaking with Hollywood conventions of the time, Hitchcock did extensive location filming, especially in New York City, and depicted a confrontation between a suspected saboteur (Cummings) and a real saboteur (Norman Lloyd) atop the Statue of Liberty.
"Shadow of a Doubt" (1943), Hitchcock's personal favourite of all his films and the second of the early Universal films, was about young Charlotte "Charlie" Newton (Teresa Wright), who suspects her beloved uncle Charlie Oakley (Joseph Cotten) of being a serial murderer. Critics have said that in its use of overlapping characters, dialogue, and closeups it has provided a generation of film theorists with psychoanalytic potential, including Jacques Lacan and Slavoj Žižek. Hitchcock again filmed extensively on location, this time in the Northern California city of Santa Rosa, California, during the summer of 1942. The director showcased his own personal fascination with crime and criminals when he had two of his characters discuss various ways of killing people, to the obvious annoyance of Charlotte.
Working at 20th Century Fox, Hitchcock adapted a script of John Steinbeck's that chronicled the experiences of the survivors of a German U-boat attack in the film "Lifeboat" (1944). The action sequences were shot on the small boat. The locale also posed problems for Hitchcock's traditional cameo appearance. That was solved by having Hitchcock's image appear in a newspaper that William Bendix is reading in the boat, showing the director in a before-and-after advertisement for "Reduco-Obesity Slayer". While at Fox, Hitchcock seriously considered directing the film version of A.J. Cronin's novel about a Catholic priest in China, "The Keys of the Kingdom", but the plans for this fell through. John M. Stahl ended up directing the 1944 film, which was produced by Joseph L. Mankiewicz and starred Gregory Peck, among other luminaries.
Returning to England for an extended visit in late 1943 and early 1944, Hitchcock made two short films for the Ministry of Information, "Bon Voyage" and "Aventure Malgache". These - made for the Free French - were the only films Hitchcock made in the French language, and "feature typical Hitchcockian touches". In the 1990s, the two films were shown by Turner Classic Movies and released on home video.
In 1945, Hitchcock served as "treatment advisor" (in effect, a film editor) for a Holocaust documentary produced by the British Army. The film, which recorded the liberation of Nazi Concentration Camps, remained unreleased until 1985, when it was completed by PBS Frontline and distributed under the title "Memory of the Camps".
Hitchcock worked for Selznick again when he directed "Spellbound", which explored the then-fashionable subject of psychoanalysis and featured a dream sequence designed by Salvador Dalí. Gregory Peck is amnesiac Dr. Anthony Edwardes under the treatment of analyst Dr. Peterson (Ingrid Bergman), who falls in love with him while trying to unlock his repressed past. The dream sequence as it actually appears in the film is considerably shorter than was originally envisioned, which was to be several minutes long, because it proved to be too disturbing for the audience. Some of the original musical score by Miklós Rózsa (which makes use of the theremin) was later adapted by the composer into a concert piano concerto.
"Notorious" (1946) followed "Spellbound". According to Hitchcock, in his book-length interview with François Truffaut, Selznick sold the director, the two stars (Grant and Bergman) and the screenplay (by Ben Hecht) to RKO Radio Pictures as a "package" for $500,000 due to cost overruns on Selznick's "Duel in the Sun" (1946). "Notorious" starred Hitchcock regulars Ingrid Bergman and Cary Grant, and features a plot about Nazis, uranium, and South America. It was a huge box office success and has remained one of Hitchcock's most acclaimed films. His use of uranium as a plot device led to Hitchcock's being briefly under FBI surveillance. McGilligan writes that Hitchcock consulted Dr. Robert Millikan of Caltech about the development of an atomic bomb. Selznick complained that the notion was "science fiction" only to be confronted by the news stories of the detonation of two atomic bombs on Hiroshima and Nagasaki in Japan in August 1945.
After completing his final film for Selznick, "The Paradine Case" (a courtroom drama that critics found lost momentum because it apparently ran too long and exhausted its resource of ideas), Hitchcock filmed his first color film, "Rope", which appeared in 1948. Here Hitchcock experimented with marshalling suspense in a confined environment, as he had done earlier with "Lifeboat" (1943). He also experimented with exceptionally long takes — up to ten minutes long. Featuring James Stewart in the leading role, "Rope" was the first of four films Stewart would make for Hitchcock. It was based on the Leopold and Loeb case of the 1920s. Somehow Hitchcock's cameraman managed to move the bulky, heavy Technicolor camera quickly around the set as it followed the continuous action of the long takes.
"Under Capricorn" (1949), set in nineteenth-century Australia, also used the short-lived technique of long takes, but to a more limited extent. He again used Technicolor in this production, then returned to black and white films for several years. For "Rope" and "Under Capricorn". Hitchcock formed a production company with Sidney Bernstein, called Transatlantic Pictures, which became inactive after these two unsuccessful pictures. Hitchcock continued to produce his own films for the rest of his life.
1950s: Peak years.
In 1950, Hitchcock filmed "Stage Fright" on location in the UK. For the first time, Hitchcock matched one of Warner Brothers' biggest stars, Jane Wyman, with the sultry German actress Marlene Dietrich. Hitchcock used a number of prominent British actors, including Michael Wilding, Richard Todd, and Alastair Sim. This was Hitchcock's first production for Warner Brothers, which had distributed "Rope" and "Under Capricorn", because Transatlantic Pictures was experiencing financial difficulties.
With the film "Strangers on a Train" (1951), based on the novel by Patricia Highsmith, Hitchcock combined many elements from his preceding films. Hitchcock approached Dashiell Hammett to write the dialogue but Raymond Chandler took over, then left over disagreements with the director. Two men casually meet and speculate on removing people who are causing them difficulty. One of the men takes this banter entirely seriously. With Farley Granger reprising some elements of his role from "Rope", "Strangers" continued the director's interest in the narrative possibilities of blackmail and murder". Robert Walker, previously known for "boy-next-door" roles, plays the villain.
MCA head Lew Wasserman, whose client list included James Stewart, Janet Leigh and other actors who would appear in Hitchcock's films, had a significant impact in packaging and marketing Hitchcock's films beginning in the 1950s.
Three very popular films starring Grace Kelly followed. "Dial M for Murder" (1954) was adapted from the popular stage play by Frederick Knott. Ray Milland plays the "suave and scheming" villain, an ex-tennis pro, who tries to murder his innocent wife Grace Kelly for her money. When the murder goes awry and the assassin is killed by her in self-defense, he manipulates the evidence to pin the murder of the assassin on his wife. Her lover Mark Halliday (Robert Cummings) and police inspector Hubbard (John Williams) work urgently to save her from execution. Hitchcock experimented with 3D cinematography, although the film was not released in this format at first. However, it was shown in 3D in the early 1980s. The film marked a return to Technicolor productions for Hitchcock.
Hitchcock moved to Paramount Pictures and filmed "Rear Window" (1954), starring James Stewart and Kelly again, as well as Thelma Ritter and Raymond Burr. Here, the wheelchair-using Stewart, a photographer based on Robert Capa, seems obsessed with observing his neighbours across the courtyard, and becomes convinced one of them (Raymond Burr) has murdered his wife. Stewart tries to sway both his glamorous model-girlfriend (Kelly) and his policeman buddy (Wendell Corey) to his theory, and eventually succeeds.. Like "Lifeboat" and "Rope", the movie was photographed almost entirely within the confines of a small space: Stewart's tiny studio apartment overlooking the massive courtyard set. Hitchcock uses closeups of Stewart's face to show his character's reactions to all he sees, "from the comic voyeurism directed at his neighbors to his helpless terror watching Kelly and Burr in the villain's apartment".
The third Kelly film "To Catch a Thief" (1955), set in the French Riviera, stars Kelly with Cary Grant again and John Williams. Grant plays retired thief John Robie who becomes the prime suspect for a spate of robberies in the Riviera. An American heiress played by Kelly surmises his true identity, attempts to seduce him with her own jewels, and even offers to assist him in his alleged life of crime. "Despite the obvious age disparity between Grant and Kelly and a lightweight plot, the witty script (loaded with double-entendres) and the good-natured acting proved a commercial success." It was Hitchcock's last film with Kelly because she married Prince Rainier of Monaco in 1956 and the residents of her new homeland refused to allow her to make any more films.
The successful remake of Hitchcock's own 1934 film, "The Man Who Knew Too Much", in 1956 followed, this time starring Stewart and Doris Day, who sang the theme song, "Whatever Will Be, Will Be (Que Sera, Sera)" (which won the Oscar for "Best Music", and became a big hit for Day). Stewart and Day, distraught over the kidnapping of their son, struggle with both their emotions and their urgent quest to find their child and stop an assassination, until the song helps re-unite the family.
"The Wrong Man" (1957), Hitchcock's final film for Warner Brothers, was a low-key black and white production based on a real-life case of mistaken identity reported in Life Magazine in 1953. This was the only film of Hitchcock's to star Henry Fonda. Fonda plays a Stork Club musician mistaken for a liquor store thief who is arrested and tried for robbery while his wife (newcomer Vera Miles) emotionally collapses under the strain. Hitchcock told Truffaut that his lifelong fear of the police attracted him to the subject and was embedded in many scenes.
"Vertigo" (1958) again starred Stewart, this time with Kim Novak and Barbara Bel Geddes. Stewart plays "Scottie", a former police investigator suffering from acrophobia, who develops an obsession with a woman he is shadowing (Kim Novak). Scottie's obsession leads to tragedy, and this time Hitchcock does not opt for a happy ending. Though the film is widely considered a classic today, "Vertigo" met with negative reviews and poor box office receipts upon its release, and marked the last collaboration between Stewart and Hitchcock. The film is now placed highly in the "Sight & Sound" decade polls. It was premiered in the San Sebastián International Film Festival, where Hitchcock won a Silver Seashell.
Late 1950s, 1960s and 1970s.
By this time, Hitchcock had filmed in many areas of the United States. He followed "Vertigo" with three more successful films. All are also recognized as among his very best films: "North by Northwest" (1959), "Psycho" (1960) and "The Birds" (1963).
In "North by Northwest", Cary Grant is Roger Thornhill, a Madison Avenue ad executive who is mistaken for a government agent. He is hotly pursued by enemy agents across America who try to kill him, one of whom is foreign agent Eve Kendall (Eva Marie Saint), who is really an American agent. She seduces Thornhill, sets him up, but then falls in love with him and aids his escape.
"Psycho" is considered by some to be Hitchcock's most famous film. Produced on a highly constrained budget of $800,000, it was shot in black-and-white on a spare set. The unprecedented violence of the shower scene, the early demise of the heroine, the innocent lives extinguished by a disturbed murderer were all hallmarks of Hitchcock, copied in many subsequent horror films. After completing "Psycho", Hitchcock moved to Universal, where he made the remainder of his films.
"The Birds", inspired by a Daphne Du Maurier short story and by an actual news story about a mysterious infestation of birds in California, was Hitchcock's 49th film. He signed up Tippi Hedren as his latest blonde heroine opposite Rod Taylor. The scenes of the birds attacking included hundreds of shots mixing actual and animated sequences. The cause of the birds' attack is left unanswered, "perhaps highlighting the mystery of forces unknown".
The latter two films were particularly notable for their unconventional soundtracks, both orchestrated by Bernard Herrmann: the screeching strings played in the murder scene in "Psycho" exceeded the limits of the time, and "The Birds" dispensed completely with conventional instruments, instead using an electronically-produced soundtrack and an unaccompanied song by school children (just prior to the infamous attack at the historic Bodega Bay School). Also notable was that Santa Cruz was mentioned again as the place where the bird-phenomenon was said to have first occurred. These films are considered his last great films, after which it is said his career started to lose pace (although some critics such as Robin Wood and Donald Spoto contend that "Marnie", from 1964, is first-class Hitchcock, and some have argued that "Frenzy" is unfairly overlooked).
Failing health took its toll on Hitchcock, reducing his output during the last two decades of his career. Hitchcock filmed two spy thrillers. The first, "Torn Curtain" (1966), with Paul Newman and Julie Andrews, was a Cold War thriller. "Torn Curtain" displays the bitter end of the twelve-year collaboration between Hitchcock and composer Bernard Herrmann. Herrmann was fired when Hitchcock was unsatisfied with his score, so John Addison was hired in Herrmann's place. In 1969, "Topaz", another Cold War-themed film (based on a Leon Uris novel), was released. Both received mixed reviews from critics.
In 1972, Hitchcock returned to London to film "Frenzy", his last major triumph. After two only moderately successful espionage films, the plot marks a return to the murder thriller genre that he made so many films out of earlier in his career. The basic story recycles his early film "The Lodger". Richard Blaney (Jon Finch), volatile barkeeper with a history of explosive anger, becomes the likely perpetrator of the "Necktie Murders", which are actually committed by his friend Bob Rusk (Barry Foster), a fruit seller. This time Hitchcock makes the victim and villain twins, rather than opposites, as in "Strangers on a Train". Only one of them, however, has crossed the line to murder. For the first time, Hitchcock allowed nudity and profane language, which had before been taboo, in one of his films. He also shows rare sympathy for the Chief Inspector and his comic domestic life. Biographers have noted that Hitchcock had always pushed the limits of film censorship, often managing to fool Joseph Breen, the longtime head of Hollywood's Production Code. Many times Hitchcock slipped in subtle hints of improprieties forbidden by censorship until the mid-1960s. Yet Patrick McGilligan wrote that Breen and others often realized that Hitchcock was inserting such things and were actually amused as well as alarmed by Hitchcock's "inescapable inferences". Beginning with "Torn Curtain", Hitchcock was finally able to blatantly include plot elements previously forbidden in American films and this continued for the remainder of his film career.
"Family Plot" (1976) was Hitchcock's last film. It related the escapades of "Madam" Blanche Tyler played by Barbara Harris, a fraudulent spiritualist, and her taxi driver lover Bruce Dern making a living from her phony powers. William Devane, Karen Black and Cathleen Nesbitt co-starred. It was the only Hitchcock film scored by John Williams.
Last film work and death.
Near the end of his life, Hitchcock had worked on the script for a projected spy thriller, "The Short Night", collaborating with screenwriters James Costigan and Ernest Lehman. Despite some preliminary work, the story was never filmed. This was due, primarily, to Hitchcock's own failing health and his concerns over the health of his wife, Alma, who had suffered a stroke. The script was eventually published posthumously, in a book on Hitchcock's last years.
Hitchcock died from kidney failure in his Bel Air, Los Angeles, California home at the age of 80. His wife Alma Reville, and their daughter, Patricia Hitchcock O'Connell, both survived him. His funeral service was held at Good Shepherd Catholic Church in Beverly Hills. Hitchcock's body was cremated and his ashes were scattered over the Pacific.
Themes, plot devices and motifs.
Hitchcock returned several times to cinematic devices such as suspense, the audience as voyeur, and his well-known "McGuffin", an apparently minor detail serving as a pivot upon which the narrative turns.
Technical innovations.
Hitchcock seemed to delight in the technical challenges of film making. In the film "Lifeboat", Hitchcock stages the entire action of the movie in a small boat, yet manages to keep the cinematography from monotonous repetition (his trademark cameo appearance was a dilemma, given the limitations of the setting; so Hitchcock appears in a fictitious magazine for a weight loss product). Similarly, the entire action in "Rear Window" either takes place in or is seen from a single apartment. In "Spellbound", two unprecedented point-of-view shots were achieved by constructing a large wooden hand (which would appear to belong to the character whose point of view the camera took) and out-sized props for it to hold: a bucket-sized glass of milk and a large wooden gun. For added novelty and impact, the climactic gunshot was hand-colored red on some copies of the black-and-white print of the film.
"Rope" (1948) was another technical challenge: a film that appears to have been shot entirely in a single take. The film was actually shot in 10 takes ranging from four and a half to 10 minutes each; a 10 minute length of film being the maximum a camera's film magazine could hold. Some transitions between reels were hidden by having a dark object fill the entire screen for a moment. Hitchcock used those points to hide the cut, and began the next take with the camera in the same place.
Hitchcock's 1958 film "Vertigo" contains a camera technique developed by Irmin Roberts that has been imitated and re-used many times by filmmakers, wherein the image appears to "stretch". This is achieved by moving the camera in the opposite direction of the camera's zoom. It has become known as the Dolly zoom or "Vertigo Effect."
Signature appearances in his films.
Hitchcock appeared briefly in many of his own films, usually playing upon his portly figure in an incongruous manner, for example, seen struggling to get a double bass onto a train.
Psychology of characters.
Hitchcock's films sometimes feature characters struggling in their relationships with their mothers. In "North by Northwest" (1959), Roger Thornhill (Cary Grant's character) is an innocent man ridiculed by his mother for insisting that shadowy, murderous men are after him. In "The Birds" (1963), the Rod Taylor character, an innocent man, finds his world under attack by vicious birds, and struggles to free himself of a clinging mother (Jessica Tandy). The killer in "Frenzy" (1972) has a loathing of women but idolizes his mother. The villain Bruno in "Strangers on a Train" hates his father, but has an incredibly close relationship with his mother (played by Marion Lorne). Sebastian (Claude Rains) in "Notorious" has a clearly conflictual relationship with his mother, who is (correctly) suspicious of his new bride Alicia Huberman (Ingrid Bergman). And, of course, Norman Bates' troubles with his mother in "Psycho" are well known.
Hitchcock heroines tend to be lovely, cool blondes who seem proper at first but, when aroused by passion or danger, respond in a more sensual, animal, or even criminal way. As noted, the famous victims in "The Lodger" are all blondes. In "The 39 Steps", Hitchcock's glamorous blonde star, Madeleine Carroll, is put in handcuffs. In "Marnie" (1964), the title character (played by Tippi Hedren) is a kleptomaniac. In "To Catch a Thief" (1955), Francie (Grace Kelly) offers to help a man she believes is a burglar. In "Rear Window", Lisa (Grace Kelly again) risks her life by breaking into Lars Thorwald's apartment. The best known example is in "Psycho" where Janet Leigh's unfortunate character steals $40,000 and is murdered by a reclusive psychopath. Hitchcock's last blonde heroine was - years after Dany Robin and her "daughter" Claude Jade in "Topaz" - Barbara Harris as a phony psychic turned amateur sleuth in his final film, 1976's "Family Plot". In the same film, the diamond smuggler played by Karen Black could also fit that role, as she wears a long blonde wig in various scenes and becomes increasingly uncomfortable about her line of work.
Some critics and Hitchcock scholars, including Donald Spoto and Roger Ebert, agree that "Vertigo" represents the director's most personal and revealing film, dealing with the obsessions of a man who crafts a woman into the woman he desires. "Vertigo" explores more frankly and at greater length his interest in the relation between sex and death than any other film in his filmography.
Hitchcock often said that his favorite film (of his own work) was "Shadow of a Doubt".
Storyboards and production.
Hitchcock's films were strongly believed to have been extensively storyboarded to the finest detail by the majority of commentators over the years. He was reported to have never even bothered looking through the viewfinder, since he didn't need to do so, though in publicity photos he was shown doing so. He also used this as an excuse to never have to change his films from his initial vision. If a studio asked him to change a film, he would claim that it was already shot in a single way, and that there were no alternate takes to consider.
However, this view of Hitchcock as a director who relied more on pre-production than on the actual production itself, has been challenged by the book, "Hitchcock At Work", written by Bill Krohn, the American correspondent of "Cahiers du cinéma". Krohn after investigating several script revisions, notes to other production personnel written by or to Hitchcock alongside inspection of storyboards and other production material has observed that Hitchcock's work often deviated from how the screenplay was written or how the film was originally envisioned. He noted that the myth of storyboards in relation to Hitchcock, often regurgitated by generations of commentators on his movies was to a great degree perpetuated by Hitchcock himself or the publicity arm of the studios. A great example would be the celebrated crop spraying sequence of "North by Northwest" which was not storyboarded at all. After the scene was filmed, the publicity department asked Hitchcock to make storyboards to promote the film and Hitchcock in turn hired an artist to match the scenes in detail.
Even on the occasions when storyboards were made, the scene which was shot did differ from it significantly. Krohn's extensive analysis of the production of Hitchcock classics like "Notorious" reveals that Hitchcock was flexible enough to change a film's conception during its production. Another example Krohn notes is the American remake of "The Man Who Knew Too Much" whose shooting schedule commenceed without a finished script and moreover went over schedule, something which as Krohn notes was not an uncommon occurrence on many of Hitchcock's films including "Strangers on a Train" and "Topaz". While Hitchcock did do a great deal of preparation for all his movies, he was fully cognizant that the actual film-making process often deviated from the best laid plans and was flexible to adapt to the changes and needs of production as his films were not free from the normal hassles faced and common routines utilised during many other film productions.
Krohn's work also sheds light on Hitchcock's practice of generally shooting in chronological order. A practice which he notes often sent many of his films over budget and over schedule and more importantly differed from the standard operating procedure of Hollywood in the Studio System Era. Equally important is Hitchcock's tendency of shooting alternate takes of scenes. This differed from coverage in that the films weren't necessarily shot from varying angles so as to give the editor options to shape the film how he/she chooses (often under the producer's aegis). Rather they represented Hitchcock's tendency of giving himself options in the editing room where he would provide advice to his editors after viewing a rough cut of the work so as to give him space for other possibilities in the editing room. According to Krohn, this and numerous other information revealed through his research of Hitchcock's personal papers, script revisions and the like refute the notion of Hitchcock as a director who was always in control of his films, whose vision of his films did not change during production, which Krohn notes has remained the central long-standing myth of Alfred Hitchcock.
His fastidiousness and attention to detail also found its way to each film poster for his films. Hitchcock preferred to work with the best talent of his day—film poster designers such as Bill Gold and Saul Bass -- and kept them busy with countless rounds of revision until he felt that the single image of the poster accurately represented his entire film.
Approach to actors.
Similarly, much of Hitchcock's supposed dislike of actors has been exaggerated. Hitchcock simply did not tolerate the method approach, as he believed that actors should only concentrate on their performances and leave work on script and character to the directors and screenwriters. In a "Sight and Sound" interview, he stated that, 'the method actor is OK in the theatre because he has a free space to move about. But when it comes to cutting the face and what he sees and so forth, there must be some discipline'. During the making of "Lifeboat", Walter Slezak, who played the German character, stated that Hitchcock knew the mechanics of acting better than anyone he knew. Several critics have observed that despite his reputation as a man who disliked actors, several actors who worked with him gave fine, often brilliant performances and these performances contribute to the film's success.
Regarding Hitchcock's sometimes less than pleasant relationship with actors, there was a persistent rumor that he had said that actors were cattle. Hitchcock later denied this, typically tongue-in-cheek, clarifying that he had only said that actors should be treated like cattle. Carole Lombard, tweaking Hitchcock and drumming up a little publicity, brought some cows along with her when she reported to the set of "Mr. and Mrs. Smith". For Hitchcock, the actors, like the props, were part of the film's setting.
In the late 1950s, French New Wave critics, especially Éric Rohmer, Claude Chabrol and François Truffaut, were among the first to see and promote Hitchcock's films as artistic works. Hitchcock was one of the first directors to whom they applied their auteur theory, which stresses the artistic authority of the director in the film-making process.
Hitchcock's innovations and vision have influenced a great number of filmmakers, producers, and actors. His influence helped start a trend for film directors to control artistic aspects of their movies without answering to the movie's producer.
Awards and honours.
"Rebecca", which Hitchcock directed, won the 1940 Best Picture Oscar for its producer David O. Selznick. In addition to "Rebecca" and "Suspicion", two other films Hitchcock directed, "Foreign Correspondent" and "Spellbound", were nominated for Best Picture. Hitchcock is considered the Best Film Director of all time by The Screen Directory website. Sixteen films directed by Hitchcock earned Oscar nominations, though only six of those films earned Hitchcock himself a nomination. The total number of Oscar nominations (including winners) earned by films he directed is fifty. Four of those films earned Best Picture nominations. "Spellbound" won the Academy Award for Best Original Music Score. Actor Joan Fontaine won the Academy Award for Best Actress for her performance in "Suspicion", the only Academy Award–winning performance under Hitchcock's direction.
Six of Hitchcock's films are in the National Film Registry: "Vertigo", "Rear Window", "North by Northwest", "Shadow of a Doubt", "Notorious", and "Psycho"; all but "Shadow of a Doubt" and "Notorious" were also in 1998's AFI's 100 best American films and the AFI's 2007 update. In 2008, four of Hitchcock's films were named among the ten best mystery films of all time in the AFI's 10 Top 10. Those films are "Vertigo" (at No. 1); "Rear Window" (No. 3); "North by Northwest" (No. 7); and "Dial M for Murder" (No. 9).
Alfred Hitchcock received the AFI Life Achievement Award in 1979.
Hitchcock was made a Knight Commander of the Order of the British Empire by Queen Elizabeth II in the 1980 New Year's Honours. Although he had adopted American citizenship in 1956, he was entitled to use the title "Sir" because he had remained a British subject. Hitchcock died just four months later, on 29 April, before he could be formally invested.
Fame.
Hitchcock became famous for his expert and largely unrivaled control of pace and suspense, and his films draw heavily on both fear and fantasy. The films are known for their droll humour and witticisms, and these cinematic works often portray innocent people caught up in circumstances beyond their control or understanding.
Hitchcock began his directing career in the United Kingdom in 1922. From 1939 onward, he worked primarily in the United States. In September, 1940, Hitchcock had purchased a mountaintop estate for the sum of $40,000. Known as the 1870 Cornwall Ranch or 'Heart o' the Mountain', the property was perched high above Scotts Valley, California, at the end of Canham Road. The Hitchcocks resided there from 1940 to 1972. The Hitchcocks became close friends with the parents of Joan Fontaine, after she starred in his film, "Rebecca". Years later, after a break-in at his estate, Hitchcock replaced all of the accumulated paintings with studio-made copies. The family sold the estate in 1974, six years before Hitchcock's death.
Hitchcock and family also purchased a second home in late 1942 at 10957 Bellagio Road in Los Angeles, just across from the Bel Air Country Club.
"Rebecca" was the only Hitchcock film to win the Academy Award for Best Picture (though the award did not go to Hitchcock); four other films were nominated. In 1967 he was awarded the Irving G. Thalberg Memorial Award for lifetime achievement. He never won an Academy Award for direction of a film.
Television and books.
Along with Walt Disney, Hitchcock was among the first prominent motion picture producers to fully envisage just how popular the medium of television would become. From 1955 to 1965, Hitchcock was the host and producer of a television series entitled "Alfred Hitchcock Presents". While his films had made Hitchcock's name strongly associated with suspense, the TV series made Hitchcock a celebrity himself. His irony-tinged voice and signature droll delivery, gallows humor, iconic image and mannerisms became instantly recognizable and were often the subject of parody.
The title-theme of the show pictured a minimalist caricature of Hitchcock's profile (he drew it himself; it is composed of only nine strokes) which his real silhouette then filled. His introductions before the stories in his program always included some sort of wry humor, such as the description of a recent multi-person execution hampered by having only one electric chair, while two are now shown with a sign "Two chairs--no waiting!" He directed a few episodes of the TV series himself, and he upset a number of movie production companies when he insisted on using his TV production crew to produce his motion picture "Psycho". In the late 1980s, a new version of "Alfred Hitchcock Presents" was produced for television, making use of Hitchcock's original introductions in a colorised form.
"Alfred Hitchcock Presents" was parodied by Friz Freleng's 1961 cartoon "The Last Hungry Cat", which contains a plot similar to "Blackmail".
"Hitch" used a curious little tune by the French composer Charles Gounod (1818–1893), the composer of the 1859 opera "Faust", as the theme "song" for his television programs, after it was suggested to him by composer Bernard Herrmann. Arthur Fiedler and the Boston Pops Orchestra included the piece, "Funeral March of a Marionette", in one of their extended play 45 rpm discs for RCA Victor during the 1950s.
Hitchcock appears as a character in the popular juvenile detective book series, "Alfred Hitchcock and the Three Investigators". The long-running detective series was created by Robert Arthur, who wrote the first several books, although other authors took over after he left the series. The Three Investigators—Jupiter Jones, Bob Andrews and Peter Crenshaw—were amateur detectives, slightly younger than the Hardy Boys. In the introduction to each book, "Alfred Hitchcock" introduces the mystery, and he sometimes refers a case to the boys to solve. At the end of each book, the boys report to Hitchcock, and sometimes give him a memento of their case.
When the real Hitchcock died, the fictional Hitchcock in the Three Investigators books was replaced by a retired detective named Hector Sebastian. At this time, the series title was changed from "Alfred Hitchcock and the Three Investigators" to "The Three Investigators".
At the height of Hitchcock's success, he was also asked to introduce a set of books with his name attached. The series was a collection of short stories by popular short-story writers, primarily focused on suspense and thrillers. These titles included "Alfred Hitchcock's Anthology", "Alfred Hitchcock Presents: Stories to be Read with the Door Locked", "Alfred Hitchcock's Monster Museum", "Alfred Hitchcock's Supernatural Tales of Terror and Suspense", "Alfred Hitchcock's Spellbinders in Suspense", "Alfred Hitchcock's Witch's Brew", "Alfred Hitchcock's Ghostly Gallery", "Alfred Hitchcock's A Hangman's Dozen" and "Alfred Hitchcock's Haunted Houseful." Hitchcock himself was not actually involved in the reading, reviewing, editing or selection of the short stories; in fact, even his introductions were ghost-written. The entire extent of his involvement with the project was to lend his name and collect a check.
Some notable writers whose works were used in the collection, include Shirley Jackson ("Strangers in Town", "The Lottery"), T.H. White ("The Once and Future King"), Robert Bloch, H. G. Wells ("The War of the Worlds"), Robert Louis Stevenson, Sir Arthur Conan Doyle, Mark Twain and the creator of "The Three Investigators", Robert Arthur.
Hitchcock also wrote a mystery story for "Look" magazine in 1943, "The Murder of Monty Woolley". This was a sequence of captioned photographs inviting the reader to inspect the pictures for clues to the murderer's identity; Hitchcock cast the performers as themselves; such as Woolley, Doris Merrick and make up man Guy Pearce, whom Hitchcock identified, in the last photo, as the murderer. The article was reprinted in "Games" Magazine in November/December 1980.
---END.OF.DOCUMENT---

Anaconda.
Anacondas are large, nonvenomous boas found in tropical South America. Although the name actually applies to a group of snakes, it is often used to refer only to one species in particular, the green anaconda, "Eunectes murinus", one of the largest snakes in the world, and (together with the "reticulated python" of southeast Asia) possibly the longest.
They live mostly in water, such as the Amazon River. While this snake poses a danger to humans, and there are several ascertained cases of people being killed by it, it does not regularly hunt humans. Its standard prey includes fish, river fowl, and occasionally domesticated goats or ponies that venture near or into the water.
Threat from dangerous anacondas is a familiar plot in comics, movies and adventure stories set in the Amazon jungle.
Etymology.
The Oxford English Dictionary gives a first source as John Ray's "List of Indian Serpents" from the Leyden Museum, as "anacandaia of the Ceylonese, i.e. he that crushes the limbs of buffaloes and yoke beasts," but that "anacandaia" is "not now a native name in Sri Lanka, and not satisfactorily explained either in Cingalese [Sinhalese] or Tamil"—though Henry Yule lists "āṇaik'k'onḍa" to means "having killed an elephant" in Tamil.
---END.OF.DOCUMENT---

Altaic languages.
Altaic is a disputed language family that is generally held by its proponents to include the Turkic, Mongolic, Tungusic, and Japonic language families and the Korean language isolate (Georg et al. 1999:73–74). These languages are spoken in a wide arc stretching from northeast Asia through Central Asia to Anatolia and eastern Europe (Turks, Kalmyks). The group is named after the Altai Mountains, a mountain range in Central Asia.
These language families share numerous characteristics. The debate is over the origin of their similarities. One camp, often called the "Altaicists", views these similarities as arising from common descent from a Proto-Altaic language spoken several thousand years ago. The other camp, often called the "anti-Altaicists", views these similarities as arising from areal interaction between the language groups concerned. Some linguists believe the case for either interpretation is about equally strong; they have been called the "skeptics" (Georg et al. 1999:81).
Another view accepts Altaic as a valid family but includes in it only Turkic, Mongolic, and Tungusic. This view was widespread prior to the 1960s, but has almost no supporters among specialists today (Georg et al. 1999:73–74). The expanded grouping, including Korean and Japanese, came to be known as "Macro-Altaic", leading to the designation by back-formation of the smaller grouping as "Micro-Altaic". Most proponents of Altaic continue to support the inclusion of Korean and Japanese.
Micro-Altaic would include about 66 living languages, to which Macro-Altaic would add Korean, Japanese, and the Ryukyuan languages for a total of about 74. (These are estimates, depending on what is considered a language and what is considered a dialect. They do not include earlier states of language, such as Old Japanese.) Micro-Altaic would have a total of about 348 million speakers today, Macro-Altaic about 558 million.
History of the Altaic idea.
The idea that the Turkic, Mongolic, and Tungusic languages are each others' closest relatives was allegedly first published in 1730 by Philip Johan von Strahlenberg, a Swedish officer who traveled in the eastern Russian Empire while a prisoner of war after the Great Northern War. However, as has been pointed out by Alexis Manaster Ramer and Paul Sidwell (1997), Strahlenberg actually opposed the idea of a closer relationship between the languages which later became known as "Altaic".
The term "Altaic", as the name for a language family, was introduced in 1844 by Matthias Castrén, a pioneering Finnish philologist who made major contributions to the study of the Uralic languages. As originally formulated by Castrén, Altaic included not only Turkic, Mongolian, and Manchu-Tungus (=Tungusic) but also Finno-Ugric and Samoyed (Poppe 1965:126). Finno-Ugric and Samoyed are not included in later formulations of Altaic. They came to be grouped in a separate family, known as Uralic (though doubts long persisted about its validity). Castrén's Altaic is thus equivalent to what later came to be known as Ural-Altaic (ib. 127). More precisely, Ural-Altaic came to subgroup Finno-Ugric and Samoyedic as "Uralic" and Turkic, Mongolic, and Tungusic as "Altaic", with Korean sometimes added to Altaic, and less often Japanese.
For much of the 19th and early 20th centuries, many linguists who studied Turkic, Mongolic, and Tungusic regarded them as members of a common Ural-Altaic family, together with Finno-Ugric and Samoyedic, based on such shared features as vowel harmony and agglutination. While the Ural-Altaic hypothesis can still be found in encyclopedias, atlases, and similar general reference works, it has not had any adherents in the linguistics community for decades. It has been characterized by Sergei Starostin as "an idea now completely discarded" (Starostin et al. 2003:8).
In 1857, the Austrian scholar Anton Boller suggested adding Japanese to Altaic or more precisely to Ural-Altaic (Miller 1986:34). For Korean, G.J. Ramstedt and E.D. Polivanov put forward additional etymologies in favor of its inclusion in the 1920s.
The culmination of decades of research and publication on the part of the author, Ramstedt's two-volume work "Einführung in die altaische Sprachwissenschaft" ('Introduction to Altaic Linguistics') was published in 1952–1957. It rejected grouping the Uralic languages in a common family with the Altaic ones and included Korean in Altaic, an inclusion followed by most leading Altaicists to date. Ramstedt's first volume, "Lautlehre" ('Phonology'), contained the first comprehensive attempt to identify regular correspondences between the sound systems of the Altaic language families. The second volume was "Formenlehre" ('Morphology'). (The second volume was actually published first, in 1952, with the first volume following in 1957.)
Ramstedt did not live to see the publication of his great work. He died in 1950, and the work was edited and seen through the press by Pentti Aalto, a student of his. In 1960, Nicholas Poppe presented what was in effect a heavily revised version of Ramstedt’s volume on phonology (Miller 1991:298) that has since set the standard in Altaic studies. Further contributions to Altaic linguistics in the 1960s were made by scholars such as Karl H. Menges and, on particular points, by Vladislav Illich-Svitych and others.
In the meantime, knowledge of the branches of Altaic and the individual languages of which they are composed made great strides, thanks in large part to the efforts of Vera Cincius (also spelled Tsintsius) on Tungusic (Poppe 1965:97–98) and of Poppe himself on Mongolic, with contributions by many other scholars.
Ramstedt and Cincius each had several students who carried on and extended their work (Poppe 1965:136, 98), as did Poppe.
Poppe (1965:148) considered the issue of the relationship of Korean to Turkic-Mongolic-Tungusic was not settled. In his view, there were three real possibilities: (1) Korean did not belong with the other three genealogically, but had been influenced by an Altaic substratum; (2) Korean was related to the other three at the same level they were related to each other; (3) Korean had split off from the other three before they underwent a series of characteristic changes. Poppe leaned toward the third possibility (ib.), but did not commit himself to it in this work.
Roy Andrew Miller's 1971 book "Japanese and the Other Altaic Languages" convinced most Altaicists that Japanese also belonged to Altaic (Poppe 1976:470). Since then, the standard set of languages included in Altaic has comprised Turkic, Mongolic, Tungusic, Korean, and Japanese.
An alternative classification, though one with much less currency among Altaicists, was proposed by John C. Street (1962), according to which Turkic-Mongolic-Tungusic forms one grouping and Korean-Japanese-Ainu another, the two being linked in a common family that Street designated as "North Asiatic". The same schema was adopted by James Patrie (1982) in the context of an attempt to classify the Ainu language. The Turkic-Mongolic-Tungusic and Korean-Japanese-Ainu groupings were also posited by Joseph Greenberg (2000–2002) who, however, treated them as independent members of a larger family, which he termed Eurasiatic.
<span id="Controversy" /> <span id="The controversy over Altaic" />
A language family or a Sprachbund?
Even as Ramstedt's "Einführung" was making converts and generating the modern school of Altaic studies, a newly invigorated attack on the validity of the Altaic language family was taking shape. Gerard Clauson (1956), Gerhard Doerfer (1963), and Alexander Shcherbak argued that the words and features shared by Turkic, Mongolic, and Tungusic were for the most part borrowings and that the rest could be attributed to chance resemblances. They argued that while there were words shared by Turkic and Mongolic, by Mongolic and Tungusic, and by all three, there were none (Doerfer: few) shared by Turkic and Tungusic but not Mongolic. If all three families had a common ancestor, we should expect losses to happen at random, not only at the geographical margins of the family; on the other hand, we should expect exactly the observed pattern if borrowing is responsible. Furthermore, they argued that many of the typological features of the supposed Altaic languages, such as agglutinative morphology and SOV word order, usually occur together in languages. In sum, the idea was that Turkic, Mongolic, and Tungusic form a Sprachbund – the result of convergence through intensive borrowing and long contact among speakers of languages that are not necessarily closely related. The proponents of this hypothesis are sometimes called "the Anti-Altaicists".
Doubt was also raised about the affinities of Korean and Japanese; in particular, some authors tried to connect Japanese to the Austronesian languages (Starostin et al. 2003:8–9).
Since then, the debate has raged back and forth, with defenses of Altaic in the wide sense (e.g. Sergei Starostin 1991), advocacy of a family consisting of Tungusic, Korean, and Japonic but not Turkic or Mongolic ("Macro-Tungusic", J. Marshall Unger 1990), and wholesale rejections (e.g. Doerfer 1988) being published.
Starostin's (1991) lexicostatistical research showed that the proposed Altaic groups shared about 15–20% of potential cognates within a 110-word Swadesh-Yakhontov list (e.g. Turkic–Mongolic 20%, Turkic–Tungusic 18%, Turkic–Korean 17%, Mongolic–Tungusic 22%, Mongolic–Korean 16%, Tungusic–Korean 21%). Altogether, Starostin concluded that the Altaic grouping was substantiated, though "older than most other language families in Eurasia, such as Indo-European or Finno-Ugric, and this is the reason why the modern Altaic languages preserve few common elements".
A further step in the debate was the publication of "An Etymological Dictionary of the Altaic Languages" by Sergei Starostin, Anna V. Dybo, and Oleg A. Mudrak in 2003. The research for the dictionary included contributions by several young Altaic scholars, among them Ilya Gruntov and Martine Robbeets. The result of some twenty years of work, it contains 2800 proposed cognate sets, a complete set of regular sound correspondences based on those proposed sets, and a number of grammatical correspondences, as well as a few important changes to the reconstruction of Proto-Altaic. For example, while most of today's Altaic languages have vowel harmony, Proto-Altaic as reconstructed by Starostin et al. lacked it – instead various vowel assimilations between the first and second syllables of words occurred in Turkic, Mongolic, Tungusic, Korean, and Japonic. It tries hard to distinguish loans between Turkic and Mongolic and between Mongolic and Tungusic from cognates, and it suggests words that occur in Turkic and Tungusic but not Mongolic (Starostin et al. 2003:20); all other combinations between the five branches also occur in the book. It lists 144 items of shared basic vocabulary (2003:230–234) (mostly already present in Starostin 1991 (2003:234)), including words for such items as 'eye', 'ear', 'neck', 'bone', 'blood', 'water', 'stone', 'sun', and 'two'.
This work has not changed the mind of any of the principal authors in the field, however. The debate continues unabated—e.g. S. Georg 2004, A. Vovin 2005, S. Georg 2005 (anti-Altaic); S. Starostin 2005, V. Blažek 2006, M. Robbeets 2007, A. Dybo and G. Starostin 2008 (pro-Altaic).
Postulated Urheimat.
The earliest known texts in a language attributed to Altaic by its proponents are the Orkhon inscriptions, dating from the 8th century AD. They are written in a Turkic language. They were deciphered in 1893 by the Danish linguist Vilhelm Thomsen in a scholarly race with his rival, the Germano-Russian linguist Wilhelm Radloff. However, Radloff was the first to publish the inscriptions.
All of these methods remain to be applied to the languages attributed to Altaic with the same degree of focus and intensity they have been applied to the Indo-European family (e.g. Mallory 1989, Anthony 2007).
In the absence of more extensive studies in this area, most claims about the prehistory of the Altaic-speaking peoples must be viewed as extremely preliminary. This includes the following remarks.
According to one line of reasoning, if the languages grouped as Altaic are genetically related, their great differences from each other would point to a very ancient date for their proto-language, in the Mesolithic or even the Upper Paleolithic period. (Miller 1991 however emphasizes the commonalities of these languages in all major areas: phonology, vocabulary, inflections, and syntax).
Speakers of an Altaic protolanguage might have entered Central Asia following the disappearance of the West Siberian Glacial Lake, which almost completely covered the flatlands of western Siberia up to the foothills of the Kuznetsk Alatau and Altai mountain ranges. With the Late Glacial warming, up to the Atlantic Phase of the Post-Glacial Optimum, Mesolithic groups moved north into this area from the Hissar (6000–4000 BCE) and Keltiminar (5500–3500 BCE) cultures. These groups brought with them the bow and arrow and the dog, elements of what Kent Flannery has called the "broad-spectrum revolution". The Keltiminar culture occupied the semi-desert and desert areas of the Karakum and Kyzyl Kum deserts and the deltas of the Amu Darya and Zeravshan rivers (Whitney Coolidge 2005). The Keltiminar people practised a mobile hunting, gathering, and fishing subsistence system. Over time, they adopted stockbreeding.
Some seek the origin of the proposed Micro-Altaic group in the spread of the Karasuk culture and the appearance of northern Mongol Dinlin elements. The Karasuk culture is the result of a migration of the eastern part of the Dinlins. Its influence extended as far as the Ordos region of China and across into Manchuria and northern Korea. The Karasuk people lived in permanent settlements in frame-type houses. The economy was complex. They bred large-horned livestock, horses, and sheep. They developed a high level of bronze metallurgy. Characteristic of the Karasuk culture are extensive cemeteries. Tombs are fenced with stone slabs laid on crest.
Others equate the Karasuk culture with the origin of the Karasuk languages, a recently proposed language family that includes the Yeniseian languages and Burushaski but none of the suggested members of Altaic. Associating languages with archeological discoveries in the absence of written evidence is always a delicate matter. This hypothesis was dealt a major blow when the Yeniseian languages were firmly linked to the Na-Dené languages of North America in a family now called Dené-Yeniseian ("Bulletin of the Society for the Study of the Indigenous Languages of the Americas" 264, 31 March 2008).
According to one view, Turkic and Mongolic are more closely related to each other than either is to Tungusic. If so, the split between Turkic and Mongolic would have been the last division within the Altaic group. It has been suggested that this occurred just prior to the Xiongnu period of Central Asian history. This would imply a considerably more shallow time depth for Proto-Altaic, or at least Proto-Micro-Altaic, than the late Stone Age. Such conflicts remain to be resolved.
List of Altaicists and critics of Altaic.
"Note: This list is limited to linguists who have worked specifically on the Altaic problem since the publication of the first volume of Ramstedt's "Einführung" in 1952. The dates given are those of works concerning Altaic. For Altaicists, the version of Altaic they favor is given at the end of the entry."
Consonants.
1 This phoneme only occurred at the beginnings of words.
2 These phonemes only occurred in the interior of words.
Vowels.
It is not clear whether /æ/, /ø/, /y/ were monophthongs as shown here (presumably) or diphthongs (); the evidence is equivocal. In any case, however, they only occurred in the first (and sometimes only) syllable of any word.
Every vowel occurred in long and short versions which were different phonemes in the first syllable. Starostin et al. (2003) treat length together with pitch as a prosodic feature.
Prosody.
As reconstructed by Starostin et al. (2003), Proto-Altaic was a pitch accent or tone language; at least the first, and probably every, syllable could have high or low pitch.
Sound correspondences.
If a Proto(-Macro)-Altaic language really existed, it should be possible to reconstruct regular sound correspondences between that protolanguage and its descendants; such correspondences would make it possible to distinguish cognates from loanwords (in many cases). Such attempts have repeatedly been made. The latest version is reproduced here, taken from Blažek's (2006) summary of the newest Altaic etymological dictionary (Starostin et al. 2003) and transcribed into the IPA.
When a Proto-Altaic phoneme developed differently depending on its position in a word (beginning, interior, or end), the special case (or all cases) is marked with a hyphen; for example, Proto-Altaic disappears (marked "0") or becomes /j/ at the beginning of a Turkic word and becomes /p/ elsewhere in a Turkic word.
Consonants.
Only single consonants are considered here. In the middle of words, clusters of two consonants were allowed in Proto-Altaic as reconstructed by Starostin et al. (2003); the correspondence table of these clusters spans almost 7 pages in their book (83–89), and most clusters are only found in one or a few of the reconstructed roots.
¹ The Khalaj language has /h/ instead. (It also retains a number of other archaisms.) However, it has also added /h/ in front of words for which no initial consonant (except in some cases /ŋ/, as expected) can be reconstructed for Proto-Altaic; therefore, and because it would make them dependent on whether Khalaj happens to have preserved any given root, Starostin et al. (2003:26–28) have not used Khalaj to decide whether to reconstruct an initial in any given word and have not reconstructed a /h/ for Proto-Turkic even though it was probably there.
² The Monguor language has /f/ here instead (Kaiser & Shevoroshkin 1988); it is therefore possible that Proto-Mongolian also had /f/ which then became /h/ (and then usually disappeared) in all descendants except Monguor. Tabgač and Kitan, two extinct Mongolic languages not considered by Starostin et al. (2003), even preserve /p/ in these places (Blažek 2006).
³ This happened when the next consonant in the word was, or.
5 When the next consonant in the word was /h/.
6 This happened "in syllables with original high pitch" (Starostin et al. 2003:135).
7 When followed by /æ/, /ø/, /y/.
8 When the next consonant in the word was /r/.
9 When the preceding consonant was,, or, or when the next consonant was /g/.
10 When the following vowel was /a/, /ə/, or followed by /j/.
11 When followed by /i/ and then another vowel, or by /j/.
12 When preceded by a vowel preceded by /i/.
14 Starostin et al. (2003) follow a minority opinion (Vovin 1993) in interpreting the sound of the Middle Korean letter as or rather than [z]. (Dybo & Starostin 2008:footnote 50)
16 When followed by /a/, /o/, or /e/.
17 When followed by /i/ or /u/.
Vowels.
1 When preceded by a bilabial consonant.
2 When followed by a trill, /l/, or.
3 When preceded or followed by a bilabial consonant.
4 When preceded by a fricative ().
Prosody.
¹ "Proto-Mongolian has lost all traces of the original prosody except for voicing *p > *b in syllables with original high pitch" (Starostin et al. 2003:135).
² "[…] several secondary metatonic processes happened […] in Korean, basically in the verb subsystem: all verbs have a strong tendency towards low pitch on the first syllable." (Starostin et al. 2003:135)
Morphological correspondences.
/V/ symbolizes an uncertain vowel. Suffixes reconstructed for Proto-Turkic, Proto-Mongolic, Proto-Korean, or Proto-Japonic, but not attested in Old Turkic, Classical Mongolian, Middle Korean, or Old Japanese are marked with asterisks.
Personal pronouns.
The table below is taken (with slight modifications) from Blažek (2006) and transcribed into IPA.
1 이기문, 국어사 개설, 탑출판사, 1991.
As above, forms not attested in Classical Mongolian or Middle Korean but reconstructed for their ancestors are marked with an asterisk, and /V/ represents an uncertain vowel.
Other basic vocabulary.
The following table is a brief selection of further proposed cognates in basic vocabulary across the Altaic family (from Starostin et al. [2003]).
1 Contains the Proto-Altaic dual suffix: "both breasts" – "chest" – "heart".
2 Contains the Proto-Altaic singulative suffix -/nV/: "one breast".
3 Compare Baekje */turak/ "stone" (Blažek 2006).
4 This is in the Jurchen language. In modern Manchu it is "usiha".
5 This is disputed by Georg (2004), who states: "The traditional Tungusological reconstruction "*yāsa" [=] cannot be replaced by the nasal-initial one espoused here, needed for the comparison." However, Starostin (2005) mentions evidence from several Tungusic languages cited by Starostin et al. (2003). Georg (2005) does not accept this, referring to Georg (1999/2000) and an upcoming paper. By that time, Starostin was already dead (Starostin 2005 was published posthumously).
Numerals and related words.
3 Kitan has "2" (Blažek 2006).
4 is probably a contraction of -/ubu/-.
5 The /y/- of "3" "may also reflect the same root, although the suffixation is not clear." (Starostin et al. 2003:223)
6 Compare Silla /mir/ "3" (Blažek 2006).
7 Compare Goguryeo /mir/ "3" (Blažek 2006).
8 "third (or next after three = fourth)", "consisting of three objects"
9 "song with three out of four verses rhyming (first, second and fourth)"
10 Kitan has /dur/ "4" (Blažek 2006).
11 Kitan has /tau/ "5" (Blažek 2006).
12 "(the prefixed i- is somewhat unclear: it is also used as a separate word meaning ‘fifty’, but the historical root here is no doubt "*tu-")" (Starostin et al. 2003:223). – Blažek (2006) also considers Goguryeo * "5" (from */uti/) to be related.
13 Kitan has /nir/ "6" (Blažek 2006).
14 Middle Korean has "6", which may fit here, but the required loss of initial "is not quite regular" (Starostin et al. 2003:224).
15 The Mongolian forms "may suggest an original proto-form" or /ladi/ "with dissimilation or metathesis in" Proto-Mongolic (Starostin et al. 2003:224). – Kitan has /dol/ "7".
16 in Early Middle Korean(タリクニ/チリクヒ in 二中歴).
17 "Problematic" (Starostin et al. 2003:224).
18 Compare Goguryeo /tok/ "10" (Blažek 2006).
19 Manchu "a very big number".
20 Orok "a bundle of 10 squirrels", Nanai "collection, gathering".
21 "Hundred" in names of hundreds.
22 Starostin et al. (2003) suspect this to be a reduplication: * "20 + 20".
23 /kata-ti/ would be expected; Starostin et al. (2003) think that this irregular change from /k/ to /p/ is due to influence from "2" /puta-tu/.
---END.OF.DOCUMENT---

Austrian German.
Austrian German ("German: Österreichisches Deutsch"), or "Austrian Standard German", is the national standard variety of the German language spoken in Austria and in South Tyrol (Italy). The standardized form of Austrian German for official texts and schools is defined by the Austrian dictionary ("Österreichisches Wörterbuch"), published under the authority of the ministry of education, art and culture.
German.
As German is a pluricentric language, Austrian German is another standard variety in addition to the German spoken in Germany. Much like the relationship between American and British English, Austrian German is simply another standard form of the German language. The "Österreichisches Wörterbuch" states specific grammar rules and is a dictionary using Austrian spelling. In addition to this standard variety, in everyday life most Austrians speak one of a number of High German dialects.
Standard German in Austria.
With German being a pluricentric language, Austrian dialects should not be confused with the variety of Standard German spoken by most Austrians, which is distinct from that of Germany or Switzerland. Distinctions in vocabulary persist, for example, in culinary terms, where communication with Germans is frequently difficult, and administrative and legal language, which is due to Austria's exclusion from the development of a German nation-state in the late 19th century and its manifold particular traditions. A comprehensive collection of Austrian-German legal, administrative and economic terms is offered in: "Markhardt, Heidemarie: Wörterbuch der österreichischen Rechts-, Wirtschafts- und Verwaltungsterminologie" (Peter Lang, 2006).
European Union.
When Austria became a member of the European Union, the Austrian variety of the German language (limited to 23 agricultural terms) was “protected” in the so-called Protocol no. 10 () regarding the use of specific Austrian terms of the German language in the framework of the European Union, which forms part of the Austrian EU accession treaty. Austrian German is the only variety of a pluricentric language recognised under international law / EU primary law. All facts concerning “Protocol no. 10” are documented in Markhardt, Heidemarie: "Das österreichische Deutsch im Rahmen der EU", Peter Lang, 2005.
Verbs.
In Austria, as in the German-speaking parts of Switzerland and in southern Germany, verbs that express a state tend to use "sein" as the auxiliary verb in the perfect tense, as well as verbs of movement. Verbs which fall into this category include "sitzen" (to sit), "liegen" (to lie) and, in parts of Carinthia, "schlafen" (to sleep). Therefore the perfect tense of these verbs would be "ich bin gesessen", "ich bin gelegen" and "ich bin geschlafen" respectively (note: "ich bin geschlafen" is a very rare form, usually you will hear "ich habe geschlafen"; but "ich bin eingeschlafen" (I fell asleep) is quite normal).
(In the variant of German that is spoken in Germany, the words "stehen" (to stand) and "gestehen" (to confess) are identical in the present perfect tense: "habe gestanden". The Austrian variant avoids this potential ambiguity ("bin gestanden" from "stehen", "habe gestanden" from "gestehen").
Also, the preterite (simple past) is very rarely used in Austria, especially in the spoken language, except for some modal verbs ("ich war", "ich wollte").
Vocabulary.
There are many official terms that differ in Austrian German from their usage in most parts of Germany. These include Jänner (January) rather than "Januar", Feber (February) rather than Februar, heuer (this year) rather than "dieses Jahr", Kasten (wardrobe) instead of "Schrank", Kiste (crate) instead of "Schachtel", Sessel (chair) instead of "Stuhl", Stiege (stairs) instead of "Treppe", Rauchfang (chimney) instead of "Schornstein", Vorzimmer (floor) instead of "Diele", many administrative, legal and political terms - and a whole series of foods and vegetables such as: Erdäpfel (potatoes) German "Kartoffeln" (but Dutch "Aardappel"), Schlagobers (whipped cream) German "Schlagsahne", Faschiertes (ground beef) German "Hackfleisch", Fisolen (green beans) German "Gartenbohne", Karfiol (cauliflower) German "Blumenkohl", Karotte (carrot) German "Möhre", Kohlsprossen (Brussels sprouts) German "Rosenkohl", Marillen (apricots) German "Aprikosen", Paradeiser (tomatoes) German "Tomaten", Palatschinken (pancakes) German "Pfannkuchen", Topfen (a semi-sweet cottage cheese) German "Quark" and Kren (horseradish) German "Meerrettich".
Austrians, in particular (and especially in the countryside and in conservative settings), will say "Grüß Gott!" (literally "greet God!", meaning "May God bless you") when greeting someone, rather than the "Guten Tag!" ("[Have a] good day!") used by many Germans. Beside the official Austrian German, occasionally also Austrian dialects from various regions are seen in written form, containing a large number of contractions and abbreviations compared to standard German, which can be hard to understand for non-native speakers (although the same applies to German dialects in Germany and Switzerland).
Dialects: Intercomprehensibility and regional accents.
While strong forms of the various dialects are not normally fully comprehensible to Northern Germans, there is virtually no communication barrier to speakers from Bavaria. The Central Austro-Bavarian dialects are more intelligible to speakers of Standard German than the Southern Austro-Bavarian dialects of Tyrol. Viennese, the Austro-Bavarian dialect of Vienna, is most frequently used in Germany for impersonations of the typical inhabitant of Austria. The people of Graz, the capital of Styria, speak yet another dialect which is not very Styrian and more easily understood by people from other parts of Austria than other Styrian dialects, for example from western Styria.
Simple words in the various dialects are very similar, but pronunciation is distinct for each and, after listening to a few spoken words it may be possible for an Austrian to realise which dialect is being spoken. However, in regard to the dialects of the deeper valleys of the Tirol, other Tyroleans are often unable to understand them. Speakers from the different states of Austria can easily be distinguished from each other by their particular accents (probably more so than Bavarians), those of Carinthia, Styria, Vienna, Upper Austria, and the Tirol being very characteristic. Speakers from those regions, even those speaking Standard German, can usually be easily identified by their accent, even by an untrained listener.
Several of the dialects have been influenced by contact with non-Germanic linguistic groups, such as the dialect of Carinthia, where in the past many speakers were bilingual with Slovene, and the dialect of Vienna, which has been influenced by immigration during the Austro-Hungarian period, particularly from what is today the Czech Republic. The German dialects of Bolzano-Bozen (Alto Adige/South Tyrol) have been influenced by local Romance languages, in particular with many loan words from Italian, and Ladin.
Interestingly, the geographic borderlines between the different accents (isoglosses) coincide strongly with the borders of the states and also with the border with Bavaria, with Bavarians having a markedly different rhythm of speech in spite of the similarities in the language.
---END.OF.DOCUMENT---